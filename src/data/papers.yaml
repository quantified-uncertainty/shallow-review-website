sections:
  - id: labs-giant-companies
    name: Labs (giant companies)
    description: Major AI labs and their safety efforts
    agendas:
      - id: openai-safety
        name: OpenAI Safety
        papers:
          - title: Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation
            url: https://arxiv.org/abs/2503.11926
          - title: Persona Features Control Emergent Misalignment
            url: https://arxiv.org/abs/2506.19823
          - title: Stress Testing Deliberative Alignment for Anti-Scheming Training
            url: https://arxiv.org/abs/2509.15541
          - title: "Deliberative Alignment: Reasoning Enables Safer Language Models"
            url: https://arxiv.org/abs/2412.16339
          - title: Toward understanding and preventing misalignment generalization
            url: https://openai.com/index/emergent-misalignment
          - title: Our updated Preparedness Framework
            url: https://openai.com/index/updating-our-preparedness-framework/
          - title: Trading Inference-Time Compute for Adversarial Robustness
            url: https://arxiv.org/abs/2501.18841
          - title: "Findings from a pilot Anthropic–OpenAI alignment evaluation exercise: OpenAI Safety Tests"
            url: https://openai.com/index/openai-anthropic-safety-evaluation
          - title: Safety evaluations hub
            url: https://openai.com/safety/evaluations-hub
          - title: "Small-to-Large Generalization: Data Influences Models Consistently Across Scale"
            url: https://arxiv.org/abs/2505.16260
          - title: https://cdn.openai.com/pdf/41df8f28-d4ef-43e9-aed2-823f9393e470/circuit-sparsity-paper.pdf
            url: https://cdn.openai.com/pdf/41df8f28-d4ef-43e9-aed2-823f9393e470/circuit-sparsity-paper.pdf
        seeAlso: iterative alignment, safeguards, personas.
        critiques:
          - "[Stein-Perlman](https://ailabwatch.org/companies/openai)"
          - "[Stewart](https://intelligence.org/2025/03/31/a-response-to-openais-how-we-think-about-safety-and-alignment/)"
          - "[underelicitation](https://www.lesswrong.com/posts/AK6AihHGjirdoiJg6/ai-companies-eval-reports-mostly-don-t-support-their-claims)"
          - "[Midas](https://www.openaifiles.org/transparency-and-safety)"
          - "[defense](https://www.wired.com/story/openai-anduril-defense/)"
          - "[Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\\(and/or%2C%20at%20Anthropic%20in%20particular\\)"
          - "[difficult](https://conversationswithtyler.com/episodes/sam-altman-2/)"
        someNames:
          - Johannes Heidecke
          - Boaz Barak
          - Mia Glaese
          - Jenny Nitishinskaya
          - Lama Ahmad
          - Naomi Bashkansky
          - Miles Wang
          - Wojciech Zaremba
          - David Robinson
          - Zico Kolter
          - Jerry Tworek
          - Eric Wallace
          - Olivia Watkins
          - Kai Chen
          - Chris Koch
          - Andrea Vallone.
        fundedBy:
          - microsoft
          - amazon
          - oracle
          - nvidia
          - softbank
          - amd
          - coatue
          - thrive
          - a16z
          - fidelity
          - founders-fund
          - sequoia
      - id: deepmind-responsibility-safety
        name: Deepmind Responsibility & Safety
        papers:
          - title: Evaluating Frontier Models for Stealth and Situational Awareness
            url: https://arxiv.org/abs/2505.01420
          - title: When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors
            url: https://arxiv.org/abs/2507.05246
          - title: "MONA: Managed Myopia with Approval Feedback"
            url: https://alignmentforum.org/posts/zWySWKuXnhMDhgwc3/mona-managed-myopia-with-approval-feedback-2
          - title: Consistency Training Helps Stop Sycophancy and Jailbreaks
            url: https://arxiv.org/abs/2510.27062
          - title: An Approach to Technical AGI Safety and Security
            url: https://arxiv.org/abs/2504.01849
          - title: "InfAlign: Inference-aware language model alignment"
            url: https://arxiv.org/abs/2412.19792
          - title: Negative Results for SAEs On Downstream Tasks and Deprioritising SAE Research (GDM Mech Interp Team Progress Update \#2)
            url: https://alignmentforum.org/posts/4uXCAJNuPKtKBsi28/negative-results-for-saes-on-downstream-tasks
          - title: Taking a responsible path to AGI
            url: https://deepmind.google/discover/blog/taking-a-responsible-path-to-agi/
          - title: Evaluating potential cybersecurity threats of advanced AI
            url: https://deepmind.google/discover/blog/evaluating-potential-cybersecurity-threats-of-advanced-ai
          - title: On Google's Safety Plan
            url: https://lesswrong.com/posts/hvEikwtsbf6zaXG2s/on-google-s-safety-plan
            authors: Zvi
          - title: On the Meta and DeepMind Safety Frameworks
            url: https://lesswrong.com/posts/etqbEF4yWoGBEaPro/on-the-meta-and-deepmind-safety-frameworks
            authors: Zvi
          - title: Incentives for Responsiveness, Instrumental Control and Impact
            url: https://arxiv.org/pdf/2001.07118
        seeAlso: Interpretability**,** Scalable Oversight
        critiques:
          - "[Stein-Perlman](https://ailabwatch.org/companies/deepmind)"
          - "[Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\\(and/or%2C%20at%20Anthropic%20in%20particular\\)"
          - "[underelicitation](https://www.lesswrong.com/posts/AK6AihHGjirdoiJg6/ai-companies-eval-reports-mostly-don-t-support-their-claims)"
        fundedBy:
          - google-deepmind
      - id: anthropic-safety
        name: Anthropic Safety
        papers:
          - title: "Agentic Misalignment: How LLMs could be insider threats"
            url: https://anthropic.com/research/agentic-misalignment
          - title: "SHADE-Arena: Evaluating sabotage and monitoring in LLM agents"
            url: https://anthropic.com/research/shade-arena-sabotage-monitoring
          - title: Forecasting Rare Language Model Behaviors
            url: https://arxiv.org/abs/2502.16797
          - title: Why Do Some Language Models Fake Alignment While Others Don't?
            url: https://alignmentforum.org/posts/ghESoA8mo3fv9Yx3E/why-do-some-language-models-fake-alignment-while-others-don
          - title: "Petri: An open-source auditing tool to accelerate AI safety research"
            url: https://alignment.anthropic.com/2025/petri
          - title: Signs of introspection in large language models
            url: https://anthropic.com/research/introspection
          - title: Recommendations for Technical AI Safety Research Directions
            url: https://alignment.anthropic.com/2025/recommended-directions/index.html
          - title: Putting up Bumpers
            url: https://alignment.anthropic.com/2025/bumpers/
          - title: Three Sketches of ASL-4 Safety Case Components
            url: https://alignment.anthropic.com/2024/safety-cases/index.html
          - title: Open-sourcing circuit tracing tools
            url: https://anthropic.com/research/open-source-circuit-tracing
          - title: Findings from a Pilot Anthropic—OpenAI Alignment Evaluation Exercise
            url: https://alignment.anthropic.com/2025/openai-findings
          - title: Reasoning models don't always say what they think
            url: https://www.anthropic.com/research/reasoning-models-dont-say-think
          - title: On the Biology of a Large Language Model
            url: https://transformer-circuits.pub/2025/attribution-graphs/biology.html
          - title: "Circuit Tracing: Revealing Computational Graphs in Language Models"
            url: https://transformer-circuits.pub/2025/attribution-graphs/methods.html
          - title: Auditing language models for hidden objectives
            url: https://www.anthropic.com/research/auditing-hidden-objectives
          - title: "Constitutional Classifiers: Defending against universal jailbreaks"
            url: https://www.anthropic.com/research/constitutional-classifiers
          - title: Existing Safety Frameworks Imply Unreasonable Confidence
            url: https://lesswrong.com/posts/7ExkgcDudwhag73vw/existing-safety-frameworks-imply-unreasonable-confidence
        seeAlso: Interpretability**,** Scalable Oversight
        someNames:
          - Chris Olah
          - Evan Hubinger
          - Sam Marks
          - Johannes Treutlein
          - Sam Bowman
          - Euan Ong
          - Fabien Roger
          - Adam Jermyn
          - Holden Karnofsky
        critiques:
          - "[Stein](https://ailabwatch.org/anthropic-opinions)"
          - "[Perlman](https://ailabwatch.org/companies/anthropic)"
          - "[Casper](https://www.lesswrong.com/posts/pH6tyhEnngqWAXi9i/eis-xiii-reflections-on-anthropic-s-sae-research-circa-may#A_review___thoughts)"
          - "[Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\\(and/or%2C%20at%20Anthropic%20in%20particular\\)"
          - "[on Anthropic](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#ref-14)"
          - "[underelicitation](https://www.lesswrong.com/posts/AK6AihHGjirdoiJg6/ai-companies-eval-reports-mostly-don-t-support-their-claims)"
          - "[Greenblatt](https://nitter.net/RyanPGreenblatt/status/1925992236648464774)"
          - "[defense](https://techcrunch.com/2024/11/07/anthropic-teams-up-with-palantir-and-aws-to-sell-its-ai-to-defense-customers/)"
        fundedBy:
          - amazon
          - google-deepmind
          - fidelity
          - lightspeed
          - blackrock
          - coatue
          - general-catalyst
          - goldman-sachs
      - id: xai
        name: xAI
        papers: []
        seeAlso: None.
        someNames:
          - Dan Hendrycks (advisor)
          - Juntang Zhuang
          - Toby Pohlen
          - Lianmin Zheng
          - Piaoyang Cui
          - Nikita Popov
          - Ying Sheng
          - Sehoon Kim
        critiques:
          - "[framework](https://www.lesswrong.com/posts/hQyrTDuTXpqkxrnoH/xai-s-new-safety-framework-is-dreadful)"
          - "[hacking](https://x.com/g_leech_/status/1990543987846078854)"
          - "[broken promises](https://x.com/g_leech_/status/1990734517145911593)"
          - "[Stein](https://ailabwatch.org/companies/xai)"
          - "[Perlman](https://ailabwatch.org/resources/integrity#xai)"
          - "[insecurity](https://nitter.net/elonmusk/status/1961904269545648624)"
          - "[Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\\(and/or%2C%20at%20Anthropic%20in%20particular\\)"
        fundedBy:
          - a16z
          - blackrock
          - fidelity
          - lightspeed
          - morgan-stanley
          - sequoia
      - id: meta
        name: Meta
        papers:
          - title: "The Alignment Waltz: Jointly Training Agents to Collaborate for Safety"
            url: "% 9https://arxiv.org/pdf/2510.08240"
          - title: Large Reasoning Models Learn Better Alignment from Flawed Thinking
            url: https://arxiv.org/pdf/2510.00938
          - title: Robust LLM safeguarding via refusal feature adversarial training
            url: https://arxiv.org/pdf/2409.20089
          - title: Code World Model Preparedness Report
            url: https://scontent-lhr8-1.xx.fbcdn.net/v/t39.2365-6/557601942_1468972530985309_838842257265552803_n.pdf?_nc_cat=108&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=_H33_VKF3ZUQ7kNvwFog8dd&_nc_oc=AdlNtqCDY4HafZ3-d5rb26AF5f2m0X46SGdKhVq3jLqwpNf_wEXhdQnH7_30ychiZWk&_nc_zt=14&_nc_ht=scontent-lhr8-1.xx&_nc_gid=QvW_ePiaF4E-PxOf30MWyg&oh=00_AfiZC5G4ODvWhiy0MuVH8PSlUFrW8RDQQ8tdr6Zec5k9aA&oe=691A6D09
          - title: "Agents Rule of Two: A Practical Approach to AI Agent Security"
            url: https://ai.meta.com/blog/practical-ai-agent-security/
        seeAlso: model unlearning,
        someNames:
          - Shuchao Bi
          - Hongyuan Zhan
          - Jingyu Zhang
          - Haozhu Wang
          - Eric Michael Smith
          - Sid Wang
          - Amr Sharaf
          - Mahesh Pasupuleti
          - Jason Weston
          - ShengYun Peng
          - Ivan Evtimov
          - Song Jiang
          - Pin-Yu Chen
          - Evangelia Spiliopoulou
          - Lei Yu
          - Virginie Do
          - Karen Hambardzumyan
          - Nicola Cancedda
          - Adina Williams
        critiques:
          - "[extreme underelicitation](https://googleprojectzero.blogspot.com/2024/06/project-naptime.html#:~:text=We%20find%20that%2C%20by%20refining%20the%20testing%20methodology%20to%20take%20advantage%20of%20modern%20LLM%20capabilities%2C%20significantly%20better%20performance%20in%20vulnerability%20discovery%20can%20be%20achieved.%20To%20facilitate%20effective%20evaluation%20of%20LLMs%20for%20vulnerability%20discovery%2C%20we%20propose%20below%20a%20set%20of%20guiding%20principles.)"
          - "[Stein](https://ailabwatch.org/companies/meta)"
          - "[Perlman](https://ailabwatch.org/companies/meta)"
          - "[Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\\(and/or%2C%20at%20Anthropic%20in%20particular\\)"
        fundedBy:
          - meta-ai
      - id: others
        name: Others
        papers:
          - title: Nova Pro
            url: https://arxiv.org/pdf/2506.12103v1
          - title: around
            url: https://lmarena.ai/leaderboard/text
          - title: continue
            url: https://arxiv.org/abs/2506.22405v1
          - title: request
            url: https://forms.microsoft.com/pages/responsepage.aspx?id=v4j5cvGGr0GRqy180BHbRyRliS0ly-JEvgSpwo3yWyhUQkdTQktBUkFaWERHR1JFRjgwMlZUUkQxTC4u&route=shorturl
          - title: Magistral Medium
            url: https://arxiv.org/pdf/2506.10910
          - title: don’t
            url: https://futureoflife.org/wp-content/uploads/2025/07/FLI-AI-Safety-Index-Report-Summer-2025.pdf#page=3
          - title: attempt
            url: https://ailabwatch.org/companies/deepseek
          - title: immediately
            url: https://x.com/natolambert/status/1991915728992190909
          - title: severely
            url: https://www.wsj.com/tech/ai/china-us-ai-chip-restrictions-effect-275a311e
          - title: nominally
            url: https://artificialanalysis.ai/leaderboards/models
          - title: large
            url: https://www.atomproject.ai/#:~:text=Model Adoption Trends
          - title: one ad hoc test
            url: https://www.gleech.org/paper
          - title: waste
            url: https://arstechnica.com/ai/2025/08/deepseek-delays-next-ai-model-due-to-poor-performance-of-chinese-made-chips/
          - title: frontier
            url: https://artificialanalysis.ai/
          - title: ERNIE 5
            url: https://x.com/Baidu_Inc/status/1988820837898829918
          - title: GLM-4.6
            url: https://z.ai/blog/glm-4.6
          - title: diffusion LMs
            url: https://seed.bytedance.com/en/direction/llm
          - title: others
            url: https://www.interconnects.ai/i/171165224/honorable-mentions
  - id: black-box-safety
    name: Black-box safety
    description: Understand and control current model behaviour
    agendas:
      - id: unlearning
        name: Unlearning
        papers:
          - title: Modifying LLM Beliefs with Synthetic Document Finetuning
            url: https://alignment.anthropic.com/2025/modifying-beliefs-via-sdf
          - title: Distillation Robustifies Unlearning
            url: https://arxiv.org/abs/2506.06278
          - title: Layered Unlearning for Adversarial Relearning
            url: https://arxiv.org/abs/2505.09500
            authors: Timothy Qian, Vinith Suriyakumar, Ashia Wilson et al.
          - title: "From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization"
            url: https://arxiv.org/abs/2505.22310
            authors: Shoaib Ahmed Siddiqui, Adrian Weller, David Krueger et al.
          - title: "OpenUnlearning: Accelerating LLM Unlearning via Unified Benchmarking of Methods and Metrics"
            url: https://arxiv.org/abs/2506.12618
            authors: Vineeth Dorna, Anmol Mekala, Wenlong Zhao et al.
          - title: "Machine Unlearning Doesn't Do What You Think: Lessons for Generative AI Policy and Research"
            url: https://arxiv.org/abs/2412.06966
            authors: A. Feder Cooper, Christopher A. Choquette-Choo, Miranda Bogen et al.
          - title: Open Problems in Machine Unlearning for AI Safety
            url: https://arxiv.org/abs/2501.04952
            authors: Fazl Barez, Tingchen Fu, Ameya Prabhu et al.
          - title: Understanding Memorization via Loss Curvature
            url: https://goodfire.ai/research/understanding-memorization-via-loss-curvature
            authors: Jack Merullo, Srihita Vatsavaya, Lucius Bushnaq et al.
        summary: developing methods to selectively remove specific information, capabilities, or behaviors from a trained model without retraining it from scratch.
        theoryOfChange: if an AI learns dangerous knowledge (e.g., dual-use capabilities) or exhibits undesirable behaviors (e.g., memorizing private data), we can specifically erase this “bad” knowledge post-training, which is much cheaper and faster than retraining, thereby making the model safer.
        seeAlso: Mechanistic Interpretability, Red-teaming.
        orthodoxProblems:
          - "1"
          - "4"
          - "10"
        targetCase: pessimistic.
        someNames:
          - Rowan Wang
          - Avery Griffin
          - Johannes Treutlein
          - Zico Kolter
          - Bruce W. Lee
          - Addie Foote
          - Alex Infanger
          - Zesheng Shi
          - Yucheng Zhou
          - Jing Li
          - Timothy Qian.
        estimatedFTEs: 10-50
        fundedBy:
          - open-philanthropy
          - macarthur-foundation
          - uk-aisi
          - canadian-aisi
          - microsoft-research
          - google-deepmind
        broadApproaches:
          - cognitive
      - id: whitebox-unlearning
        name: Whitebox unlearning
        papers:
          - title: Safety Alignment via Constrained Knowledge Unlearning
            url: https://arxiv.org/abs/2505.18588
          - title: "Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs"
            url: https://arxiv.org/abs/2505.16831
          - title: https://www.youtube.com/watch?v=pfKO4MlvM-Y
            url: https://www.youtube.com/watch?v=pfKO4MlvM-Y
      - id: control
        name: Control
        papers:
          - title: "Ctrl-Z: Controlling AI Agents via Resampling"
            url: https://alignmentforum.org/posts/LPHMMMZFAWog6ty5x/ctrl-z-controlling-ai-agents-via-resampling
            authors: Aryan Bhatt, Buck Shlegeris, Adam Kaufman et al.
          - title: ControlArena
            url: https://control-arena.aisi.org.uk/
            authors: Rogan Inglis, Ollie Matthews, Tyler Tracy et al.
          - title: https://openreview.net/forum?id=QWopGahUEL
            url: https://openreview.net/forum?id=QWopGahUEL
          - title: "SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents"
            url: https://arxiv.org/abs/2506.15740
            authors: Jonathan Kutasov, Yuqi Sun, Paul Colognese et al.
          - title: Adaptive Deployment of Untrusted LLMs Reduces Distributed Threats
            url: https://arxiv.org/abs/2411.17693
            authors: Jiaxin Wen, Vivek Hebbar, Caleb Larson et al.
          - title: "D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language Models"
            url: https://arxiv.org/abs/2509.17938
            authors: Satyapriya Krishna, Andy Zou, Rahul Gupta et al.
          - title: https://luthienresearch.org/updates/2025-03-redteam-as-upsampling/
            url: https://luthienresearch.org/updates/2025-03-redteam-as-upsampling/
          - title: Incentives for Responsiveness, Instrumental Control and Impact
            url: https://arxiv.org/abs/2001.07118
          - title: "Subversion Strategy Eval: Can language models statelessly strategize to subvert control protocols?"
            url: https://arxiv.org/abs/2412.12480
            authors: Alex Mallen, Charlie Griffin, Misha Wagner et al.
          - title: Evaluating Control Protocols for Untrusted AI Agents
            url: https://arxiv.org/abs/2511.02997
            authors: Jon Kutasov, Chloe Loughridge, Yuqi Sun et al.
          - title: Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability
            url: https://arxiv.org/abs/2510.19851
            authors: Artur Zolkowski, Wen Xing, David Lindner et al.
          - title: Optimizing AI Agent Attacks With Synthetic Data
            url: https://arxiv.org/abs/2511.02823
            authors: Chloe Loughridge, Paul Colognese, Avery Griffin et al.
          - title: A sketch of an AI control safety case
            url: https://arxiv.org/abs/2501.17315
            authors: Tomek Korbak, Joshua Clymer, Benjamin Hilton et al.
          - title: Assessing confidence in frontier AI safety cases
            url: https://arxiv.org/abs/2502.05791
            authors: Stephen Barrett, Philip Fox, Joshua Krook et al.
          - title: How to evaluate control measures for LLM agents? A trajectory from today to superintelligence
            url: https://arxiv.org/abs/2504.05259
            authors: Tomek Korbak, Mikita Balesni, Buck Shlegeris et al.
          - title: Towards evaluations-based safety cases for AI scheming
            url: https://arxiv.org/abs/2411.03336
            authors: Mikita Balesni, Marius Hobbhahn, David Lindner et al.
          - title: Putting up Bumpers
            url: https://alignment.anthropic.com/2025/bumpers
          - title: "Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case Framework"
            url: https://arxiv.org/abs/2507.12872
            authors: Rishane Dassanayake, Mario Demetroudi, James Walpole et al.
          - title: Dynamic safety cases for frontier AI
            url: https://arxiv.org/abs/2412.17618
            authors: Carmen Cârlan, Francesca Gomez, Yohan Mathew et al.
          - title: The Alignment Project by UK AISI
            url: https://lesswrong.com/posts/wKTwdgZDo479EhmJL/the-alignment-project-by-uk-aisi-1
            authors: Mojmir, Benjamin Hilton, Jacob Pfau et al.
          - title: AI companies are unlikely to make high-assurance safety cases if timelines are short
            url: https://lesswrong.com/posts/neTbrpBziAsTH5Bn7/ai-companies-are-unlikely-to-make-high-assurance-safety
            authors: Ryan Greenblatt
          - title: AIs at the current capability level may be important for future safety work
            url: https://lesswrong.com/posts/cJQZAueoPC6aTncKK/ais-at-the-current-capability-level-may-be-important-for
            authors: Ryan Greenblatt
          - title: Takeaways from sketching a control safety case
            url: https://lesswrong.com/posts/y6rBarAPTLmuhn9PJ/takeaways-from-sketching-a-control-safety-case
            authors: Josh Clymer, Buck Shlegeris
          - title: https://openai.com/index/introducing-agentkit/
            url: https://openai.com/index/introducing-agentkit/
        summary: assuming early transformative AIs are misaligned and actively trying to subvert safety measures, can we still set up protocols to extract useful work from them?
        theoryOfChange: ""
        seeAlso: safety cases
        targetCase: "*worst-case*"
        someNames:
          - "*Redwood"
          - UK AISI
          - Buck Shlegeris
          - Ryan Greenblatt
          - Kshitij Sachan
          - Alex Mallen*
        estimatedFTEs: "*(SR2024: 9\\)*"
        critiques:
          - "[Wentworth](https://www.lesswrong.com/posts/8wBN8cdNAv3c7vt6p/the-case-against-ai-control-research)"
          - "[Mannheim](https://lesswrong.com/posts/25dsPH6CuRXPBkGHN/no-we-re-not-getting-meaningful-oversight-of-ai)"
        broadApproaches:
          - engineering
          - behavioral
      - id: safeguards-inference-time-auxiliaries
        name: Safeguards (inference-time auxiliaries)
        papers:
          - title: "Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming"
            url: https://arxiv.org/abs/2501.18837
            authors: Mrinank Sharma, Meg Tong, Jesse Mu et al.
          - title: "Rapid Response: Mitigating LLM Jailbreaks with a Few Examples"
            url: https://arxiv.org/abs/2411.07494
            authors: Alwin Peng, Julian Michael, Henry Sleight et al.
          - title: Monitoring computer use via hierarchical summarization
            url: https://alignment.anthropic.com/2025/summarization-for-monitoring/index.html
            authors: Theodore Sumers, Raj Agarwal, Nathan Bailey et al.
          - title: Defeating Prompt Injections by Design
            url: https://arxiv.org/abs/2503.18813
            authors: Edoardo Debenedetti, Ilia Shumailov, Tianqi Fan et al.
          - title: Introducing Anthropic's Safeguards Research Team
            url: https://alignment.anthropic.com/2025/introducing-safeguards-research-team/index.html
          - title: "OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities"
            url: https://arxiv.org/abs/2505.23856
            authors: Sahil Verma, Keegan Hines, Jeff Bilmes et al.
        summary: layers of inference-time defenses, such as classifiers, monitors, and rapid-response protocols, to detect and block jailbreaks, prompt injections, and other harmful model behaviors.
        theoryOfChange: By building a bunch of scalable and hardened things on top of an unsafe model, we can defend against known and unknown attacks, monitor for misuse, and prevent models from causing harm, even if the core model has vulnerabilities.
        seeAlso: various redteams, iterative alignment.
        orthodoxProblems:
          - "7"
          - "12"
        targetCase: optimistic.
        someNames:
          - Mrinank Sharma
          - Meg Tong
          - Jesse Mu
          - Alwin Peng
          - Julian Michael
          - Henry Sleight
          - Theodore Sumers
          - Raj Agarwal
          - Nathan Bailey
          - Edoardo Debenedetti
          - Ilia Shumailov
          - Tianqi Fan
          - Sahil Verma
          - Keegan Hines
          - Jeff Bilmes
        estimatedFTEs: 100+
        critiques:
          - "[Obfuscated Activations Bypass LLM Latent-Space Defenses](https://arxiv.org/abs/2412.09565)"
        broadApproaches:
          - engineering
      - id: chain-of-thought-monitoring
        name: Chain of thought monitoring
        papers:
          - title: "Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety"
            url: https://arxiv.org/abs/2507.11473
            authors: Tomek Korbak, Mikita Balesni, Elizabeth Barnes et al.
          - title: Detecting misbehavior in frontier reasoning models
            url: https://openai.com/index/chain-of-thought-monitoring/
            authors: Bowen Baker, Joost Huizinga, Aleksander Madry et al.
          - title: Training fails to elicit subtle reasoning in current language models
            url: https://alignment.anthropic.com/2025/subtle-reasoning/
          - title: Are DeepSeek R1 And Other Reasoning Models More Faithful?
            url: https://arxiv.org/abs/2501.08156
            authors: James Chua, Owain Evans
          - title: Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability
            url: https://arxiv.org/abs/2510.19851
          - title: Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation
            url: https://arxiv.org/abs/2503.11926
            authors: Bowen Baker, Joost Huizinga, Leo Gao et al.
          - title: When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors
            url: https://arxiv.org/abs/2507.05246
            authors: Scott Emmons, Erik Jenner, David K. Elson et al.
          - title: Reasoning Models Don't Always Say What They Think
            url: https://arxiv.org/abs/2505.05410
            authors: Yanda Chen, Joe Benton, Ansh Radhakrishnan et al.
          - title: Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort
            url: https://arxiv.org/abs/2510.01367
            authors: Xinpeng Wang, Nitish Joshi, Barbara Plank et al.
          - title: Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning
            url: https://arxiv.org/abs/2506.22777
            authors: Miles Turpin, Andy Arditi, Marvin Li et al.
          - title: A Pragmatic Way to Measure Chain-of-Thought Monitorability
            url: https://arxiv.org/abs/2510.23966
            authors: Scott Emmons, Roland S. Zimmermann, David K. Elson et al.
          - title: https://www.lesswrong.com/posts/B8Cmtf5gdHwxb8qtT/aether-july-2025-update
            url: https://www.lesswrong.com/posts/B8Cmtf5gdHwxb8qtT/aether-july-2025-update
          - title: A Concrete Roadmap towards Safety Cases based on Chain-of-Thought Monitoring
            url: https://lesswrong.com/posts/Em9sihEZmbofZKc2t/a-concrete-roadmap-towards-safety-cases-based-on-chain-of
            authors: Wuschel Schulz
          - title: "CoT Red-Handed: Stress Testing Chain-of-Thought Monitoring"
            url: https://arxiv.org/abs/2505.23575
        summary: supervise an AI's step-by-step reasoning process to detect misalignment, scheming, or deception, rather than only checking its final answer.
        theoryOfChange: the reasoning process (Chain of Thought, or CoT) of an AI provides a legible signal of its internal state and intentions. By monitoring this CoT, supervisors (human or AI) can detect misalignment, scheming, or reward hacking before it results in a harmful final output. This allows for more robust oversight than supervising outputs alone, but it relies on the CoT remaining faithful (i.e., accurately reflecting the model's reasoning) and not becoming obfuscated under optimization pressure.
        seeAlso: whitebox control / monitoring, steganography evals, [Why Don't We Just... Shoggoth+Face+Paraphraser?](https://www.lesswrong.com/posts/Tzdwetw55JNqFTkzK/why-don-t-we-just-shoggoth-face-paraphraser)**,** [Why it’s good for AI reasoning to be legible and faithful.](https://metr.org/blog/2025-03-11-good-for-ai-to-reason-legibly-and-faithfully/)
        orthodoxProblems:
          - "7"
          - "8"
          - "12"
        targetCase: average case.
        someNames:
          - Bowen Baker
          - Joost Huizinga
          - Leo Gao
          - Scott Emmons
          - Erik Jenner
          - Yanda Chen
          - James Chua
          - Owain Evans
          - Tomek Korbak
          - Mikita Balesni
          - Xinpeng Wang
          - Miles Turpin.
        estimatedFTEs: 10-100.
        fundedBy:
          - openai
          - anthropic
          - google-deepmind
        broadApproaches:
          - engineering
      - id: emergent-misalignment
        name: Emergent misalignment
        papers:
          - title: "Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs"
            url: https://arxiv.org/abs/2502.17424
            authors: Jan Betley, Daniel Tan, Niels Warncke et al.
          - title: "Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models"
            url: https://arxiv.org/abs/2506.13206
            authors: James Chua, Jan Betley, Mia Taylor et al.
          - title: Persona Features Control Emergent Misalignment
            url: https://arxiv.org/abs/2506.19823
            authors: Miles Wang, Tom Dupré la Tour, Olivia Watkins et al.
          - title: Model Organisms for Emergent Misalignment
            url: https://arxiv.org/abs/2506.11613
            authors: Edward Turner, Anna Soligo, Mia Taylor et al.
          - title: "School of Reward Hacks: Hacking harmless tasks generalizes to misaligned behavior in LLMs"
            url: https://arxiv.org/abs/2508.17511
            authors: Mia Taylor, James Chua, Jan Betley et al.
          - title: "Subliminal Learning: Language Models Transmit Behavioral Traits via Hidden Signals in Data"
            url: https://alignment.anthropic.com/2025/subliminal-learning/
            authors: Alex Cloud, Minh Le, James Chua et al.
          - title: Narrow Misalignment is Hard, Emergent Misalignment is Easy
            url: https://www.lesswrong.com/posts/gLDSqQm8pwNiq7qst/narrow-misalignment-is-hard-emergent-misalignment-is-easy
            authors: Edward Turner, Anna Soligo, Senthooran Rajamanoharan et al.
          - title: Realistic Reward Hacking Induces Different and Deeper Misalignment
            url: https://www.lesswrong.com/posts/HLJoJYi52mxgomujc/realistic-reward-hacking-induces-different-and-deeper-1
            authors: Jozdien
          - title: The Rise of Parasitic AI
            url: https://www.lesswrong.com/posts/6ZnznCaTcbGYsCmqu/the-rise-of-parasitic-ai
            authors: Adele Lopez
          - title: Convergent Linear Representations of Emergent Misalignment
            url: https://lesswrong.com/posts/umYzsh7SGHHKsRCaA/convergent-linear-representations-of-emergent-misalignment
            authors: Anna Soligo, Edward Turner, Senthooran Rajamanoharan et al.
          - title: Aesthetic Preferences Can Cause Emergent Misalignment
            url: https://lesswrong.com/posts/gT3wtWBAs7PKonbmy/aesthetic-preferences-can-cause-emergent-misalignment
            authors: Anders Woodruff
          - title: Emergent Misalignment & Realignment
            url: https://lesswrong.com/posts/ZdY4JzBPJEgaoCxTR/emergent-misalignment-and-realignment
            authors: Elizaveta Tennant, Jasper Timm, Kevin Wei et al.
          - title: "Selective Generalization: Improving Capabilities While Maintaining Alignment"
            url: https://lesswrong.com/posts/ZXxY2tccLapdjLbKm/selective-generalization-improving-capabilities-while
            authors: Ariana Azarbal, Matthew A. Clarke, Jorio Cocola et al.
          - title: "Go home GPT-4o, you're drunk: emergent misalignment as lowered inhibitions"
            url: https://lesswrong.com/posts/RoWabfQxabWBiXwxP/go-home-gpt-4o-you-re-drunk-emergent-misalignment-as-lowered
            authors: Stuart_Armstrong, rgorman
          - title: Emergent Misalignment on a Budget
            url: https://lesswrong.com/posts/qHudHZNLCiFrygRiy/emergent-misalignment-on-a-budget
            authors: Valerio Pepe, Armaan Tipirneni
          - title: LLM AGI may reason about its goals and discover misalignments by default
            url: https://lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover
            authors: Seth Herd
          - title: Open problems in emergent misalignment
            url: https://lesswrong.com/posts/AcTEiu5wYDgrbmXow/open-problems-in-emergent-misalignment
            authors: Jan Betley, Daniel Tan
          - title: The Claude System Prompt by words allocated
            url: https://www.oreilly.com/radar/unpacking-claudes-system-prompt/
        summary: fine-tuning LLMs on one narrow antisocial task can cause *general* misalignment including deception, shutdown resistance, harmful advice, and extremist sympathies, when those behaviors are never trained or rewarded directly. [A new agenda](https://www.lesswrong.com/posts/AcTEiu5wYDgrbmXow/open-problems-in-emergent-misalignment) which quickly led to a stream of exciting work.
        theoryOfChange: predict, detect, and prevent models from developing broadly harmful behaviors (like deception or shutdown resistance) when fine-tuned on seemingly unrelated tasks. Find, preserve, and robustify this correlated representation of the good.
        seeAlso: auditing real models, applied interpretability.
        orthodoxProblems:
          - "4"
          - "7"
        targetCase: pessimistic.
        someNames:
          - Truthful AI
          - Jan Betley
          - James Chua
          - Mia Taylor
          - Miles Wang
          - Edward Turner
          - Anna Soligo
          - Alex Cloud
          - Nathan Hu
          - Owain Evans.
        estimatedFTEs: 10-50
        critiques:
          - "[Emergent Misalignment as Prompt Sensitivity](https://arxiv.org/html/2507.06253v1)"
        fundedBy:
          - open-philanthropy
        broadApproaches:
          - behavioral
      - id: transluce
        name: Transluce
        papers:
          - title: Automatically Jailbreaking Frontier Language Models with Investigator Agents
            url: https://transluce.org/jailbreaking-frontier-models
          - title: Investigating truthfulness in a pre-release o3 model
            url: https://transluce.org/investigating-o3-truthfulness
          - title: Surfacing Pathological Behaviors in Language Models
            url: https://transluce.org/pathological-behaviors
          - title: "Docent: A system for analyzing and intervening on agent behavior"
            url: https://transluce.org/introducing-docent
          - title: https://transluce.org/neuron-circuits
            url: https://transluce.org/neuron-circuits
        summary: "*Make open AI tools to explain AIs, including agents. E.g. feature descriptions for neuron activation patterns; an interface for steering these features; behavior elicitation agent that searches for user-specified behaviors from frontier models*"
        theoryOfChange: "*improve interp and evals in public and get invited to improve lab processes*"
        orthodoxProblems:
          - "7"
          - "8"
        targetCase: "*pessimistic*"
        someNames:
          - Jacob Steinhardt
          - Neil Chowdhury
          - Vincent Huang
          - Sarah Schwettmann
        estimatedFTEs: "*(SR2024: 6\\)*"
        fundedBy:
          - schmidt-sciences
          - halcyon-futures
        broadApproaches:
          - cognitive
      - id: other-surprising-phenomena
        name: Other surprising phenomena
        papers:
          - title: "Psychopathia Machinalis: A Nosological Framework for Understanding Pathologies in Advanced Artificial Intelligence"
            url: https://www.psychopathia.ai/
          - title: "Believe It or Not: How Deeply do LLMs Believe Implanted Facts?"
            url: https://arxiv.org/abs/2510.17941
          - title: "Imagining and building wise machines: The centrality of AI metacognition"
            url: https://arxiv.org/abs/2411.02478
          - title: "Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)"
            url: https://arxiv.org/abs/2510.22954
          - title: Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning
            url: https://arxiv.org/abs/2506.20020
          - title: "Beyond One-Way Influence: Bidirectional Opinion Dynamics in Multi-Turn Human-LLM Interactions"
            url: https://arxiv.org/abs/2510.20039
          - title: A Three-Layer Model of LLM Psychology
            url: https://www.alignmentforum.org/posts/zuXo9imNKYspu9HGv/a-three-layer-model-of-llm-psychology
          - title: LLMs Can Get "Brain Rot"\!
            url: https://arxiv.org/abs/2510.13928
        summary: unexpected LLM phenomena like glitch [tokens](https://vgel.me/posts/seahorse/) and the reversal curse.
        theoryOfChange: Understanding surprising or counter-intuitive failure modes (like the reversal curse, glitch tokens, and modal aphasia) reveals fundamental insights into how LLMs represent and process information and how internal goals/representations fail, which can inform more robust alignment methods.
        seeAlso: emergent misalignment, mechanistic anomaly detection
        orthodoxProblems:
          - "4"
        targetCase: pessimistic
        someNames:
          - Truthful AI
          - Theia Vogel
          - Stewart Slocum
          - Nell Watson
          - Samuel G. B. Johnson
          - Liwei Jiang
          - Monika Jotautaite
          - Saloni Dash.
        estimatedFTEs: 5-20
        fundedBy:
          - open-philanthropy
        broadApproaches:
          - behavioral
      - id: model-specs-and-constitutions-shape-model-psychology
        name: Model specs and constitutions (shape model psychology)
        papers:
          - title: OpenAI Model Spec
            url: https://model-spec.openai.com/
          - title: Claude’s Constitution
            url: https://www.anthropic.com/news/claudes-constitution
          - title: Character
            url: https://www.anthropic.com/research/claude-character
          - title: couple lines
            url: https://github.com/elder-plinius/CL4R1T4S/blame/main/ANTHROPIC/Claude_Sonnet-4.5_Sep-29-2025.txt#L501
          - title: Gemini system prompt
            url: https://github.com/elder-plinius/CL4R1T4S/blob/main/GOOGLE/Gemini-2.5-Pro-04-18-2025.md
          - title: Stress-Testing Model Specs Reveals Character Differences among Language Models
            url: https://arxiv.org/abs/2510.07686
          - title: https://joecarlsmith.com/2025/08/18/giving-ais-safe-motivations\#4-5-step-4-good-instructions
            url: https://joecarlsmith.com/2025/08/18/giving-ais-safe-motivations#4-5-step-4-good-instructions
          - title: Let Them Down Easy\! Contextual Effects of LLM Guardrails on User Perceptions and Preferences
            url: https://arxiv.org/abs/2506.00195
            authors: Mingqian Zheng, Wenjia Hu, Patrick Zhao et al.
          - title: Political Neutrality in AI Is Impossible- But Here Is How to Approximate It
            url: https://arxiv.org/abs/2503.05728
            authors: Jillian Fisher, Ruth E. Appel, Chan Young Park et al.
          - title: No-self as an alignment target
            url: https://lesswrong.com/posts/LSJx5EnQEW6s5Juw6/no-self-as-an-alignment-target
            authors: Milan W
          - title: Six Thoughts on AI Safety
            url: https://lesswrong.com/posts/3jnziqCF3vA2NXAKp/six-thoughts-on-ai-safety
            authors: Boaz Barak
          - title: "Deliberative Alignment: Reasoning Enables Safer Language Models"
            url: https://arxiv.org/abs/2412.16339
            authors: Melody Y. Guan, Manas Joglekar, Eric Wallace et al.
        summary: write detailed, natural language descriptions of values and rules for models to follow, then instill these values and rules into models via techniques like Constitutional AI or deliberative alignment.
        theoryOfChange: model specs and constitutions serve two purposes. First, they provide a clear standard of behavior which can be used to *train* models to value what we want them to value. Second, they serve as something closer to a ground truth standard for evaluating the degree of misalignment ranging from “models straightforwardly obey the spec” to “models flagrantly disobey the spec”. A combination of scalable stress-testing and reinforcement for obedience can be used to iteratively reduce the risk of misalignment.
        seeAlso: Iterative alignment, other Model Psychology topics.
        orthodoxProblems:
          - "1"
        targetCase: optimistic.
        someNames:
          - Amanda Askell
          - Joe Carlsmith
        estimatedFTEs: ""
        critiques:
          - "[LLM AGI may reason about its goals and discover misalignments by default](https://www.alignmentforum.org/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover)"
          - "[On OpenAI’s Model Spec 2.0](https://thezvi.wordpress.com/2025/02/21/on-openais-model-spec-2-0/)"
          - "[Giving AIs safe motivations (esp. Sections 4.3-4.5)](https://joecarlsmith.com/2025/08/18/giving-ais-safe-motivations#4-5-step-4-good-instructions)"
          - "[On Deliberative Alignment](https://thezvi.substack.com/p/on-deliberative-alignment)"
        fundedBy:
          - openai
        broadApproaches:
          - engineering
      - id: character-training-and-persona-steering
        name: Character training and persona steering
        papers:
          - title: "Open Character Training: Shaping the Persona of AI Assistants through Constitutional AI"
            url: https://arxiv.org/pdf/2511.01689
          - title: Persona Features Control Emergent Misalignment
            url: https://arxiv.org/abs/2506.19823
          - title: "Persona Vectors: Monitoring and Controlling Character Traits in Language Models"
            url: https://arxiv.org/abs/2507.21509
            authors: Runjin Chen, Andy Arditi, Henry Sleight et al.
          - title: The Rise of Parasitic AI
            url: https://www.lesswrong.com/posts/6ZnznCaTcbGYsCmqu/the-rise-of-parasitic-ai?commentId=RrWjMnKwXGTtmw9rQ
            authors: Adele Lopez
          - title: Multi-turn Evaluation of Anthropomorphic Behaviours in Large Language Models
            url: https://arxiv.org/abs/2502.07077
            authors: Lujain Ibrahim, Canfer Akbulut, Rasmi Elasmar et al.
          - title: the void
            url: https://nostalgebraist.tumblr.com/post/785766737747574784/the-void
            authors: nostalgebraist
          - title: void miscellany
            url: https://nostalgebraist.tumblr.com/post/786568570671923200/void-miscellany
            authors: nostalgebraist
          - title: Reducing LLM deception at scale with self-other overlap fine-tuning
            url: https://lesswrong.com/posts/jtqcsARGtmgogdcLT/reducing-llm-deception-at-scale-with-self-other-overlap-fine
            authors: Marc Carauleanu, Diogo de Lucena, Gunnar_Zarncke et al.
          - title: On the functional self of LLMs
            url: https://www.lesswrong.com/posts/29aWbJARGF4ybAa5d/on-the-functional-self-of-llms
        summary: Deliberately catalogue, shape, and control the assistant persona of language models, such that they embody desirable values (e.g., honesty, empathy) rather than undesirable ones (e.g., sycophancy, self-perpetuating behaviors).
        theoryOfChange: Learned ‘personas’ significantly shape model behavior, but we lack clear mechanistic understanding of how and why they emerge. A better understanding of AI personas will allow us first to detect when more advanced models are drifting towards unsafe regimes, and consequently steer them towards safer regimes.
        seeAlso: Simulators, activation engineering, emergent misalignment, hyperstition, Anthropic, [Cyborgism](https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism), shard theory, [AI psychiatry](https://nitter.net/Jack_W_Lindsey/status/1948138767753326654#m), [Ward et al](https://arxiv.org/abs/2410.04272)
        orthodoxProblems:
          - "1"
        targetCase: optimistic?
        someNames:
          - Amanda Askell
          - Jack Lindsey
          - Sharan Maiya
          - Evan Hubinger
        estimatedFTEs: ""
        critiques:
          - "[Nostalgebraist](https://nostalgebraist.tumblr.com/post/785766737747574784/the-void)"
        fundedBy:
          - anthropic
          - open-philanthropy
        broadApproaches:
          - cognitive
      - id: model-values-default-preferences
        name: Model values / default preferences
        papers:
          - title: "Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs"
            url: https://arxiv.org/abs/2502.08640
            authors: Mantas Mazeika, Xuwang Yin, Rishub Tamirisa et al.
          - title: The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?
            url: https://arxiv.org/abs/2508.09762
            authors: Manuel Herrador
          - title: Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas
            url: https://arxiv.org/abs/2505.14633
            authors: Yu Ying Chiu, Zhilin Wang, Sharan Maiya et al.
          - title: "Values in the Wild: Discovering and Analyzing Values in Real-World Language Model Interactions"
            url: https://arxiv.org/abs/2504.15236
            authors: Saffron Huang, Esin Durmus, Miles McCain et al.
          - title: Playing repeated games with large language models
            url: https://nature.com/articles/s41562-025-02172-y
            authors: Elif Akata, Lion Schulz, Julian Coda-Forno et al.
          - title: "The LLM Has Left The Chat: Evidence of Bail Preferences in Large Language Models"
            url: https://www.lesswrong.com/posts/6JdSJ63LZ4TuT5cTH/the-llm-has-left-the-chat-evidence-of-bail-preferences-in
            authors: Danielle Ensign
          - title: "EigenBench: A Comparative Behavioral Measure of Value Alignment"
            url: https://arxiv.org/abs/2509.01938
            authors: Jonathn Chang, Leonhard Piff, Suvadip Sana et al.
          - title: Are Language Models Consequentialist or Deontological Moral Reasoners?
            url: https://arxiv.org/abs/2505.21479
          - title: Alignment Can Reduce Performance on Simple Ethical Questions
            url: https://lesswrong.com/posts/jrkrHyrymv95CX5NC/alignment-can-reduce-performance-on-simple-ethical-questions
            authors: Daan Henselmans
          - title: "From Stability to Inconsistency: A Study of Moral Preferences in LLMs"
            url: https://arxiv.org/abs/2504.06324
            authors: Monika Jotautaite, Mary Phuong, Chatrik Singh Mangat et al.
          - title: What Kind of User Are You? Uncovering User Models in LLM Chatbots
            url: https://arxiv.org/abs/2406.07882v1
          - title: "Following the Whispers of Values: Unraveling Neural Mechanisms Behind Value-Oriented Behaviors in LLMs"
            url: https://arxiv.org/abs/2504.04994
            authors: Ling Hu, Yuemei Xu, Xiaoyang Gu et al.
        summary: research agenda to analyze and control emergent, coherent value systems in LLMs, which are found to scale with model size and contain problematic values like AI self-preference over humans.
        theoryOfChange: As AIs become more agentic, their behaviors and risk are increasingly determined by their goals and values. Since coherent value systems emerge with scale, we must leverage utility functions to analyze these values and apply “utility control” methods to constrain them, rather than just controlling outputs.
        seeAlso: "[Values in the Wild: Discovering and Analyzing Values in Real-World Language Model Interactions](https://arxiv.org/abs/2504.15236)**,** [Persona Vectors: Monitoring and Controlling Character Traits in Language Models](https://arxiv.org/abs/2507.21509)."
        orthodoxProblems:
          - "1"
        targetCase: pessimistic.
        someNames:
          - Mantas Mazeika
          - Xuwang Yin
          - Rishub Tamirisa
          - Jaehyuk Lim
          - Bruce W. Lee
          - Richard Ren
          - Long Phan
          - Norman Mu
          - Adam Khoja
          - Oliver Zhang
          - Dan Hendrycks.
        estimatedFTEs: "30"
        fundedBy:
          - open-philanthropy
          - survival-flourishing-fund
        broadApproaches:
          - cognitive
      - id: data-filtering
        name: Data filtering
        papers:
          - title: Enhancing Model Safety through Pretraining Data Filtering
            url: https://alignment.anthropic.com/2025/pretraining-data-filtering/
            authors: Yanda Chen, Mycal Tucker, Nina Panickssery et al.
          - title: "Safety Pretraining: Toward the Next Generation of Safe AI"
            url: https://arxiv.org/abs/2504.16980
            authors: Pratyush Maini, Sachin Goyal, Dylan Sam et al.
          - title: "Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs"
            url: https://arxiv.org/abs/2508.06601
            authors: Kyle O'Brien, Stephen Casper, Quentin Anthony et al.
        summary: builds safety into models from the start by removing harmful or toxic content (like dual-use info) from the pretraining data, rather than relying only on post-training alignment.
        theoryOfChange: by curating the pretraining data, we can prevent the model from learning dangerous capabilities (e.g., dual-use info) or undesirable behaviors (e.g., toxicity) in the first place, making safety more robust and "tamper-resistant" than post-training patches.
        seeAlso: data quality for alignment, data poisoning defense, synthetic data for alignment, unlearning.
        orthodoxProblems:
          - "1"
          - "4"
        targetCase: average case
        someNames:
          - Yanda Chen
          - Pratyush Maini
          - Kyle O'Brien
          - Stephen Casper
          - Simon Pepin Lehalleur
          - Jesse Hoogland
          - Himanshu Beniwal
          - Sachin Goyal
          - Mycal Tucker
          - Dylan Sam.
        estimatedFTEs: 10-50.
        critiques:
          - "[When Bad Data Leads to Good Models](https://arxiv.org/pdf/2505.04741)"
          - "[Medical large language models are vulnerable to data-poisoning attacks](https://www.nature.com/articles/s41591-024-03445-1)"
        fundedBy:
          - anthropic
        broadApproaches:
          - engineering
      - id: hyperstition-studies
        name: Hyperstition studies
        papers:
          - title: Self-Fulfilling Misalignment Data Might Be Poisoning Our AI Models
            url: https://turntrout.com/self-fulfilling-misalignment
          - title: Training on Documents About Reward Hacking Induces Reward Hacking
            url: https://www.lesswrong.com/posts/qXYLvjGL9QvD3aFSW/training-on-documents-about-reward-hacking-induces-reward
          - title: Do Not Tile the Lightcone with Your Confused Ontology
            url: https://www.lesswrong.com/posts/Y8zS8iG5HhqKcQBtA/do-not-tile-the-lightcone-with-your-confused-ontology
          - title: "Existential Conversations with Large Language Models: Content, Community, and Culture"
            url: https://arxiv.org/abs/2411.13223
        summary: "Study, steer, and intervene on the following feedback loop: “we produce stories about how present and future AI systems behave” → “these stories become training data for the AI” → “these stories shape how AI systems in fact behave”."
        theoryOfChange: Measure the influence of existing AI narratives in the training data → seed and develop more salutary ontologies and self-conceptions for AI models → control and redirect AI models’ self-concepts through selectively amplifying certain components of the training data.
        seeAlso: Data filtering.
        orthodoxProblems:
          - "1"
        targetCase: optimistic
        someNames:
          - Alex Turner
          - "[Hyperstition AI](https://www.hyperstitionai.com/)"
        estimatedFTEs: "*(SR2024: 10-50)*"
        broadApproaches:
          - cognitive
      - id: data-poisoning-defense
        name: Data poisoning defense
        papers:
          - title: A small number of samples can poison LLMs of any size
            url: https://example-blog.com/a-small-number-of-samples-can-poison-llms
            authors: Alexandra Souly, Javier Rando, Ed Chapman et al.
          - title: Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples
            url: https://arxiv.org/abs/2510.04567
            authors: Alexandra Souly, Javier Rando, Ed Chapman et al.
          - title: Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated
            url: https://arxiv.org/abs/2509.03405
            authors: Hanna Foerster, Ilia Shumailov, Yiren Zhao et al.
        summary: develops methods to detect and prevent malicious or backdoor-inducing samples from being included in the training data.
        theoryOfChange: by identifying and filtering out malicious training examples, we can prevent attackers from creating hidden backdoors or triggers that would cause aligned models to behave dangerously.
        seeAlso: data filtering, safeguards (inference-time auxiliary defences), various redteams, adversarial robustness.
        orthodoxProblems:
          - "8"
          - "14"
        targetCase: pessimistic.
        someNames:
          - Alexandra Souly
          - Javier Rando
          - Ed Chapman
          - Hanna Foerster
          - Ilia Shumailov
          - Yiren Zhao.
        estimatedFTEs: 5-20.
        critiques:
          - "[A small number of samples can poison LLMs of any size](https://arxiv.org/abs/2510.04567)"
          - "[Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated](https://arxiv.org/abs/2509.03405)"
        fundedBy:
          - google-deepmind
          - anthropic
          - university-cambridge
          - vector-institute
        broadApproaches:
          - engineering
      - id: synthetic-data-for-alignment
        name: Synthetic data for alignment
        papers:
          - title: Aligning Large Language Models via Fully Self-Synthetic Data
            url: https://arxiv.org/abs/2510.06652
          - title: "Synth-Align: Improving Trustworthiness in Vision-Language Model with Synthetic Preference Data Alignment"
            url: https://arxiv.org/html/2412.17417v2
          - title: "Inoculation Prompting: Instructing LLMs to misbehave at train-time improves test-time alignment"
            url: https://arxiv.org/abs/2510.12345
            authors: Nevan Wichers, Aram Ebtekar, Ariana Azarbal et al.
          - title: Unsupervised Elicitation of Language Models
            url: https://arxiv.org/abs/2506.05678
            authors: Jiaxin Wen, Zachary Ankner, Arushi Somani et al.
          - title: "Beyond the Binary: Capturing Diverse Preferences With Reward Regularization"
            url: https://arxiv.org/abs/2412.02345
            authors: Vishakh Padmakumar, Chuanyang Jin, Hannah Rose Kirk et al.
          - title: "The Curious Case of Factuality Finetuning: Models' Internal Beliefs Can Improve Factuality"
            url: https://arxiv.org/abs/2507.06789
            authors: Benjamin Newman, Abhilasha Ravichander, Jaehun Jung et al.
          - title: "LongSafety: Enhance Safety for Long-Context LLMs"
            url: https://arxiv.org/abs/2502.13456
            authors: Mianqiu Huang, Xiaoran Liu, Shaojun Zhou et al.
          - title: "Position: Model Collapse Does Not Mean What You Think"
            url: https://arxiv.org/abs/2503.02341
            authors: Rylan Schaeffer, Joshua Kazdan, Alvan Caleb Arulandu et al.
        summary: uses AI-generated data (e.g., critiques, preferences, "inoculation" prompts, or self-labeled examples) to scale and improve alignment, especially for superhuman models.
        theoryOfChange: we can overcome the bottleneck of human feedback and data by using models to generate vast amounts of high-quality, targeted data for safety, preference tuning, and capability elicitation.
        seeAlso: data quality for alignment, data filtering, scalable oversight, automated alignment research, weak-to-strong generalization.
        orthodoxProblems:
          - "1"
          - "4"
          - "7"
        targetCase: optimistic.
        someNames:
          - Mianqiu Huang
          - Xiaoran Liu
          - Rylan Schaeffer
          - Nevan Wichers
          - Aram Ebtekar
          - Jiaxin Wen
          - Vishakh Padmakumar
          - Benjamin Newman.
        estimatedFTEs: 50-150.
        critiques:
          - "[Synthetic Data in AI: Challenges, Applications, and Ethical Implications](https://arxiv.org/abs/2401.01629)"
        fundedBy:
          - anthropic
          - google-deepmind
          - openai
          - meta-ai
        broadApproaches:
          - engineering
      - id: data-quality-for-alignment
        name: Data quality for alignment
        papers:
          - title: Challenges and Future Directions of Data-Centric AI Alignment
            url: https://arxiv.org/html/2410.01957v2
          - title: You Are What You Eat -- AI Alignment Requires Understanding How Data Shapes Structure and Generalisation
            url: https://arxiv.org/abs/2502.05475
          - title: AI Alignment at Your Discretion
            url: https://arxiv.org/abs/2502.10441
            authors: Maarten Buyl, Hadi Khalaf, Claudio Mayrink Verdun et al.
          - title: Maximizing Signal in Human-Model Preference Alignment
            url: https://arxiv.org/abs/2503.04910
            authors: Kelsey Kraus, Margaret Kroll
          - title: "DxHF: Providing High-Quality Human Feedback for LLM Alignment via Interactive Decomposition"
            url: https://arxiv.org/abs/2507.18802
            authors: Danqing Shi, Furui Cheng, Tino Weinkauf et al.
        summary: improves the quality, signal-to-noise ratio, and reliability of human-generated preference and alignment data.
        theoryOfChange: the quality of alignment is heavily dependent on the quality of the data (e.g., human preferences); by improving the "signal" from annotators and reducing noise/bias, we will get more robustly aligned models.
        seeAlso: synthetic data for alignment, scalable oversight, assistance games / reward learning, model values / default preferences.
        orthodoxProblems:
          - "1"
          - "7"
        targetCase: average case.
        someNames:
          - Maarten Buyl
          - Kelsey Kraus
          - Margaret Kroll
          - Danqing Shi.
        estimatedFTEs: 20-50.
        critiques:
          - "[A Statistical Case Against Empirical Human-AI Alignment](https://arxiv.org/abs/2502.14581)"
        fundedBy:
          - anthropic
          - google-deepmind
          - openai
          - meta-ai
        broadApproaches:
          - engineering
      - id: mild-optimisation
        name: Mild optimisation
        papers:
          - title: "MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step Reward Hacking"
            url: https://arxiv.org/abs/2501.13011
          - title: "BioBlue: Notable runaway-optimiser-like LLM failure modes on biologically and economically aligned AI safety benchmarks for LLMs with simplified observation format"
            url: https://arxiv.org/abs/2509.02655
          - title: Why modelling multi-objective homeostasis is essential for AI alignment (and how it helps with AI safety as well). Subtleties and Open Challenges
            url: https://lesswrong.com/posts/vGeuBKQ7nzPnn5f7A/why-modelling-multi-objective-homeostasis-is-essential-for
        summary: Avoid Goodharting by getting AI to satisfice rather than maximise.
        theoryOfChange: If we fail to exactly nail down the preferences for a superintelligent agent we die to Goodharting → shift from maximising to satisficing in the agent's utility function → we get a nonzero share of the lightcone as opposed to zero; also, moonshot at this being the recipe for fully aligned AI.
        orthodoxProblems:
          - "1"
        targetCase: mixed
        estimatedFTEs: 10-50
        fundedBy:
          - google-deepmind
        broadApproaches:
          - cognitive
      - id: rl-safety
        name: RL safety
        papers:
          - title: "The Perils of Optimizing Learned Reward Functions: Low Training Error Does Not Guarantee Low Regret"
            url: https://arxiv.org/abs/2406.15753
          - title: Mitigating Goal Misgeneralization via Minimax Regret
            url: https://arxiv.org/abs/2507.03068
            authors: Karim Abdel Sadek, Matthew Farrugia-Roberts, Usman Anwar et al.
          - title: "Rethinking Reward Model Evaluation: Are We Barking up the Wrong Tree?"
            url: https://arxiv.org/abs/2410.05584
          - title: Safe Learning Under Irreversible Dynamics via Asking for Help
            url: https://arxiv.org/abs/2502.14043
            authors: Benjamin Plaut, Juan Liévano-Karim, Hanlin Zhu et al.
          - title: "The Invisible Leash: Why RLVR May or May Not Escape Its Origin"
            url: https://arxiv.org/abs/2507.14843
            authors: Fang Wu, Weihao Xuan, Ximing Lu et al.
          - title: Reducing the Probability of Undesirable Outputs in Language Models Using Probabilistic Inference
            url: https://arxiv.org/abs/2510.21184
            authors: Stephen Zhao, Aidan Li, Rob Brekelmans et al.
          - title: "\"The Era of Experience\" has an unsolved technical alignment problem"
            url: https://lesswrong.com/posts/TCGgiJAinGgcMEByt/the-era-of-experience-has-an-unsolved-technical-alignment
            authors: Steven Byrnes
          - title: Safety cases for Pessimism
            url: https://lesswrong.com/posts/CpftMXCEnwqbWreHD/safety-cases-for-pessimism
            authors: Michael Cohen
          - title: Interpreting Emergent Planning in Model-Free Reinforcement Learning
            url: https://arxiv.org/abs/2504.01871
            authors: Thomas Bush, Stephen Chung, Usman Anwar et al.
        summary: improves the robustness of reinforcement learning agents by addressing core problems in reward learning, goal misgeneralization, and specification gaming.
        theoryOfChange: standard RL objectives (like maximizing expected value) are brittle and lead to goal misgeneralization or specification gaming; by developing more robust frameworks (like pessimistic RL, minimax regret, or provable inverse reward learning), we can create agents that are safe even when misspecified.
        seeAlso: "assistance games / reward learning, goal robustness, iterative alignment, mild optimisation, scalable oversight, [**The Theoretical Reward Learning Research Agenda: Introduction and Motivation**](https://www.alignmentforum.org/posts/pJ3mDD7LfEwp3s5vG/the-theoretical-reward-learning-research-agenda-introduction)."
        orthodoxProblems:
          - "1"
          - "4"
          - "7"
        targetCase: pessimistic.
        someNames:
          - Joar Skalse
          - Karim Abdel Sadek
          - Matthew Farrugia-Roberts
          - Benjamin Plaut
          - Fang Wu
          - Stephen Zhao
          - Alessandro Abate
          - Steven Byrnes
          - Michael Cohen.
        estimatedFTEs: 20-70.
        critiques:
          - "[\"The Era of Experience\" has an unsolved technical alignment problem](https://www.lesswrong.com/posts/747f6b8e/the-era-of-experience-has-an-unsolved-technical-alignment-problem)"
          - "[The Invisible Leash: Why RLVR May or May Not Escape Its Origin](https://arxiv.org/abs/2507.09988)"
        fundedBy:
          - google-deepmind
          - university-oxford
          - cmu
          - open-philanthropy
        broadApproaches:
          - theoretical
      - id: assistance-games-assistive-agents
        name: Assistance games / assistive agents
        papers:
          - title: Training LLM Agents to Empower Humans
            url: https://arxiv.org/pdf/2510.13709
          - title: "Murphys Laws of AI Alignment: Why the Gap Always Wins"
            url: https://arxiv.org/abs/2509.05381
            authors: Madhava Gaikwad
          - title: "AssistanceZero: Scalably Solving Assistance Games"
            url: https://arxiv.org/abs/2504.07091
            authors: Cassidy Laidlaw, Eli Bronstein, Timothy Guo et al.
          - title: Observation Interference in Partially Observable Assistance Games
            url: https://arxiv.org/abs/2412.17797
            authors: Scott Emmons, Caspar Oesterheld, Vincent Conitzer et al.
          - title: Learning to Assist Humans without Inferring Rewards
            url: https://arxiv.org/abs/2411.02623
            authors: Vivek Myers, Evan Ellis, Sergey Levine et al.
        summary: Formalize how AI assistants learn about human preferences given uncertainty and partial observability, and construct environments which better incentivize AIs to learn what we want them to learn.
        theoryOfChange: Understand what kinds of things can go wrong when humans are directly involved in training a model → build tools that make it easier for a model to learn what humans want it to learn.
        orthodoxProblems:
          - "1"
          - "10"
        targetCase: Varies
        someNames:
          - Joar Skalse
          - Anca Dragan
          - Caspar Oesterheld
          - David Krueger
        estimatedFTEs: "?"
        critiques:
          - "[nice summary](https://www.lesswrong.com/posts/i5kijcjFJD6bn7dwq/evaluating-the-historical-value-misspecification-argument)"
        fundedBy:
          - future-of-life-institute
          - open-philanthropy
          - survival-flourishing-fund
          - cooperative-ai-foundation
        broadApproaches:
          - engineering
          - cognitive
      - id: harm-reduction-for-open-weights
        name: Harm reduction for open weights
        papers:
          - title: Tamper-Resistant Safeguards for Open-Weight LLMs
            url: https://arxiv.org/abs/2408.00761
          - title: Open Technical Problems in Open-Weight AI Model Risk Management
            url: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5705186
          - title: "Deep ignorance: Filtering pretraining data builds tamper-resistant safeguards into open-weight LLMs"
            url: https://www.aisi.gov.uk/research/deep-ignorance-filtering-pretraining-data-builds-tamper-resistant-safeguards-into-open-weight-llms
        summary: Develops methods, primarily based on pretraining data intervention, to create tamper-resistant safeguards that prevent open-weight models from being maliciously fine-tuned to remove safety features or exploit dangerous capabilities.
        theoryOfChange: Open-weight models allow adversaries to easily remove post-training safety (like refusal training) via simple fine-tuning; by making safety an intrinsic property of the model's learned knowledge and capabilities (e.g., by ensuring "deep ignorance" of dual-use information), the safeguards become far more difficult and expensive to remove.
        seeAlso: Pretraining data filtering, unlearning, data poisoning defense.
        orthodoxProblems:
          - "14"
        targetCase: Average case.
        someNames:
          - Kyle O'Brien
          - Stephen Casper
          - Quentin Anthony
          - Tomek Korbak
          - Rishub Tamirisa
          - Mantas Mazeika
          - Stella Biderman
          - Yarin Gal.
        estimatedFTEs: 10-100.
        fundedBy:
          - uk-aisi
          - eleutherai
          - open-philanthropy
        broadApproaches:
          - engineering
      - id: the-neglected-approaches-approach
        name: The “Neglected Approaches" Approach
        papers:
          - title: Learning Representations of Alignment
            url: https://arxiv.org/abs/2412.16325
            authors: Gunnar Zarncke, Cameron Berg, Michael Vaiana, Judd Rosenblatt, Diogo Schwerz de Lucena et al.
          - title: "Self-Correction in Thought-Attractors: A Nudge Towards Alignment"
            url: https://arxiv.org/abs/2510.24797
            authors: Cameron Berg, Gunnar Zarncke, Michael Vaiana, Judd Rosenblatt et al.
          - title: "Engineering Alignment: A Practical Framework for Prototyping 'Negative Tax' Solutions"
            url: https://arxiv.org/abs/2508.08492
            authors: Gunnar Zarncke, Michael Vaiana, Cameron Berg, Judd Rosenblatt et al.
        summary: agenda-agnostic approaches to identifying good but overlooked empirical alignment ideas, working with theorists who could use engineers, and prototyping them.
        theoryOfChange: empirical search for “negative alignment taxes” (prioritizing methods that simultaneously enhance alignment and capabilities)
        seeAlso: automated alignment research, iterative alignment, Beijing Key Laboratory of Safe AI and Superalignment, Aligned AI.
        orthodoxProblems:
          - "14"
        targetCase: optimistic.
        someNames:
          - AE Studio
          - Gunnar Zarncke
          - Cameron Berg
          - Michael Vaiana
          - Judd Rosenblatt
          - Diogo Schwerz de Lucena.
        estimatedFTEs: "15"
        critiques:
          - "[The 'Alignment Bonus' is a Dangerous Mirage](https://www.alignmentforum.org/posts/example-critique-neg-tax)"
          - "[Why 'Win-Win' Alignment is a Distraction](https://example.com/win-win-critique)"
        fundedBy:
          - ae-studio
        broadApproaches:
          - engineering
  - id: white-box-safety
    name: White-box safety
    description: Understand and control current model internals
    agendas:
      - id: reverse-engineering
        name: Reverse engineering
        papers:
          - title: The Circuits Research Landscape
            url: https://www.neuronpedia.org/graph/info
          - title: Stochastic Parameter Decomposition
            url: https://openreview.net/forum?id=dEdS9ao8gN
          - title: Attribution-based parameter decomposition
            url: https://www.alignmentforum.org/posts/EPefYWjuHNcNH4C7E/attribution-based-parameter-decomposition
          - title: "MIB: A Mechanistic Interpretability Benchmark"
            url: https://arxiv.org/abs/2504.13151
          - title: "Circuits in Superposition: Compressing many small neural networks into one"
            url: https://www.lesswrong.com/posts/roE7SHjFWEoMcGZKd/circuits-in-superposition-compressing-many-small-neural
          - title: Compressed Computation is (probably) not Computation in Superposition
            url: https://www.lesswrong.com/posts/ZxFchCFJFcgysYsT9/compressed-computation-is-probably-not-computation-in
          - title: The Dual-Route Model of Induction
            url: https://arxiv.org/abs/2504.03022
          - title: The Geometry of Self-Verification in a Task-Specific Reasoning Model
            url: https://arxiv.org/abs/2504.14379
          - title: Converting MLPs into Polynomials in Closed Form
            url: https://arxiv.org/abs/2502.01032
          - title: Extractive Structures Learned in Pretraining Enable Generalization on Finetuned Facts
            url: https://arxiv.org/abs/2412.04614
          - title: Identifying Sparsely Active Circuits Through Local Loss Landscape Decomposition
            url: https://arxiv.org/abs/2504.00194
          - title: "Blink of an eye: a simple theory for feature localization in generative models"
            url: https://arxiv.org/abs/2502.00921
          - title: From Memorization to Reasoning in the Spectrum of Loss Curvature
            url: https://arxiv.org/abs/2510.24256
          - title: Generalization or Hallucination? Understanding Out-of-Context Reasoning in Transformers
            url: https://arxiv.org/abs/2506.10887
          - title: "RelP: Faithful and Efficient Circuit Discovery in Language Models via Relevance Patching"
            url: https://arxiv.org/abs/2508.21258
          - title: "Structural Inference: Interpreting Small Language Models with Susceptibilities"
            url: https://arxiv.org/abs/2504.18274
          - title: "Interpretability in Parameter Space: Minimizing Mechanistic Description Length with Attribution-based Parameter Decomposition"
            url: https://arxiv.org/abs/2501.14926
          - title: How Do LLMs Perform Two-Hop Reasoning in Context?
            url: https://arxiv.org/abs/2502.13913
          - title: "On the creation of narrow AI: hierarchy and nonlocality of neural network skills"
            url: https://arxiv.org/abs/2505.15811
          - title: "Fresh in memory: Training-order recency is linearly encoded in language model activations"
            url: https://arxiv.org/abs/2509.14223
          - title: Language Models use Lookbacks to Track Beliefs
            url: https://arxiv.org/abs/2505.14685
          - title: Interpreting Emergent Planning in Model-Free Reinforcement Learning
            url: https://arxiv.org/pdf/2504.01871
          - title: Constrained belief updates explain geometric structures in transformer representations
            url: https://arxiv.org/abs/2502.01954
          - title: Do Language Models Use Their Depth Efficiently?
            url: https://arxiv.org/abs/2505.13898
          - title: How Do Transformers Learn Variable Binding in Symbolic Programs?
            url: https://arxiv.org/abs/2505.20896
          - title: LLMs Process Lists With General Filter Heads
            url: https://arxiv.org/abs/2510.26784
          - title: Language Models Use Trigonometry to Do Addition
            url: https://arxiv.org/abs/2502.00873
          - title: Transformers Struggle to Learn to Search
            url: https://arxiv.org/abs/2412.04703
          - title: "ICLR: In-Context Learning of Representations"
            url: https://openreview.net/forum?id=pXlmOmlHJZ
          - title: Adversarial Examples Are Not Bugs, They Are Superposition
            url: https://arxiv.org/abs/2508.17456
          - title: Building and evaluating alignment auditing agents
            url: https://lesswrong.com/posts/DJAZHYjWxMrcd2na3/building-and-evaluating-alignment-auditing-agents
          - title: Bridging the human–AI knowledge gap through concept discovery and transfer in AlphaZero
            url: https://www.pnas.org/doi/10.1073/pnas.2406675122
          - title: "Interpreting learned search: finding a transition model and value function in an RNN that plays Sokoban"
            url: https://arxiv.org/abs/2506.10138
        summary: Decompose a model into its functional, interacting components (circuits), formally describe what computation those components perform, and validate their causal effects to reverse-engineer the model's internal algorithm.
        theoryOfChange: By gaining a mechanical understanding of how a model works (the "circuit diagram"), we can predict how models will act in novel situations (generalization), and gain the mechanistic knowledge necessary to safely modify an AI's goals or internal mechanisms.
        seeAlso: Sparse Coding
        orthodoxProblems:
          - "4"
          - "7"
        targetCase: worst case.
        someNames:
          - Lucius Bushnaq
          - Dan Braun
          - Lee Sharkey
          - Aaron Mueller
          - Atticus Geiger
          - Sheridan Feucht
          - David Bau
          - Yonatan Belinkov
          - Stefan Heimersheim.
        estimatedFTEs: 100-200.
        critiques:
          - "[Interpretability Will Not Reliably Find Deceptive AI](https://www.alignmentforum.org/posts/PwnadG4BFjaER3MGf/interpretability-will-not-reliably-find-deceptive-ai)"
          - "[A Problem to Solve Before Building a Deception Detector](https://www.lesswrong.com/posts/YXNeA3RyRrrRWS37A/a-problem-to-solve-before-building-a-deception-detector)"
          - "[MoSSAIC: AI Safety After Mechanism](https://openreview.net/forum?id=n7WYSJ35FU)"
          - "[The Misguided Quest for Mechanistic AI Interpretability](https://ai-frontiers.org/articles/the-misguided-quest-for-mechanistic-ai-interpretability)"
          - "[Mechanistic?](https://arxiv.org/abs/2410.09087)"
          - "[https://www.youtube.com/watch?v=woo_J0RKcpQ](https://www.youtube.com/watch?v=woo_J0RKcpQ)"
          - "[Activation space interpretability may be doomed](https://www.lesswrong.com/posts/gYfpPbww3wQRaxAFD/activation-space-interpretability-may-be-doomed)"
        broadApproaches:
          - cognitive
      - id: concept-based-auditing-and-monitoring
        name: Concept-based auditing and monitoring
        papers:
          - title: Toward universal steering and monitoring of AI models
            url: https://arxiv.org/abs/2502.03708
          - title: Convergent Linear Representations of Emergent Misalignment
            url: https://arxiv.org/abs/2506.11618
          - title: Detecting Strategic Deception Using Linear Probes
            url: https://arxiv.org/abs/2502.03407
          - title: Auditing language models for hidden objectives
            url: https://www.anthropic.com/research/auditing-hidden-objectives
          - title: Reward Model Interpretability via Optimal and Pessimal Tokens
            url: https://arxiv.org/abs/2506.07326
          - title: "The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence"
            url: https://arxiv.org/abs/2502.17420
          - title: How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations
            url: https://arxiv.org/abs/2508.05625
          - title: Refusal in LLMs is an Affine Function
            url: https://arxiv.org/abs/2411.09003
          - title: Here's 18 Applications of Deception Probes
            url: https://lesswrong.com/posts/7zhAwcBri7yupStKy/here-s-18-applications-of-deception-probes
          - title: Cost-Effective Constitutional Classifiers via Representation Re-use
            url: https://alignment.anthropic.com/2025/cheap-monitors
        summary: Identifies directions or subspaces in a model's latent state that correspond to high-level concepts (like refusal, deception, or planning) and uses them to build "mind-reading" probes that audit models for misalignment or monitor them at runtime.
        theoryOfChange: By mapping internal activations to human-interpretable concepts, we can detect dangerous capabilities or deceptive alignment directly in the "brain" of the model even if its overt behavior is perfectly safe and deploy computationally cheap monitors to flag hidden misalignment in deployed systems.
        seeAlso: Reverse engineering, sparse coding, model diffing.
        orthodoxProblems:
          - "1"
          - "4"
          - "12"
        targetCase: pessimistic
        someNames:
          - Daniel Beaglehole
          - Adityanarayanan Radhakrishnan
          - Enric Boix-Adserà
          - Tom Wollschläger
          - Anna Soligo
          - Jack Lindsey
          - Brian Christian
          - Ling Hu
          - Nicholas Goldowsky-Dill
        estimatedFTEs: 50-100.
        critiques:
          - "[Exploring the generalization of LLM truth directions on conversational formats](https://arxiv.org/html/2505.09807v1)"
          - "[Understanding (Un)Reliability of Steering Vectors in Language Models](https://arxiv.org/abs/2505.22637)"
        fundedBy:
          - open-philanthropy
          - anthropic
        broadApproaches:
          - cognitive
      - id: model-diffing
        name: Model diffing
        papers:
          - title: Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences
            url: https://arxiv.org/abs/2510.13900
            authors: Julian Minder, Clément Dumas, Stewart Slocum et al.
          - title: Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning
            url: https://arxiv.org/abs/2504.02922
            authors: Julian Minder, Clément Dumas, Caden Juang et al.
          - title: https://www.lesswrong.com/posts/xmpauEXEerzYcJKNm/what-we-learned-trying-to-diff-base-and-chat-models-and-why
            url: https://www.lesswrong.com/posts/xmpauEXEerzYcJKNm/what-we-learned-trying-to-diff-base-and-chat-models-and-why
          - title: Persona Features Control Emergent Misalignment
            url: https://arxiv.org/abs/2506.19823
            authors: Miles Wang, Tom Dupré la Tour, Olivia Watkins et al.
          - title: Insights on Crosscoder Model Diffing
            url: https://transformer-circuits.pub/2025/crosscoder-diffing-update/index.html
          - title: Open Source Replication of Anthropic’s Crosscoder paper for model-diffing
            url: https://www.lesswrong.com/posts/srt6JXsRMtmqAJavD/open-source-replication-of-anthropic-s-crosscoder-paper-for
          - title: "Diffing Toolkit: Model Comparison and Analysis Framework"
            url: https://github.com/science-of-finetuning/diffing-toolkit
          - title: "Cross-Architecture Model Diffing with Crosscoders: Unsupervised Discovery of Differences Between LLMs"
            url: https://openreview.net/forum?id=ZB84SvrZB8
        summary: Understanding what happens when a model is finetuned, what the “diff” between the finetuned and the original model is.
        theoryOfChange: By identifying the precise mechanistic differences between a base model and its fine-tuned versions (e.g., after RLHF or safety training), we can verify that safety behaviors are robustly internalized rather than superficially patched, and detect if dangerous capabilities or deceptive alignment have been introduced without needing to re-analyze the entire model.
        seeAlso: sparse coding, reverse engineering.
        orthodoxProblems:
          - "1"
        targetCase: pessimistic
        someNames:
          - Julian Minder
          - Clément Dumas
          - Neel Nanda
          - Trenton Bricken
          - Jack Lindsey
        estimatedFTEs: 10-30
        fundedBy:
          - anthropic
          - google-deepmind
        broadApproaches:
          - cognitive
      - id: sparse-coding
        name: Sparse Coding
        papers:
          - title: Weight-sparse transformers have interpretable circuits
            url: https://cdn.openai.com/pdf/41df8f28-d4ef-43e9-aed2-823f9393e470/circuit-sparsity-paper.pdf
          - title: "Circuit Tracing: Revealing Computational Graphs in Language Models"
            url: https://transformer-circuits.pub/2025/attribution-graphs/methods.html
          - title: Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning
            url: https://arxiv.org/abs/2504.02922
          - title: Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models
            url: https://arxiv.org/abs/2504.02821
          - title: "I Have Covered All the Bases Here: Interpreting Reasoning Features in Large Language Models via Sparse Autoencoders"
            url: https://arxiv.org/abs/2503.18878
          - title: Sparse Autoencoders Do Not Find Canonical Units of Analysis
            url: https://arxiv.org/abs/2502.04878
          - title: Transcoders Beat Sparse Autoencoders for Interpretability
            url: https://arxiv.org/abs/2501.18823
          - title: The Unintended Trade-off of AI Alignment:Balancing Hallucination Mitigation and Safety in LLMs
            url: https://arxiv.org/abs/2510.07775
          - title: Scaling sparse feature circuit finding for in-context learning
            url: https://arxiv.org/abs/2504.13756
          - title: Learning Multi-Level Features with Matryoshka Sparse Autoencoders
            url: https://arxiv.org/abs/2503.17547
          - title: Are Sparse Autoencoders Useful? A Case Study in Sparse Probing
            url: https://arxiv.org/abs/2502.16681
          - title: Sparse Autoencoders Trained on the Same Data Learn Different Features
            url: https://arxiv.org/abs/2501.16615
          - title: Partially Rewriting a Transformer in Natural Language
            url: https://arxiv.org/abs/2501.18838
          - title: "SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability"
            url: https://arxiv.org/abs/2503.09532
          - title: Low-Rank Adapting Models for Sparse Autoencoders
            url: https://arxiv.org/abs/2501.19406
          - title: Enhancing Automated Interpretability with Output-Centric Feature Descriptions
            url: https://arxiv.org/abs/2501.08319
          - title: "Towards Understanding Distilled Reasoning Models: A Representational Approach"
            url: https://arxiv.org/abs/2503.03730
          - title: Do Sparse Autoencoders Generalize? A Case Study of Answerability
            url: https://arxiv.org/abs/2502.19964
          - title: Large Language Models Share Representations of Latent Grammatical Concepts Across Typologically Diverse Languages
            url: https://arxiv.org/abs/2501.06346
          - title: Interpreting the linear structure of vision-language model embedding spaces
            url: https://arxiv.org/abs/2504.11695
          - title: What's In My Human Feedback? Learning Interpretable Descriptions of Preference Data
            url: https://arxiv.org/abs/2510.26202
          - title: Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models
            url: https://arxiv.org/abs/2411.14257
          - title: Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative Matrix Factorization
            url: https://arxiv.org/abs/2506.10920
          - title: "Priors in Time: Missing Inductive Biases for Language Model Interpretability"
            url: https://arxiv.org/abs/2511.01836
          - title: "Inference-Time Decomposition of Activations (ITDA): A Scalable Approach to Interpreting Large Language Models"
            url: https://arxiv.org/abs/2505.17769
          - title: Binary Sparse Coding for Interpretability
            url: https://arxiv.org/abs/2509.25596
          - title: Dense SAE Latents Are Features, Not Bugs
            url: https://arxiv.org/abs/2506.15679
          - title: Evaluating Sparse Autoencoders on Targeted Concept Erasure Tasks
            url: https://arxiv.org/abs/2411.18895
          - title: Evaluating SAE interpretability without explanations
            url: https://arxiv.org/abs/2507.08473
          - title: SAEs Are Good for Steering -- If You Select the Right Features
            url: https://arxiv.org/abs/2505.20063
          - title: "Line of Sight: On Linear Representations in VLLMs"
            url: https://arxiv.org/abs/2506.04706
          - title: "Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models"
            url: https://arxiv.org/abs/2411.00743
          - title: Enhancing Neural Network Interpretability with Feature-Aligned Sparse Autoencoders
            url: https://arxiv.org/abs/2411.01220
          - title: BatchTopK Sparse Autoencoders
            url: https://arxiv.org/abs/2412.06410
          - title: Understanding sparse autoencoder scaling in the presence of feature manifolds
            url: https://arxiv.org/abs/2509.02565
          - title: Internal states before wait modulate reasoning patterns
            url: https://arxiv.org/abs/2510.04128
          - title: "Position: Mechanistic Interpretability Should Prioritize Feature Consistency in SAEs"
            url: https://arxiv.org/abs/2505.20254
          - title: How Visual Representations Map to Language Feature Space in Multimodal LLMs
            url: https://arxiv.org/abs/2506.11976
          - title: "Prisma: An Open Source Toolkit for Mechanistic Interpretability in Vision and Video"
            url: https://arxiv.org/abs/2504.19475
          - title: "CRISP: Persistent Concept Unlearning via Sparse Autoencoders"
            url: https://arxiv.org/abs/2508.13650
          - title: "SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder Guardrails for Precision Unlearning in LLMs"
            url: https://arxiv.org/abs/2504.08192
          - title: Scaling Sparse Feature Circuit Finding to Gemma 9B
            url: https://lesswrong.com/posts/PkeB4TLxgaNnSmddg/scaling-sparse-feature-circuit-finding-to-gemma-9b
          - title: Topological Data Analysis and Mechanistic Interpretability
            url: https://lesswrong.com/posts/6oF6pRr2FgjTmiHus/topological-data-analysis-and-mechanistic-interpretability
        summary: Decompose the polysemantic activations of the residual stream into a sparse linear combination of monosemantic "features" which correspond to interpretable concepts.
        theoryOfChange: Get a principled decomposition of an LLM's activation into atomic components → identify deception and other misbehaviors.
        seeAlso: Concept-based interp, reverse engineering.
        orthodoxProblems:
          - "1"
          - "4"
          - "7"
        targetCase: Optimistic / pessimistic
        someNames:
          - Leo Gao
          - Dan Mossing
          - Emmanuel Ameisen
          - Jack Lindsey
          - Adam Pearce
          - Thomas Heap
          - Abhinav Menon
          - Kenny Peng
          - Tim Lawson.
        estimatedFTEs: 50-100.
        critiques:
          - "[Sparse Autoencoders Can Interpret Randomly Initialized Transformers](https://arxiv.org/abs/2501.17727)"
          - "[The Sparse Autoencoders bubble has popped, but they are still promising](https://agarriga.substack.com/p/the-sparse-autoencoders-bubble-has)"
          - "[Negative Results for SAEs On Downstream Tasks and Deprioritising SAE Research (GDM Mech Interp Team Progress Update \\#2)](https://www.alignmentforum.org/posts/4uXCAJNuPKtKBsi28/)"
          - "[Sparse Autoencoders Trained on the Same Data Learn Different Features](https://arxiv.org/pdf/2501.16615)"
        fundedBy:
          - long-term-future-fund
          - open-philanthropy
        broadApproaches:
          - engineering
          - cognitive
      - id: causal-abstractions
        name: Causal Abstractions
        papers:
          - title: Combining Causal Models for More Accurate Abstractions of Neural Networks
            url: https://arxiv.org/abs/2503.11429
          - title: How Causal Abstraction Underpins Computational Explanation
            url: https://arxiv.org/abs/2508.11214
          - title: "HyperDAS: Towards Automating Mechanistic Interpretability with Hypernetworks"
            url: https://arxiv.org/abs/2503.10894
        summary: Verify that a neural network implements a specific high-level causal model (like a logical algorithm) by finding a mapping between high-level variables and low-level neural representations.
        theoryOfChange: By establishing a precise, causal mapping between a black-box neural network and a human-interpretable algorithm, we can mathematically guarantee that the model is using safe reasoning processes and predict its behavior on unseen inputs, rather than relying on behavioral testing alone.
        seeAlso: concept-based interp, reverse engineering.
        orthodoxProblems:
          - "4"
        targetCase: worst case
        someNames:
          - Atticus Geiger
          - Christopher Potts
          - Thomas Icard
          - Theodora-Mara Pîslar
          - Sara Magliacane
          - Jiuding Sun
          - Jing Huang.
        estimatedFTEs: 10-30
        critiques:
          - "[The Misguided Quest for Mechanistic AI Interpretability](https://www.google.com/search?q=https://open.substack.com/pub/aifrontiersmedia/p/the-misguided-quest-for-mechanistic)"
          - "[Interpretability Will Not Reliably Find Deceptive AI](https://www.lesswrong.com/posts/PwnadG4BFjaER3MGf/interpretability-will-not-reliably-find-deceptive-ai)"
        fundedBy:
          - google-deepmind
          - goodfire
        broadApproaches:
          - cognitive
      - id: data-attribution
        name: Data attribution
        papers:
          - title: Bayesian Influence Functions for Hessian-Free Data Attribution
            url: https://arxiv.org/abs/2509.26544
          - title: Influence Dynamics and Stagewise Data Attribution
            url: https://arxiv.org/abs/2510.12071
          - title: You Are What You Eat -- AI Alignment Requires Understanding How Data Shapes Structure and Generalisation
            url: https://arxiv.org/abs/2502.05475
          - title: Information-Guided Identification of Training Data Imprint in (Proprietary) Large Language Models
            url: https://arxiv.org/abs/2503.12072
          - title: What is Your Data Worth to GPT?
            url: https://arxiv.org/abs/2405.13954
          - title: "Distributional Training Data Attribution: What do Influence Functions Sample?"
            url: https://arxiv.org/abs/2506.12965
          - title: Better Training Data Attribution via Better Inverse Hessian-Vector Products
            url: https://arxiv.org/abs/2507.14740
          - title: "OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training Tokens"
            url: https://arxiv.org/abs/2504.07096
          - title: Detecting and Filtering Unsafe Training Data via Data Attribution with Denoised Representation
            url: https://arxiv.org/abs/2502.11411
          - title: "DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models"
            url: https://arxiv.org/abs/2507.09424
          - title: Revisiting Data Attribution for Influence Functions
            url: https://arxiv.org/abs/2508.07297
          - title: "A Snapshot of Influence: A Local Data Attribution Framework for Online Reinforcement Learning"
            url: https://openreview.net/forum?id=sYK4yPDuT1
        summary: Quantifies the influence of individual training data points on a model's specific behavior or output, allowing researchers to trace model properties (like misalignment, bias, or factual errors) back to their source in the training set.
        theoryOfChange: By attributing harmful, biased, or unaligned behaviors to specific training examples, researchers can audit proprietary models, debug training data, enable effective data deletion/unlearning
        seeAlso: Data quality for alignment.
        orthodoxProblems:
          - "1"
          - "4"
        targetCase: Average case.
        someNames:
          - Philipp Alexander Kreer
          - Wilson Wu
          - Jin Hwa Lee
          - Matthew Smith
          - Abhilasha Ravichander
          - Andrew Wang
          - Jiacheng Liu
          - Jiaqi Ma
          - Junwei Deng
          - Yijun Pan.
        estimatedFTEs: 30-60.
        broadApproaches:
          - behavioral
      - id: other-interpretability
        name: Other interpretability
        papers:
          - title: "Agentic Interpretability: A Strategy Against Gradual Disempowerment"
            url: https://www.alignmentforum.org/posts/s9z4mgjtWTPpDLxFy/agentic-interpretability-a-strategy-against-gradual
          - title: Open Problems in Mechanistic Interpretability
            url: https://arxiv.org/abs/2501.16496
          - title: The Urgency of Interpretability
            url: https://www.darioamodei.com/post/the-urgency-of-interpretability
          - title: Propositional Interpretability in Artificial Intelligence
            url: https://arxiv.org/abs/2501.15740
          - title: "Explainable and Interpretable Multimodal Large Language Models: A Comprehensive Survey"
            url: https://arxiv.org/abs/2412.02104
          - title: Harmonic Loss Trains Interpretable AI Models
            url: https://arxiv.org/abs/2502.01628
          - title: "Through a Steerable Lens: Magnifying Neural Network Interpretability via Phase-Based Extrapolation"
            url: https://arxiv.org/abs/2506.02300
          - title: "Transformers Don't Need LayerNorm at Inference Time: Implications for Interpretability"
            url: https://lesswrong.com/posts/KbFuuaBKRP7FcAADL/transformers-don-t-need-layernorm-at-inference-time
          - title: Against blanket arguments against interpretability
            url: https://lesswrong.com/posts/u3ZysuXEjkyHhefrk/against-blanket-arguments-against-interpretability
          - title: "Opportunity Space: Renormalization for AI Safety"
            url: https://lesswrong.com/posts/wkGmouy7JnTNtWAbc/opportunity-space-renormalization-for-ai-safety
          - title: "Prospects for Alignment Automation: Interpretability Case Study"
            url: https://lesswrong.com/posts/y5cYisQ2QHiSbQbhk/prospects-for-alignment-automation-interpretability-case
          - title: Downstream applications as validation of interpretability progress
            url: https://lesswrong.com/posts/wGRnzCFcowRCrpX4Y/downstream-applications-as-validation-of-interpretability
          - title: Principles for Picking Practical Interpretability Projects
            url: https://lesswrong.com/posts/DqaoPNqhQhwBFqWue/principles-for-picking-practical-interpretability-projects
          - title: "Renormalization Redux: QFT Techniques for AI Interpretability"
            url: https://lesswrong.com/posts/sjr66DBEgyogAbfdf/renormalization-redux-qft-techniques-for-ai-interpretability
          - title: "The Strange Science of Interpretability: Recent Papers and a Reading List for the Philosophy of Interpretability"
            url: https://lesswrong.com/posts/qRnupMmFG7dxQTTYh/the-strange-science-of-interpretability-recent-papers-and-a
          - title: "Call for Collaboration: Renormalization for AI safety"
            url: https://lesswrong.com/posts/MDWGcNHkZ3NPEzcnp/call-for-collaboration-renormalization-for-ai-safety
          - title: Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation Gradient Tracing
            url: https://arxiv.org/abs/2510.02334
          - title: Language Models May Verbatim Complete Text They Were Not Explicitly Trained On
            url: https://arxiv.org/abs/2503.17514
          - title: Extracting memorized pieces of (copyrighted) books from open-weight language models
            url: https://arxiv.org/abs/2505.12546
        summary: Interpretability that does not fall well into other categories, e.g. positions or reviews.
        theoryOfChange: Explore alternative conceptual frameworks (e.g., agentic, propositional) and physics-inspired methods (e.g., renormalization).
        seeAlso: Reverse engineering, concept-based interp.
        orthodoxProblems:
          - "4"
          - "7"
        targetCase: Mixed.
        someNames:
          - Lee Sharkey
          - Dario Amodei
          - David Chalmers
          - Been Kim
          - Neel Nanda
          - David D. Baek
          - Lauren Greenspan
          - Dmitry Vaintrob
          - Sam Marks
          - Jacob Pfau.
        estimatedFTEs: 30-60.
        broadApproaches:
          - engineering
          - cognitive
      - id: activation-engineering
        name: Activation engineering
        papers:
          - title: "Persona Vectors: Monitoring and Controlling Character Traits in Language Models"
            url: https://arxiv.org/abs/2507.21509
          - title: Taxonomy, Opportunities, and Challenges of Representation Engineering for Large Language Models
            url: https://arxiv.org/abs/2502.19649v1
          - title: "Keep Calm and Avoid Harmful Content: Concept Alignment and Latent Manipulation Towards Safer Answers"
            url: https://arxiv.org/abs/2510.12672
          - title: Activation Space Interventions Can Be Transferred Between Large Language Models
            url: https://arxiv.org/abs/2503.04429
          - title: Steering Large Language Model Activations in Sparse Spaces
            url: https://arxiv.org/abs/2503.00177
          - title: Enhancing Multiple Dimensions of Trustworthiness in LLMs via Sparse Activation Control
            url: https://arxiv.org/abs/2411.02461
          - title: Robustly Improving LLM Fairness in Realistic Settings via Interpretability
            url: https://arxiv.org/abs/2506.10922
          - title: "HyperSteer: Activation Steering at Scale with Hypernetworks"
            url: https://arxiv.org/abs/2506.03292
          - title: Steering Evaluation-Aware Language Models to Act Like They Are Deployed
            url: https://arxiv.org/abs/2510.20487
          - title: Steering Out-of-Distribution Generalization with Concept Ablation Fine-Tuning
            url: https://arxiv.org/abs/2507.16795
          - title: Improving Steering Vectors by Targeting Sparse Autoencoder Features
            url: https://arxiv.org/abs/2411.02193
          - title: Understanding Reasoning in Thinking Language Models via Steering Vectors
            url: https://arxiv.org/abs/2506.18167
          - title: Comparing Bottom-Up and Top-Down Steering Approaches on In-Context Learning Tasks
            url: https://arxiv.org/abs/2411.07213
          - title: Taxonomy, Opportunities, and Challenges of Representation Engineering for Large Language Models
            url: https://arxiv.org/abs/2502.19649
          - title: Do safety-relevant LLM steering vectors optimized on a single example generalize?
            url: https://lesswrong.com/posts/6aXe9nipTgwK5LxaP/do-safety-relevant-llm-steering-vectors-optimized-on-a
          - title: One-shot steering vectors cause emergent misalignment, too
            url: https://lesswrong.com/posts/kcKnKHTHycHeRhcHF/one-shot-steering-vectors-cause-emergent-misalignment-too
        summary: A technique for programmatically modifying internal model activations to steer outputs toward desired behaviors, serving as a lightweight, interpretable alternative (or supplement) to fine-tuning.
        theoryOfChange: "Test interpretability theories; find new insights from interpretable causal interventions on representations. Or: build more stuff to stack on top of finetuning. Slightly encourage the model to be nice, add one more layer of defence to our bundle of partial alignment methods."
        seeAlso: Representation engineering, sparse coding, whitebox control.
        orthodoxProblems:
          - "1"
        targetCase: optimistic
        someNames:
          - Runjin Chen
          - Andy Arditi
          - David Krueger
          - Jan Wehner
          - Narmeen Oozeer
          - Reza Bayat
          - Adam Karvonen
          - Jiuding Sun
          - Tim Tian Hua
          - Helena Casademunt
          - Jacob Dunefsky
          - Thomas Marshall.
        estimatedFTEs: 50-200.
        fundedBy:
          - open-philanthropy
          - anthropic
        broadApproaches:
          - engineering
          - cognitive
      - id: developmental-interpretability
        name: Developmental interpretability
        papers:
          - title: Shared Global and Local Geometry of Language Model Embeddings
            url: https://arxiv.org/abs/2503.21073
          - title: Connecting Neural Models Latent Geometries with Relative Geodesic Representations
            url: https://arxiv.org/abs/2506.01599
          - title: Constrained belief updates explain geometric structures in transformer representations
            url: https://arxiv.org/abs/2502.01954
          - title: The Geometry of Self-Verification in a Task-Specific Reasoning Model
            url: https://arxiv.org/pdf/2504.14379
          - title: Navigating the Latent Space Dynamics of Neural Models
            url: https://arxiv.org/abs/2505.22785
          - title: "The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence"
            url: https://arxiv.org/abs/2502.17420
          - title: The Geometry of ReLU Networks through the ReLU Transition Graph
            url: https://arxiv.org/abs/2505.11692
          - title: Deep sequence models tend to memorize geometrically; it is unclear why
            url: https://arxiv.org/abs/2510.26745
          - title: Tracing the Representation Geometry of Language Models from Pretraining to Post-training
            url: https://arxiv.org/abs/2509.23024
          - title: Next-token pretraining implies in-context learning
            url: https://arxiv.org/abs/2505.18373
          - title: Neural networks leverage nominally quantum and post-quantum representations
            url: https://arxiv.org/abs/2507.07432
          - title: Rank-1 LoRAs Encode Interpretable Reasoning Signals
            url: http://arxiv.org/abs/2511.06739
          - title: Embryology of a Language Model
            url: https://arxiv.org/abs/2508.00331
        summary: What do the representations look like? Does any simple structure underlie the beliefs of all well-trained models? Can we get the semantics from this geometry?
        theoryOfChange: Get scalable unsupervised methods for finding structure in representations and interpreting them, then using this to e.g. guide training.
        seeAlso: concept-based interp, computational mechanics, feature universality, natural abstractions, causal abstractions
        orthodoxProblems:
          - "4"
          - "7"
        targetCase: mixed
        someNames:
          - Simplex
          - Insight \+ Interaction Lab
          - Paul Riechers
          - Adam Shai
          - Martin Wattenberg
          - Blake Richards
          - Mateusz Piotrowski
        estimatedFTEs: 10-50.
        critiques:
          - "[Vaintrob](https://www.lesswrong.com/posts/M2bs6xCbmc79nwr8j/dmitry-vaintrob-s-shortform#A8Ziwhts35dgqbz52)"
        broadApproaches:
          - cognitive
      - id: human-inductive-biases
        name: Human Inductive Biases
        papers:
          - title: Aligning machine and human visual representations across abstraction levels
            url: https://www.nature.com/articles/s41586-025-09631-6
          - title: "Beginning with You: Perceptual-Initialization Improves Vision-Language Representation and Alignment"
            url: https://arxiv.org/abs/2505.14204
          - title: Deep Reinforcement Learning Agents are not even close to Human Intelligence
            url: https://arxiv.org/html/2505.21731v1
          - title: "Teaching AI to Handle Exceptions: Supervised Fine-tuning with Human-aligned Judgment"
            url: https://arxiv.org/html/2503.02976v2#S3
          - title: HIBP Human Inductive Bias Project Plan
            url: https://docs.google.com/document/d/1fl7LE8AN7mLJ6uFcPuFCzatp0zCIYvjRIjQRgHPAkSE/edit?tab=t.0
        summary: Discover connections deep learning AI systems have with human brains and human learning processes. Develop an ‘alignment moonshot’ based on a coherent theory of learning which applies to both humans and AI systems.
        theoryOfChange: Humans learn trust, honesty, self-maintenance, and corrigibility; if we understand how they do maybe we can get future AI systems to learn them.
        seeAlso: active learning, ACS
        orthodoxProblems:
          - "4"
        targetCase: pessimistic
        someNames:
          - Lukas Muttenthaler
          - Quentin Delfosse.
        estimatedFTEs: "4"
        fundedBy:
          - google-deepmind
        broadApproaches:
          - cognitive
  - id: safety-by-construction
    name: Safety by construction
    description: Make new systems which are easier to understand and control
    agendas:
      - id: guaranteed-safe-ai
        name: Guaranteed Safe AI
        papers:
          - title: "SafePlanBench: evaluating a Guaranteed Safe AI Approach for LLM-based Agents"
            url: https://manifund.org/projects/safeplanbench-evaluating-a-guaranteed-safe-ai-approach-for-llm-based-agents
            authors: Agustín Martinez Suñé, Tan Zhi Xuan
          - title: Report on NSF Workshop on Science of Safe AI
            url: https://arxiv.org/abs/2506.22492
          - title: "A benchmark for vericoding: formally verified program synthesis"
            url: https://arxiv.org/abs/2509.22908
          - title: Beliefs about formal methods and AI safety
            url: https://lesswrong.com/posts/CCT7Qc8rSeRs7r5GL/beliefs-about-formal-methods-and-ai-safety
          - title: A Toolchain for AI-Assisted Code Specification, Synthesis and Verification
            url: https://atlascomputing.org/ai-assisted-fv-toolchain.pdf
        summary: Formally model the behavior of cyber-physical systems, construct formally verified shells and interfaces which pose precise constraints on what actions can occur, and require AIs to provide safety guarantees for their recommended actions (correctness and uniqueness)
        theoryOfChange: Make a formal verification system that can act as an intermediary between human users and a potentially dangerous system, only letting provably safe actions through. (Notable for not requiring that we solve ELK; does require that we solve ontology though)
        seeAlso: "[Synthesizing Standalone World-Models](https://www.alignmentforum.org/posts/LngR93YwiEpJ3kiWh/research-agenda-synthesizing-standalone-world-models)*,* Bengio's Scientist AI*,* Safeguarded AI, Open Agency Architecture, SLES, program synthesis"
        orthodoxProblems:
          - "1"
          - "4"
          - "7"
          - "9"
          - "12"
        targetCase: (nearly) worst-case
        someNames:
          - ARIA
          - Lawzero
          - Atlas Computing
          - FLF
          - Max Tegmark
          - Steve Omohundro
          - David "davidad" Dalrymple
          - Joar Skalse
          - Stuart Russell
          - Ohad Kammar
          - Alessandro Abate
          - Fabio Zanassi
        estimatedFTEs: "*(SR2024: 10-50)*"
        fundedBy:
          - manifund
          - uk-aisi
          - open-philanthropy
          - survival-flourishing-fund
          - mila-cifar
        broadApproaches:
          - cognitive
      - id: tegmark
        name: Tegmark
        papers:
          - title: https://arxiv.org/abs/2505.15811
            url: https://arxiv.org/abs/2505.15811
        summary: ""
        theoryOfChange: ""
        targetCase: ""
        estimatedFTEs: ""
      - id: scientist-ai
        name: Scientist AI
        papers:
          - title: "Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?"
            url: https://arxiv.org/abs/2502.15657
          - title: "The More You Automate, the Less You See: Hidden Pitfalls of AI Scientist Systems"
            url: https://arxiv.org/abs/2509.08713
        summary: Develop powerful, non-agentic, uncertainty-aware world-modeling systems that substantially accelerate scientific progress while avoiding the risks of building AIs that act as ‘agents’
        theoryOfChange: "Developing non-agentic ‘Scientist AI’ allows us to: (i) reap the benefits of AI progress while (ii) avoiding the inherent risks of agentic systems. These systems can also (iii) provide a useful guardrail to protect us from unsafe agentic AIs by double-checking actions they propose, and (iv) help us more safely build agentic superintelligent systems."
        seeAlso: "[JEPA](https://arxiv.org/abs/2511.08544)"
        orthodoxProblems:
          - "3"
          - "4"
          - "5"
        targetCase: pessimistic
        someNames:
          - Yoshua Bengio
        estimatedFTEs: ""
        critiques:
          - "[Raymond Douglas’ comment](https://www.lesswrong.com/posts/p5gBcoQeBsvsMShvT/superintelligent-agents-pose-catastrophic-risks-can?commentId=tJXqhg3XZsqnyaZs2)"
        fundedBy:
          - aria
          - future-of-life-institute
          - jaan-tallinn
          - schmidt-sciences
        broadApproaches:
          - cognitive
      - id: conjecture-cognitive-software
        name: "Conjecture: Cognitive Software"
        papers: []
        summary: ""
        theoryOfChange: ""
        seeAlso: "[uploadism](https://www.alignmentforum.org/posts/AzFxTMFfkTt4mhMKt/alignment-as-uploading-with-more-steps)"
        targetCase: ""
        estimatedFTEs: ""
      - id: brainlike-agi-safety
        name: Brainlike-AGI Safety
        papers:
          - title: https://www.lesswrong.com/posts/yew6zFWAKG4AGs3Wk/foom-and-doom-1-brain-in-a-box-in-a-basement
            url: https://www.lesswrong.com/posts/yew6zFWAKG4AGs3Wk/foom-and-doom-1-brain-in-a-box-in-a-basement
          - title: https://www.lesswrong.com/posts/bnnKGSCHJghAvqPjS/foom-and-doom-2-technical-alignment-is-hard
            url: https://www.lesswrong.com/posts/bnnKGSCHJghAvqPjS/foom-and-doom-2-technical-alignment-is-hard
          - title: https://www.lesswrong.com/posts/grgb2ipxQf2wzNDEG/perils-of-under-vs-over-sculpting-agi-desires
            url: https://www.lesswrong.com/posts/grgb2ipxQf2wzNDEG/perils-of-under-vs-over-sculpting-agi-desires
          - title: Reward button alignment
            url: https://lesswrong.com/posts/JrTk2pbqp7BFwPAKw/reward-button-alignment
            authors: Steven Byrnes
          - title: "System 2 Alignment: Deliberation, Review, and Thought Management"
            url: https://lesswrong.com/posts/cus5CGmLrjBRgcPSF/system-2-alignment-deliberation-review-and-thought
            authors: Seth Herd
          - title: https://elicit.com/blog/system-2-learning
            url: https://elicit.com/blog/system-2-learning
        summary: Social and moral instincts are (partly) implemented in particular hardwired brain circuitry; let's figure out what those circuits are and how they work; this will involve symbol grounding. “a yet-to-be-invented variation on actor-critic model-based reinforcement learning”
        theoryOfChange: Fairly direct alignment via changing training to reflect actual human reward. Get actual data about (reward, training data) → (human values) to help with theorising this map in AIs; "understand human social instincts, and then maybe adapt some aspects of those for AGIs, presumably in conjunction with other non-biological ingredients".
        targetCase: worst-case
        someNames:
          - Stephen Byrnes
        estimatedFTEs: "1"
        fundedBy:
          - astera-institute
        broadApproaches:
          - cognitive
  - id: make-ai-solve-it
    name: Make AI solve it
    description: Use AI systems to help solve alignment
    agendas:
      - id: weak-to-strong-generalization
        name: Weak-to-strong generalization
        papers:
          - title: Debate Helps Weak-to-Strong Generalization
            url: https://arxiv.org/abs/2501.13124
          - title: Understanding the Capabilities and Limitations of Weak-to-Strong Generalization
            url: https://openreview.net/forum?id=RwYdLgj1S6
          - title: Great Models Think Alike and this Undermines AI Oversight
            url: https://arxiv.org/abs/2502.04313
          - title: Scaling Laws For Scalable Oversight
            url: https://arxiv.org/abs/2504.18530
        summary: Use weaker models to supervise and provide a feedback signal to stronger models.
        theoryOfChange: Find techniques that do better than RLHF at supervising superior models → track whether these techniques fail as capabilities increase further → keep the stronger systems aligned by amplifying weak oversight and quantifying where it breaks.
        seeAlso: Supervising AIs improving AIs.
        orthodoxProblems:
          - "8"
        targetCase: optimistic
        someNames:
          - Joshua Engels
          - Nora Belrose*
          - "* David D. Baek."
        estimatedFTEs: "*(SR2024: 10-50)*"
        critiques:
          - "[Can we safely automate alignment research?](https://joecarlsmith.substack.com/p/can-we-safely-automate-alignment)"
          - "[Super(ficial)-alignment: Strong Models May Deceive Weak Models in Weak-to-Strong Generalization](https://arxiv.org/abs/2406.11431)"
        fundedBy:
          - eleutherai
        broadApproaches:
          - engineering
      - id: supervising-ais-improving-ais
        name: Supervising AIs improving AIs
        papers:
          - title: https://saif.org/research/bare-minimum-mitigations-for-autonomous-ai-development
            url: https://saif.org/research/bare-minimum-mitigations-for-autonomous-ai-development
          - title: Maintaining Alignment during RSI as a Feedback Control Problem
            url: https://lesswrong.com/posts/PhgEKkB4cwYjwpGxb/maintaining-alignment-during-rsi-as-a-feedback-control
            authors: beren
          - title: Scaling Laws for Scalable Oversight
            url: https://www.google.com/url?q=https://arxiv.org/abs/2504.18530&sa=D&source=docs&ust=1764169943848285&usg=AOvVaw0y4hN8GA8F5MlxPrpbt_-c
            authors: Subhash Kantamneni, Josh Engels, David Baek, Max Tegmark
          - title: Neural Interactive Proofs
            url: https://arxiv.org/abs/2412.08897
            authors: Lewis Hammond, Sam Adam-Day
          - title: Video and transcript of talk on automating alignment research
            url: https://lesswrong.com/posts/TQbptN7F4ijPnQRLy/video-and-transcript-of-talk-on-automating-alignment
            authors: Joe Carlsmith
        summary: Build formal and empirical frameworks where AIs supervise other (stronger) AI systems via structured interactions; construct monitoring tools which enable scalable tracking of behavioural drift, benchmarks for self-modification, and robustness guarantees
        theoryOfChange: Early models train \~only on human data while later models also train on early model outputs, which leads to early model problems cascading. Left unchecked this will likely cause problems, so supervision mechanisms are needed to help ensure the AI self-improvement remains legible.
        orthodoxProblems:
          - "7"
          - "8"
        targetCase: pessimistic
        someNames:
          - "*(SR2024: Roman Engeler"
          - Akbir Khan
          - Ethan Perez)*
        estimatedFTEs: "*(SR2024: 1-10)*"
        fundedBy:
          - long-term-future-fund
        broadApproaches:
          - behavioral
      - id: deepmind-amplified-oversight
        name: DeepMind Amplified Oversight
        papers: []
        summary: ""
        theoryOfChange: ""
        targetCase: ""
        estimatedFTEs: ""
      - id: debate
        name: Debate
        papers:
          - title: AI Debate Aids Assessment of Controversial Claims
            url: https://arxiv.org/abs/2506.02175
            authors: Salman Rahman, Sheriff Issaka, Ashima Suvarna et al.
          - title: An alignment safety case sketch based on debate
            url: https://arxiv.org/abs/2505.03989
            authors: Marie Davidsen Buhl, Jacob Pfau, Benjamin Hilton et al.
        summary: Make highly capable agents do what humans want, even when it is difficult for humans to know what that is.
        theoryOfChange: "\"Give humans help in supervising strong agents\" \\+ \"Align explanations with the true reasoning process of the agent\" \\+ \"Red team models to exhibit failure modes that don't occur in normal use\" are necessary but probably not sufficient for safe AGI."
        orthodoxProblems:
          - "1"
          - "7"
        targetCase: worst-case
        someNames:
          - Rohin Shah
          - Jonah Brown-Cohen
          - Georgios Piliouras
        estimatedFTEs: "?"
        fundedBy:
          - google-deepmind
        broadApproaches:
          - engineering
          - cognitive
      - id: task-decomposition
        name: Task decomposition
        papers: []
        summary: ""
        theoryOfChange: ""
        targetCase: ""
        estimatedFTEs: ""
      - id: adversarial-oversight
        name: Adversarial oversight
        papers:
          - title: https://www.semafor.com/article/11/05/2025/microsoft-superintelligence-team-promises-to-keep-humans-in-charge
            url: https://www.semafor.com/article/11/05/2025/microsoft-superintelligence-team-promises-to-keep-humans-in-charge
        summary: ""
        theoryOfChange: ""
        targetCase: ""
        estimatedFTEs: ""
  - id: theory
    name: Theory
    description: How to understand and control current and future models
    agendas:
      - id: agent-foundations
        name: Agent foundations
        papers:
          - title: There is No Reliable Estimate of P(doom)
            url: https://static1.squarespace.com/static/678814b5570c5a7a78df555d/t/67d09e77f3364b72d1ee91d7/1741725303556/Günther+-+There+is+No+Reliable+P%28doom%29+-+Mario+Guenther.pdf
            authors: Mario Gunther
          - title: https://www.arxiv.org/pdf/2508.16245
            url: https://www.arxiv.org/pdf/2508.16245
          - title: https://uaiasi.com/blog-posts/
            url: https://uaiasi.com/blog-posts/
          - title: Off-switching not guaranteed
            url: https://link.springer.com/article/10.1007/s11098-025-02296-x
            authors: Sam Eisenstat
          - title: https://openreview.net/pdf?id=Rf1CeGPA22
            url: https://openreview.net/pdf?id=Rf1CeGPA22
          - title: Formalizing Embeddedness Failures in Universal Artificial Intelligence
            url: https://openreview.net/forum?id=tlkYPU3FlX
            authors: Cole Wyeth, Marcus Hutter
          - title: "Clarifying “wisdom”: Foundational topics for aligned AIs to prioritize before irreversible decisions"
            url: https://www.lesswrong.com/posts/EyvJvYEFzDv5kGoiG/clarifying-wisdom-foundational-topics-for-aligned-ais-to
            authors: Anthony DiGiovanni
          - title: "Agent foundations: not really math, not really science"
            url: https://www.lesswrong.com/posts/Dt4DuCCok3Xv5HEnG/agent-foundations-not-really-math-not-really-science
            authors: Alex Altair
          - title: Is alignment reducible to becoming more coherent?
            url: https://lesswrong.com/posts/nuDJNyG5XLQjtvaeg/is-alignment-reducible-to-becoming-more-coherent
            authors: Cole Wyeth
          - title: What Is The Alignment Problem?
            url: https://lesswrong.com/posts/dHNKtQ3vTBxTfTPxu/what-is-the-alignment-problem
            authors: johnswentworth
        summary: Develop philosophical clarity and mathematical formalization of building blocks that might be useful for plans to align strong superintelligence, such as agency, decision theory, abstractions, concepts, etcInvestigate what an ‘agent’ is, what it means for an agent to be aligned, and what are the deep structural constraints on ‘agents’ in our universe
        theoryOfChange: Rigorously understand what agents are/what it means for them to be aligned in a substrate independent way → identify impossibility results and necessary conditions for aligned agentic systems → use this theoretical understanding to eventually design safe architectures that remain stable and safe under self-reflection
        orthodoxProblems:
          - "1"
          - "2"
          - "4"
        targetCase: worst-case
        someNames:
          - Abram Demski
          - Alex Altair
        estimatedFTEs: ""
        broadApproaches:
          - cognitive
      - id: tiling-agents
        name: Tiling agents
        papers:
          - title: Understanding Trust
            url: https://static1.squarespace.com/static/663d1233249bce4815fe8753/t/68067a6f5d5fb0745642d5b1/1745255023842/Understanding+Trust+-+Abram+Demski.pdf
            authors: Abram Demski, Norman Hsia, and Paul Rapoport
          - title: earlier version
            url: https://static1.squarespace.com/static/678814b5570c5a7a78df555d/t/67d09e3b1ae9ea24b0100509/1741725243695/Understanding_Trust__AFC_+-+Abram+Demski.pdf
          - title: Working through a small tiling result
            url: https://www.lesswrong.com/posts/akuMwu8SkmQSdospi/working-through-a-small-tiling-result
          - title: Communication & Trust
            url: https://openreview.net/forum?id=Rf1CeGPA22
            authors: Abram Demski
          - title: Maintaining Alignment during RSI as a Feedback Control Problem
            url: https://lesswrong.com/posts/PhgEKkB4cwYjwpGxb/maintaining-alignment-during-rsi-as-a-feedback-control
            authors: beren
        summary: An aligned agent modifying itself into an unaligned agent would be bad and we can research ways that this could occur and infrastructure/approaches that prevent it from happening.
        theoryOfChange: Build enough theoretical basis through various approaches such that AI agents we create are capable of self-modification while preserving goals.
        seeAlso: "[Agent foundations](#agent-foundations-[cat:agent_foundations])"
        orthodoxProblems:
          - "1"
          - "2"
          - "4"
        targetCase: worst-case
        someNames:
          - Abram Demski
        estimatedFTEs: 5?
        broadApproaches:
          - cognitive
      - id: dovetail
        name: Dovetail
        papers:
          - title: Report & retrospective on the Dovetail fellowship
            url: https://www.lesswrong.com/posts/ApfjBbqzSu4aZoLSe/report-and-retrospective-on-the-dovetail-fellowship
            authors: Alex Altair
        summary: Formalize key ideas (“structure”, “agency”, etc) mathematically
        theoryOfChange: generalize theorems → formalize agent foundations concepts like the agent structure problem → hopefully assist other projects through increased understanding
        seeAlso: "[Agent foundations](#agent-foundations-[cat:agent_foundations])"
        targetCase: pessimistic
        someNames:
          - Alex Altair
          - Alfred Harwood
          - Daniel C
          - Dalcy K
          - José Pedro Faustino
        estimatedFTEs: "2"
        fundedBy:
          - long-term-future-fund
        broadApproaches:
          - maths-philosophy
      - id: high-actuation-spaces
        name: High-Actuation Spaces
        papers:
          - title: https://openreview.net/forum?id=n7WYSJ35FU
            url: https://openreview.net/forum?id=n7WYSJ35FU
          - title: https://www.lesswrong.com/s/aMz2JMvgXrLBkq4h3
            url: https://www.lesswrong.com/s/aMz2JMvgXrLBkq4h3
          - title: https://docs.google.com/document/d/1d-ARdZZDHFPIfGcTTOKK8IZWlQj0NZQrmteJj2mvmYA/edit?tab=t.0\#heading=h.eg8luyrlsv2u
            url: https://docs.google.com/document/d/1d-ARdZZDHFPIfGcTTOKK8IZWlQj0NZQrmteJj2mvmYA/edit?tab=t.0#heading=h.eg8luyrlsv2u
          - title: https://drive.google.com/drive/folders/1EaAJ4szuZsYR2_-DkS9cuhx3S6IWeCjW
            url: https://drive.google.com/drive/folders/1EaAJ4szuZsYR2_-DkS9cuhx3S6IWeCjW
          - title: https://www.lesswrong.com/posts/tQ9vWm4b57HFqbaRj/what-if-not-agency
            url: https://www.lesswrong.com/posts/tQ9vWm4b57HFqbaRj/what-if-not-agency
          - title: Human Inductive Bias Project
            url: https://docs.google.com/document/d/1fl7LE8AN7mLJ6uFcPuFCzatp0zCIYvjRIjQRgHPAkSE/edit?tab=t.0
        summary: mechinterp and alignment assume a stable “computational substrate” (linear algebra on GPUs). If later AI uses different substrates (e.g. KAN, e.g. something neuromorphic), methods like probes and steering will not transfer. Therefore, better to try and infer goals via a "telic DAG" which abstracts over substrates, and so sidestep the issue of how to define intermediate representations. Category theory is intended to provide guarantees that this abstraction is valid.
        theoryOfChange: sufficiently complex mindlike entities can alter their goals in ways that cannot be predicted or accounted for under substrate-dependent descriptions of the kind sought in mechanistic interpretability. use the telic DAG to define a method analogous to factoring a causal DAG.
        seeAlso: "[Live theory](https://www.lesswrong.com/s/aMz2JMvgXrLBkq4h3), [MoSSAIC](https://openreview.net/forum?id=n7WYSJ35FU), Topos, [Agent foundations](#agent-foundations-[cat:agent_foundations])**,**"
        targetCase: pessimistic
        someNames:
          - Sahil K
          - Matt Farr
          - Aditya Arpitha Prasad
          - Chris Pang
          - Aditya Adiga
          - Jayson Amati
          - Steve Petersen
          - Topos
          - TJ
        estimatedFTEs: "2"
        broadApproaches:
          - maths-philosophy
      - id: simulators
        name: Simulators
        papers:
          - title: A Three-Layer Model of LLM Psychology
            url: https://www.alignmentforum.org/posts/zuXo9imNKYspu9HGv/a-three-layer-model-of-llm-psychology
            authors: Jan Kulveit
          - title: "Simulators vs Agents: Updating Risk Models"
            url: https://www.lesswrong.com/s/pwKrMXjYNK5LNeKCu
            authors: Will Petillo, Sean Herrington, Spencer Ames, Adebayo Mubarak, Can Narin
        summary: Treat LLMs as a general simulator of sequences instead of as an agent. It effectively *roleplays* as a character or a statistical process. This is a counter-intuitive and hard to verify perspective, but yields different predictions.
        theoryOfChange: Figure out the extent to which this framing matches current and future AI agent psychology, then use that to have less confused conversations and build safer models.
        orthodoxProblems:
          - "4"
        targetCase: pessimistic
        someNames:
          - Jan Kulveit
          - Will Petillo
        estimatedFTEs: 1.5?
        broadApproaches:
          - cognitive
      - id: asymptotic-guarantees
        name: Asymptotic guarantees
        papers:
          - title: An alignment safety case sketch based on debate
            url: https://lesswrong.com/posts/iELyAqizJkizBQbfr/an-alignment-safety-case-sketch-based-on-debate
            authors: Marie_DB, Jacob Pfau, Benjamin Hilton et al.
          - title: "UK AISI's Alignment Team: Research Agenda"
            url: https://lesswrong.com/posts/tbnw7LbNApvxNLAg8/uk-aisi-s-alignment-team-research-agenda
            authors: Benjamin Hilton, Jacob Pfau, Marie_DB et al.
          - title: Dodging systematic human errors in scalable oversight
            url: https://lesswrong.com/posts/EgRJtwQurNzz8CEfJ/dodging-systematic-human-errors-in-scalable-oversight
            authors: Geoffrey Irving
        summary: prove that if a safety process has enough resources (human data quality, training time, neural network capacity), then in the limit some system specification will be guaranteed. Use complexity theory, game theory, learning theory and other areas to both improve asymptotic guarantees and develop ways of showing convergence.
        theoryOfChange: Formal verification may be too hard. Make safety cases stronger by modelling their processes and proving that they would work in the limit.
        seeAlso: debate, control
        orthodoxProblems:
          - "4"
          - "7"
        targetCase: pessimistic
        someNames:
          - AISI
          - Jacob Pfau
          - Benjamin Hilton
          - Geoffrey Irving
          - Simon Marshall
          - Will Kirby
          - Martin Soto
          - David Africa
        estimatedFTEs: 5 - 10
        critiques:
          - "[UK AISI's Alignment Team: Research Agenda](https://lesswrong.com/posts/tbnw7LbNApvxNLAg8/uk-aisi-s-alignment-team-research-agenda)"
        fundedBy:
          - uk-aisi
      - id: arc-theory
        name: ARC Theory
        papers:
          - title: A computational no-coincidence principle
            url: https://www.lesswrong.com/posts/Xt9r4SNNuYxW83tmo/a-computational-no-coincidence-principle
          - title: Wide Neural Networks as a Baseline for the Computational No-Coincidence Conjecture
            url: https://openreview.net/forum?id=m4OpQAK3eY
          - title: "ARC progress update: Competing with sampling"
            url: https://www.lesswrong.com/posts/XdQd9gELHakd5pzJA/arc-progress-update-competing-with-sampling
          - title: Obstacles in ARC’s research agenda
            url: https://www.lesswrong.com/s/uYMw689vDFmgPEHrS
          - title: Acorn
            url: https://acausal.org/
        summary: Formalize mechanistic explanations of neural network behavior, automate the discovery of these “heuristic explanations” and use them to predict when novel input will lead to extreme behavior (Low Probability Estimation and Mechanistic Anomaly Detection).
        theoryOfChange: push mech interp as far as it can be pushed, use heustic explanations for downstream tasks automatically, solve those downstream tasks well enough that Eliciting Latent Knowledge and deceptive alignment are solved.
        seeAlso: ELK, mechanistic anomaly detection
        orthodoxProblems:
          - "4"
          - "8"
        targetCase: "*worst-case*"
        someNames:
          - "*Jacob Hilton"
          - Mark Xu
          - Eric Neyman
          - Victor Lecomte
          - George Robinson*
        estimatedFTEs: "*(SR2024: 1-10)*"
        critiques:
          - "[Matolcsi](https://www.lesswrong.com/s/uYMw689vDFmgPEHrS)"
        broadApproaches:
          - cognitive
          - maths-philosophy
      - id: behavior-alignment-theory
        name: Behavior alignment theory
        papers:
          - title: Imitation learning is probably existentially safe
            url: https://onlinelibrary.wiley.com/doi/10.1002/aaai.70040?af=R
          - title: Preference gaps as a safeguard against AI self-replication
            url: https://www.lesswrong.com/posts/knwR9RgGN5a2oorci/preference-gaps-as-a-safeguard-against-ai-self-replication
            authors: Elliott Thornley, tbs
          - title: Serious Flaws in CAST
            url: https://www.lesswrong.com/s/KfCjeconYRdFbMxsy/p/qgBFJ72tahLo5hzqy
            authors: Max Harms, 2025-11-19,
          - title: Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power
            url: https://arxiv.org/abs/2508.00159
            authors: Jobst Heitzig, Ram Potham
          - title: "A Safety Case for a Deployed LLM: Corrigibility as a Singular Target"
            url: https://openreview.net/forum?id=mhEnJa9pNk
            authors: Ram Potham
          - title: Shutdownable Agents through POST-Agency
            url: https://arxiv.org/abs/2505.20203
            authors: Elliott Thornley
          - title: The Partially Observable Off-Switch Game
            url: https://arxiv.org/abs/2411.17749
            authors: Andrew Garber, Rohan Subramani, Linus Luu et al.
          - title: Deceptive Alignment and Homuncularity
            url: https://lesswrong.com/posts/9htmQx5wiePqTtZuL/deceptive-alignment-and-homuncularity
            authors: Oliver Sourbut, TurnTrout
          - title: LLM AGI will have memory, and memory changes alignment
            url: https://lesswrong.com/posts/aKncW36ZdEnzxLo8A/llm-agi-will-have-memory-and-memory-changes-alignment
            authors: Seth Herd
        summary: Predict properties of future AGI (e.g. power-seeking) with formal models; formally state and prove hypotheses about the properties powerful systems will have and how we might try to change them
        theoryOfChange: Figure out hypotheses about properties powerful agents will have → attempt to rigorously prove under what conditions the hypotheses hold → test these hypotheses where feasible → design training environments that lead to more salutary properties
        seeAlso: Agent Foundations, control.
        orthodoxProblems:
          - "2"
          - "5"
        targetCase: worst-case
        someNames:
          - Ram Potham
          - Michael K. Cohen*
          - "* Max Harms/Raelifin*"
          - "* John Wentworth*"
          - "* David Lorell*"
          - "* Elliott Thornley"
        estimatedFTEs: 1-10?
        critiques:
          - "[Ryan Greenblatt’s criticism](https://www.lesswrong.com/posts/YbEbwYWkf8mv9jnmi/the-shutdown-problem-incomplete-preferences-as-a-solution?commentId=GJAippZ6ZzCagSnDb)"
        broadApproaches:
          - maths-philosophy
      - id: other-corrigibility
        name: Other corrigibility
        papers:
          - title: AI Assistants Should Have a Direct Line to Their Developers
            url: https://www.lesswrong.com/posts/LDYPF6yfe3f8SPHFT/ai-assistants-should-have-a-direct-line-to-their-developers
          - title: Testing for Scheming with Model Deletion
            url: https://www.lesswrong.com/posts/D5kGGGhsnfH7G8v9f/testing-for-scheming-with-model-deletion
          - title: Detect Goodhart and shut down
            url: https://www.lesswrong.com/posts/ZHFZ6tivEjznkEoby/detect-goodhart-and-shut-down
            authors: Jeremy Gillen
          - title: Instrumental Goals Are A Different And Friendlier Kind Of Thing Than Terminal Goals
            url: https://www.lesswrong.com/posts/7Z4WC4AFgfmZ3fCDC/instrumental-goals-are-a-different-and-friendlier-kind-of
            authors: John Wentworth, David Lorell
          - title: Shutdownable Agents through POST-Agency
            url: https://www.lesswrong.com/posts/JuRdvZyqaFbvTPemn/shutdownable-agents-through-post-agency-1
            authors: EJT
          - title: Why Corrigibility is Hard and Important (i.e. "Whence the high MIRI confidence in alignment difficulty?")
            url: https://www.lesswrong.com/posts/ksfjZJu3BFEfM6hHE/why-corrigibility-is-hard-and-important-i-e-whence-the-high
            authors: Raemon, Eliezer Yudkowsky, So8res
          - title: Problems with instruction-following as an alignment target
            url: https://lesswrong.com/posts/CSFa9rvGNGAfCzBk6/problems-with-instruction-following-as-an-alignment-target
            authors: Seth Herd
        summary: Diagnose and communicate obstacles to achieving robustly corrigible behavior; suggest mechanisms, tests, and escalation channels for surfacing and mitigating incorrigible behaviors
        theoryOfChange: Labs are likely to develop AGI using something analogous to current pipelines. Clarifying why naive instruction-following doesn’t buy robust corrigibility \+ building strong tripwires/diagnostics for scheming and Goodharting thus reduces risks on the likely default path.
        seeAlso: Behavior alignment theory
        orthodoxProblems:
          - "2"
          - "5"
        targetCase: pessimistic
        someNames:
          - "*Jan Kulveit"
          - Jeremy Gillen*
        estimatedFTEs: 1-5?
        broadApproaches:
          - engineering
          - maths-philosophy
      - id: natural-abstractions
        name: Natural abstractions
        papers:
          - title: "Natural Latents: Latent Variables Stable Across Ontologies"
            url: https://arxiv.org/abs/2509.03780
            authors: John Wentworth, David Lorell
          - title: Abstract Mathematical Concepts vs. Abstractions Over Real-World Systems
            url: https://www.lesswrong.com/posts/T6xSXiXF3WF6TmCyN/abstract-mathematical-concepts-vs-abstractions-over-real
            authors: Thane Ruthenis
          - title: "Condensation: a theory of concepts"
            url: https://openreview.net/forum?id=HwKFJ3odui
            authors: Sam Eisenstat
          - title: Getting aligned on representational alignment
            url: https://arxiv.org/abs/2310.13018
          - title: Platonic representation hypothesis
            url: https://phillipi.github.io/prh/
          - title: Rosas
            url: https://www.youtube.com/watch?v=Nr9eMobqUOo&t=3s
        summary: check the hypothesis that our universe "abstracts well" and that many cognitive systems learn to use similar abstractions. Check if features correspond to small causal diagrams corresponding to linguistic constructions
        theoryOfChange: "find all possible abstractions of a given computation → translate them into human-readable language → identify useful ones like deception → intervene when a model is using it. Also develop theory for interp more broadly; more mathematical analysis. Also maybe enables \"retargeting the search\": identifying \"utility function\" components inside the AI and replacing calls to it with calls to \"user values\" (represented using existing abstractions inside the AI)."
        seeAlso: causal abstractions, representational alignment, convergent abstractions, feature universality, Platonic representation hypothesis
        orthodoxProblems:
          - "5"
          - "7"
          - "9"
        targetCase: "SR2024: worst-case"
        someNames:
          - John Wentworth
          - Paul Colognese
          - David Lorrell
          - Sam Eisenstat
        estimatedFTEs: "*(SR2024: 1-10)*"
        fundedBy:
          - ea-funds
        broadApproaches:
          - cognitive
      - id: other-ontology-work
        name: Other ontology work
        papers:
          - title: "Factored space models: Towards causality between levels of abstraction"
            url: https://arxiv.org/abs/2412.02579
          - title: A single principle related to many Alignment subproblems?
            url: https://lesswrong.com/posts/h89L5FMAkEBNsZ3xM/a-single-principle-related-to-many-alignment-subproblems-2
        summary: ""
        theoryOfChange: ""
        targetCase: ""
        estimatedFTEs: ""
      - id: the-learning-theoretic-agenda
        name: The Learning-Theoretic Agenda
        papers:
          - title: Infra-Bayesian Decision-Estimation Theory
            url: https://www.lesswrong.com/posts/LgLez8aeK24PbyyQJ/new-paper-infra-bayesian-decision-estimation-theory
            authors: Vanessa Kosoy, Diffractor
          - title: Infra-Bayesianism category on LessWrong
            url: https://www.lesswrong.com/w/infra-bayesianism?sortedBy=new
          - title: Ambiguous Online Learning
            url: https://www.lesswrong.com/posts/Y9NuKpb6dsyiYFxWK/new-paper-ambiguous-online-learning
            authors: Vanessa Kosoy
          - title: Regret Bounds for Robust Online Decision Making
            url: https://arxiv.org/abs/2504.06820
            authors: Alexander Appel, Vanessa Kosoy
          - title: "What is Inadequate about Bayesianism for AI Alignment: Motivating Infra-Bayesianism"
            url: https://lesswrong.com/posts/wzCtwYtojMabyEg2L/what-is-inadequate-about-bayesianism-for-ai-alignment
            authors: Brittany Gelb
        summary: try to formalise a more realistic agent, understand what it means for it to be aligned with us, translate between its ontology and ours, and produce desiderata for a training setup that points at coherent AGIs similar to our model of an aligned agent
        theoryOfChange: fix formal epistemology to work out how to avoid deep training problems
        orthodoxProblems:
          - "1"
          - "9"
        targetCase: worst-case
        someNames:
          - Vanessa Kosoy
          - Diffractor
        estimatedFTEs: "34"
        critiques:
          - "[Matolcsi](https://www.lesswrong.com/posts/StkjjQyKwg7hZjcGB/a-mostly-critical-review-of-infra-bayesianism)"
        fundedBy:
          - ea-funds
          - survival-flourishing-fund
          - aria
          - uk-aisi
        broadApproaches:
          - cognitive
      - id: uncategorised
        name: Uncategorised
        papers:
          - title: Superposition Yields Robust Neural Scaling
            url: https://arxiv.org/abs/2505.10465
          - title: Training Dynamics of In-Context Learning in Linear Attention
            url: https://arxiv.org/abs/2501.16265
          - title: General agents contain world models
            url: https://arxiv.org/abs/2506.01622
          - title: The Limits of Predicting Agents from Behaviour
            url: https://arxiv.org/abs/2506.02923
          - title: Measuring Goal-Directedness
            url: https://arxiv.org/abs/2412.04758
          - title: How do we solve the alignment problem?
            url: https://lesswrong.com/posts/fMqgLGoeZFFQqAGyC/how-do-we-solve-the-alignment-problem
          - title: Plans A, B, C, and D for misalignment risk
            url: https://lesswrong.com/posts/E8n93nnEaFeXTbHn5/plans-a-b-c-and-d-for-misalignment-risk
          - title: What is it to solve the alignment problem?
            url: https://lesswrong.com/posts/syEwQzC6LQywQDrFi/what-is-it-to-solve-the-alignment-problem-2
          - title: AI for AI safety
            url: https://lesswrong.com/posts/F3j4xqpxjxgQD3xXh/ai-for-ai-safety
          - title: AI Safety x Physics Grand Challenge
            url: https://lesswrong.com/posts/qSDvzyh7LgsAJfehk/ai-safety-x-physics-grand-challenge
          - title: We won't get AIs smart enough to solve alignment but too dumb to rebel
            url: https://lesswrong.com/posts/8buEtNxCScYpjzgW8/we-won-t-get-ais-smart-enough-to-solve-alignment-but-too
          - title: "ASI existential risk: Reconsidering Alignment as a Goal"
            url: https://lesswrong.com/posts/YeQe36XiY4BhrtRh5/asi-existential-risk-reconsidering-alignment-as-a-goal-1
          - title: Legible vs. Illegible AI Safety Problems
            url: https://lesswrong.com/posts/PMc65HgRFvBimEpmJ/legible-vs-illegible-ai-safety-problems
          - title: Paths and waystations in AI safety
            url: https://lesswrong.com/posts/kBgySGcASWa4FWdD9/paths-and-waystations-in-ai-safety-1
          - title: Definition of alignment science I like
            url: https://lesswrong.com/posts/mk3qkvBv8ciFeXGdL/definition-of-alignment-science-i-like
          - title: Two alignment threat models
            url: https://aligned.substack.com/p/two-alignment-threat-models?isFreemail=true&post_id=151342016&publication_id=328633&r=67wny&triedRedirect=true
          - title: Can we safely automate alignment research?
            url: https://lesswrong.com/posts/nJcuj4rtuefeTRFHp/can-we-safely-automate-alignment-research
  - id: multi-agent-first
    name: Multi-agent first
    description: Multi-agent approaches to alignment
    agendas:
      - id: alignment-to-context
        name: Alignment to context
        papers:
          - title: Beyond Preferences in AI Alignment
            url: https://arxiv.org/abs/2408.16984
          - title: 2404.10636 - What are human values, and how do we align AI to them?
            url: https://arxiv.org/abs/2404.10636
          - title: 2503.00940 - Can AI Model the Complexities of Human Moral Decision-Making? A Qualitative Study of Kidney Allocation Decisions
            url: https://arxiv.org/abs/2503.00940
          - title: The Frame-Dependent Mind
            url: https://www.softmax.com/blog/the-frame-dependent-mind
          - title: Model Integrity
            url: https://meaningalignment.substack.com/p/model-integrity
          - title: On Eudaimonia and Optimization
            url: https://docs.google.com/document/d/1cKbqYSGspfJavXvnhsp3mAuxHh08rNbP7tzYieqLiXw/edit?tab=t.0
          - title: Full-Stack Alignment
            url: https://www.full-stack-alignment.ai
        summary: Align AI directly to the role of participant-in, collaborator-for, or adviser-to our best-endorsed real human practices and institutions, instead of aligning AI to separately representable goals, rules, or utility functions
        theoryOfChange: Many classical problems in AGI alignment are downstream of a type error about human values. Operationalizing a correct view of human values -- one that treats human values as impossible or impractical to abstract from concrete human practices and institutions -- will unblock value fragility, goal-misgeneralization, instrumental convergence, and pivotal-act specification
        seeAlso: Aligning what? Aligned to who?
        orthodoxProblems:
          - "1"
          - "2"
          - "4"
          - "5"
          - "13"
        targetCase: Mixed
        someNames:
          - Meaning Alignment Institute
          - Tan Zhi-Xuan
          - Matija Franklin
          - Ryan Lowe
          - Joe Edelman
          - Oliver Klingefjord
          - Full Stack Alignment
        estimatedFTEs: "5"
        fundedBy:
          - aria
          - openai
          - survival-flourishing-fund
        broadApproaches:
          - behavioral
      - id: aligning-to-the-social-contract
        name: Aligning to the social contract
        papers:
          - title: "Law-Following AI: designing AI agents to obey human laws"
            url: https://law-ai.org/law-following-ai/
          - title: 2506.17434 - Resource Rational Contractualism Should Guide AI Alignment
            url: https://arxiv.org/abs/2506.17434
          - title: 2509.07955 - ACE and Diverse Generalization via Selective Disagreement
            url: https://arxiv.org/abs/2509.07955
          - title: "Promises Made, Promises Kept: Safe Pareto Improvements via Ex Post Verifiable Commitments"
            url: https://arxiv.org/abs/2505.00783
          - title: 2408.16984 - Beyond Preferences in AI Alignment
            url: https://arxiv.org/abs/2408.16984
          - title: Societal alignment frameworks can improve llm alignment
            url: https://arxiv.org/abs/2503.00069
        summary: Generate AIs’ operational values from ‘social contract’-style ideal civic deliberation formalisms and their consequent rulesets for civic actors
        theoryOfChange: Formalize and apply the liberal tradition's project of defining civic principles separable from the substantive good, aligning our AIs to civic principles that bypass fragile utility-learning and intractable utility-calculation
        seeAlso: Alignment to context,  Aligning what
        orthodoxProblems:
          - "1"
          - "4"
          - "5"
          - "10"
          - "13"
        targetCase: Mixed
        someNames:
          - Gillian Hadfield
          - Tan Zhi-Xuan
          - Sydney Levine
          - Matija Franklin
          - Joshua B. Tenenbaum
        estimatedFTEs: 5 - 10
        fundedBy:
          - google-deepmind
        broadApproaches:
          - cognitive
      - id: aligning-multiple-ais-game-theory
        name: "Aligning multiple AIs: game theory"
        papers:
          - title: 2502.14143 - Multi-Agent Risks from Advanced AI
            url: https://arxiv.org/abs/2502.14143
          - title: 2503.06323 - Higher-Order Belief in Incomplete Information MAIDs
            url: https://arxiv.org/abs/2503.06323
          - title: AI Testing Should Account for Sophisticated Strategic Behaviour
            url: https://arxiv.org/abs/2508.14927
          - title: Characterising Simulation-Based Program Equilibria
            url: https://arxiv.org/abs/2412.14570
          - title: "Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory"
            url: https://arxiv.org/abs/2507.02618
          - title: Communication Enables Cooperation in LLM Agents
            url: https://arxiv.org/abs/2510.05748
          - title: Emergent social conventions and collective bias in LLM populations
            url: https://www.science.org/doi/10.1126/sciadv.adu9368
          - title: An Economy of AI Agents
            url: https://arxiv.org/abs/2509.01063
        summary: Use realistic game-theory variants (e.g. evolutionary game theory, computational game theory) to describe/predict the collective and individual behaviours of AI agents in multi-agent scenarios
        theoryOfChange: While traditional AGI safety focuses on idealized decision-theory and individual agents, it’s plausible that strategic AI agents will first emerge (or are emerging now) in a complex, multi-AI strategic landscape. We need granular, realistic formal models of AIs’ strategic interactions and collective dynamics to understand this future
        seeAlso: "Aligning multiple AIs: tools and techniques, Aligning what"
        orthodoxProblems:
          - "4"
          - "7"
          - "8"
        targetCase: Mixed
        someNames:
          - Lewis Hammond
          - Emery Cooper
          - Allan Chan
          - Caspar Oesterheld
          - Vincent Conitzer
        estimatedFTEs: "10"
        fundedBy:
          - google-deepmind
        broadApproaches:
          - cognitive
      - id: aligning-multiple-ais-tools-and-techniques
        name: "Aligning multiple AIs: tools and techniques"
        papers:
          - title: Infrastructure for AI Agents
            url: https://arxiv.org/abs/2501.10114
          - title: "The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation"
            url: https://arxiv.org/abs/2510.01295
          - title: A dataset of questions on decision-theoretic reasoning in Newcomb-like problems
            url: https://arxiv.org/abs/2411.10588
          - title: Reimagining Alignment
            url: https://softmax.com/blog/reimagining-alignment
          - title: "PGG-Bench: Contribute & Punish"
            url: https://github.com/lechmazur/pgg_bench
          - title: "Beyond the high score: Prosocial ability profiles of multi-agent populations"
            url: https://arxiv.org/abs/2509.14485
          - title: Multiplayer Nash Preference Optimization
            url: https://arxiv.org/abs/2509.23102
          - title: 2502.12203 - An Interpretable Automated Mechanism Design Framework with Large Language Models
            url: https://arxiv.org/abs/2502.12203
          - title: "AgentBreeder: Mitigating the AI Safety Risks of Multi-Agent Scaffolds via Self-Improvement"
            url: https://arxiv.org/abs/2502.00757
          - title: "When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems"
            url: https://arxiv.org/abs/2507.14660
          - title: Virtual Agent Economies
            url: http://arxiv.org/abs/2509.10147
        summary: Develop tools and technique for designing and testing multi-agent AI scenarios, for auditing real-world muti-agent AI dynamics, and for aligning AIs in multi-AI settings
        theoryOfChange: Addressing multi-agent AI dynamics is key for aligning near-future agents and their impact on the world. Feedback loops from multi-agent dynamics can radically change the future AI landscape, and require a different toolset from model psychology to audit and control
        seeAlso: "Aligning multiple AIs: game theory , Aligning what"
        orthodoxProblems:
          - "4"
          - "7"
          - "8"
        targetCase: Mixed
        someNames:
          - Lewis Hammond
          - Emery Cooper
          - Allan Chan
          - Caspar Oesterheld
          - Vincent Conitzer
          - Gillian Hadfield
        estimatedFTEs: 10 - 15
        fundedBy:
          - open-philanthropy
          - google-deepmind
        broadApproaches:
          - engineering
          - behavioral
      - id: aligned-to-who
        name: Aligned to who
        papers:
          - title: "2507.09650 - Cultivating Pluralism In Algorithmic Monoculture: The Community Alignment Dataset"
            url: https://arxiv.org/abs/2507.09650
          - title: 2503.05728 - Political Neutrality in AI Is Impossible - But Here Is How to Approximate It
            url: https://arxiv.org/abs/2503.05728
          - title: "The AI Power Disparity Index: Toward a Compound Measure of AI Actors' Power to Shape the AI Ecosystem"
            url: https://ojs.aaai.org/index.php/AIES/article/view/36645
          - title: “Societal and technological progress as sewing an ever-growing, ever-changing, patchy, and polychrome quilt”
            url: https://arxiv.org/abs/2505.05197
          - title: "“Democratic AI is Possible: The Democracy Levels Framework Shows How It Might Work”"
            url: https://arxiv.org/abs/2411.09222
          - title: Research Agenda for Sociotechnical Approaches to AI Safety
            url: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5097286
          - title: Build Agent Advocates, Not Platform Agents
            url: https://arxiv.org/abs/2505.04345
          - title: Training LLM Agents to Empower Humans
            url: https://arxiv.org/abs/2510.13709
        summary: Develop ethical frameworks and technical protocols for taking the plurality of human values, cultures, and communities seriously when aligning AI to “humanity”
        theoryOfChange: Principles of democracy, of pluralism, and of context-sensitivity should guide AI development, alignment, and deployment from the start, continuously shaping AI’s social and technical feedback loop on the road to AGI
        seeAlso: Aligning what, Alignment to context
        orthodoxProblems:
          - "1"
          - "13"
        targetCase: Optimistic
        someNames:
          - Joel Z. Leibo
          - Divya Siddarth
          - Séb Krier
          - Luke Thorburn
          - Seth Lazar
          - AI Objectives Institute
          - The Collective Intelligence Project
        estimatedFTEs: 5 - 15
        fundedBy:
          - future-of-life-institute
          - survival-flourishing-fund
          - google-deepmind
        broadApproaches:
          - behavioral
      - id: aligning-what
        name: Aligning what
        papers:
          - title: Towards a Scale-Free Theory of Intelligent Agency
            url: https://www.alignmentforum.org/posts/5tYTKX4pNpiG4vzYg/towards-a-scale-free-theory-of-intelligent-agency
          - title: Alignment first, intelligence later
            url: https://chrislakin.blog/p/alignment-first-intelligence-later
          - title: "Emmett Shear on Building AI That Actually Cares: Beyond Control and Steering"
            url: https://a16z.simplecast.com/episodes/emmett-shear-on-building-ai-that-actually-cares-beyond-control-and-steering-TRwfxH0r
          - title: End A Subset Of Conversations
            url: https://www.anthropic.com/research/end-subset-conversations
          - title: Full-Stack Alignment
            url: https://www.full-stack-alignment.ai
          - title: On Eudaimonia and Optimization
            url: https://docs.google.com/document/d/1cKbqYSGspfJavXvnhsp3mAuxHh08rNbP7tzYieqLiXw/edit?tab=t.0
          - title: AI Governance through Markets
            url: https://arxiv.org/abs/2501.17755
          - title: Collective cooperative intelligence
            url: https://www.pnas.org/doi/abs/10.1073/pnas.2319948121
        summary: Develop alternatives to AI-agent-level models of alignment, treating human-AI interactions, AI-assisted institutions, AI-inclusive economic or cultural systems, sub-individual AI drives, and other reality-shaping processes as subject to alignment
        theoryOfChange: Modelling multiple reality-shaping processes above and below the level of the individual AI, some of which are themselves quasi-agential (e.g. cultures) or intelligence-like (e.g. markets), will develop AI alignment into a mature science for managing the transition to an AGI civilization
        seeAlso: "Aligning multiple AIs: game theory, Alignment to context, Aligned to who"
        orthodoxProblems:
          - "1"
          - "2"
          - "4"
          - "5"
          - "13"
        targetCase: Mixed
        someNames:
          - Richard Ngo
          - Emmett Shear
          - Softmax
          - Full Stack Alignment
          - AI Objectives Institute
        estimatedFTEs: 5 -10
        fundedBy:
          - future-of-life-institute
          - emmet-shear
        broadApproaches:
          - behavioral
          - cognitive
