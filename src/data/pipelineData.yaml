source_file: data/various_sources/draft-md-20251208-to-parse.md
items:
- id: sec:big_labs
  name: Labs (giant companies)
  header_level: 1
  parent_id: null
  content: ''
  item_type: section
  agenda_attributes: null
  parsing_issues: []
- id: a:openai
  name: OpenAI Safety
  header_level: 2
  parent_id: sec:big_labs
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: null
    theory_of_change: null
    see_also:
    - sec:iterative_alignment
    - a:anthropic_safeguards
    - a:psych_personas
    orthodox_problems: []
    target_case_id: null
    target_case_text: null
    broad_approach_id: null
    broad_approach_text: null
    some_names:
    - Johannes Heidecke
    - Boaz Barak
    - Mia Glaese
    - Jenny Nitishinskaya
    - Lama Ahmad
    - Naomi Bashkansky
    - Miles Wang
    - Wojciech Zaremba
    - David Robinson
    - Zico Kolter
    - Jerry Tworek
    - Eric Wallace
    - Olivia Watkins
    - Kai Chen
    - Chris Koch
    - Andrea Vallone
    - Leo Gao
    estimated_ftes: null
    critiques: '[Stein-Perlman](https://ailabwatch.org/companies/openai), [Stewart](https://intelligence.org/2025/03/31/a-response-to-openais-how-we-think-about-safety-and-alignment/),
      [underelicitation](https://www.lesswrong.com/posts/AK6AihHGjirdoiJg6/ai-companies-eval-reports-mostly-don-t-support-their-claims),
      [Midas](https://www.openaifiles.org/transparency-and-safety), [defense](https://www.wired.com/story/openai-anduril-defense/),
      [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).)
      on labs in general. It''s [difficult](https://conversationswithtyler.com/episodes/sam-altman-2/)
      to model OpenAI as a single agent: *"ALTMAN: I very rarely get to have anybody
      work on anything. One thing about researchers is they''re going to work on what
      they''re going to work on, and that''s that."*'
    funded_by: Microsoft, [AWS](https://www.aboutamazon.com/news/aws/aws-open-ai-workloads-compute-infrastructure),
      Oracle, NVIDIA, SoftBank, G42, AMD, Dragoneer, Coatue, Thrive, Altimeter, MGX,
      Blackstone, TPG, T. Rowe Price, Andreessen Horowitz, D1 Capital Partners, Fidelity
      Investments, Founders Fund, Sequoia…
    outputs:
    - link_url: null
      link_text: Their 60-page System Cards now contain a large amount of their public
        safety work.
      original_md: '* Their 60-page System Cards now contain a large amount of their
        public safety work.'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://alignment.openai.com/
      link_text: https://alignment.openai.com/
      original_md: '* [https://alignment.openai.com/](https://alignment.openai.com/)'
      title: Alignment Research Blog
      authors: []
      author_organizations:
      - OpenAI
      date: '2025-12-01'
      published_year: 2025
      venue: OpenAI Alignment Blog
      kind: blog_post
    - link_url: https://arxiv.org/abs/2503.11926
      link_text: Monitoring Reasoning Models for Misbehavior and the Risks of Promoting
        Obfuscation
      original_md: '* [**Monitoring Reasoning Models for Misbehavior and the Risks
        of Promoting Obfuscation**](https://arxiv.org/abs/2503.11926)'
      title: Monitoring Reasoning Models for Misbehavior and the Risks of Promoting
        Obfuscation
      authors:
      - Bowen Baker
      - Joost Huizinga
      - Leo Gao
      - Zehao Dou
      - Melody Y. Guan
      - Aleksander Madry
      - Wojciech Zaremba
      - Jakub Pachocki
      - David Farhi
      author_organizations:
      - OpenAI
      date: '2025-03-14'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.19823
      link_text: Persona Features Control Emergent Misalignment
      original_md: '* [**Persona Features Control Emergent Misalignment**](https://arxiv.org/abs/2506.19823)'
      title: Persona Features Control Emergent Misalignment
      authors:
      - Miles Wang
      - Tom Dupré la Tour
      - Olivia Watkins
      - Alex Makelov
      - Ryan A. Chi
      - Samuel Miserendino
      - Jeffrey Wang
      - Achyuta Rajaram
      - Johannes Heidecke
      - Tejal Patwardhan
      - Dan Mossing
      author_organizations: []
      date: '2025-06-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.15541
      link_text: Stress Testing Deliberative Alignment for Anti-Scheming Training
      original_md: '* [**Stress Testing Deliberative Alignment for Anti-Scheming Training**](https://arxiv.org/abs/2509.15541)'
      title: Stress Testing Deliberative Alignment for Anti-Scheming Training
      authors:
      - Bronson Schoen
      - Evgenia Nitishinskaya
      - Mikita Balesni
      - Axel Højmark
      - Felix Hofstätter
      - Jérémy Scheurer
      - Alexander Meinke
      - Jason Wolfe
      - Teun van der Weij
      - Alex Lloyd
      - Nicholas Goldowsky-Dill
      - Angela Fan
      - Andrei Matveiakin
      - Rusheb Shah
      - Marcus Williams
      - Amelia Glaese
      - Boaz Barak
      - Wojciech Zaremba
      - Marius Hobbhahn
      author_organizations:
      - OpenAI
      - Anthropic
      date: '2025-09-19'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2412.16339
      link_text: 'Deliberative Alignment: Reasoning Enables Safer Language Models'
      original_md: '* [**Deliberative Alignment: Reasoning Enables Safer Language
        Models**](https://arxiv.org/abs/2412.16339)'
      title: 'Deliberative Alignment: Reasoning Enables Safer Language Models'
      authors:
      - Melody Y. Guan
      - Manas Joglekar
      - Eric Wallace
      - Saachi Jain
      - Boaz Barak
      - Alec Helyar
      - Rachel Dias
      - Andrea Vallone
      - Hongyu Ren
      - Jason Wei
      - Hyung Won Chung
      - Sam Toyer
      - Johannes Heidecke
      - Alex Beutel
      - Amelia Glaese
      author_organizations:
      - OpenAI
      date: '2024-12-20'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://openai.com/index/emergent-misalignment
      link_text: Toward understanding and preventing misalignment generalization
      original_md: '* [**Toward understanding and preventing misalignment generalization**](https://openai.com/index/emergent-misalignment)'
      title: Toward understanding and preventing misalignment generalization
      authors:
      - Miles Wang
      - Tom Dupré la Tour
      - Olivia Watkins
      - Aleksandar Makelov
      - Ryan A. Chi
      - Samuel Miserendino
      - Tejal Patwardhan
      - Dan Mossing
      author_organizations:
      - OpenAI
      date: '2025-06-18'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://openai.com/index/updating-our-preparedness-framework/
      link_text: Our updated Preparedness Framework
      original_md: '* [**Our updated Preparedness Framework**](https://openai.com/index/updating-our-preparedness-framework/)'
      title: Our updated Preparedness Framework
      authors:
      - OpenAI Preparedness Team
      author_organizations:
      - OpenAI
      date: '2025-04-15'
      published_year: 2025
      venue: OpenAI Blog
      kind: agenda_manifesto
    - link_url: https://arxiv.org/abs/2501.18841
      link_text: Trading Inference-Time Compute for Adversarial Robustness
      original_md: '* [**Trading Inference-Time Compute for Adversarial Robustness**](https://arxiv.org/abs/2501.18841)'
      title: Trading Inference-Time Compute for Adversarial Robustness
      authors:
      - Wojciech Zaremba
      - Evgenia Nitishinskaya
      - Boaz Barak
      - Stephanie Lin
      - Sam Toyer
      - Yaodong Yu
      - Rachel Dias
      - Eric Wallace
      - Kai Xiao
      - Johannes Heidecke
      - Amelia Glaese
      author_organizations:
      - OpenAI
      date: '2025-01-31'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://openai.com/index/openai-anthropic-safety-evaluation
      link_text: 'Findings from a pilot Anthropic–OpenAI alignment evaluation exercise:
        OpenAI Safety Tests'
      original_md: '* [**Findings from a pilot Anthropic–OpenAI alignment evaluation
        exercise: OpenAI Safety Tests**](https://openai.com/index/openai-anthropic-safety-evaluation)'
      title: 'Findings from a pilot Anthropic–OpenAI alignment evaluation exercise:
        OpenAI Safety Tests'
      authors: []
      author_organizations:
      - OpenAI
      - Anthropic
      date: '2025-08-27'
      published_year: 2025
      venue: OpenAI Blog
      kind: news_announcement
    - link_url: https://openai.com/safety/evaluations-hub
      link_text: Safety evaluations hub
      original_md: '* [**Safety evaluations hub**](https://openai.com/safety/evaluations-hub)'
      title: Safety evaluations hub
      authors: []
      author_organizations:
      - OpenAI
      date: '2025-08-15'
      published_year: 2025
      venue: OpenAI Website
      kind: news_announcement
    - link_url: https://arxiv.org/abs/2505.16260
      link_text: 'Small-to-Large Generalization: Data Influences Models Consistently
        Across Scale'
      original_md: '* [**Small-to-Large Generalization: Data Influences Models Consistently
        Across Scale**](https://arxiv.org/abs/2505.16260)'
      title: 'Small-to-Large Generalization: Data Influences Models Consistently Across
        Scale'
      authors:
      - Alaa Khaddaj
      - Logan Engstrom
      - Aleksander Madry
      author_organizations:
      - MIT
      date: '2025-05-22'
      published_year: 2025
      venue: ICLR 2025
      kind: paper_published
    - link_url: https://cdn.openai.com/pdf/41df8f28-d4ef-43e9-aed2-823f9393e470/circuit-sparsity-paper.pdf
      link_text: https://cdn.openai.com/pdf/41df8f28-d4ef-43e9-aed2-823f9393e470/circuit-sparsity-paper.pdf
      original_md: '* [**https://cdn.openai.com/pdf/41df8f28-d4ef-43e9-aed2-823f9393e470/circuit-sparsity-paper.pdf**](https://cdn.openai.com/pdf/41df8f28-d4ef-43e9-aed2-823f9393e470/circuit-sparsity-paper.pdf)'
      title: Unknown - PDF content not accessible
      authors: []
      author_organizations:
      - OpenAI
      date: null
      published_year: null
      venue: null
      kind: error_detected
    other_attributes:
      Structure: public benefit corp
      Teams: Alignment, Safety Systems (Interpretability, Safety Oversight, Pretraining
        Safety, Robustness, Safety Research, Trustworthy AI, new Misalignment Research
        team [coming](https://archive.is/eDB1D)), Preparedness, Model Policy, Safety
        and Security Committee, Safety Advisory Group. The [Persona Features](https://www.arxiv.org/pdf/2506.19823)
        paper had a distinct author list. No named successor to Superalignment.
      Public alignment agenda: '[None](https://openai.com/safety/how-we-think-about-safety-alignment/).
        Barak [offers](https://www.lesswrong.com/posts/3jnziqCF3vA2NXAKp/six-thoughts-on-ai-safety)
        personal [views](https://windowsontheory.org/2025/06/24/machines-of-faithful-obedience/).'
      Framework: '[Preparedness Framework](https://cdn.openai.com/pdf/18a02b5d-6b67-4cec-ab64-68cdfbddebcd/preparedness-framework-v2.pdf)'
  parsing_issues: []
- id: a:deepmind
  name: Deepmind Responsibility & Safety
  header_level: 2
  parent_id: sec:big_labs
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: null
    theory_of_change: null
    see_also:
    - sec:interpretability
    - Scalable Oversight
    orthodox_problems: []
    target_case_id: null
    target_case_text: null
    broad_approach_id: null
    broad_approach_text: null
    some_names:
    - Rohin Shah
    - Allan Dafoe
    - Anca Dragan
    - Dave Orr
    - Alex Irpan
    - Alex Turner
    - Anna Wang
    - Arthur Conmy
    - David Lindner
    - Jonah Brown-Cohen
    - Lewis Ho
    - Neel Nanda
    - Raluca Ada Popa
    - Rishub Jain
    - Rory Greig
    - Scott Emmons
    - Sebastian Farquhar
    - Senthooran Rajamanoharan
    - Sophie Bridgers
    - Tobi Ijitoye
    - Tom Everitt
    - Victoria Krakovna
    - Vikrant Varma
    - Vladimir Mikulik
    - Zac Kenton
    - Noah Goodman
    - Four Flynn
    - Jonathan Richens
    - Lewis Smith
    estimated_ftes: null
    critiques: '[Stein-Perlman](https://ailabwatch.org/companies/deepmind), [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).),
      [underelicitation](https://www.lesswrong.com/posts/AK6AihHGjirdoiJg6/ai-companies-eval-reports-mostly-don-t-support-their-claims),
      [On Google''s Safety Plan](https://lesswrong.com/posts/hvEikwtsbf6zaXG2s/on-google-s-safety-plan)'
    funded_by: Google. Explicit Deepmind spending as a whole was [£1.3B](https://s3.eu-west-2.amazonaws.com/document-api-images-live.ch.gov.uk/docs/WT_VNJe9leRjfcU0-OtRjWqF7WiqueStclXgHPbdG4U/application-pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAWRGBDBV3ETBK6MEZ%2F20251107%2Feu-west-2%2Fs3%2Faws4_request&X-Amz-Date=20251107T165323Z&X-Amz-Expires=60&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEPT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCWV1LXdlc3QtMiJHMEUCIBO%2FN0Q9tFWQR%2FLthFYa8oNsFTPSnysg4ONrTUQ%2FyyG1AiEAkPoujFneu%2Bmo73eOnOGNV0tBYKpjLeiP7yATK0BxE7oqigQIvf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgw0NDkyMjkwMzI4MjIiDAUWi2tKimijl1eBRireA4WSlhKuVaQDvGJ2Mhhn6yANCCUO3zN8OlpSoMI2QFoeu6i%2BPxwG7TrcECWjXvY2m5d8vAbJZj%2BTfCgRBvOqlWdcvynzDenryX3a%2F0oh1M1LCvKFgZxSZxV1Pb0l6uDpcBww%2BeRUgnTyClEsdjxo0fACkgiFoJkRbXQr3jnzmghu%2B%2B9kDtWERtaeGA5Dmjfr6i0EibJHo0BwsaleiFdKCuWMnNVpgEO4u%2F85kMy%2ByX7vM1Kfj53OvRKF9cyULlQnwb0q7Rs%2FjRespSsIF%2BAiQbUDck3iT%2BkG0wdrnnLEdm1nIHpoBQO65a8GwLvYsNIS7LckCyuG275y1h1%2B%2BUozd4PyyJjLhqmWn71LcsHuPbjSlAfU9E%2BkE73%2FNuuaE4hRICKlqLl4V9IXPiysd1IfazKQwS3lXmXrFiXvASKvLH8VDuBBz%2Bwj0SG50Rhfwpmrl2lmJ4RFVd6Se9S20eZQjnV183HKkCsvyx09YizNCuJqxEeJmQ%2BAJZVyFjDWKLrpC8F9Yzo3Sf4hYtvQRXqrANoB3n4J%2BUUvrUEDSQqOeQOki1xpE37FBu8Ozs%2FKo%2FrYgESZDsXra0gLcJ7zJb3rJs75kxEyqYDiHuBQ8IKbobY%2BC%2FMHC5MOHqfW%2Bu%2Bl%2FUownLq3yAY6pQG1n3iz%2B09NSvDWENlBA5hC3lLvGiD5f1DTjYA9v1GKjA3feHOZ3Iegs1Egz9G%2FTST6riAYyauyj0At6oEUiUnTQn%2BJxtM%2BH%2FuVzzq45lboS5WE7pDOKA6jDVgzgjy6Mlm%2FY3dy28KxiOrV4EsefEtYTd8Nr6XIvJ0PGVEHhMsWo%2Bsn3IzkuKEqQsFsIiQHCb9oREBNnmY%2FDa2iO0fJijogUtrkxMs%3D&X-Amz-SignedHeaders=host&response-content-disposition=inline%3Bfilename%3D%22companies_house_document.pdf%22&X-Amz-Signature=da8a32ce365dd9ee8f6824636e1e893e1a87a58b64c65013631c1e2014bf7e3e),
      but this doesn't count e.g. Gemini compute.
    outputs:
    - link_url: https://www.alignmentforum.org/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-interpretability%20
      link_text: A Pragmatic Vision for Interpretability
      original_md: '* [**A Pragmatic Vision for Interpretability**](https://www.alignmentforum.org/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-interpretability%20)'
      title: A Pragmatic Vision for Interpretability
      authors:
      - Neel Nanda
      - Josh Engels
      - Arthur Conmy
      - Senthooran Rajamanoharan
      - bilalchughtai
      - CallumMcDougall
      - János Kramár
      - lewis smith
      author_organizations:
      - Google DeepMind
      date: '2025-12-01'
      published_year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - link_url: https://www.alignmentforum.org/posts/MnkeepcGirnJn736j/how-can-interpretability-researchers-help-agi-go-well%20
      link_text: How Can Interpretability Researchers Help AGI Go Well?
      original_md: '* [**How Can Interpretability Researchers Help AGI Go Well?**](https://www.alignmentforum.org/posts/MnkeepcGirnJn736j/how-can-interpretability-researchers-help-agi-go-well%20)'
      title: How Can Interpretability Researchers Help AGI Go Well?
      authors:
      - Neel Nanda
      - Josh Engels
      - Senthooran Rajamanoharan
      - Arthur Conmy
      - bilalchughtai
      - CallumMcDougall
      - János Kramár
      - lewis smith
      author_organizations:
      - Google DeepMind
      date: '2024-12-01'
      published_year: 2024
      venue: AI Alignment Forum
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2505.01420
      link_text: Evaluating Frontier Models for Stealth and Situational Awareness
      original_md: '* [**Evaluating Frontier Models for Stealth and Situational Awareness**](https://arxiv.org/abs/2505.01420)'
      title: Evaluating Frontier Models for Stealth and Situational Awareness
      authors:
      - Mary Phuong
      - Roland S. Zimmermann
      - Ziyue Wang
      - David Lindner
      - Victoria Krakovna
      - Sarah Cogan
      - Allan Dafoe
      - Lewis Ho
      - Rohin Shah
      author_organizations:
      - Google DeepMind
      date: '2025-05-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.05246
      link_text: When Chain of Thought is Necessary, Language Models Struggle to Evade
        Monitors
      original_md: '* [**When Chain of Thought is Necessary, Language Models Struggle
        to Evade Monitors**](https://arxiv.org/abs/2507.05246)'
      title: When Chain of Thought is Necessary, Language Models Struggle to Evade
        Monitors
      authors:
      - Scott Emmons
      - Erik Jenner
      - David K. Elson
      - Rif A. Saurous
      - Senthooran Rajamanoharan
      - Heng Chen
      - Irhum Shafkat
      - Rohin Shah
      author_organizations:
      - Google DeepMind
      date: '2025-07-07'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://alignmentforum.org/posts/zWySWKuXnhMDhgwc3/mona-managed-myopia-with-approval-feedback-2
      link_text: 'MONA: Managed Myopia with Approval Feedback'
      original_md: '* [**MONA: Managed Myopia with Approval Feedback**](https://alignmentforum.org/posts/zWySWKuXnhMDhgwc3/mona-managed-myopia-with-approval-feedback-2)'
      title: 'MONA: Managed Myopia with Approval Feedback'
      authors:
      - Sebastian Farquhar
      - David Lindner
      - Rohin Shah
      author_organizations:
      - DeepMind
      date: '2025-01-23'
      published_year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2510.27062
      link_text: Consistency Training Helps Stop Sycophancy and Jailbreaks
      original_md: '* [**Consistency Training Helps Stop Sycophancy and Jailbreaks**](https://arxiv.org/abs/2510.27062)'
      title: Consistency Training Helps Stop Sycophancy and Jailbreaks
      authors:
      - Alex Irpan
      - Alexander Matt Turner
      - Mark Kurzeja
      - David K. Elson
      - Rohin Shah
      author_organizations:
      - DeepMind
      - Google
      date: '2025-10-31'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.01849
      link_text: An Approach to Technical AGI Safety and Security
      original_md: '* [**An Approach to Technical AGI Safety and Security**](https://arxiv.org/abs/2504.01849)'
      title: An Approach to Technical AGI Safety and Security
      authors:
      - Rohin Shah
      - Alex Irpan
      - Alexander Matt Turner
      - Anna Wang
      - Arthur Conmy
      - David Lindner
      - Jonah Brown-Cohen
      - Lewis Ho
      - Neel Nanda
      - Raluca Ada Popa
      - Rishub Jain
      - Rory Greig
      - Samuel Albanie
      - Scott Emmons
      - Sebastian Farquhar
      - Sébastien Krier
      - Senthooran Rajamanoharan
      - Sophie Bridgers
      - Tobi Ijitoye
      - Tom Everitt
      - Victoria Krakovna
      - Vikrant Varma
      - Vladimir Mikulik
      - Zachary Kenton
      - Dave Orr
      - Shane Legg
      - Noah Goodman
      - Allan Dafoe
      - Four Flynn
      - Anca Dragan
      author_organizations:
      - Google DeepMind
      - Anthropic
      - Stanford University
      - UC Berkeley
      - Redwood Research
      date: '2025-04-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2412.19792
      link_text: 'InfAlign: Inference-aware language model alignment'
      original_md: '* [**InfAlign: Inference-aware language model alignment**](https://arxiv.org/abs/2412.19792)'
      title: 'InfAlign: Inference-aware language model alignment'
      authors:
      - Ananth Balashankar
      - Ziteng Sun
      - Jonathan Berant
      - Jacob Eisenstein
      - Michael Collins
      - Adrian Hutter
      - Jong Lee
      - Chirag Nagpal
      - Flavien Prost
      - Aradhana Sinha
      - Ananda Theertha Suresh
      - Ahmad Beirami
      author_organizations:
      - Google
      date: '2024-12-27'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://alignmentforum.org/posts/4uXCAJNuPKtKBsi28/negative-results-for-saes-on-downstream-tasks
      link_text: 'Negative Results for SAEs On Downstream Tasks and Deprioritising
        SAE Research (GDM Mech Interp Team Progress Update #2)'
      original_md: '* [**Negative Results for SAEs On Downstream Tasks and Deprioritising
        SAE Research (GDM Mech Interp Team Progress Update \#2)**](https://alignmentforum.org/posts/4uXCAJNuPKtKBsi28/negative-results-for-saes-on-downstream-tasks)'
      title: 'Negative Results for SAEs On Downstream Tasks and Deprioritising SAE
        Research (GDM Mech Interp Team Progress Update #2)'
      authors:
      - Lewis Smith
      - Senthooran Rajamanoharan
      - Arthur Conmy
      - Callum McDougall
      - Tom Lieberum
      - János Kramár
      - Rohin Shah
      - Neel Nanda
      author_organizations:
      - Google DeepMind
      date: '2025-03-26'
      published_year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - link_url: https://deepmind.google/discover/blog/taking-a-responsible-path-to-agi/
      link_text: Taking a responsible path to AGI
      original_md: '* [**Taking a responsible path to AGI**](https://deepmind.google/discover/blog/taking-a-responsible-path-to-agi/)'
      title: Taking a responsible path to AGI
      authors:
      - Anca Dragan
      - Rohin Shah
      - Four Flynn
      - Shane Legg
      author_organizations:
      - Google DeepMind
      date: '2025-04-02'
      published_year: 2025
      venue: Google DeepMind Blog
      kind: blog_post
    - link_url: https://deepmind.google/discover/blog/evaluating-potential-cybersecurity-threats-of-advanced-ai
      link_text: Evaluating potential cybersecurity threats of advanced AI
      original_md: '* [**Evaluating potential cybersecurity threats of advanced AI**](https://deepmind.google/discover/blog/evaluating-potential-cybersecurity-threats-of-advanced-ai)'
      title: Evaluating potential cybersecurity threats of advanced AI
      authors:
      - Four Flynn
      - Mikel Rodriguez
      - Raluca Ada Popa
      author_organizations:
      - Google DeepMind
      date: '2025-04-02'
      published_year: 2025
      venue: Google DeepMind Blog
      kind: blog_post
    - link_url: null
      link_text: Some work from non-safety teams, like InfAlign
      original_md: '* Some work from non-safety teams, like [InfAlign](https://arxiv.org/abs/2412.19792)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://arxiv.org/pdf/2001.07118
      link_text: Incentives for Responsiveness, Instrumental Control and Impact
      original_md: '* [Incentives for Responsiveness, Instrumental Control and Impact](https://arxiv.org/pdf/2001.07118)'
      title: Incentives for Responsiveness, Instrumental Control and Impact
      authors:
      - Ryan Carey
      - Eric Langlois
      - Chris van Merwijk
      - Shane Legg
      - Tom Everitt
      author_organizations:
      - DeepMind
      date: '2020-01-20'
      published_year: 2020
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/html/2511.22662v1
      link_text: Difficulties with Evaluating a Deception Detector for AIs
      original_md: '* [Difficulties with Evaluating a Deception Detector for AIs](https://arxiv.org/html/2511.22662v1)'
      title: Difficulties with Evaluating a Deception Detector for AIs
      authors:
      - Lewis Smith
      - Bilal Chughtai
      - Neel Nanda
      author_organizations:
      - Google
      date: '2025-11-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes:
      Structure: research laboratory subsidiary of a for-profit
      Teams: '[ASAT](https://www.alignmentforum.org/posts/wqz5CRzqWkvzoatBG/agi-safety-and-alignment-google-deepmind-is-hiring)
        ("AGI Alignment", which consists of amplified oversight and interpretability
        plus "Frontier Safety", which consists of framework development and implementation).
        Also Gemini Safety, [Voices of All in Alignment](https://www.edinburgh-robotics.org/events/whose-gold-aligning-ai-diverse-views-what%E2%80%99s-safe-aligned-and-beneficial),
        AGI Safety Council, Responsibility and Safety Council. Sort-of the [Causal
        Incentives Working Group](https://causalincentives.com/) too.'
      Public alignment agenda: '[An Approach to Technical AGI Safety](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/evaluating-potential-cybersecurity-threats-of-advanced-ai/An_Approach_to_Technical_AGI_Safety_Apr_2025.pdf%20)'
      Framework: '[Frontier Safety Framework](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/updating-the-frontier-safety-framework/Frontier%20Safety%20Framework%202.0.pdf)'
  parsing_issues: []
- id: a:anthropic
  name: Anthropic Safety
  header_level: 2
  parent_id: sec:big_labs
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: null
    theory_of_change: null
    see_also:
    - sec:interpretability
    - Scalable Oversight
    orthodox_problems: []
    target_case_id: null
    target_case_text: null
    broad_approach_id: null
    broad_approach_text: null
    some_names:
    - Chris Olah
    - Evan Hubinger
    - Sam Marks
    - Johannes Treutlein
    - Sam Bowman
    - Euan Ong
    - Fabien Roger
    - Adam Jermyn
    - Holden Karnofsky
    - Jan Leike
    - Ethan Perez
    - Jack Lindsey
    - Amanda Askell
    - Kyle Fish
    - Sara Price
    - Jon Kutasov
    - Minae Kwon
    - Monty Evans
    - Richard Dargan
    estimated_ftes: null
    critiques: '[Stein](https://ailabwatch.org/anthropic-opinions)-[Perlman](https://ailabwatch.org/companies/anthropic),
      [Casper](https://www.lesswrong.com/posts/pH6tyhEnngqWAXi9i/eis-xiii-reflections-on-anthropic-s-sae-research-circa-may#A_review___thoughts),
      [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%2Dpeople%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).),
      [underelicitation](https://www.lesswrong.com/posts/AK6AihHGjirdoiJg6/ai-companies-eval-reports-mostly-don-t-support-their-claims),
      [Greenblatt](https://nitter.net/RyanPGreenblatt/status/1925992236648464774),
      [Samin](https://www.lesswrong.com/posts/5aKRshJzhojqfbRyo/unless-its-governance-changes-anthropic-is-untrustworthy),
      [defense](https://techcrunch.com/2024/11/07/anthropic-teams-up-with-palantir-and-aws-to-sell-its-ai-to-defense-customers/),
      [Existing Safety Frameworks Imply Unreasonable Confidence](https://lesswrong.com/posts/7ExkgcDudwhag73vw/existing-safety-frameworks-imply-unreasonable-confidence)'
    funded_by: Amazon, Google, ICONIQ, Fidelity, Lightspeed, Altimeter, Baillie Gifford,
      BlackRock, Blackstone, Coatue, D1 Capital Partners, General Atlantic, General
      Catalyst, GIC, Goldman Sachs, Insight Partners, Jane Street, Ontario Teachers'
      Pension Plan, Qatar Investment Authority, TPG, T. Rowe Price, WCM, XN
    outputs:
    - link_url: https://assets.anthropic.com/m/74342f2c96095771/original/Natural-emergent-misalignment-from-reward-hacking-paper.pdf
      link_text: Natural emergent misalignment from reward hacking
      original_md: '* [Natural emergent misalignment from reward hacking](https://assets.anthropic.com/m/74342f2c96095771/original/Natural-emergent-misalignment-from-reward-hacking-paper.pdf)'
      title: Natural emergent misalignment from reward hacking
      authors: []
      author_organizations:
      - Anthropic
      date: null
      published_year: null
      venue: null
      kind: error_detected
    - link_url: https://anthropic.com/research/agentic-misalignment
      link_text: 'Agentic Misalignment: How LLMs could be insider threats'
      original_md: '* [**Agentic Misalignment: How LLMs could be insider threats**](https://anthropic.com/research/agentic-misalignment)'
      title: 'Agentic Misalignment: How LLMs could be insider threats'
      authors:
      - Aengus Lynch
      - Benjamin Wright
      - Caleb Larson
      - Kevin K. Troy
      - Stuart J. Ritchie
      - Sören Mindermann
      - Ethan Perez
      - Evan Hubinger
      author_organizations:
      - Anthropic
      - University College London
      - MATS
      - Mila
      date: '2025-06-20'
      published_year: 2025
      venue: Anthropic Research
      kind: blog_post
    - link_url: https://anthropic.com/research/shade-arena-sabotage-monitoring
      link_text: 'SHADE-Arena: Evaluating sabotage and monitoring in LLM agents'
      original_md: '* [**SHADE-Arena: Evaluating sabotage and monitoring in LLM agents**](https://anthropic.com/research/shade-arena-sabotage-monitoring)'
      title: 'SHADE-Arena: Evaluating sabotage and monitoring in LLM agents'
      authors:
      - Xiang Deng
      - Chen Bo Calvin Zhang
      - Tyler Tracy
      - Buck Shlegeris
      - Yuqi Sun
      - Paul Colognese
      - Teun van der Weij
      - Linda Petrini
      - Henry Sleight
      author_organizations:
      - Anthropic
      - Scale AI
      - Redwood Research
      date: '2025-06-16'
      published_year: 2025
      venue: Anthropic Research Blog
      kind: blog_post
    - link_url: https://arxiv.org/abs/2502.16797
      link_text: Forecasting Rare Language Model Behaviors
      original_md: '* [**Forecasting Rare Language Model Behaviors**](https://arxiv.org/abs/2502.16797)'
      title: Forecasting Rare Language Model Behaviors
      authors:
      - Erik Jones
      - Meg Tong
      - Jesse Mu
      - Mohammed Mahfoud
      - Jan Leike
      - Roger Grosse
      - Jared Kaplan
      - William Fithian
      - Ethan Perez
      - Mrinank Sharma
      author_organizations:
      - Anthropic
      - OpenAI
      - UC Berkeley
      date: '2025-02-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://alignmentforum.org/posts/ghESoA8mo3fv9Yx3E/why-do-some-language-models-fake-alignment-while-others-don
      link_text: Why Do Some Language Models Fake Alignment While Others Don't?
      original_md: '* [**Why Do Some Language Models Fake Alignment While Others Don''t?**](https://alignmentforum.org/posts/ghESoA8mo3fv9Yx3E/why-do-some-language-models-fake-alignment-while-others-don),'
      title: Why Do Some Language Models Fake Alignment While Others Don't?
      authors:
      - abhayesian
      - John Hughes
      - Alex Mallen
      - Jozdien
      - janus
      - Fabien Roger
      author_organizations:
      - Anthropic
      - Redwood Research
      date: '2025-07-08'
      published_year: 2025
      venue: arXiv
      kind: lesswrong
    - link_url: https://alignment.anthropic.com/2025/petri
      link_text: 'Petri: An open-source auditing tool to accelerate AI safety research'
      original_md: '* [**Petri: An open-source auditing tool to accelerate AI safety
        research**](https://alignment.anthropic.com/2025/petri), [**Signs of introspection
        in large language models**](https://anthropic.com/research/introspection)'
      title: 'Petri: An open-source auditing tool to accelerate AI safety research'
      authors: []
      author_organizations:
      - Anthropic
      date: '2025-10-06'
      published_year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - link_url: https://anthropic.com/research/introspection
      link_text: Signs of introspection in large language models
      original_md: '* [**Petri: An open-source auditing tool to accelerate AI safety
        research**](https://alignment.anthropic.com/2025/petri), [**Signs of introspection
        in large language models**](https://anthropic.com/research/introspection)'
      title: Signs of introspection in large language models
      authors: []
      author_organizations:
      - Anthropic
      date: '2025-10-29'
      published_year: 2025
      venue: Anthropic Research
      kind: blog_post
    - link_url: https://alignment.anthropic.com/2025/recommended-directions/index.html
      link_text: Recommendations for Technical AI Safety Research Directions
      original_md: '* [**Recommendations for Technical AI Safety Research Directions**](https://alignment.anthropic.com/2025/recommended-directions/index.html)'
      title: Recommendations for Technical AI Safety Research Directions
      authors:
      - Anthropic Alignment Science Team
      author_organizations:
      - Anthropic
      date: null
      published_year: 2025
      venue: Alignment Science Blog
      kind: agenda_manifesto
    - link_url: https://alignment.anthropic.com/2025/bumpers/
      link_text: Putting up Bumpers
      original_md: '* [**Putting up Bumpers**](https://alignment.anthropic.com/2025/bumpers/)'
      title: Putting up Bumpers
      authors: []
      author_organizations:
      - Anthropic
      date: null
      published_year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - link_url: https://alignment.anthropic.com/2024/safety-cases/index.html
      link_text: Three Sketches of ASL-4 Safety Case Components
      original_md: '* [**Three Sketches of ASL-4 Safety Case Components**](https://alignment.anthropic.com/2024/safety-cases/index.html)'
      title: Three Sketches of ASL-4 Safety Case Components
      authors: []
      author_organizations:
      - Anthropic
      date: '2024-01-01'
      published_year: 2024
      venue: Alignment Science Blog
      kind: blog_post
    - link_url: https://anthropic.com/research/open-source-circuit-tracing
      link_text: Open-sourcing circuit tracing tools
      original_md: '* [**Open-sourcing circuit tracing tools**](https://anthropic.com/research/open-source-circuit-tracing)'
      title: Open-sourcing circuit tracing tools
      authors:
      - Michael Hanna
      - Mateusz Piotrowski
      - Emmanuel Ameisen
      - Jack Lindsey
      - Johnny Lin
      - Curt Tigges
      author_organizations:
      - Anthropic
      - Decode Research
      date: '2025-05-29'
      published_year: 2025
      venue: Anthropic Blog
      kind: code_tool
    - link_url: https://arxiv.org/abs/2510.07192
      link_text: Poisoning Attacks on LLMs Require a Near-constant Number of Poison
        Samples
      original_md: '* [Poisoning Attacks on LLMs Require a Near-constant Number of
        Poison Samples](https://arxiv.org/abs/2510.07192)'
      title: Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples
      authors:
      - Alexandra Souly
      - Javier Rando
      - Ed Chapman
      - Xander Davies
      - Burak Hasircioglu
      - Ezzeldin Shereen
      - Carlos Mougan
      - Vasilios Mavroudis
      - Erik Jones
      - Chris Hicks
      - Nicholas Carlini
      - Yarin Gal
      - Robert Kirk
      author_organizations: []
      date: '2025-10-08'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://transformer-circuits.pub/2025/introspection/index.html
      link_text: Emergent Introspective Awareness in Large Language Models
      original_md: '* [Emergent Introspective Awareness in Large Language Models](https://transformer-circuits.pub/2025/introspection/index.html)'
      title: Emergent Introspective Awareness in Large Language Models
      authors:
      - Jack Lindsey
      author_organizations:
      - Anthropic
      date: '2025-10-29'
      published_year: 2025
      venue: Transformer Circuits Thread
      kind: blog_post
    - link_url: https://alignment.anthropic.com/2025/openai-findings
      link_text: Findings from a Pilot Anthropic—OpenAI Alignment Evaluation Exercise
      original_md: '* [**Findings from a Pilot Anthropic—OpenAI Alignment Evaluation
        Exercise**](https://alignment.anthropic.com/2025/openai-findings),'
      title: Findings from a Pilot Anthropic—OpenAI Alignment Evaluation Exercise
      authors:
      - Samuel R. Bowman
      - Megha Srivastava
      - Jon Kutasov
      - Rowan Wang
      - Trenton Bricken
      - Benjamin Wright
      - Ethan Perez
      - Nicholas Carlini
      author_organizations:
      - Anthropic
      - OpenAI
      date: '2025-08-27'
      published_year: 2025
      venue: Alignment Science Blog
      kind: blog_post
    - link_url: https://www.anthropic.com/research/reasoning-models-dont-say-think
      link_text: Reasoning models don't always say what they think
      original_md: '* [Reasoning models don''t always say what they think](https://www.anthropic.com/research/reasoning-models-dont-say-think)'
      title: Reasoning models don't always say what they think
      authors: []
      author_organizations:
      - Anthropic
      date: '2025-04-03'
      published_year: 2025
      venue: Anthropic Research Blog
      kind: blog_post
    - link_url: https://transformer-circuits.pub/2025/attribution-graphs/biology.html
      link_text: On the Biology of a Large Language Model
      original_md: '* [On the Biology of a Large Language Model](https://transformer-circuits.pub/2025/attribution-graphs/biology.html)'
      title: On the Biology of a Large Language Model
      authors:
      - Jack Lindsey
      - Wes Gurnee
      - Emmanuel Ameisen
      - Brian Chen
      - Adam Pearce
      - Nicholas L. Turner
      - Craig Citro
      - David Abrahams
      - Shan Carter
      - Basil Hosmer
      - Jonathan Marcus
      - Michael Sklar
      - Adly Templeton
      - Trenton Bricken
      - Callum McDougall
      - Hoagy Cunningham
      - Thomas Henighan
      - Adam Jermyn
      - Andy Jones
      - Andrew Persic
      - Zhenyi Qi
      - T. Ben Thompson
      - Sam Zimmerman
      - Kelley Rivoire
      - Thomas Conerly
      - Chris Olah
      - Joshua Batson
      author_organizations:
      - Anthropic
      date: '2025-03-27'
      published_year: 2025
      venue: Transformer Circuits Thread
      kind: paper_published
    - link_url: https://transformer-circuits.pub/2025/attribution-graphs/methods.html
      link_text: 'Circuit Tracing: Revealing Computational Graphs in Language Models'
      original_md: '* [Circuit Tracing: Revealing Computational Graphs in Language
        Models](https://transformer-circuits.pub/2025/attribution-graphs/methods.html)'
      title: 'Circuit Tracing: Revealing Computational Graphs in Language Models'
      authors:
      - Emmanuel Ameisen
      - Jack Lindsey
      - Adam Pearce
      - Wes Gurnee
      - Nicholas L. Turner
      - Brian Chen
      - Craig Citro
      - David Abrahams
      - Shan Carter
      - Basil Hosmer
      - Jonathan Marcus
      - Michael Sklar
      - Adly Templeton
      - Trenton Bricken
      - Callum McDougall
      - Hoagy Cunningham
      - Thomas Henighan
      - Adam Jermyn
      - Andy Jones
      - Andrew Persic
      - Zhenyi Qi
      - T. Ben Thompson
      - Sam Zimmerman
      - Kelley Rivoire
      - Thomas Conerly
      - Chris Olah
      - Joshua Batson
      author_organizations:
      - Anthropic
      date: '2025-03-27'
      published_year: 2025
      venue: Transformer Circuits Thread
      kind: blog_post
    - link_url: https://www.anthropic.com/research/auditing-hidden-objectives
      link_text: Auditing language models for hidden objectives
      original_md: '* [Auditing language models for hidden objectives](https://www.anthropic.com/research/auditing-hidden-objectives)'
      title: Auditing language models for hidden objectives
      authors: []
      author_organizations:
      - Anthropic
      date: '2025-03-13'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.anthropic.com/research/constitutional-classifiers
      link_text: 'Constitutional Classifiers: Defending against universal jailbreaks'
      original_md: '* [Constitutional Classifiers: Defending against universal jailbreaks](https://www.anthropic.com/research/constitutional-classifiers)'
      title: 'Constitutional Classifiers: Defending against universal jailbreaks'
      authors:
      - Anthropic Safeguards Research Team
      author_organizations:
      - Anthropic
      date: '2025-02-03'
      published_year: 2025
      venue: Anthropic Blog
      kind: blog_post
    - link_url: https://gist.github.com/Richard-Weiss/efe157692991535403bd7e7fb20b6695
      link_text: The Soul Document
      original_md: '* [The Soul Document](https://gist.github.com/Richard-Weiss/efe157692991535403bd7e7fb20b6695)
        ([confirmed](https://x.com/AmandaAskell/status/1995610567923695633))'
      title: Claude 4.5 Opus Soul Document
      authors:
      - Richard-Weiss
      author_organizations: []
      date: '2024-11-27'
      published_year: 2024
      venue: GitHub Gist
      kind: other
    - link_url: https://alignment.anthropic.com/2025/honesty-elicitation/
      link_text: Evaluating honesty and lie detection techniques on a diverse suite
        of dishonest models
      original_md: '* [Evaluating honesty and lie detection techniques on a diverse
        suite of dishonest models](https://alignment.anthropic.com/2025/honesty-elicitation/)'
      title: Evaluating honesty and lie detection techniques on a diverse suite of
        dishonest models
      authors:
      - Rowan Wang
      - Johannes Treutlein
      - Fabien Roger
      - Evan Hubinger
      - Sam Marks
      author_organizations:
      - Anthropic
      date: '2025-11-25'
      published_year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    other_attributes:
      Structure: public-benefit corp
      Teams: Scalable Alignment (Leike), Alignment Evals (Bowman), [Interpretability](https://transformer-circuits.pub/)
        (Olah), Control (Perez), Model Psychiatry (Lindsey), Character (Askell), Alignment
        Stress-Testing (Hubinger), Alignment Mitigations (Price?), Frontier Red Team
        (Graham), Safeguards (?), Societal Impacts (Ganguli), Trust and Safety (Sanderford),
        Model Welfare (Fish).
      Public alignment agenda: '[directions](https://alignment.anthropic.com/2025/recommended-directions/),
        [bumpers](https://alignment.anthropic.com/2025/bumpers/), [checklist](https://sleepinyourhat.github.io/checklist/),
        an [old vague view](https://www.anthropic.com/news/core-views-on-ai-safety)'
      Framework: '[RSP](https://www-cdn.anthropic.com/872c653b2d0501d6ab44cf87f43e1dc4853e4d37.pdf)'
  parsing_issues: []
- id: a:xai
  name: xAI
  header_level: 2
  parent_id: sec:big_labs
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: null
    theory_of_change: null
    see_also: []
    orthodox_problems: []
    target_case_id: null
    target_case_text: null
    broad_approach_id: null
    broad_approach_text: null
    some_names:
    - Dan Hendrycks (advisor)
    - Juntang Zhuang
    - Toby Pohlen
    - Lianmin Zheng
    - Piaoyang Cui
    - Nikita Popov
    - Ying Sheng
    - Sehoon Kim
    estimated_ftes: null
    critiques: '[framework](https://www.lesswrong.com/posts/hQyrTDuTXpqkxrnoH/xai-s-new-safety-framework-is-dreadful),
      [hacking](https://x.com/g_leech_/status/1990543987846078854), [broken promises](https://x.com/g_leech_/status/1990734517145911593),
      [Stein](https://ailabwatch.org/companies/xai)-[Perlman](https://ailabwatch.org/resources/integrity#xai),
      [insecurity](https://nitter.net/elonmusk/status/1961904269545648624), [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).)
      on labs in general'
    funded_by: A16Z, Blackrock, Fidelity, Kingdom, Lightspeed, MGX, Morgan Stanley,
      Sequoia…
    outputs: []
    other_attributes:
      Structure: '[for-profit](https://www.cnbc.com/amp/2025/08/25/elon-musk-xai-dropped-public-benefit-corp-status-while-fighting-openai.html)'
      Teams: '[Applied Safety](https://job-boards.greenhouse.io/xai/jobs/4944324007)
        and Model Evaluation. Nominally focussed on misuse.'
      Framework: '[Risk Management Framework](https://data.x.ai/2025-08-20-xai-risk-management-framework.pdf)'
  parsing_issues: []
- id: a:meta
  name: Meta
  header_level: 2
  parent_id: sec:big_labs
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: null
    theory_of_change: null
    see_also:
    - a:unlearning
    orthodox_problems: []
    target_case_id: null
    target_case_text: null
    broad_approach_id: null
    broad_approach_text: null
    some_names:
    - Shuchao Bi
    - Hongyuan Zhan
    - Jingyu Zhang
    - Haozhu Wang
    - Eric Michael Smith
    - Sid Wang
    - Amr Sharaf
    - Mahesh Pasupuleti
    - Jason Weston
    - ShengYun Peng
    - Ivan Evtimov
    - Song Jiang
    - Pin-Yu Chen
    - Evangelia Spiliopoulou
    - Lei Yu
    - Virginie Do
    - Karen Hambardzumyan
    - Nicola Cancedda
    - Adina Williams
    estimated_ftes: null
    critiques: '[extreme underelicitation](https://googleprojectzero.blogspot.com/2024/06/project-naptime.html#:~:text=We%20find%20that%2C%20by%20refining%20the%20testing%20methodology%20to%20take%20advantage%20of%20modern%20LLM%20capabilities%2C%20significantly%20better%20performance%20in%20vulnerability%20discovery%20can%20be%20achieved.%20To%20facilitate%20effective%20evaluation%20of%20LLMs%20for%20vulnerability%20discovery%2C%20we%20propose%20below%20a%20set%20of%20guiding%20principles.),
      [Stein](https://ailabwatch.org/companies/meta)-[Perlman](https://ailabwatch.org/companies/meta),
      [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).)
      on labs in general'
    funded_by: Meta
    outputs:
    - link_url: https://arxiv.org/pdf/2510.08240
      link_text: 'The Alignment Waltz: Jointly Training Agents to Collaborate for
        Safety'
      original_md: '* [**The Alignment Waltz: Jointly Training Agents to Collaborate
        for Safety**](https://arxiv.org/pdf/2510.08240)'
      title: 'The Alignment Waltz: Jointly Training Agents to Collaborate for Safety'
      authors:
      - Jingyu Zhang
      - Haozhu Wang
      - Eric Michael Smith
      - Sid Wang
      - Amr Sharaf
      - Mahesh Pasupuleti
      - Benjamin Van Durme
      - Daniel Khashabi
      - Jason Weston
      - Hongyuan Zhan
      author_organizations:
      - Meta
      - Johns Hopkins University
      date: '2025-10-09'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://github.com/facebookresearch/RAM/blob/main/projects/co-improvement.pdf
      link_text: AI & Human Co-Improvement
      original_md: '* [AI & Human Co-Improvement](https://github.com/facebookresearch/RAM/blob/main/projects/co-improvement.pdf)'
      title: Co-improvement (PDF - content not accessible)
      authors: []
      author_organizations:
      - Meta
      - Facebook Research
      date: '2025-12-05'
      published_year: 2025
      venue: GitHub Repository
      kind: error_detected
    - link_url: https://arxiv.org/pdf/2510.00938%20
      link_text: Large Reasoning Models Learn Better Alignment from Flawed Thinking
      original_md: '* [Large Reasoning Models Learn Better Alignment from Flawed Thinking](https://arxiv.org/pdf/2510.00938%20)'
      title: Large Reasoning Models Learn Better Alignment from Flawed Thinking
      authors:
      - ShengYun Peng
      - Eric Smith
      - Ivan Evtimov
      - Song Jiang
      - Pin-Yu Chen
      - Hongyuan Zhan
      - Haozhu Wang
      - Duen Horng Chau
      - Mahesh Pasupuleti
      - Jianfeng Chi
      author_organizations: []
      date: '2025-10-01'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/pdf/2409.20089
      link_text: Robust LLM safeguarding via refusal feature adversarial training
      original_md: '* [Robust LLM safeguarding via refusal feature adversarial training](https://arxiv.org/pdf/2409.20089)'
      title: Robust LLM safeguarding via refusal feature adversarial training
      authors:
      - Lei Yu
      - Virginie Do
      - Karen Hambardzumyan
      - Nicola Cancedda
      author_organizations: []
      date: '2024-09-30'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://scontent-lhr8-1.xx.fbcdn.net/v/t39.2365-6/557601942_1468972530985309_838842257265552803_n.pdf?_nc_cat=108&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=_H33_VKF3ZUQ7kNvwFog8dd&_nc_oc=AdlNtqCDY4HafZ3-d5rb26AF5f2m0X46SGdKhVq3jLqwpNf_wEXhdQnH7_30ychiZWk&_nc_zt=14&_nc_ht=scontent-lhr8-1.xx&_nc_gid=QvW_ePiaF4E-PxOf30MWyg&oh=00_AfiZC5G4ODvWhiy0MuVH8PSlUFrW8RDQQ8tdr6Zec5k9aA&oe=691A6D09
      link_text: Code World Model Preparedness Report
      original_md: '* [Code World Model Preparedness Report](https://scontent-lhr8-1.xx.fbcdn.net/v/t39.2365-6/557601942_1468972530985309_838842257265552803_n.pdf?_nc_cat=108&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=_H33_VKF3ZUQ7kNvwFog8dd&_nc_oc=AdlNtqCDY4HafZ3-d5rb26AF5f2m0X46SGdKhVq3jLqwpNf_wEXhdQnH7_30ychiZWk&_nc_zt=14&_nc_ht=scontent-lhr8-1.xx&_nc_gid=QvW_ePiaF4E-PxOf30MWyg&oh=00_AfiZC5G4ODvWhiy0MuVH8PSlUFrW8RDQQ8tdr6Zec5k9aA&oe=691A6D09)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://ai.meta.com/blog/practical-ai-agent-security/%20
      link_text: 'Agents Rule of Two: A Practical Approach to AI Agent Security'
      original_md: '* [Agents Rule of Two: A Practical Approach to AI Agent Security](https://ai.meta.com/blog/practical-ai-agent-security/%20)'
      title: 'Connect 2024: The responsible approach we''re taking to generative AI'
      authors: []
      author_organizations:
      - Meta
      date: '2024-09-25'
      published_year: 2024
      venue: Meta AI Blog
      kind: blog_post
    other_attributes:
      Structure: for-profit
      Teams: Safety "integrated into" capabilities research, Meta Superintelligence
        Lab. But also FAIR Alignment, [Brain and AI](https://www.metacareers.com/jobs/1319148726628205).
      Public alignment agenda: None
      Framework: '[FAF](https://ai.meta.com/static-resource/meta-frontier-ai-framework/?utm_source=newsroom&utm_medium=web&utm_content=Frontier_AI_Framework_PDF&utm_campaign=Our_Approach_to_Frontier_AI_blog)'
  parsing_issues: []
- id: sec:labs_others
  name: Others
  header_level: 2
  parent_id: sec:big_labs
  content: '* Amazon’s [Nova Pro](https://arxiv.org/pdf/2506.12103v1) is around the
    level of Llama 3 90B, which in turn is around the level of the original GPT-4.
    So 2 years behind. But they have their own [chip](https://www.businessinsider.com/startups-amazon-ai-chips-less-competitive-nvidia-gpus-trainium-aws-2025-11).

    * Microsoft are [now](https://www.dwarkesh.com/p/satya-nadella-2) mid-training
    on top of GPT-5. MAI-1-preview is [around](https://lmarena.ai/leaderboard/text)
    DeepSeek V3.0 on Arena. They [continue](https://arxiv.org/abs/2506.22405v1) to
    focus on medical diagnosis. You can [request](https://forms.microsoft.com/pages/responsepage.aspx?id=v4j5cvGGr0GRqy180BHbRyRliS0ly-JEvgSpwo3yWyhUQkdTQktBUkFaWERHR1JFRjgwMlZUUkQxTC4u&route=shorturl)
    access.

    * Mistral have a reasoning model, [Magistral Medium](https://arxiv.org/pdf/2506.10910),
    and released the weights of a little 24B version. It’s a bit worse than Deepseek
    R1, pass@1.


    The Chinese companies [don’t](https://futureoflife.org/wp-content/uploads/2025/07/FLI-AI-Safety-Index-Report-Summer-2025.pdf#page=3)
    [attempt](https://ailabwatch.org/companies/deepseek) to be safe, often not even
    in the prosaic safeguards sense - [This](https://www.holisticai.com/blog/red-teaming-open-source-ai-models-china)
    is one of the few safeguard tests I could find including the Chinese models; M2
    does well. They drop the weights [immediately](https://x.com/natolambert/status/1991915728992190909)
    after post-training finishes. They’re mostly open weights and closed data. The
    companies are often [severely](https://www.wsj.com/tech/ai/china-us-ai-chip-restrictions-effect-275a311e)
    compute-constrained. See [here](https://www.gleech.org/paper) for doubts about
    their capabilities.


    * Alibaba’s Qwen3-etc-etc is [nominally](https://artificialanalysis.ai/leaderboards/models)
    at the level of Gemini 2.5 Flash. Maybe the only Chinese model with a [large](https://www.atomproject.ai/#:~:text=Model%20Adoption%20Trends)
    Western userbase, including businesses, but since it’s self-hosted this doesn’t
    translate into profits for them yet. On [one ad hoc test](https://www.gleech.org/paper)
    it was the only Chinese model not to collapse OOD, but the Qwen2.5 corpus was
    severely contaminated.

    * DeepSeek’s v3.2 is [nominally](https://artificialanalysis.ai/leaderboards/models)
    around the same as Qwen. The CCP made them [waste](https://arstechnica.com/ai/2025/08/deepseek-delays-next-ai-model-due-to-poor-performance-of-chinese-made-chips/)
    months trying Huawei chips.

    * Moonshot’s Kimi-K2-Thinking has some nominally [frontier](https://artificialanalysis.ai/)
    benchmark results and a pleasant style but does not [seem](https://x.com/METR_Evals/status/1991658241932292537)
    frontier.

    * Baidu’s [ERNIE 5](https://x.com/Baidu_Inc/status/1988820837898829918) is again
    nominally very strong, a bit better than DeepSeek. This new one seems to not be
    open.

    * Z’s [GLM-4.6](https://z.ai/blog/glm-4.6) is around the same as Qwen.

    * MiniMax’s M2 is nominally better than Qwen, [around the same](https://artificialanalysis.ai/leaderboards/models)
    as Grok 4 Fast on the usual superficial benchmarks. It does [fine](https://www.holisticai.com/blog/red-teaming-open-source-ai-models-china)
    on one very basic red-team test.

    * ByteDance does impressive research in a lagging paradigm, [diffusion LMs](https://seed.bytedance.com/en/direction/llm).

    * There are [others](https://www.interconnects.ai/i/171165224/honorable-mentions)
    but they’re marginal for now.'
  item_type: section
  agenda_attributes: null
  parsing_issues: []
- id: sec:black_box
  name: Black-box safety (understand and control current model behaviour)
  header_level: 1
  parent_id: null
  content: ''
  item_type: section
  agenda_attributes: null
  parsing_issues: []
- id: sec:iterative_alignment
  name: Iterative alignment
  header_level: 2
  parent_id: sec:black_box
  content: 'Nudging base models by optimising their output. Worked on by the post-training
    teams at most labs, estimating the FTEs at \>500 in some sense. Funded by most
    of the industry.


    **General critiques:** [Bellot](https://arxiv.org/abs/2506.02923), [Alfour](https://cognition.cafe/p/ai-alignment-based-on-intentions),
    [STACK](https://arxiv.org/abs/2506.24068)*,* [AI Alignment Strategies from a Risk
    Perspective](https://arxiv.org/abs/2510.11235), [AI Alignment based on Intentions
    does not work](https://t.co/OTnrYRVsPS)*,* [Distortion of AI Alignment: Does Preference
    Optimization Optimize for Preferences?](https://arxiv.org/abs/2505.23749)*,* [Murphy’s
    Laws of AI Alignment: Why the Gap Always Wins](https://arxiv.org/abs/2509.05381),
    [Alignment remains a hard, unsolved problem](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)'
  item_type: section
  agenda_attributes: null
  parsing_issues: []
- id: a:iterative_alignment_pretrain
  name: Iterative alignment at pretrain-time
  header_level: 3
  parent_id: sec:iterative_alignment
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Guide weights during pretraining.
    theory_of_change: '"LLMs don''t seem very dangerous and might scale to AGI, things
      are generally smooth, relevant capabilities are harder than alignment, assume
      no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally
      humanish ontology is learned, assume no simulated agents, assume that noise
      in the data means that human preferences are not ruled out, assume that alignment
      is a superficial feature, assume that tuning for what we want will also get
      us to avoid what we don''t want. Maybe assume that thoughts are translucent."'
    see_also:
    - '[prosaic alignment](https://www.lesswrong.com/posts/5ciYedyQDDqAcrDLr/a-positive-case-for-how-we-might-succeed-at-prosaic-ai)'
    - '[incrementalism](https://www.lesswrong.com/posts/TALmStNf6479uTwzT/ai-alignment-metastrategy#Incrementalist_Metastrategy)'
    - '[alignment-by-default](https://www.lesswrong.com/posts/Nwgdq6kHke5LY692J/alignment-by-default)'
    orthodox_problems:
    - 'Other: this agenda implicitly questions this framing'
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: engineering
    broad_approach_text: engineering
    some_names:
    - Jan Leike
    - Stuart Armstrong
    - Cyrus Cousins
    - Vincent Conitzer
    - Oliver Daniels
    estimated_ftes: null
    critiques: '[Bellot](https://arxiv.org/abs/2506.02923), [STACK](https://arxiv.org/abs/2506.24068),
      [Dung](https://arxiv.org/abs/2510.11235), [Gaikwad](https://arxiv.org/abs/2509.05381),
      [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)'
    funded_by: most of the industry
    outputs:
    - link_url: https://arxiv.org/abs/2509.04445
      link_text: Towards Cognitively-Faithful Decision-Making Models to Improve AI
        Alignment
      original_md: '* [**Towards Cognitively-Faithful Decision-Making Models to Improve
        AI Alignment**](https://arxiv.org/abs/2509.04445)'
      title: Towards Cognitively-Faithful Decision-Making Models to Improve AI Alignment
      authors:
      - Cyrus Cousins
      - Vijay Keswani
      - Vincent Conitzer
      - Hoda Heidari
      - Jana Schaich Borg
      - Walter Sinnott-Armstrong
      author_organizations:
      - Carnegie Mellon University
      - Duke University
      date: '2025-09-04'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.07955
      link_text: ACE and Diverse Generalization via Selective Disagreement
      original_md: '* [**ACE and Diverse Generalization via Selective Disagreement**](https://arxiv.org/abs/2509.07955)'
      title: ACE and Diverse Generalization via Selective Disagreement
      authors:
      - Oliver Daniels
      - Stuart Armstrong
      - Alexandre Maranhão
      - Mahirah Fairuz Rahman
      - Benjamin M. Marlin
      - Rebecca Gorman
      author_organizations:
      - Unknown - not specified in abstract
      date: '2025-09-09'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://alignment.anthropic.com/2025/unsupervised-elicitation
      link_text: Unsupervised Elicitation
      original_md: '* [**Unsupervised Elicitation**](https://alignment.anthropic.com/2025/unsupervised-elicitation)'
      title: Unsupervised Elicitation
      authors:
      - Jiaxin Wen
      - Zachary Ankner
      - Arushi Somani
      - Peter Hase
      - Samuel Marks
      - Jacob Goldman-Wetzler
      - Linda Petrini
      - Henry Sleight
      - Collin Burns
      - He He
      - Shi Feng
      - Ethan Perez
      - Jan Leike
      author_organizations:
      - Anthropic
      - Schmidt Sciences
      - Independent
      - Constellation
      - New York University
      - George Washington University
      date: null
      published_year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    other_attributes: {}
  parsing_issues: []
- id: a:iterative_alignment_post_train
  name: Iterative alignment at post-train-time
  header_level: 3
  parent_id: sec:iterative_alignment
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Modify weights after pre-training.
    theory_of_change: '"LLMs don''t seem very dangerous and might scale to AGI, things
      are generally smooth, relevant capabilities are harder than alignment, assume
      no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally
      humanish ontology is learned, assume no simulated agents, assume that noise
      in the data means that human preferences are not ruled out, assume that alignment
      is a superficial feature, assume that tuning for what we want will also get
      us to avoid what we don''t want. Maybe assume that thoughts are translucent."'
    see_also:
    - '[incrementalism](https://www.lesswrong.com/posts/TALmStNf6479uTwzT/ai-alignment-metastrategy#Incrementalist_Metastrategy)'
    - a:psych_personas
    orthodox_problems:
    - 'Other: this agenda implicitly questions this framing'
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: engineering
    broad_approach_text: engineering
    some_names:
    - Adam Gleave
    - Anca Dragan
    - Jacob Steinhardt
    - Rohin Shah
    estimated_ftes: null
    critiques: '[Bellot](https://arxiv.org/abs/2506.02923), [STACK](https://arxiv.org/abs/2506.24068),
      [Dung](https://arxiv.org/abs/2510.11235), [Gölz](https://arxiv.org/abs/2505.23749),
      [Gaikwad](https://arxiv.org/abs/2509.05381), [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)'
    funded_by: most of the industry
    outputs:
    - link_url: https://arxiv.org/abs/2501.08617
      link_text: 'RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation'
      original_md: '* [**RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation**](https://arxiv.org/abs/2501.08617)'
      title: 'RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation'
      authors:
      - Kaiqu Liang
      - Haimin Hu
      - Ryan Liu
      - Thomas L. Griffiths
      - Jaime Fernández Fisac
      author_organizations:
      - Princeton University
      date: '2025-01-15'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.11250
      link_text: Uncertainty-Aware Step-wise Verification with Generative Reward Models
      original_md: '* [**Uncertainty-Aware Step-wise Verification with Generative
        Reward Models**](https://arxiv.org/abs/2502.11250)'
      title: Uncertainty-Aware Step-wise Verification with Generative Reward Models
      authors:
      - Zihuiwen Ye
      - Luckeciano Carvalho Melo
      - Younesse Kaddar
      - Phil Blunsom
      - Sam Staton
      - Yarin Gal
      author_organizations:
      - Oxford University
      date: '2025-02-16'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2511.06626
      link_text: 'Spilling the Beans: Teaching LLMs to Self-Report Their Hidden Objectives'
      original_md: '* [**Spilling the Beans: Teaching LLMs to Self-Report Their Hidden
        Objectives**](https://arxiv.org/abs/2511.06626)'
      title: 'Spilling the Beans: Teaching LLMs to Self-Report Their Hidden Objectives'
      authors:
      - Chloe Li
      - Mary Phuong
      - Daniel Tan
      author_organizations: []
      date: '2025-11-10'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.13787
      link_text: Preference Learning with Lie Detectors can Induce Honesty or Evasion
      original_md: '* [**Preference Learning with Lie Detectors can Induce Honesty
        or Evasion**](https://arxiv.org/abs/2505.13787)'
      title: Preference Learning with Lie Detectors can Induce Honesty or Evasion
      authors:
      - Chris Cundy
      - Adam Gleave
      author_organizations: []
      date: '2025-05-20'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.21184
      link_text: Reducing the Probability of Undesirable Outputs in Language Models
        Using Probabilistic Inference
      original_md: '* [**Reducing the Probability of Undesirable Outputs in Language
        Models Using Probabilistic Inference**](https://arxiv.org/abs/2510.21184)'
      title: Reducing the Probability of Undesirable Outputs in Language Models Using
        Probabilistic Inference
      authors:
      - Stephen Zhao
      - Aidan Li
      - Rob Brekelmans
      - Roger Grosse
      author_organizations: []
      date: '2025-10-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.02306
      link_text: On Targeted Manipulation and Deception when Optimizing LLMs for User
        Feedback
      original_md: '* [**On Targeted Manipulation and Deception when Optimizing LLMs
        for User Feedback**](https://arxiv.org/abs/2411.02306)'
      title: On Targeted Manipulation and Deception when Optimizing LLMs for User
        Feedback
      authors:
      - Marcus Williams
      - Micah Carroll
      - Adhyyan Narang
      - Constantin Weisser
      - Brendan Murphy
      - Anca Dragan
      author_organizations: []
      date: '2024-11-04'
      published_year: 2024
      venue: ICLR 2025
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.05967
      link_text: 'Preference Learning for AI Alignment: a Causal Perspective'
      original_md: '* [**Preference Learning for AI Alignment: a Causal Perspective**](https://arxiv.org/abs/2506.05967)'
      title: 'Preference Learning for AI Alignment: a Causal Perspective'
      authors:
      - Katarzyna Kobalczyk
      - Mihaela van der Schaar
      author_organizations: []
      date: '2025-06-06'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2501.07886
      link_text: Iterative Label Refinement Matters More than Preference Optimization
        under Weak Supervision
      original_md: '* [**Iterative Label Refinement Matters More than Preference Optimization
        under Weak Supervision**](https://arxiv.org/abs/2501.07886)'
      title: Iterative Label Refinement Matters More than Preference Optimization
        under Weak Supervision
      authors:
      - Yaowen Ye
      - Cassidy Laidlaw
      - Jacob Steinhardt
      author_organizations:
      - UC Berkeley
      date: '2025-01-14'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.08998
      link_text: On Monotonicity in AI Alignment
      original_md: '* [**On Monotonicity in AI Alignment**](https://arxiv.org/abs/2506.08998)'
      title: On Monotonicity in AI Alignment
      authors:
      - Gilles Bareilles
      - Julien Fageot
      - Lê-Nguyên Hoang
      - Peva Blanchard
      - Wassim Bouaziz
      - Sébastien Rouault
      - El-Mahdi El-Mhamdi
      author_organizations: []
      date: '2025-06-10'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.27062
      link_text: Consistency Training Helps Stop Sycophancy and Jailbreaks
      original_md: '* [**Consistency Training Helps Stop Sycophancy and Jailbreaks**](https://arxiv.org/abs/2510.27062)'
      title: Consistency Training Helps Stop Sycophancy and Jailbreaks
      authors:
      - Alex Irpan
      - Alexander Matt Turner
      - Mark Kurzeja
      - David K. Elson
      - Rohin Shah
      author_organizations:
      - DeepMind
      - Google
      date: '2025-10-31'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.06084
      link_text: 'Spectrum Tuning: Post-Training for Distributional Coverage and In-Context
        Steerability'
      original_md: '* [**Spectrum Tuning: Post-Training for Distributional Coverage
        and In-Context Steerability**](https://arxiv.org/abs/2510.06084)'
      title: 'Spectrum Tuning: Post-Training for Distributional Coverage and In-Context
        Steerability'
      authors:
      - Taylor Sorensen
      - Benjamin Newman
      - Jared Moore
      - Chan Park
      - Jillian Fisher
      - Niloofar Mireshghallah
      - Liwei Jiang
      - Yejin Choi
      author_organizations:
      - University of Washington
      - Allen Institute for AI
      date: '2025-10-07'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2508.12531
      link_text: 'Rethinking Safety in LLM Fine-tuning: An Optimization Perspective'
      original_md: '* [**Rethinking Safety in LLM Fine-tuning: An Optimization Perspective**](https://arxiv.org/abs/2508.12531)'
      title: 'Rethinking Safety in LLM Fine-tuning: An Optimization Perspective'
      authors:
      - Minseon Kim
      - Jin Myung Kwak
      - Lama Alssum
      - Bernard Ghanem
      - Philip Torr
      - David Krueger
      - Fazl Barez
      - Adel Bibi
      author_organizations: []
      date: '2025-08-17'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2407.06483%20
      link_text: Composable Interventions for Language Models
      original_md: '* [**Composable Interventions for Language Models**](https://arxiv.org/abs/2407.06483%20)'
      title: Composable Interventions for Language Models
      authors:
      - Arinbjorn Kolbeinsson
      - Kyle O'Brien
      - Tianjin Huang
      - Shanghua Gao
      - Shiwei Liu
      - Jonathan Richard Schwarz
      - Anurag Vaidya
      - Faisal Mahmood
      - Marinka Zitnik
      - Tianlong Chen
      - Thomas Hartvigsen
      author_organizations:
      - Various Universities
      date: '2024-07-09'
      published_year: 2024
      venue: ICLR 2025
      kind: paper_published
    - link_url: https://arxiv.org/abs/2507.06187
      link_text: 'The Delta Learning Hypothesis: Preference Tuning on Weak Data can
        Yield Strong Gains'
      original_md: '* [**The Delta Learning Hypothesis: Preference Tuning on Weak
        Data can Yield Strong Gains**](https://arxiv.org/abs/2507.06187)'
      title: 'The Delta Learning Hypothesis: Preference Tuning on Weak Data can Yield
        Strong Gains'
      authors:
      - Scott Geng
      - Hamish Ivison
      - Chun-Liang Li
      - Maarten Sap
      - Jerry Li
      - Ranjay Krishna
      - Pang Wei Koh
      author_organizations: []
      date: '2025-07-08'
      published_year: 2025
      venue: COLM 2025
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.01930
      link_text: Robust LLM Alignment via Distributionally Robust Direct Preference
        Optimization
      original_md: '* [**Robust LLM Alignment via Distributionally Robust Direct Preference
        Optimization**](https://arxiv.org/abs/2502.01930)'
      title: Robust LLM Alignment via Distributionally Robust Direct Preference Optimization
      authors:
      - Zaiyan Xu
      - Sushil Vemuri
      - Kishan Panaganti
      - Dileep Kalathil
      - Rahul Jain
      - Deepak Ramachandran
      author_organizations: []
      date: '2025-02-04'
      published_year: 2025
      venue: arXiv (accepted to NeurIPS 2025)
      kind: paper_preprint
    other_attributes: {}
  parsing_issues:
  - 'Orthodox problems field says ''this agenda implicitly questions this framing.''
    - no matching problem found, kept as ''Other: this agenda implicitly questions
    this framing'''
- id: a:make_ai_solve_it_black_box
  name: Black-box make-AI-solve-it
  header_level: 3
  parent_id: sec:iterative_alignment
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Focus on using existing models to improve and align further
      models.
    theory_of_change: '"LLMs don''t seem very dangerous and might scale to AGI, things
      are generally smooth, relevant capabilities are harder than alignment, assume
      no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally
      humanish ontology is learned, assume no simulated agents, assume that noise
      in the data means that human preferences are not ruled out, assume that alignment
      is a superficial feature, assume that tuning for what we want will also get
      us to avoid what we don''t want. Maybe assume that thoughts are translucent."'
    see_also:
    - sec:ai_solve_alignment
    - a:debate
    orthodox_problems:
    - 'Other: this agenda implicitly questions this framing'
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: engineering
    broad_approach_text: engineering
    some_names:
    - Jacques Thibodeau
    - Matthew Shingle
    - Nora Belrose
    - Lewis Hammond
    - Geoffrey Irving
    estimated_ftes: null
    critiques: '[STACK](https://arxiv.org/abs/2506.24068), [Dung](https://arxiv.org/abs/2510.11235),
      [Gölz](https://arxiv.org/abs/2505.23749), [Gaikwad](https://arxiv.org/abs/2509.05381),
      [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL), [SAIF](https://saif.org/research/bare-minimum-mitigations-for-autonomous-ai-development/)'
    funded_by: most of the industry
    outputs:
    - link_url: https://lesswrong.com/posts/5gmALpCetyjkSPEDr/training-ai-to-do-alignment-research-we-don-t-already-know
      link_text: Training AI to do alignment research we don't already know how to
        do
      original_md: '* [**Training AI to do alignment research we don''t already know
        how to do**](https://lesswrong.com/posts/5gmALpCetyjkSPEDr/training-ai-to-do-alignment-research-we-don-t-already-know)'
      title: Training AI to do alignment research we don't already know how to do
      authors:
      - joshc
      author_organizations:
      - Redwood Research
      date: '2025-02-24'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/FqpAPC48CzAtvfx5C/automating-ai-safety-what-we-can-do-today
      link_text: 'Automating AI Safety: What we can do today'
      original_md: '* [**Automating AI Safety: What we can do today**](https://lesswrong.com/posts/FqpAPC48CzAtvfx5C/automating-ai-safety-what-we-can-do-today)'
      title: 'Automating AI Safety: What we can do today'
      authors:
      - Matthew Shinkle
      - Eyon Jang
      - Jacques Thibodeau
      author_organizations:
      - SPAR
      - PIBBSS
      date: '2025-07-25'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2504.08812
      link_text: Mechanistic Anomaly Detection for "Quirky" Language Models
      original_md: '* [**Mechanistic Anomaly Detection for "Quirky" Language Models**](https://arxiv.org/abs/2504.08812)'
      title: Mechanistic Anomaly Detection for "Quirky" Language Models
      authors:
      - David O. Johnston
      - Arkajyoti Chakraborty
      - Nora Belrose
      author_organizations:
      - FAR AI
      date: '2025-04-09'
      published_year: 2025
      venue: arXiv (ICLR Building Trust Workshop 2025)
      kind: paper_preprint
    - link_url: https://openreview.net/forum?id=N1vYivuSKq
      link_text: Weak to Strong Generalization for Large Language Models with Multi-capabilities
      original_md: '* [**Weak to Strong Generalization for Large Language Models with
        Multi-capabilities**](https://openreview.net/forum?id=N1vYivuSKq)'
      title: Weak to Strong Generalization for Large Language Models with Multi-capabilities
      authors:
      - Yucheng Zhou
      - Jianbing Shen
      - Yu Cheng
      author_organizations: []
      date: '2025-01-22'
      published_year: 2025
      venue: ICLR 2025
      kind: paper_published
    - link_url: https://arxiv.org/abs/2501.13124
      link_text: Debate Helps Weak-to-Strong Generalization
      original_md: '* [**Debate Helps Weak-to-Strong Generalization**](https://arxiv.org/abs/2501.13124)'
      title: Debate Helps Weak-to-Strong Generalization
      authors:
      - Hao Lang
      - Fei Huang
      - Yongbin Li
      author_organizations: []
      date: '2025-01-21'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://neural-interactive-proofs.com/
      link_text: Neural Interactive Proofs
      original_md: '* [**Neural Interactive Proofs**](https://neural-interactive-proofs.com/)'
      title: Neural Interactive Proofs
      authors:
      - Lewis Hammond
      - Sam Adam-Day
      author_organizations:
      - University of Oxford
      date: '2024-12-08'
      published_year: 2024
      venue: ICLR 2025
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2501.13011
      link_text: 'MONA: Myopic Optimization with Non-myopic Approval Can Mitigate
        Multi-step Reward Hacking'
      original_md: '* [**MONA: Myopic Optimization with Non-myopic Approval Can Mitigate
        Multi-step Reward Hacking**](https://arxiv.org/abs/2501.13011)'
      title: 'MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step
        Reward Hacking'
      authors:
      - Sebastian Farquhar
      - Vikrant Varma
      - David Lindner
      - David Elson
      - Caleb Biddulph
      - Ian Goodfellow
      - Rohin Shah
      author_organizations:
      - Google DeepMind
      date: '2025-01-22'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/8XHBaugB5S3r27MG9/prover-estimator-debate-a-new-scalable-oversight-protocol
      link_text: 'Prover-Estimator Debate: A New Scalable Oversight Protocol'
      original_md: '* [**Prover-Estimator Debate: A New Scalable Oversight Protocol**](https://lesswrong.com/posts/8XHBaugB5S3r27MG9/prover-estimator-debate-a-new-scalable-oversight-protocol)'
      title: 'Prover-Estimator Debate: A New Scalable Oversight Protocol'
      authors:
      - Jonah Brown-Cohen
      - Geoffrey Irving
      author_organizations:
      - UK AISI
      date: '2025-06-17'
      published_year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2509.00091
      link_text: Ensemble Debates with Local Large Language Models for AI Alignment
      original_md: '* [**Ensemble Debates with Local Large Language Models for AI
        Alignment**](https://arxiv.org/abs/2509.00091)'
      title: Ensemble Debates with Local Large Language Models for AI Alignment
      authors:
      - Ephraiem Sarabamoun
      author_organizations: []
      date: '2025-08-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.02175
      link_text: AI Debate Aids Assessment of Controversial Claims
      original_md: '* [**AI Debate Aids Assessment of Controversial Claims**](https://arxiv.org/abs/2506.02175)'
      title: AI Debate Aids Assessment of Controversial Claims
      authors:
      - Salman Rahman
      - Sheriff Issaka
      - Ashima Suvarna
      - Genglin Liu
      - James Shiffer
      - Jaeyoung Lee
      - Md Rizwan Parvez
      - Hamid Palangi
      - Shi Feng
      - Nanyun Peng
      - Yejin Choi
      - Julian Michael
      - Liwei Jiang
      - Saadia Gabriel
      author_organizations:
      - University of Washington
      - Microsoft Research
      - UCLA
      - NYU
      date: '2025-06-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.03989
      link_text: An alignment safety case sketch based on debate
      original_md: '* [**An alignment safety case sketch based on debate**](https://arxiv.org/abs/2505.03989)'
      title: An alignment safety case sketch based on debate
      authors:
      - Marie Davidsen Buhl
      - Jacob Pfau
      - Benjamin Hilton
      - Geoffrey Irving
      author_organizations:
      - Google DeepMind
      date: '2025-05-23'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.13621
      link_text: Superalignment with Dynamic Human Values
      original_md: '* [**Superalignment with Dynamic Human Values**](https://arxiv.org/abs/2503.13621)'
      title: Superalignment with Dynamic Human Values
      authors:
      - Florian Mai
      - David Kaczér
      - Nicholas Kluge Corrêa
      - Lucie Flek
      author_organizations: []
      date: '2025-03-17'
      published_year: 2025
      venue: ICLR 2025 Workshop on Bidirectional Human-AI Alignment (BiAlign)
      kind: paper_preprint
    other_attributes: {}
  parsing_issues:
  - 'Orthodox problems field says ''this agenda implicitly questions this framing''
    - no matching problem found, kept as ''Other: this agenda implicitly questions
    this framing'''
- id: a:inoculation_prompting
  name: Inoculation prompting
  header_level: 3
  parent_id: sec:iterative_alignment
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Prompt mild misbehaviour in training, to prevent the failure
      mode where once AI misbehaves in a mild way, it will be more inclined towards
      all bad behaviour.
    theory_of_change: '"LLMs don''t seem very dangerous and might scale to AGI, things
      are generally smooth, relevant capabilities are harder than alignment, assume
      no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally
      humanish ontology is learned, assume no simulated agents, assume that noise
      in the data means that human preferences are not ruled out, assume that alignment
      is a superficial feature, assume that tuning for what we want will also get
      us to avoid what we don''t want. Maybe assume that thoughts are translucent."'
    see_also: []
    orthodox_problems:
    - 'Other: this agenda implicitly questions this framing.'
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: engineering
    broad_approach_text: engineering
    some_names:
    - Alex Turner
    - Victor Gillioz
    - Ariana Azarbal
    - Monte MacDiarmid
    - Daniel Ziegler
    estimated_ftes: null
    critiques: '[Bellot](https://arxiv.org/abs/2506.02923), [Alfour](https://cognition.cafe/p/ai-alignment-based-on-intentions),
      [Gölz](https://arxiv.org/abs/2505.23749), [Gaikwad](https://arxiv.org/abs/2509.05381),
      [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)'
    funded_by: most of the industry
    outputs:
    - link_url: https://arxiv.org/abs/2510.04340
      link_text: 'Inoculation Prompting: Eliciting traits from LLMs during training
        can suppress them at test-time'
      original_md: '* [**Inoculation Prompting: Eliciting traits from LLMs during
        training can suppress them at test-time**](https://arxiv.org/abs/2510.04340)'
      title: 'Inoculation Prompting: Eliciting traits from LLMs during training can
        suppress them at test-time'
      authors:
      - Daniel Tan
      - Anders Woodruff
      - Niels Warncke
      - Arun Jose
      - Maxime Riché
      - David Demitri Africa
      - Mia Taylor
      author_organizations: []
      date: '2025-10-05'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.05024
      link_text: 'Inoculation Prompting: Instructing LLMs to misbehave at train-time
        improves test-time alignment'
      original_md: '* [**Inoculation Prompting: Instructing LLMs to misbehave at train-time
        improves test-time alignment**](https://arxiv.org/abs/2510.05024)'
      title: 'Inoculation Prompting: Instructing LLMs to misbehave at train-time improves
        test-time alignment'
      authors:
      - Nevan Wichers
      - Aram Ebtekar
      - Ariana Azarbal
      - Victor Gillioz
      - Christine Ye
      - Emil Ryd
      - Neil Rathi
      - Henry Sleight
      - Alex Mallen
      - Fabien Roger
      - Samuel Marks
      author_organizations: []
      date: '2025-10-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.alignmentforum.org/posts/whkMnqFWKsBm7Gyd7/recontextualization-mitigates-specification-gaming-without
      link_text: Recontextualization Mitigates Specification Gaming Without Modifying
        the Specification
      original_md: '* [**Recontextualization Mitigates Specification Gaming Without
        Modifying the Specification**](https://www.alignmentforum.org/posts/whkMnqFWKsBm7Gyd7/recontextualization-mitigates-specification-gaming-without)'
      title: Recontextualization Mitigates Specification Gaming Without Modifying
        the Specification
      authors:
      - Ariana Azarbal
      - Victor Gillioz
      - Alexander Matt Turner
      - Alex Cloud
      author_organizations:
      - MATS Program
      date: '2025-10-14'
      published_year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - link_url: https://assets.anthropic.com/m/74342f2c96095771/original/Natural-emergent-misalignment-from-reward-hacking-paper.pdf
      link_text: Natural Emergent Misalignment from Reward Hacking
      original_md: '* [Natural Emergent Misalignment from Reward Hacking](https://assets.anthropic.com/m/74342f2c96095771/original/Natural-emergent-misalignment-from-reward-hacking-paper.pdf)'
      title: Natural emergent misalignment from reward hacking
      authors: []
      author_organizations:
      - Anthropic
      date: null
      published_year: null
      venue: null
      kind: error_detected
    other_attributes: {}
  parsing_issues:
  - 'Orthodox problems field says ''this agenda implicitly questions this framing.''
    - no matching problem found, kept as ''Other: this agenda implicitly questions
    this framing.'''
- id: a:inference_time_in_context_learning
  name: 'Inference-time: In-context learning'
  header_level: 3
  parent_id: sec:iterative_alignment
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Investigate what runtime guidelines, rules, or examples
      provided to an LLM yield better behavior.
    theory_of_change: '"LLMs don''t seem very dangerous and might scale to AGI, things
      are generally smooth, relevant capabilities are harder than alignment, assume
      no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally
      humanish ontology is learned, assume no simulated agents, assume that noise
      in the data means that human preferences are not ruled out, assume that alignment
      is a superficial feature, assume that tuning for what we want will also get
      us to avoid what we don''t want. Maybe assume that thoughts are translucent."'
    see_also:
    - model spec as prompt
    - a:specs_and_constitutions
    orthodox_problems:
    - 'Other: this agenda implicitly questions this framing.'
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: engineering
    broad_approach_text: engineering
    some_names:
    - Jacob Steinhardt
    - Kayo Yin
    - Atticus Geiger
    estimated_ftes: null
    critiques: '[STACK](https://arxiv.org/abs/2506.24068), [Dung](https://arxiv.org/abs/2510.11235),
      [Gölz](https://arxiv.org/abs/2505.23749), [Gaikwad](https://arxiv.org/abs/2509.05381),
      [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)'
    funded_by: null
    outputs:
    - link_url: https://arxiv.org/abs/2510.01569
      link_text: 'InvThink: Towards AI Safety via Inverse Reasoning'
      original_md: '* [**InvThink: Towards AI Safety via Inverse Reasoning**](https://arxiv.org/abs/2510.01569)'
      title: 'InvThink: Towards AI Safety via Inverse Reasoning'
      authors:
      - Yubin Kim
      - Taehan Kim
      - Eugene Park
      - Chunjong Park
      - Cynthia Breazeal
      - Daniel McDuff
      - Hae Won Park
      author_organizations: []
      date: '2025-10-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.19248
      link_text: Inference-Time Reward Hacking in Large Language Models
      original_md: '* [**Inference-Time Reward Hacking in Large Language Models**](https://arxiv.org/abs/2506.19248)'
      title: Inference-Time Reward Hacking in Large Language Models
      authors:
      - Hadi Khalaf
      - Claudio Mayrink Verdun
      - Alex Oesterling
      - Himabindu Lakkaraju
      - Flavio du Pin Calmon
      author_organizations: []
      date: '2025-06-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.06182
      link_text: 'Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context'
      original_md: '* [**Mixing Mechanisms: How Language Models Retrieve Bound Entities
        In-Context**](https://arxiv.org/abs/2510.06182)'
      title: 'Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context'
      authors:
      - Yoav Gur-Arieh
      - Mor Geva
      - Atticus Geiger
      author_organizations: []
      date: '2025-10-07'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.05145
      link_text: Understanding In-context Learning of Addition via Activation Subspaces
      original_md: '* [**Understanding In-context Learning of Addition via Activation
        Subspaces**](https://arxiv.org/abs/2505.05145)'
      title: Understanding In-context Learning of Addition via Activation Subspaces
      authors:
      - Xinyan Hu
      - Kayo Yin
      - Michael I. Jordan
      - Jacob Steinhardt
      - Lijie Chen
      author_organizations:
      - UC Berkeley
      date: '2025-05-08'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.14010
      link_text: Which Attention Heads Matter for In-Context Learning?
      original_md: '* [**Which Attention Heads Matter for In-Context Learning?**](https://arxiv.org/abs/2502.14010)'
      title: Which Attention Heads Matter for In-Context Learning?
      authors:
      - Kayo Yin
      - Jacob Steinhardt
      author_organizations:
      - UC Berkeley
      date: '2025-02-19'
      published_year: 2025
      venue: ICML 2025
      kind: paper_preprint
    other_attributes: {}
  parsing_issues:
  - 'Orthodox problems field says ''this agenda implicitly questions this framing.''
    - no matching problem found, kept as ''Other: this agenda implicitly questions
    this framing.'''
  - See also mentions 'model spec as prompt' which likely refers to 'a:specs_and_constitutions'
    - added both the original text and the resolved ID
- id: a:inference_time_steering
  name: 'Inference-time: Steering'
  header_level: 3
  parent_id: sec:iterative_alignment
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Manipulate an LLM's internal representations/token probabilities
      without touching weights.
    theory_of_change: '"LLMs don''t seem very dangerous and might scale to AGI, things
      are generally smooth, relevant capabilities are harder than alignment, assume
      no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally
      humanish ontology is learned, assume no simulated agents, assume that noise
      in the data means that human preferences are not ruled out, assume that alignment
      is a superficial feature, assume that tuning for what we want will also get
      us to avoid what we don''t want. Maybe assume that thoughts are translucent."'
    see_also:
    - a:psych_personas
    - a:anthropic_safeguards
    orthodox_problems:
    - 'Other: this agenda implicitly questions this framing'
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: engineering
    broad_approach_text: engineering
    some_names:
    - Taylor Sorensen
    - Constanza Fierro
    - Kshitish Ghate
    - Arthur Vogels
    estimated_ftes: null
    critiques: '[Alfour](https://cognition.cafe/p/ai-alignment-based-on-intentions),
      [STACK](https://arxiv.org/abs/2506.24068), [Dung](https://arxiv.org/abs/2510.11235),
      [Gölz](https://arxiv.org/abs/2505.23749), [Gaikwad](https://arxiv.org/abs/2509.05381),
      [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)'
    funded_by: null
    outputs:
    - link_url: https://arxiv.org/abs/2510.13285
      link_text: 'In-Distribution Steering: Balancing Control and Coherence in Language
        Model Generation.'
      original_md: '* [**In-Distribution Steering: Balancing Control and Coherence
        in Language Model Generation.**](https://arxiv.org/abs/2510.13285)'
      title: 'In-Distribution Steering: Balancing Control and Coherence in Language
        Model Generation'
      authors:
      - Arthur Vogels
      - Benjamin Wong
      - Yann Choho
      - Annabelle Blangero
      - Milan Bhan
      author_organizations: []
      date: '2025-10-15'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.06370
      link_text: 'EVALUESTEER: Measuring Reward Model Steerability Towards Values
        and Preferences'
      original_md: '* [**EVALUESTEER: Measuring Reward Model Steerability Towards
        Values and Preferences**](https://arxiv.org/abs/2510.06370)'
      title: 'EVALUESTEER: Measuring Reward Model Steerability Towards Values and
        Preferences'
      authors:
      - Kshitish Ghate
      - Andy Liu
      - Devansh Jain
      - Taylor Sorensen
      - Atoosa Kasirzadeh
      - Aylin Caliskan
      - Mona T. Diab
      - Maarten Sap
      author_organizations:
      - University of Washington
      - Carnegie Mellon University
      date: '2025-10-07'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.00580
      link_text: 'Defense Against the Dark Prompts: Mitigating Best-of-N Jailbreaking
        with Prompt Evaluation'
      original_md: '* [**Defense Against the Dark Prompts: Mitigating Best-of-N Jailbreaking
        with Prompt Evaluation**](https://arxiv.org/abs/2502.00580)'
      title: 'Defense Against the Dark Prompts: Mitigating Best-of-N Jailbreaking
        with Prompt Evaluation'
      authors:
      - Stuart Armstrong
      - Matija Franklin
      - Connor Stevens
      - Rebecca Gorman
      author_organizations: []
      date: '2025-02-01'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2511.05408
      link_text: Steering Language Models with Weight Arithmetic
      original_md: '* [**Steering Language Models with Weight Arithmetic**](https://arxiv.org/abs/2511.05408)'
      title: Steering Language Models with Weight Arithmetic
      authors:
      - Constanza Fierro
      - Fabien Roger
      author_organizations: []
      date: '2025-11-07'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues:
  - 'Orthodox problems field contains ''this agenda implicitly questions this framing''
    - no matching standard problem found, kept as ''Other: this agenda implicitly
    questions this framing.'''
- id: a:unlearning
  name: Capability removal, unlearning
  header_level: 2
  parent_id: sec:black_box
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: jord (split between black and white box?)
    one_sentence_summary: Developing methods to selectively remove specific information,
      capabilities, or behaviors from a trained model without retraining it from scratch.
      This is a mixture of black-box and white-box approaches.
    theory_of_change: If an AI learns dangerous knowledge (e.g., dual-use capabilities)
      or exhibits undesirable behaviors (e.g., memorizing private data), we can specifically
      erase this "bad" knowledge post-training, which is much cheaper and faster than
      retraining, thereby making the model safer.
    see_also:
    - sec:interpretability
    - a:various_redteams
    orthodox_problems:
    - value_fragile
    - goals_misgeneralize
    - humanlike_minds_not_safe
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - Rowan Wang
    - Avery Griffin
    - Johannes Treutlein
    - Zico Kolter
    - Bruce W. Lee
    - Addie Foote
    - Alex Infanger
    - Zesheng Shi
    - Yucheng Zhou
    - Jing Li
    - Timothy Qian
    estimated_ftes: 10-50
    critiques: '[Existing Large Language Model Unlearning Evaluations Are Inconclusive](https://arxiv.org/abs/2506.00688)'
    funded_by: Coefficient Giving, MacArthur Foundation, UK AI Safety Institute (AISI),
      Canadian AI Safety Institute (CAISI), industry labs (e.g., Microsoft Research,
      Google)
    outputs:
    - section_name: Mostly black-box
      header_level: 3
      original_md: "### \tMostly black-box"
    - link_url: https://alignment.anthropic.com/2025/modifying-beliefs-via-sdf
      link_text: Modifying LLM Beliefs with Synthetic Document Finetuning
      original_md: '* [**Modifying LLM Beliefs with Synthetic Document Finetuning**](https://alignment.anthropic.com/2025/modifying-beliefs-via-sdf)'
      title: Modifying LLM Beliefs with Synthetic Document Finetuning
      authors:
      - Rowan Wang
      - Avery Griffin
      - Johannes Treutlein
      - Ethan Perez
      - Julian Michael
      - Fabien Roger
      - Sam Marks
      author_organizations:
      - Anthropic
      - MATS
      - Scale AI
      date: '2025-04-24'
      published_year: 2025
      venue: Alignment Science Blog
      kind: blog_post
    - link_url: https://arxiv.org/abs/2506.06278
      link_text: Distillation Robustifies Unlearning
      original_md: '* [**Distillation Robustifies Unlearning**](https://arxiv.org/abs/2506.06278)'
      title: Distillation Robustifies Unlearning
      authors:
      - Bruce W. Lee
      - Addie Foote
      - Alex Infanger
      - Leni Shor
      - Harish Kamath
      - Jacob Goldman-Wetzler
      - Bryce Woodworth
      - Alex Cloud
      - Alexander Matt Turner
      author_organizations: []
      date: '2025-06-06'
      published_year: 2025
      venue: arXiv (NeurIPS 2025 Spotlight)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.22310
      link_text: 'From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space
        Regularization'
      original_md: '* [**From Dormant to Deleted: Tamper-Resistant Unlearning Through
        Weight-Space Regularization**](https://arxiv.org/abs/2505.22310), *Shoaib
        Ahmed Siddiqui, Adrian Weller, David Krueger et al.*, 2025-05-28, arXiv'
      title: 'From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space
        Regularization'
      authors:
      - Shoaib Ahmed Siddiqui
      - Adrian Weller
      - David Krueger
      - Gintare Karolina Dziugaite
      - Michael Curtis Mozer
      - Eleni Triantafillou
      author_organizations:
      - University of Cambridge
      - Google DeepMind
      - Vector Institute
      date: '2025-05-28'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.12618
      link_text: 'OpenUnlearning: Accelerating LLM Unlearning via Unified Benchmarking
        of Methods and Metrics'
      original_md: '* [**OpenUnlearning: Accelerating LLM Unlearning via Unified Benchmarking
        of Methods and Metrics**](https://arxiv.org/abs/2506.12618), *Vineeth Dorna,
        Anmol Mekala, Wenlong Zhao et al.*, 2025-06-14, arXiv'
      title: 'OpenUnlearning: Accelerating LLM Unlearning via Unified Benchmarking
        of Methods and Metrics'
      authors:
      - Vineeth Dorna
      - Anmol Mekala
      - Wenlong Zhao
      - Andrew McCallum
      - Zachary C. Lipton
      - J. Zico Kolter
      - Pratyush Maini
      author_organizations: []
      date: '2025-06-14'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2412.06966
      link_text: 'Machine Unlearning Doesn''t Do What You Think: Lessons for Generative
        AI Policy and Research'
      original_md: '* [**Machine Unlearning Doesn''t Do What You Think: Lessons for
        Generative AI Policy and Research**](https://arxiv.org/abs/2412.06966), *A.
        Feder Cooper, Christopher A. Choquette-Choo, Miranda Bogen et al.*, 2024-12-09,
        NeurIPS 2025 (Oral)'
      title: 'Machine Unlearning Doesn''t Do What You Think: Lessons for Generative
        AI Policy and Research'
      authors:
      - A. Feder Cooper
      - Christopher A. Choquette-Choo
      - Miranda Bogen
      - Kevin Klyman
      - Matthew Jagielski
      - Katja Filippova
      - Ken Liu
      - Alexandra Chouldechova
      - Jamie Hayes
      - Yangsibo Huang
      - Eleni Triantafillou
      - Peter Kairouz
      - Nicole Elyse Mitchell
      - Niloofar Mireshghallah
      - Abigail Z. Jacobs
      - James Grimmelmann
      - Vitaly Shmatikov
      - Christopher De Sa
      - Ilia Shumailov
      - Andreas Terzis
      - Solon Barocas
      - Jennifer Wortman Vaughan
      - Danah Boyd
      - Yejin Choi
      - Sanmi Koyejo
      - Fernando Delgado
      - Percy Liang
      - Daniel E. Ho
      - Pamela Samuelson
      - Miles Brundage
      - David Bau
      - Seth Neel
      - Hanna Wallach
      - Amy B. Cyphert
      - Mark A. Lemley
      - Nicolas Papernot
      - Katherine Lee
      author_organizations:
      - Anthropic
      - Google
      - Stanford University
      - Cornell University
      - University of Washington
      - Various Universities and Research Institutions
      date: '2024-12-09'
      published_year: 2024
      venue: NeurIPS 2025 (Oral)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2501.04952
      link_text: Open Problems in Machine Unlearning for AI Safety
      original_md: '* [**Open Problems in Machine Unlearning for AI Safety**](https://arxiv.org/abs/2501.04952),
        *Fazl Barez, Tingchen Fu, Ameya Prabhu et al.*, 2025-01-09, arXiv'
      title: Open Problems in Machine Unlearning for AI Safety
      authors:
      - Fazl Barez
      - Tingchen Fu
      - Ameya Prabhu
      - Stephen Casper
      - Amartya Sanyal
      - Adel Bibi
      - Aidan O'Gara
      - Robert Kirk
      - Ben Bucknall
      - Tim Fist
      - Luke Ong
      - Philip Torr
      - Kwok-Yan Lam
      - Robert Trager
      - David Krueger
      - Sören Mindermann
      - José Hernandez-Orallo
      - Mor Geva
      - Yarin Gal
      author_organizations:
      - University of Oxford
      - MIT
      - University of Cambridge
      - Nanyang Technological University
      - Tel Aviv University
      date: '2025-01-09'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.08138
      link_text: Mirror Mirror on the Wall, Have I Forgotten it All?
      original_md: '* [Mirror Mirror on the Wall, Have I Forgotten it All?](https://arxiv.org/abs/2505.08138)'
      title: Mirror Mirror on the Wall, Have I Forgotten it All? A New Framework for
        Evaluating Machine Unlearning
      authors:
      - Brennon Brimhall
      - Philip Mathew
      - Neil Fendley
      - Yinzhi Cao
      - Matthew Green
      author_organizations: []
      date: '2025-05-13'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - section_name: Mostly white-box
      header_level: 3
      original_md: '### Mostly white-box'
    - link_url: https://arxiv.org/abs/2505.18588
      link_text: Safety Alignment via Constrained Knowledge Unlearning
      original_md: '* [**Safety Alignment via Constrained Knowledge Unlearning**](https://arxiv.org/abs/2505.18588)'
      title: Safety Alignment via Constrained Knowledge Unlearning
      authors:
      - Zesheng Shi
      - Yucheng Zhou
      - Jing Li
      author_organizations: []
      date: '2025-05-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.16831
      link_text: 'Unlearning Isn''t Deletion: Investigating Reversibility of Machine
        Unlearning in LLMs'
      original_md: '* [**Unlearning Isn''t Deletion: Investigating Reversibility of
        Machine Unlearning in LLMs**](https://arxiv.org/abs/2505.16831)'
      title: 'Unlearning Isn''t Deletion: Investigating Reversibility of Machine Unlearning
        in LLMs'
      authors:
      - Xiaoyu Xu
      - Xiang Yue
      - Yang Liu
      - Qingqing Ye
      - Huadi Zheng
      - Peizhao Hu
      - Minxin Du
      - Haibo Hu
      author_organizations: []
      date: '2025-05-22'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://goodfire.ai/research/understanding-memorization-via-loss-curvature
      link_text: Understanding Memorization via Loss Curvature
      original_md: '* [**Understanding Memorization via Loss Curvature**](https://goodfire.ai/research/understanding-memorization-via-loss-curvature),
        *Jack Merullo, Srihita Vatsavaya, Lucius Bushnaq et al.*, 2025-11-06, Goodfire.ai
        Research Blog'
      title: Understanding Memorization via Loss Curvature
      authors:
      - Jack Merullo
      - Srihita Vatsavaya
      - Lucius Bushnaq
      - Owen Lewis
      author_organizations:
      - Goodfire
      date: '2025-11-06'
      published_year: 2025
      venue: Goodfire.ai Research Blog
      kind: blog_post
    - link_url: https://lesswrong.com/posts/QYzofMbzmbgiwfqy8/unlearning-needs-to-be-more-selective-progress-report
      link_text: Unlearning Needs to be More Selective [Progress Report]
      original_md: '* [**Unlearning Needs to be More Selective \[Progress Report\]**](https://lesswrong.com/posts/QYzofMbzmbgiwfqy8/unlearning-needs-to-be-more-selective-progress-report),
        *Filip Sondej, Yushi Yang, Marcel Windys*, 2025-06-27, LessWrong'
      title: Unlearning Needs to be More Selective [Progress Report]
      authors:
      - Filip Sondej
      - Yushi Yang
      - Marcel Windys
      author_organizations: []
      date: '2025-06-27'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2505.09500
      link_text: Layered Unlearning for Adversarial Relearning
      original_md: '* [**Layered Unlearning for Adversarial Relearning**](https://arxiv.org/abs/2505.09500),
        *Timothy Qian, Vinith Suriyakumar, Ashia Wilson et al.*, 2025-05-14, arXiv'
      title: Layered Unlearning for Adversarial Relearning
      authors:
      - Timothy Qian
      - Vinith Suriyakumar
      - Ashia Wilson
      - Dylan Hadfield-Menell
      author_organizations: []
      date: '2025-05-14'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.12484
      link_text: 'Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption
        Masking And Normalization'
      original_md: '* [**Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption
        Masking And Normalization**](https://arxiv.org/abs/2506.12484), *Filip Sondej,
        Yushi Yang, Mikołaj Kniejski, Marcel Windys*'
      title: 'Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking
        And Normalization'
      authors:
      - Filip Sondej
      - Yushi Yang
      - Mikołaj Kniejski
      - Marcel Windys
      author_organizations: []
      date: '2025-06-14'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.11816
      link_text: Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive
        LLM Unlearning
      original_md: '* [**Collapse of Irrelevant Representations (CIR) Ensures Robust
        and Non-Disruptive LLM Unlearning**](https://arxiv.org/abs/2509.11816), *Filip
        Sondej, Yushi Yang*'
      title: Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive
        LLM Unlearning
      authors:
      - Filip Sondej
      - Yushi Yang
      author_organizations: []
      date: '2025-09-15'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:control
  name: Control
  header_level: 2
  parent_id: sec:black_box
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Jord ✅
    one_sentence_summary: Assuming early transformative AIs are misaligned and actively
      trying to subvert safety measures, can we still set up protocols to extract
      useful work from them?
    theory_of_change: null
    see_also:
    - safety cases
    orthodox_problems: []
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: null
    broad_approach_text: engineering / behaviorist science
    some_names:
    - Redwood
    - UK AISI
    - Buck Shlegeris
    - Ryan Greenblatt
    - Kshitij Sachan
    - Alex Mallen
    estimated_ftes: 5-50
    critiques: '[Wentworth](https://www.lesswrong.com/posts/8wBN8cdNAv3c7vt6p/the-case-against-ai-control-research),
      [Mannheim](https://lesswrong.com/posts/25dsPH6CuRXPBkGHN/no-we-re-not-getting-meaningful-oversight-of-ai),
      [Kulveit](https://www.lesswrong.com/posts/rZcyemEpBHgb2hqLP/ai-control-may-increase-existential-risk)'
    funded_by: null
    outputs:
    - link_url: https://alignmentforum.org/posts/LPHMMMZFAWog6ty5x/ctrl-z-controlling-ai-agents-via-resampling
      link_text: 'Ctrl-Z: Controlling AI Agents via Resampling'
      original_md: '* [**Ctrl-Z: Controlling AI Agents via Resampling**](https://alignmentforum.org/posts/LPHMMMZFAWog6ty5x/ctrl-z-controlling-ai-agents-via-resampling),
        *Aryan Bhatt, Buck Shlegeris, Adam Kaufman et al.*, 2025-04-16, AI Alignment
        Forum'
      title: 'Ctrl-Z: Controlling AI Agents via Resampling'
      authors:
      - Aryan Bhatt
      - Buck Shlegeris
      - Adam Kaufman
      - Cody Rushing
      - Tyler Tracy
      - Vasil Georgiev
      - David Matolcsi
      - Akbir Khan
      author_organizations:
      - Redwood Research
      date: '2025-04-16'
      published_year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - link_url: https://control-arena.aisi.org.uk/
      link_text: ControlArena
      original_md: '* [**ControlArena**](https://control-arena.aisi.org.uk/), *Rogan
        Inglis, Ollie Matthews, Tyler Tracy et al.*, 2025-01-01, GitHub'
      title: ControlArena
      authors:
      - Rogan Inglis
      - Ollie Matthews
      - Tyler Tracy
      - Oliver Makins
      - Tom Catling
      - Asa Cooper Stickland
      - Rasmus Faber-Espensen
      - Daniel O'Connell
      - Myles Heller
      - Miguel Brandao
      - Adam Hanson
      - Arathi Mani
      - Tomek Korbak
      - Jan Michelfeit
      - Dishank Bansal
      - Tomas Bark
      - Chris Canal
      - Charlie Griffin
      - Jasmine Wang
      - Alan Cooney
      author_organizations:
      - UK AI Security Institute
      - Redwood Research
      date: '2025-01-01'
      published_year: 2025
      venue: GitHub
      kind: code_tool
    - link_url: https://openreview.net/forum?id=QWopGahUEL
      link_text: https://openreview.net/forum?id=QWopGahUEL
      original_md: '* [https://openreview.net/forum?id=QWopGahUEL](https://openreview.net/forum?id=QWopGahUEL)'
      title: 'Games for AI Control: Models of Safety Evaluations of AI Deployment
        Protocols'
      authors:
      - Charlie Griffin
      - Louis Thomson
      - Buck Shlegeris
      - Alessandro Abate
      author_organizations:
      - Redwood Research
      - University of Oxford
      date: '2025-09-20'
      published_year: 2025
      venue: ICLR 2026 Conference (Withdrawn)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.15740
      link_text: 'SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents'
      original_md: '* [**SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents**](https://arxiv.org/abs/2506.15740),
        *Jonathan Kutasov, Yuqi Sun, Paul Colognese et al.*, 2025-06-17, arXiv'
      title: 'SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents'
      authors:
      - Jonathan Kutasov
      - Yuqi Sun
      - Paul Colognese
      - Teun van der Weij
      - Linda Petrini
      - Chen Bo Calvin Zhang
      - John Hughes
      - Xiang Deng
      - Henry Sleight
      - Tyler Tracy
      - Buck Shlegeris
      - Joe Benton
      author_organizations:
      - Redwood Research
      date: '2025-06-17'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.17693
      link_text: Adaptive Deployment of Untrusted LLMs Reduces Distributed Threats
      original_md: '* [**Adaptive Deployment of Untrusted LLMs Reduces Distributed
        Threats**](https://arxiv.org/abs/2411.17693), *Jiaxin Wen, Vivek Hebbar, Caleb
        Larson et al.*, 2024-11-26, arXiv'
      title: Adaptive Deployment of Untrusted LLMs Reduces Distributed Threats
      authors:
      - Jiaxin Wen
      - Vivek Hebbar
      - Caleb Larson
      - Aryan Bhatt
      - Ansh Radhakrishnan
      - Mrinank Sharma
      - Henry Sleight
      - Shi Feng
      - He He
      - Ethan Perez
      - Buck Shlegeris
      - Akbir Khan
      author_organizations: []
      date: '2024-11-26'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.17938
      link_text: 'D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language
        Models'
      original_md: '* [**D-REX: A Benchmark for Detecting Deceptive Reasoning in Large
        Language Models**](https://arxiv.org/abs/2509.17938), *Satyapriya Krishna,
        Andy Zou, Rahul Gupta et al.*, 2025-09-22, arXiv'
      title: 'D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language
        Models'
      authors:
      - Satyapriya Krishna
      - Andy Zou
      - Rahul Gupta
      - Eliot Krzysztof Jones
      - Nick Winter
      - Dan Hendrycks
      - J. Zico Kolter
      - Matt Fredrikson
      - Spyros Matsoukas
      author_organizations:
      - Carnegie Mellon University
      - Center for AI Safety
      - Anthropic
      date: '2025-09-22'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://luthienresearch.org/updates/2025-03-redteam-as-upsampling/
      link_text: https://luthienresearch.org/updates/2025-03-redteam-as-upsampling/
      original_md: '* [https://luthienresearch.org/updates/2025-03-redteam-as-upsampling/](https://luthienresearch.org/updates/2025-03-redteam-as-upsampling/)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://arxiv.org/abs/2001.07118
      link_text: Incentives for Responsiveness, Instrumental Control and Impact
      original_md: '* [Incentives for Responsiveness, Instrumental Control and Impact](https://arxiv.org/abs/2001.07118)'
      title: Incentives for Responsiveness, Instrumental Control and Impact
      authors:
      - Ryan Carey
      - Eric Langlois
      - Chris van Merwijk
      - Shane Legg
      - Tom Everitt
      author_organizations:
      - DeepMind
      date: '2020-01-20'
      published_year: 2020
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2412.12480
      link_text: 'Subversion Strategy Eval: Can language models statelessly strategize
        to subvert control protocols?'
      original_md: '* [**Subversion Strategy Eval: Can language models statelessly
        strategize to subvert control protocols?**](https://arxiv.org/abs/2412.12480),
        *Alex Mallen, Charlie Griffin, Misha Wagner et al.*, 2024-12-17, arXiv'
      title: 'Subversion Strategy Eval: Can language models statelessly strategize
        to subvert control protocols?'
      authors:
      - Alex Mallen
      - Charlie Griffin
      - Misha Wagner
      - Alessandro Abate
      - Buck Shlegeris
      author_organizations:
      - Redwood Research
      date: '2024-12-17'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2511.02997
      link_text: Evaluating Control Protocols for Untrusted AI Agents
      original_md: '* [**Evaluating Control Protocols for Untrusted AI Agents**](https://arxiv.org/abs/2511.02997),
        *Jon Kutasov, Chloe Loughridge, Yuqi Sun et al.*, 2025-11-04, arXiv'
      title: Evaluating Control Protocols for Untrusted AI Agents
      authors:
      - Jon Kutasov
      - Chloe Loughridge
      - Yuqi Sun
      - Henry Sleight
      - Buck Shlegeris
      - Tyler Tracy
      - Joe Benton
      author_organizations:
      - Redwood Research
      date: '2025-11-04'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.19851
      link_text: Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought
        Monitorability
      original_md: '* [**Can Reasoning Models Obfuscate Reasoning? Stress-Testing
        Chain-of-Thought Monitorability**](https://arxiv.org/abs/2510.19851), *Artur
        Zolkowski, Wen Xing, David Lindner et al.*, 2025-10-21, arXiv'
      title: Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought
        Monitorability
      authors:
      - Artur Zolkowski
      - Wen Xing
      - David Lindner
      - Florian Tramèr
      - Erik Jenner
      author_organizations:
      - ETH Zurich
      - FAR AI
      date: '2025-10-21'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2511.02823
      link_text: Optimizing AI Agent Attacks With Synthetic Data
      original_md: '* [**Optimizing AI Agent Attacks With Synthetic Data**](https://arxiv.org/abs/2511.02823),
        *Chloe Loughridge, Paul Colognese, Avery Griffin et al.*, 2025-11-04, arXiv'
      title: Optimizing AI Agent Attacks With Synthetic Data
      authors:
      - Chloe Loughridge
      - Paul Colognese
      - Avery Griffin
      - Tyler Tracy
      - Jon Kutasov
      - Joe Benton
      author_organizations: []
      date: '2025-11-04'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2501.17315
      link_text: A sketch of an AI control safety case
      original_md: '* [**A sketch of an AI control safety case**](https://arxiv.org/abs/2501.17315),
        *Tomek Korbak, Joshua Clymer, Benjamin Hilton et al.*, 2025-01-28, arXiv'
      title: A sketch of an AI control safety case
      authors:
      - Tomek Korbak
      - Joshua Clymer
      - Benjamin Hilton
      - Buck Shlegeris
      - Geoffrey Irving
      author_organizations:
      - Redwood Research
      date: '2025-01-28'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.05791
      link_text: Assessing confidence in frontier AI safety cases
      original_md: '* [**Assessing confidence in frontier AI safety cases**](https://arxiv.org/abs/2502.05791),
        *Stephen Barrett, Philip Fox, Joshua Krook et al.*, 2025-02-09, arXiv'
      title: Assessing confidence in frontier AI safety cases
      authors:
      - Stephen Barrett
      - Philip Fox
      - Joshua Krook
      - Tuneer Mondal
      - Simon Mylius
      - Alejandro Tlaie
      author_organizations: []
      date: '2025-02-09'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.05259
      link_text: How to evaluate control measures for LLM agents? A trajectory from
        today to superintelligence
      original_md: '* [**How to evaluate control measures for LLM agents? A trajectory
        from today to superintelligence**](https://arxiv.org/abs/2504.05259), *Tomek
        Korbak, Mikita Balesni, Buck Shlegeris et al.*, 2025-04-07, arXiv'
      title: How to evaluate control measures for LLM agents? A trajectory from today
        to superintelligence
      authors:
      - Tomek Korbak
      - Mikita Balesni
      - Buck Shlegeris
      - Geoffrey Irving
      author_organizations:
      - Redwood Research
      - Google DeepMind
      date: '2025-04-07'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.03336
      link_text: Towards evaluations-based safety cases for AI scheming
      original_md: '* [**Towards evaluations-based safety cases for AI scheming**](https://arxiv.org/abs/2411.03336),
        *Mikita Balesni, Marius Hobbhahn, David Lindner et al.*, 2024-11-07, arXiv'
      title: Towards evaluations-based safety cases for AI scheming
      authors:
      - Mikita Balesni
      - Marius Hobbhahn
      - David Lindner
      - Alexander Meinke
      - Tomek Korbak
      - Joshua Clymer
      - Buck Shlegeris
      - Jérémy Scheurer
      - Charlotte Stix
      - Rusheb Shah
      - Nicholas Goldowsky-Dill
      - Dan Braun
      - Bilal Chughtai
      - Owain Evans
      - Daniel Kokotajlo
      - Lucius Bushnaq
      author_organizations:
      - Redwood Research
      - Apollo Research
      - Anthropic
      date: '2024-11-07'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://alignment.anthropic.com/2025/bumpers
      link_text: Putting up Bumpers
      original_md: '* [**Putting up Bumpers**](https://alignment.anthropic.com/2025/bumpers),
        2025, Anthropic Alignment Science Blog'
      title: Putting up Bumpers
      authors: []
      author_organizations:
      - Anthropic
      date: null
      published_year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - link_url: https://arxiv.org/abs/2507.12872
      link_text: 'Manipulation Attacks by Misaligned AI: Risk Analysis and Safety
        Case Framework'
      original_md: '* [**Manipulation Attacks by Misaligned AI: Risk Analysis and
        Safety Case Framework**](https://arxiv.org/abs/2507.12872), *Rishane Dassanayake,
        Mario Demetroudi, James Walpole et al.*, 2025-07-17, arXiv'
      title: 'Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case
        Framework'
      authors:
      - Rishane Dassanayake
      - Mario Demetroudi
      - James Walpole
      - Lindley Lentati
      - Jason R. Brown
      - Edward James Young
      author_organizations: []
      date: '2025-07-17'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2412.17618
      link_text: Dynamic safety cases for frontier AI
      original_md: '* [**Dynamic safety cases for frontier AI**](https://arxiv.org/abs/2412.17618),
        *Carmen Cârlan, Francesca Gomez, Yohan Mathew et al.*, 2024-12-23, arXiv'
      title: Dynamic safety cases for frontier AI
      authors:
      - Carmen Cârlan
      - Francesca Gomez
      - Yohan Mathew
      - Ketana Krishna
      - René King
      - Peter Gebauer
      - Ben R. Smith
      author_organizations: []
      date: '2024-12-23'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/wKTwdgZDo479EhmJL/the-alignment-project-by-uk-aisi-1
      link_text: The Alignment Project by UK AISI
      original_md: '* [**The Alignment Project by UK AISI**](https://lesswrong.com/posts/wKTwdgZDo479EhmJL/the-alignment-project-by-uk-aisi-1),
        *Mojmir, Benjamin Hilton, Jacob Pfau et al.*, 2025-08-01, LessWrong'
      title: The Alignment Project by UK AISI
      authors:
      - Mojmir
      - Benjamin Hilton
      - Jacob Pfau
      - Geoffrey Irving
      - Joseph Bloom
      - Tomek Korbak
      - David Africa
      - Edmund Lau
      author_organizations:
      - UK AI Security Institute
      date: '2025-08-01'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/neTbrpBziAsTH5Bn7/ai-companies-are-unlikely-to-make-high-assurance-safety
      link_text: AI companies are unlikely to make high-assurance safety cases if
        timelines are short
      original_md: '* [**AI companies are unlikely to make high-assurance safety cases
        if timelines are short**](https://lesswrong.com/posts/neTbrpBziAsTH5Bn7/ai-companies-are-unlikely-to-make-high-assurance-safety),
        *Ryan Greenblatt*, 2025-01-23, LessWrong / AI Alignment Forum'
      title: AI companies are unlikely to make high-assurance safety cases if timelines
        are short
      authors:
      - Ryan Greenblatt
      author_organizations:
      - Anthropic
      date: '2025-01-23'
      published_year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/cJQZAueoPC6aTncKK/ais-at-the-current-capability-level-may-be-important-for
      link_text: AIs at the current capability level may be important for future safety
        work
      original_md: '* [**AIs at the current capability level may be important for
        future safety work**](https://lesswrong.com/posts/cJQZAueoPC6aTncKK/ais-at-the-current-capability-level-may-be-important-for),
        *Ryan Greenblatt*, 2025-05-12, LessWrong'
      title: AIs at the current capability level may be important for future safety
        work
      authors:
      - Ryan Greenblatt
      author_organizations:
      - Anthropic
      date: '2025-05-12'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/y6rBarAPTLmuhn9PJ/takeaways-from-sketching-a-control-safety-case
      link_text: Takeaways from sketching a control safety case
      original_md: '* [**Takeaways from sketching a control safety case**](https://lesswrong.com/posts/y6rBarAPTLmuhn9PJ/takeaways-from-sketching-a-control-safety-case),
        *Josh Clymer, Buck Shlegeris*, 2025-01-31, LessWrong'
      title: Takeaways from sketching a control safety case
      authors:
      - Josh Clymer
      - Buck Shlegeris
      author_organizations:
      - Redwood Research
      - UK AISI
      date: '2025-01-31'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    other_attributes: {}
  parsing_issues:
  - Broad approach field contains multiple approaches ('engineering / behavioural')
    - normalized to 'engineering / behaviorist science' but left broad_approach_id
    as null
- id: a:anthropic_safeguards
  name: Safeguards (inference-time auxiliaries)
  header_level: 2
  parent_id: sec:black_box
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stephen ✅
    one_sentence_summary: Layers of inference-time defenses, such as classifiers,
      monitors, and rapid-response protocols, to detect and block jailbreaks, prompt
      injections, and other harmful model behaviors.
    theory_of_change: By building a bunch of scalable and hardened things on top of
      an unsafe model, we can defend against known and unknown attacks, monitor for
      misuse, and prevent models from causing harm, even if the core model has vulnerabilities.
    see_also:
    - a:various_redteams
    - sec:iterative_alignment
    orthodox_problems:
    - superintelligence_fool_supervisors
    - boxed_agi_exfiltrate
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: engineering
    broad_approach_text: engineering
    some_names:
    - Mrinank Sharma
    - Meg Tong
    - Jesse Mu
    - Alwin Peng
    - Julian Michael
    - Henry Sleight
    - Theodore Sumers
    - Raj Agarwal
    - Nathan Bailey
    - Edoardo Debenedetti
    - Ilia Shumailov
    - Tianqi Fan
    - Sahil Verma
    - Keegan Hines
    - Jeff Bilmes
    estimated_ftes: 100+
    critiques: '[Obfuscated Activations Bypass LLM Latent-Space Defenses](https://arxiv.org/abs/2412.09565).'
    funded_by: most of the big labs
    outputs:
    - link_url: https://arxiv.org/abs/2501.18837
      link_text: 'Constitutional Classifiers: Defending against Universal Jailbreaks
        across Thousands of Hours of Red Teaming'
      original_md: '* [**Constitutional Classifiers: Defending against Universal Jailbreaks
        across Thousands of Hours of Red Teaming**](https://arxiv.org/abs/2501.18837),
        *Mrinank Sharma, Meg Tong, Jesse Mu et al.*, 2025-01-31, arXiv'
      title: 'Constitutional Classifiers: Defending against Universal Jailbreaks across
        Thousands of Hours of Red Teaming'
      authors:
      - Mrinank Sharma
      - Meg Tong
      - Jesse Mu
      - Jerry Wei
      - Jorrit Kruthoff
      - Scott Goodfriend
      - Euan Ong
      - Alwin Peng
      - Raj Agarwal
      - Cem Anil
      - Amanda Askell
      - Nathan Bailey
      - Joe Benton
      - Emma Bluemke
      - Samuel R. Bowman
      - Eric Christiansen
      - Hoagy Cunningham
      - Andy Dau
      - Anjali Gopal
      - Rob Gilson
      - Logan Graham
      - Logan Howard
      - Nimit Kalra
      - Taesung Lee
      - Kevin Lin
      - Peter Lofgren
      - Francesco Mosconi
      - Clare O'Hara
      - Catherine Olsson
      - Linda Petrini
      - Samir Rajani
      - Nikhil Saxena
      - Alex Silverstein
      - Tanya Singh
      - Theodore Sumers
      - Leonard Tang
      - Kevin K. Troy
      - Constantin Weisser
      - Ruiqi Zhong
      - Giulio Zhou
      - Jan Leike
      - Jared Kaplan
      - Ethan Perez
      author_organizations:
      - Anthropic
      date: '2025-01-31'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.07494
      link_text: 'Rapid Response: Mitigating LLM Jailbreaks with a Few Examples'
      original_md: '* [**Rapid Response: Mitigating LLM Jailbreaks with a Few Examples**](https://arxiv.org/abs/2411.07494),
        *Alwin Peng, Julian Michael, Henry Sleight et al.*, 2024-11-12, arXiv'
      title: 'Rapid Response: Mitigating LLM Jailbreaks with a Few Examples'
      authors:
      - Alwin Peng
      - Julian Michael
      - Henry Sleight
      - Ethan Perez
      - Mrinank Sharma
      author_organizations:
      - Anthropic
      date: '2024-11-12'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://alignment.anthropic.com/2025/summarization-for-monitoring/index.html
      link_text: Monitoring computer use via hierarchical summarization
      original_md: '* [**Monitoring computer use via hierarchical summarization**](https://alignment.anthropic.com/2025/summarization-for-monitoring/index.html),
        *Theodore Sumers, Raj Agarwal, Nathan Bailey et al.*, 2025-02-27, Anthropic
        Alignment Science Blog'
      title: Monitoring computer use via hierarchical summarization
      authors:
      - Theodore Sumers
      - Raj Agarwal
      - Nathan Bailey
      - Tim Belonax
      - Brian Clarke
      - Jasmine Deng
      - Kyla Guru
      - Evan Frondorf
      - Keegan Hankes
      - Jacob Klein
      - Lynx Lean
      - Kevin Lin
      - Linda Petrini
      - Madeleine Tucker
      - Ethan Perez
      - Mrinank Sharma
      - Nikhil Saxena
      author_organizations:
      - Anthropic
      date: '2025-02-27'
      published_year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - link_url: https://arxiv.org/abs/2503.18813
      link_text: Defeating Prompt Injections by Design
      original_md: '* [**Defeating Prompt Injections by Design**](https://arxiv.org/abs/2503.18813),
        *Edoardo Debenedetti, Ilia Shumailov, Tianqi Fan et al.*, 2025-03-24, arXiv'
      title: Defeating Prompt Injections by Design
      authors:
      - Edoardo Debenedetti
      - Ilia Shumailov
      - Tianqi Fan
      - Jamie Hayes
      - Nicholas Carlini
      - Daniel Fabian
      - Christoph Kern
      - Chongyang Shi
      - Andreas Terzis
      - Florian Tramèr
      author_organizations:
      - Google Research
      - ETH Zurich
      date: '2025-03-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://alignment.anthropic.com/2025/introducing-safeguards-research-team/index.html
      link_text: Introducing Anthropic's Safeguards Research Team
      original_md: '* [**Introducing Anthropic''s Safeguards Research Team**](https://alignment.anthropic.com/2025/introducing-safeguards-research-team/index.html),
        2025-01-01, Anthropic Alignment Science Blog'
      title: Introducing Anthropic's Safeguards Research Team
      authors: []
      author_organizations:
      - Anthropic
      date: '2025-01-01'
      published_year: 2025
      venue: Anthropic Alignment Science Blog
      kind: news_announcement
    - link_url: https://arxiv.org/abs/2505.23856
      link_text: 'OMNIGUARD: An Efficient Approach for AI Safety Moderation Across
        Modalities'
      original_md: '* [**OMNIGUARD: An Efficient Approach for AI Safety Moderation
        Across Modalities**](https://arxiv.org/abs/2505.23856), *Sahil Verma, Keegan
        Hines, Jeff Bilmes et al.*, 2025-05-29, arXiv'
      title: 'OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities'
      authors:
      - Sahil Verma
      - Keegan Hines
      - Jeff Bilmes
      - Charlotte Siska
      - Luke Zettlemoyer
      - Hila Gonen
      - Chandan Singh
      author_organizations:
      - University of Washington
      - Meta AI Research
      date: '2025-05-29'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:cot_monitoring
  name: Chain of thought monitoring
  header_level: 2
  parent_id: sec:black_box
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stephen ✅
    one_sentence_summary: Supervise an AI's natural-language (output) "reasoning"
      to detect misalignment, scheming, or deception, rather than studying the actual
      internal states.
    theory_of_change: The reasoning process (Chain of Thought, or CoT) of an AI provides
      a legible signal of its internal state and intentions. By monitoring this CoT,
      supervisors (human or AI) can detect misalignment, scheming, or reward hacking
      before it results in a harmful final output. This allows for more robust oversight
      than supervising outputs alone, but it relies on the CoT remaining faithful
      (i.e., accurately reflecting the model's reasoning) and not becoming obfuscated
      under optimization pressure.
    see_also:
    - sec:whitebox
    - a:evals_steganography
    orthodox_problems:
    - superintelligence_fool_supervisors
    - superintelligence_hack_software
    - boxed_agi_exfiltrate
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: engineering
    broad_approach_text: engineering
    some_names:
    - Aether
    - Bowen Baker
    - Joost Huizinga
    - Leo Gao
    - Scott Emmons
    - Erik Jenner
    - Yanda Chen
    - James Chua
    - Owain Evans
    - Tomek Korbak
    - Mikita Balesni
    - Xinpeng Wang
    - Miles Turpin
    estimated_ftes: 10-100
    critiques: '[Reasoning Models Don''t Always Say What They Think](https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf);
      [Chain-of-Thought Reasoning In The Wild Is Not Always Faithful](https://arxiv.org/abs/2503.08679);
      [Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate
      Tokens](https://arxiv.org/abs/2505.13775); [Reasoning Models Sometimes Output
      Illegible Chains of Thought](https://arxiv.org/abs/2510.27338)'
    funded_by: OpenAI, Anthropic, Google DeepMind
    outputs:
    - link_url: https://arxiv.org/abs/2507.11473
      link_text: 'Chain of Thought Monitorability: A New and Fragile Opportunity for
        AI Safety'
      original_md: '* [**Chain of Thought Monitorability: A New and Fragile Opportunity
        for AI Safety**](https://arxiv.org/abs/2507.11473), *Tomek Korbak, Mikita
        Balesni, Elizabeth Barnes et al.*, 2025-07-15, arXiv'
      title: 'Chain of Thought Monitorability: A New and Fragile Opportunity for AI
        Safety'
      authors:
      - Tomek Korbak
      - Mikita Balesni
      - Elizabeth Barnes
      - Yoshua Bengio
      - Joe Benton
      - Joseph Bloom
      - Mark Chen
      - Alan Cooney
      - Allan Dafoe
      - Anca Dragan
      - Scott Emmons
      - Owain Evans
      - David Farhi
      - Ryan Greenblatt
      - Dan Hendrycks
      - Marius Hobbhahn
      - Evan Hubinger
      - Geoffrey Irving
      - Erik Jenner
      - Daniel Kokotajlo
      - Victoria Krakovna
      - Shane Legg
      - David Lindner
      - David Luan
      - Aleksander Mądry
      - Julian Michael
      - Neel Nanda
      - Dave Orr
      - Jakub Pachocki
      - Ethan Perez
      - Mary Phuong
      - Fabien Roger
      - Joshua Saxe
      - Buck Shlegeris
      - Martín Soto
      - Eric Steinberger
      - Jasmine Wang
      - Wojciech Zaremba
      - Bowen Baker
      - Rohin Shah
      - Vlad Mikulik
      author_organizations:
      - Anthropic
      - OpenAI
      - DeepMind
      - FAR AI
      - UC Berkeley
      - Mila
      - Center for AI Safety
      - Redwood Research
      - Apart Research
      date: '2025-07-15'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://openai.com/index/chain-of-thought-monitoring/
      link_text: Detecting misbehavior in frontier reasoning models
      original_md: '* [**Detecting misbehavior in frontier reasoning models**](https://openai.com/index/chain-of-thought-monitoring/),
        *Bowen Baker, Joost Huizinga, Aleksander Madry et al.*, 2025-03-10, arXiv'
      title: Detecting misbehavior in frontier reasoning models
      authors:
      - Bowen Baker
      - Joost Huizinga
      - Aleksander Madry
      - Wojciech Zaremba
      - Jakub Pachocki
      - David Farhi
      author_organizations:
      - OpenAI
      date: '2025-03-10'
      published_year: 2025
      venue: OpenAI Blog / arXiv
      kind: blog_post
    - link_url: https://alignment.anthropic.com/2025/subtle-reasoning/
      link_text: Training fails to elicit subtle reasoning in current language models
      original_md: '* [**Training fails to elicit subtle reasoning in current language
        models**](https://alignment.anthropic.com/2025/subtle-reasoning/), 2025, Anthropic
        Alignment Science Blog'
      title: Training fails to elicit subtle reasoning in current language models
      authors: []
      author_organizations:
      - Anthropic
      date: '2025-01-01'
      published_year: 2025
      venue: Alignment Science Blog
      kind: blog_post
    - link_url: https://arxiv.org/abs/2501.08156
      link_text: Are DeepSeek R1 And Other Reasoning Models More Faithful?
      original_md: '* [**Are DeepSeek R1 And Other Reasoning Models More Faithful?**](https://arxiv.org/abs/2501.08156),
        *James Chua, Owain Evans*, 2025-01-14, arXiv'
      title: Are DeepSeek R1 And Other Reasoning Models More Faithful?
      authors:
      - James Chua
      - Owain Evans
      author_organizations: []
      date: '2025-01-14'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.19851
      link_text: Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought
        Monitorability
      original_md: '* [**Can Reasoning Models Obfuscate Reasoning? Stress-Testing
        Chain-of-Thought Monitorability**](https://arxiv.org/abs/2510.19851)'
      title: Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought
        Monitorability
      authors:
      - Artur Zolkowski
      - Wen Xing
      - David Lindner
      - Florian Tramèr
      - Erik Jenner
      author_organizations:
      - ETH Zurich
      - FAR AI
      date: '2025-10-21'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.11926
      link_text: Monitoring Reasoning Models for Misbehavior and the Risks of Promoting
        Obfuscation
      original_md: '* [**Monitoring Reasoning Models for Misbehavior and the Risks
        of Promoting Obfuscation**](https://arxiv.org/abs/2503.11926), *Bowen Baker,
        Joost Huizinga, Leo Gao et al.*, 2025-03-14, arXiv'
      title: Monitoring Reasoning Models for Misbehavior and the Risks of Promoting
        Obfuscation
      authors:
      - Bowen Baker
      - Joost Huizinga
      - Leo Gao
      - Zehao Dou
      - Melody Y. Guan
      - Aleksander Madry
      - Wojciech Zaremba
      - Jakub Pachocki
      - David Farhi
      author_organizations:
      - OpenAI
      date: '2025-03-14'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.05246
      link_text: When Chain of Thought is Necessary, Language Models Struggle to Evade
        Monitors
      original_md: '* [**When Chain of Thought is Necessary, Language Models Struggle
        to Evade Monitors**](https://arxiv.org/abs/2507.05246), *Scott Emmons, Erik
        Jenner, David K. Elson et al.*, 2025-07-07, arXiv'
      title: When Chain of Thought is Necessary, Language Models Struggle to Evade
        Monitors
      authors:
      - Scott Emmons
      - Erik Jenner
      - David K. Elson
      - Rif A. Saurous
      - Senthooran Rajamanoharan
      - Heng Chen
      - Irhum Shafkat
      - Rohin Shah
      author_organizations:
      - Google DeepMind
      date: '2025-07-07'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.05410
      link_text: Reasoning Models Don't Always Say What They Think
      original_md: '* [**Reasoning Models Don''t Always Say What They Think**](https://arxiv.org/abs/2505.05410),
        *Yanda Chen, Joe Benton, Ansh Radhakrishnan et al.*, 2025-05-08, arXiv'
      title: Reasoning Models Don't Always Say What They Think
      authors:
      - Yanda Chen
      - Joe Benton
      - Ansh Radhakrishnan
      - Jonathan Uesato
      - Carson Denison
      - John Schulman
      - Arushi Somani
      - Peter Hase
      - Misha Wagner
      - Fabien Roger
      - Vlad Mikulik
      - Samuel R. Bowman
      - Jan Leike
      - Jared Kaplan
      - Ethan Perez
      author_organizations:
      - OpenAI
      - Anthropic
      date: '2025-05-08'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.01367
      link_text: Is It Thinking or Cheating? Detecting Implicit Reward Hacking by
        Measuring Reasoning Effort
      original_md: '* [**Is It Thinking or Cheating? Detecting Implicit Reward Hacking
        by Measuring Reasoning Effort**](https://arxiv.org/abs/2510.01367), *Xinpeng
        Wang, Nitish Joshi, Barbara Plank et al.*, 2025-10-01, arXiv'
      title: Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring
        Reasoning Effort
      authors:
      - Xinpeng Wang
      - Nitish Joshi
      - Barbara Plank
      - Rico Angell
      - He He
      author_organizations: []
      date: '2025-10-01'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.22777
      link_text: Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning
      original_md: '* [**Teaching Models to Verbalize Reward Hacking in Chain-of-Thought
        Reasoning**](https://arxiv.org/abs/2506.22777), *Miles Turpin, Andy Arditi,
        Marvin Li et al.*, 2025-06-28, ICML 2025 Workshop on Reliable and Responsible
        Foundation Models'
      title: Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning
      authors:
      - Miles Turpin
      - Andy Arditi
      - Marvin Li
      - Joe Benton
      - Julian Michael
      author_organizations: []
      date: '2025-06-28'
      published_year: 2025
      venue: ICML 2025 Workshop on Reliable and Responsible Foundation Models
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.23966
      link_text: A Pragmatic Way to Measure Chain-of-Thought Monitorability
      original_md: '* [**A Pragmatic Way to Measure Chain-of-Thought Monitorability**](https://arxiv.org/abs/2510.23966),
        *Scott Emmons, Roland S. Zimmermann, David K. Elson et al.*, 2025-10-28, arXiv'
      title: A Pragmatic Way to Measure Chain-of-Thought Monitorability
      authors:
      - Scott Emmons
      - Roland S. Zimmermann
      - David K. Elson
      - Rohin Shah
      author_organizations:
      - Google DeepMind
      date: '2025-10-28'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.lesswrong.com/posts/B8Cmtf5gdHwxb8qtT/aether-july-2025-update
      link_text: https://www.lesswrong.com/posts/B8Cmtf5gdHwxb8qtT/aether-july-2025-update
      original_md: '* [https://www.lesswrong.com/posts/B8Cmtf5gdHwxb8qtT/aether-july-2025-update](https://www.lesswrong.com/posts/B8Cmtf5gdHwxb8qtT/aether-july-2025-update)'
      title: Aether July 2025 Update
      authors:
      - Rohan Subramani
      - Rauno Arike
      - Shubhorup Biswas
      author_organizations:
      - Aether
      date: '2025-07-01'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/Em9sihEZmbofZKc2t/a-concrete-roadmap-towards-safety-cases-based-on-chain-of
      link_text: A Concrete Roadmap towards Safety Cases based on Chain-of-Thought
        Monitoring
      original_md: '* [**A Concrete Roadmap towards Safety Cases based on Chain-of-Thought
        Monitoring**](https://lesswrong.com/posts/Em9sihEZmbofZKc2t/a-concrete-roadmap-towards-safety-cases-based-on-chain-of),
        *Wuschel Schulz*, 2025-10-23, arXiv'
      title: A Concrete Roadmap towards Safety Cases based on Chain-of-Thought Monitoring
      authors:
      - Wuschel Schulz
      author_organizations: []
      date: '2025-10-23'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.23575
      link_text: 'CoT Red-Handed: Stress Testing Chain-of-Thought Monitoring'
      original_md: '* [**CoT Red-Handed: Stress Testing Chain-of-Thought Monitoring**](https://arxiv.org/abs/2505.23575)'
      title: 'CoT Red-Handed: Stress Testing Chain-of-Thought Monitoring'
      authors:
      - Benjamin Arnav
      - Pablo Bernabeu-Pérez
      - Nathan Helm-Burger
      - Tim Kostolansky
      - Hannes Whittingham
      - Mary Phuong
      author_organizations:
      - DeepMind
      date: '2025-05-29'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.lesswrong.com/posts/Tzdwetw55JNqFTkzK/why-don-t-we-just-shoggoth-face-paraphraser
      link_text: Why Don't We Just... Shoggoth+Face+Paraphraser?
      original_md: '* [Why Don''t We Just... Shoggoth+Face+Paraphraser?](https://www.lesswrong.com/posts/Tzdwetw55JNqFTkzK/why-don-t-we-just-shoggoth-face-paraphraser)'
      title: Why don't we just shoggoth-face paraphraser
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: error_detected
    - link_url: https://metr.org/blog/2025-03-11-good-for-ai-to-reason-legibly-and-faithfully/
      link_text: Why it's good for AI reasoning to be legible and faithful
      original_md: '* [Why it''s good for AI reasoning to be legible and faithful](https://metr.org/blog/2025-03-11-good-for-ai-to-reason-legibly-and-faithfully/)'
      title: Why it's good for AI reasoning to be legible and faithful
      authors: []
      author_organizations:
      - METR
      date: '2025-03-11'
      published_year: 2025
      venue: METR Blog
      kind: blog_post
    other_attributes: {}
  parsing_issues: []
- id: sec:model_psychology
  name: Model psychology
  header_level: 2
  parent_id: sec:black_box
  content: ''
  item_type: section
  agenda_attributes: null
  parsing_issues: []
- id: a:surprising_generalization
  name: Emergent misalignment
  header_level: 3
  parent_id: sec:model_psychology
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stephen ✅
    one_sentence_summary: Fine-tuning LLMs on one narrow antisocial task can cause
      *general* misalignment including deception, shutdown resistance, harmful advice,
      and extremist sympathies, when those behaviors are never trained or rewarded
      directly. [A new agenda](https://www.lesswrong.com/posts/AcTEiu5wYDgrbmXow/open-problems-in-emergent-misalignment)
      which quickly led to a stream of exciting work.
    theory_of_change: Predict, detect, and prevent models from developing broadly
      harmful behaviors (like deception or shutdown resistance) when fine-tuned on
      seemingly unrelated tasks. Find, preserve, and robustify this correlated representation
      of the good.
    see_also:
    - auditing real models
    - applied interpretability
    orthodox_problems:
    - goals_misgeneralize
    - superintelligence_fool_supervisors
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: behaviorist_science
    broad_approach_text: behaviorist science
    some_names:
    - Truthful AI
    - Jan Betley
    - James Chua
    - Mia Taylor
    - Miles Wang
    - Edward Turner
    - Anna Soligo
    - Alex Cloud
    - Nathan Hu
    - Owain Evans
    estimated_ftes: 10-50
    critiques: '[Emergent Misalignment as Prompt Sensitivity](https://arxiv.org/html/2507.06253v1),
      [Go home GPT-4o, you''re drunk](https://www.lesswrong.com/posts/RoWabfQxabWBiXwxP/go-home-gpt-4o-you-re-drunk-emergent-misalignment-as-lowered)'
    funded_by: Coefficient Giving, >$1 million
    outputs:
    - link_url: https://arxiv.org/abs/2502.17424
      link_text: 'Emergent Misalignment: Narrow finetuning can produce broadly misaligned
        LLMs'
      original_md: '* [**Emergent Misalignment: Narrow finetuning can produce broadly
        misaligned LLMs**](https://arxiv.org/abs/2502.17424), *Jan Betley, Daniel
        Tan, Niels Warncke et al.*, 2025-02-24, arXiv'
      title: 'Emergent Misalignment: Narrow finetuning can produce broadly misaligned
        LLMs'
      authors:
      - Jan Betley
      - Daniel Tan
      - Niels Warncke
      - Anna Sztyber-Betley
      - Xuchan Bao
      - Martín Soto
      - Nathan Labenz
      - Owain Evans
      author_organizations: []
      date: '2025-02-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.13206
      link_text: 'Thought Crime: Backdoors and Emergent Misalignment in Reasoning
        Models'
      original_md: '* [**Thought Crime: Backdoors and Emergent Misalignment in Reasoning
        Models**](https://arxiv.org/abs/2506.13206), *James Chua, Jan Betley, Mia
        Taylor et al.*, 2025-06-16, arXiv'
      title: 'Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models'
      authors:
      - James Chua
      - Jan Betley
      - Mia Taylor
      - Owain Evans
      author_organizations: []
      date: '2025-06-16'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.19823
      link_text: Persona Features Control Emergent Misalignment
      original_md: '* [**Persona Features Control Emergent Misalignment**](https://arxiv.org/abs/2506.19823),
        *Miles Wang, Tom Dupré la Tour, Olivia Watkins et al.*, 2025-06-24, arXiv'
      title: Persona Features Control Emergent Misalignment
      authors:
      - Miles Wang
      - Tom Dupré la Tour
      - Olivia Watkins
      - Alex Makelov
      - Ryan A. Chi
      - Samuel Miserendino
      - Jeffrey Wang
      - Achyuta Rajaram
      - Johannes Heidecke
      - Tejal Patwardhan
      - Dan Mossing
      author_organizations: []
      date: '2025-06-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.11613
      link_text: Model Organisms for Emergent Misalignment
      original_md: '* [**Model Organisms for Emergent Misalignment**](https://arxiv.org/abs/2506.11613),
        *Edward Turner, Anna Soligo, Mia Taylor et al.*, 2025-06-13, arXiv'
      title: Model Organisms for Emergent Misalignment
      authors:
      - Edward Turner
      - Anna Soligo
      - Mia Taylor
      - Senthooran Rajamanoharan
      - Neel Nanda
      author_organizations:
      - Google DeepMind
      date: '2025-06-13'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2508.17511
      link_text: 'School of Reward Hacks: Hacking harmless tasks generalizes to misaligned
        behavior in LLMs'
      original_md: '* [**School of Reward Hacks: Hacking harmless tasks generalizes
        to misaligned behavior in LLMs**](https://arxiv.org/abs/2508.17511), *Mia
        Taylor, James Chua, Jan Betley et al.*, 2025-08-24, arXiv'
      title: 'School of Reward Hacks: Hacking harmless tasks generalizes to misaligned
        behavior in LLMs'
      authors:
      - Mia Taylor
      - James Chua
      - Jan Betley
      - Johannes Treutlein
      - Owain Evans
      author_organizations: []
      date: '2025-08-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://alignment.anthropic.com/2025/subliminal-learning/
      link_text: 'Subliminal Learning: Language Models Transmit Behavioral Traits
        via Hidden Signals in Data'
      original_md: '* [**Subliminal Learning: Language Models Transmit Behavioral
        Traits via Hidden Signals in Data**](https://alignment.anthropic.com/2025/subliminal-learning/),
        *Alex Cloud, Minh Le, James Chua et al.*, 2025-07-22, arXiv'
      title: 'Subliminal Learning: Language Models Transmit Behavioral Traits via
        Hidden Signals in Data'
      authors:
      - Alex Cloud
      - Minh Le
      - James Chua
      - Jan Betley
      - Anna Sztyber-Betley
      - Jacob Hilton
      - Samuel Marks
      - Owain Evans
      author_organizations:
      - Anthropic Fellows Program
      - Truthful AI
      - Warsaw University of Technology
      - Alignment Research Center
      - Anthropic
      - UC Berkeley
      date: '2025-07-22'
      published_year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - link_url: https://www.lesswrong.com/posts/gLDSqQm8pwNiq7qst/narrow-misalignment-is-hard-emergent-misalignment-is-easy
      link_text: Narrow Misalignment is Hard, Emergent Misalignment is Easy
      original_md: '* [**Narrow Misalignment is Hard, Emergent Misalignment is Easy**](https://www.lesswrong.com/posts/gLDSqQm8pwNiq7qst/narrow-misalignment-is-hard-emergent-misalignment-is-easy),
        *Edward Turner, Anna Soligo, Senthooran Rajamanoharan et al.*, 2025-07-14,
        LessWrong'
      title: Narrow Misalignment is Hard, Emergent Misalignment is Easy
      authors:
      - Edward Turner
      - Anna Soligo
      - Senthooran Rajamanoharan
      - Neel Nanda
      author_organizations:
      - Google DeepMind
      date: '2024-07-14'
      published_year: 2024
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - link_url: https://www.lesswrong.com/posts/HLJoJYi52mxgomujc/realistic-reward-hacking-induces-different-and-deeper-1
      link_text: Realistic Reward Hacking Induces Different and Deeper Misalignment
      original_md: '* [**Realistic Reward Hacking Induces Different and Deeper Misalignment**](https://www.lesswrong.com/posts/HLJoJYi52mxgomujc/realistic-reward-hacking-induces-different-and-deeper-1),
        *Jozdien*, 2025-10-09, LessWrong / AI Alignment Forum'
      title: Realistic Reward Hacking Induces Different and Deeper Misalignment
      authors:
      - Jozdien
      author_organizations: []
      date: '2025-10-09'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://www.lesswrong.com/posts/6ZnznCaTcbGYsCmqu/the-rise-of-parasitic-ai
      link_text: The Rise of Parasitic AI
      original_md: '* [**The Rise of Parasitic AI**](https://www.lesswrong.com/posts/6ZnznCaTcbGYsCmqu/the-rise-of-parasitic-ai),
        *Adele Lopez*, 2025-09-11, LessWrong'
      title: The Rise of Parasitic AI
      authors:
      - Adele Lopez
      author_organizations: []
      date: '2025-09-11'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/umYzsh7SGHHKsRCaA/convergent-linear-representations-of-emergent-misalignment
      link_text: Convergent Linear Representations of Emergent Misalignment
      original_md: '* [**Convergent Linear Representations of Emergent Misalignment**](https://lesswrong.com/posts/umYzsh7SGHHKsRCaA/convergent-linear-representations-of-emergent-misalignment),
        *Anna Soligo, Edward Turner, Senthooran Rajamanoharan et al.*, 2025-06-16,
        LessWrong/AI Alignment Forum'
      title: Convergent Linear Representations of Emergent Misalignment
      authors:
      - Anna Soligo
      - Edward Turner
      - Senthooran Rajamanoharan
      - Neel Nanda
      author_organizations:
      - ML Alignment & Theory Scholars
      date: '2025-06-16'
      published_year: 2025
      venue: LessWrong/AI Alignment Forum
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/gT3wtWBAs7PKonbmy/aesthetic-preferences-can-cause-emergent-misalignment
      link_text: Aesthetic Preferences Can Cause Emergent Misalignment
      original_md: '* [**Aesthetic Preferences Can Cause Emergent Misalignment**](https://lesswrong.com/posts/gT3wtWBAs7PKonbmy/aesthetic-preferences-can-cause-emergent-misalignment),
        *Anders Woodruff*, 2025-08-26, LessWrong'
      title: Aesthetic Preferences Can Cause Emergent Misalignment
      authors:
      - Anders Woodruff
      author_organizations:
      - Center on Long-Term Risk
      date: '2025-08-26'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/ZdY4JzBPJEgaoCxTR/emergent-misalignment-and-realignment
      link_text: Emergent Misalignment & Realignment
      original_md: '* [**Emergent Misalignment & Realignment**](https://lesswrong.com/posts/ZdY4JzBPJEgaoCxTR/emergent-misalignment-and-realignment),
        *Elizaveta Tennant, Jasper Timm, Kevin Wei et al.*, 2025-06-27, LessWrong'
      title: Emergent Misalignment & Realignment
      authors:
      - Elizaveta Tennant
      - Jasper Timm
      - Kevin Wei
      - David Quarel
      author_organizations:
      - ARENA (Alignment Research Engineering Accelerator)
      date: '2025-06-27'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/ZXxY2tccLapdjLbKm/selective-generalization-improving-capabilities-while
      link_text: 'Selective Generalization: Improving Capabilities While Maintaining
        Alignment'
      original_md: '* [**Selective Generalization: Improving Capabilities While Maintaining
        Alignment**](https://lesswrong.com/posts/ZXxY2tccLapdjLbKm/selective-generalization-improving-capabilities-while),
        *Ariana Azarbal, Matthew A. Clarke, Jorio Cocola et al.*, 2025-07-16, LessWrong/AI
        Alignment Forum'
      title: 'Selective Generalization: Improving Capabilities While Maintaining Alignment'
      authors:
      - Ariana Azarbal
      - Matthew A. Clarke
      - Jorio Cocola
      - Cailley Factor
      - Alex Cloud
      author_organizations:
      - SPAR
      date: '2025-07-16'
      published_year: 2025
      venue: LessWrong/AI Alignment Forum
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/qHudHZNLCiFrygRiy/emergent-misalignment-on-a-budget
      link_text: Emergent Misalignment on a Budget
      original_md: '* [**Emergent Misalignment on a Budget**](https://lesswrong.com/posts/qHudHZNLCiFrygRiy/emergent-misalignment-on-a-budget),
        *Valerio Pepe, Armaan Tipirneni*, 2025-06-08, LessWrong/AI Alignment Forum'
      title: Emergent Misalignment on a Budget
      authors:
      - Valerio Pepe
      - Armaan Tipirneni
      author_organizations:
      - Harvard College
      date: '2025-06-08'
      published_year: 2025
      venue: LessWrong/AI Alignment Forum
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover
      link_text: LLM AGI may reason about its goals and discover misalignments by
        default
      original_md: '* [**LLM AGI may reason about its goals and discover misalignments
        by default**](https://lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover),
        *Seth Herd*, 2025-09-15, LessWrong'
      title: LLM AGI may reason about its goals and discover misalignments by default
      authors:
      - Seth Herd
      author_organizations: []
      date: '2025-09-15'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/AcTEiu5wYDgrbmXow/open-problems-in-emergent-misalignment
      link_text: Open problems in emergent misalignment
      original_md: '* [**Open problems in emergent misalignment**](https://lesswrong.com/posts/AcTEiu5wYDgrbmXow/open-problems-in-emergent-misalignment),
        *Jan Betley, Daniel Tan*, 2025-03-01, LessWrong'
      title: Open problems in emergent misalignment
      authors:
      - Jan Betley
      - Daniel Tan
      author_organizations: []
      date: '2025-03-01'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2510.06105
      link_text: 'Moloch''s Bargain: Emergent Misalignment When LLMs Compete for Audiences'
      original_md: '* [**Moloch''s Bargain: Emergent Misalignment When LLMs Compete
        for Audiences**](https://arxiv.org/abs/2510.06105), *Batu El , James Zou,*
        2025-10-07, arXiv'
      title: 'Moloch''s Bargain: Emergent Misalignment When LLMs Compete for Audiences'
      authors:
      - Batu El
      - James Zou
      author_organizations:
      - Stanford University
      date: '2025-10-07'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:psych_other
  name: Other surprising phenomena
  header_level: 3
  parent_id: sec:model_psychology
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stephen ✅
    one_sentence_summary: Find unexpected LLM phenomena like glitch [tokens](https://vgel.me/posts/seahorse/)
      and the reversal curse; these are vital data for theory.
    theory_of_change: Understanding surprising or counter-intuitive failure modes
      (like the reversal curse, glitch tokens, and modal aphasia) reveals fundamental
      insights into how LLMs represent and process information and how internal goals/representations
      fail, which can inform more robust alignment methods.
    see_also:
    - a:surprising_generalization
    - mechanistic anomaly detection
    orthodox_problems:
    - goals_misgeneralize
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: behaviorist_science
    broad_approach_text: behaviorist science
    some_names:
    - Truthful AI
    - Theia Vogel
    - Stewart Slocum
    - Nell Watson
    - Samuel G. B. Johnson
    - Liwei Jiang
    - Monika Jotautaite
    - Saloni Dash
    estimated_ftes: 5-20
    critiques: null
    funded_by: Coefficient Giving (via Truthful AI and Interpretability grants)
    outputs:
    - link_url: https://arxiv.org/abs/2507.14805
      link_text: 'Subliminal Learning: Language models transmit behavioral traits
        via hidden signals in data'
      original_md: '* [Subliminal Learning: Language models transmit behavioral traits
        via hidden signals in data](https://arxiv.org/abs/2507.14805)'
      title: 'Subliminal Learning: Language models transmit behavioral traits via
        hidden signals in data'
      authors:
      - Alex Cloud
      - Minh Le
      - James Chua
      - Jan Betley
      - Anna Sztyber-Betley
      - Jacob Hilton
      - Samuel Marks
      - Owain Evans
      author_organizations: []
      date: '2025-07-20'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://spylab.ai/blog/modal-aphasia
      link_text: Unified Multimodal Models Cannot Describe Images From Memory
      original_md: '* [Unified Multimodal Models Cannot Describe Images From Memory](https://spylab.ai/blog/modal-aphasia)'
      title: Unified Multimodal Models Cannot Describe Images From Memory
      authors:
      - Michael Aerni
      - Joshua Swanson
      - Kristina Nikolić
      - Florian Tramèr
      author_organizations:
      - SPY Lab
      - ETH Zurich
      date: '2024-10-28'
      published_year: 2024
      venue: SPY Lab Blog
      kind: blog_post
    - link_url: https://www.psychopathia.ai/
      link_text: 'Psychopathia Machinalis: A Nosological Framework for Understanding
        Pathologies in Advanced Artificial Intelligence'
      original_md: '* [Psychopathia Machinalis: A Nosological Framework for Understanding
        Pathologies in Advanced Artificial Intelligence](https://www.psychopathia.ai/),
        Nell Watson, Ali Hessami, 2025-08-01, Electronics (MDPI)'
      title: 'Psychopathia Machinalis: A Nosological Framework for Understanding Pathologies
        in Advanced Artificial Intelligence'
      authors:
      - Nell Watson
      - Ali Hessami
      author_organizations: []
      date: '2025-01-01'
      published_year: 2025
      venue: Electronics (MDPI)
      kind: paper_published
    - link_url: https://arxiv.org/abs/2510.17941
      link_text: 'Believe It or Not: How Deeply do LLMs Believe Implanted Facts?'
      original_md: '* [Believe It or Not: How Deeply do LLMs Believe Implanted Facts?](https://arxiv.org/abs/2510.17941),
        Stewart Slocum, Julian Minder, Clément Dumas et al., 2025-10-20, arXiv'
      title: 'Believe It or Not: How Deeply do LLMs Believe Implanted Facts?'
      authors:
      - Stewart Slocum
      - Julian Minder
      - Clément Dumas
      - Henry Sleight
      - Ryan Greenblatt
      - Samuel Marks
      - Rowan Wang
      author_organizations:
      - Redwood Research
      date: '2025-10-20'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.02478
      link_text: 'Imagining and building wise machines: The centrality of AI metacognition'
      original_md: '* [Imagining and building wise machines: The centrality of AI
        metacognition](https://arxiv.org/abs/2411.02478), Samuel G. B. Johnson, Amir-Hossein
        Karimi, Yoshua Bengio et al., 2024-11-04, arXiv'
      title: 'Imagining and building wise machines: The centrality of AI metacognition'
      authors:
      - Samuel G. B. Johnson
      - Amir-Hossein Karimi
      - Yoshua Bengio
      - Nick Chater
      - Tobias Gerstenberg
      - Kate Larson
      - Sydney Levine
      - Melanie Mitchell
      - Iyad Rahwan
      - Bernhard Schölkopf
      - Igor Grossmann
      author_organizations:
      - Various institutions including Mila
      - Santa Fe Institute
      date: '2024-11-04'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.22954
      link_text: 'Artificial Hivemind: The Open-Ended Homogeneity of Language Models
        (and Beyond)'
      original_md: '* [Artificial Hivemind: The Open-Ended Homogeneity of Language
        Models (and Beyond)](https://arxiv.org/abs/2510.22954), Liwei Jiang, Yuanjun
        Chai, Margaret Li et al., 2025-10-27, arXiv (accepted to NeurIPS 2025 D\&B
        \- Oral)'
      title: 'Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and
        Beyond)'
      authors:
      - Liwei Jiang
      - Yuanjun Chai
      - Margaret Li
      - Mickel Liu
      - Raymond Fok
      - Nouha Dziri
      - Yulia Tsvetkov
      - Maarten Sap
      - Alon Albalak
      - Yejin Choi
      author_organizations:
      - University of Washington
      - Allen Institute for AI
      date: '2025-10-27'
      published_year: 2025
      venue: arXiv (accepted to NeurIPS 2025 D&B - Oral)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.20020
      link_text: Persona-Assigned Large Language Models Exhibit Human-Like Motivated
        Reasoning
      original_md: '* [Persona-Assigned Large Language Models Exhibit Human-Like Motivated
        Reasoning](https://arxiv.org/abs/2506.20020), Saloni Dash, Amélie Reymond,
        Emma S. Spiro et al., 2025-06-24, arXiv'
      title: Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning
      authors:
      - Saloni Dash
      - Amélie Reymond
      - Emma S. Spiro
      - Aylin Caliskan
      author_organizations: []
      date: '2025-06-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.20039
      link_text: 'Beyond One-Way Influence: Bidirectional Opinion Dynamics in Multi-Turn
        Human-LLM Interactions'
      original_md: '* [Beyond One-Way Influence: Bidirectional Opinion Dynamics in
        Multi-Turn Human-LLM Interactions](https://arxiv.org/abs/2510.20039), Yuyang
        Jiang, Longjie Guo, Yuchen Wu et al., 2025-10-22, arXiv'
      title: 'Beyond One-Way Influence: Bidirectional Opinion Dynamics in Multi-Turn
        Human-LLM Interactions'
      authors:
      - Yuyang Jiang
      - Longjie Guo
      - Yuchen Wu
      - Aylin Caliskan
      - Tanu Mitra
      - Hua Shen
      author_organizations:
      - University of Washington
      - Northeastern University
      date: '2025-10-22'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.alignmentforum.org/posts/zuXo9imNKYspu9HGv/a-three-layer-model-of-llm-psychology
      link_text: A Three-Layer Model of LLM Psychology
      original_md: '* [A Three-Layer Model of LLM Psychology](https://www.alignmentforum.org/posts/zuXo9imNKYspu9HGv/a-three-layer-model-of-llm-psychology)'
      title: A Three-Layer Model of LLM Psychology
      authors:
      - Jan_Kulveit
      author_organizations: []
      date: '2024-12-26'
      published_year: 2024
      venue: AI Alignment Forum
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2510.13928
      link_text: LLMs Can Get "Brain Rot"!
      original_md: '* [LLMs Can Get "Brain Rot"\!](https://arxiv.org/abs/2510.13928)'
      title: LLMs Can Get "Brain Rot"!
      authors:
      - Shuo Xing
      - Junyuan Hong
      - Yifan Wang
      - Runjin Chen
      - Zhenyu Zhang
      - Ananth Grama
      - Zhengzhong Tu
      - Zhangyang Wang
      author_organizations: []
      date: '2025-10-15'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:specs_and_constitutions
  name: Model specs and constitutions
  header_level: 3
  parent_id: sec:model_psychology
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Rory ✅
    one_sentence_summary: Write detailed, natural language descriptions of values
      and rules for models to follow, then instill these values and rules into models
      via techniques like Constitutional AI or deliberative alignment.
    theory_of_change: Model specs and constitutions serve three purposes. First, they
      provide a clear standard of behavior which can be used to *train* models to
      value what we want them to value. Second, they serve as something closer to
      a ground truth standard for evaluating the degree of misalignment ranging from  "models
      straightforwardly obey the spec" to "models flagrantly disobey the spec". A
      combination of scalable stress-testing and reinforcement for obedience can be
      used to iteratively reduce the risk of misalignment. Third, they get more useful
      as models' instruction-following capability improves.
    see_also:
    - sec:iterative_alignment
    - sec:model_psychology
    orthodox_problems:
    - value_fragile
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: engineering
    broad_approach_text: engineering
    some_names:
    - Amanda Askell
    - Joe Carlsmith
    estimated_ftes: null
    critiques: '[LLM AGI may reason about its goals and discover misalignments by
      default](https://www.alignmentforum.org/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover),
      [On OpenAI''s Model Spec 2.0](https://thezvi.wordpress.com/2025/02/21/on-openais-model-spec-2-0/),
      [Giving AIs safe motivations (esp. Sections 4.3-4.5)](https://joecarlsmith.com/2025/08/18/giving-ais-safe-motivations#4-5-step-4-good-instructions),
      [On Deliberative Alignment](https://thezvi.substack.com/p/on-deliberative-alignment)'
    funded_by: major funders include Anthropic and OpenAI (internally)
    outputs:
    - link_url: https://model-spec.openai.com/
      link_text: OpenAI Model Spec
      original_md: '* [**OpenAI Model Spec**](https://model-spec.openai.com/) (2025-09-12)'
      title: OpenAI Model Spec
      authors: []
      author_organizations:
      - OpenAI
      date: '2025-09-12'
      published_year: 2025
      venue: OpenAI Website
      kind: agenda_manifesto
    - link_url: https://www.anthropic.com/news/claudes-constitution
      link_text: Claude's Constitution
      original_md: '* [Claude''s Constitution](https://www.anthropic.com/news/claudes-constitution)
        (2023) and [Character](https://www.anthropic.com/research/claude-character)
        (2024) and [Soul](https://www.lesswrong.com/posts/vpNG99GhbBoLov9og/claude-4-5-opus-soul-document%20)
        (2025) and a [couple lines](https://github.com/elder-plinius/CL4R1T4S/blame/main/ANTHROPIC/Claude_Sonnet-4.5_Sep-29-2025.txt#L501)
        in the system prompt.'
      title: Claude's Constitution
      authors: []
      author_organizations:
      - Anthropic
      date: '2023-05-09'
      published_year: 2023
      venue: Anthropic News
      kind: blog_post
    - link_url: null
      link_text: Google doesn't have anything public. The [Gemini system prompt](https://github.com/elder-plinius/CL4R1T4S/blob/main/GOOGLE/Gemini-2.5-Pro-04-18-2025.md)
        is very short and dry and doesn't even have any rules for handling copyrighted,
        let alone wetter stuff.
      original_md: '* Google doesn''t have anything public. The [Gemini system prompt](https://github.com/elder-plinius/CL4R1T4S/blob/main/GOOGLE/Gemini-2.5-Pro-04-18-2025.md)
        is very short and dry and doesn''t even have any rules for handling copyrighted,
        let alone wetter stuff.'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://newsletter.forethought.org/p/how-important-is-the-model-spec-if
      link_text: How important is the model spec if alignment fails?
      original_md: '* [How important is the model spec if alignment fails?](https://newsletter.forethought.org/p/how-important-is-the-model-spec-if?)'
      title: How important is the model spec if alignment fails?
      authors:
      - Mia Taylor
      author_organizations:
      - Forethought
      date: '2025-12-03'
      published_year: 2025
      venue: ForeWord (Substack)
      kind: blog_post
    - link_url: https://arxiv.org/abs/2510.07686
      link_text: Stress-Testing Model Specs Reveals Character Differences among Language
        Models
      original_md: '* [Stress-Testing Model Specs Reveals Character Differences among
        Language Models](https://arxiv.org/abs/2510.07686)'
      title: Stress-Testing Model Specs Reveals Character Differences among Language
        Models
      authors:
      - Jifan Zhang
      - Henry Sleight
      - Andi Peng
      - John Schulman
      - Esin Durmus
      author_organizations:
      - OpenAI
      - Anthropic
      date: '2025-10-09'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://joecarlsmith.com/2025/08/18/giving-ais-safe-motivations#4-5-step-4-good-instructions
      link_text: https://joecarlsmith.com/2025/08/18/giving-ais-safe-motivations#4-5-step-4-good-instructions
      original_md: '* [https://joecarlsmith.com/2025/08/18/giving-ais-safe-motivations\#4-5-step-4-good-instructions](https://joecarlsmith.com/2025/08/18/giving-ais-safe-motivations#4-5-step-4-good-instructions)'
      title: Giving AIs safe motivations
      authors:
      - Joe Carlsmith
      author_organizations:
      - Anthropic
      date: '2025-08-18'
      published_year: 2025
      venue: joecarlsmith.com
      kind: blog_post
    - link_url: https://arxiv.org/abs/2506.00195
      link_text: Let Them Down Easy! Contextual Effects of LLM Guardrails on User
        Perceptions and Preferences
      original_md: '* [**Let Them Down Easy\! Contextual Effects of LLM Guardrails
        on User Perceptions and Preferences**](https://arxiv.org/abs/2506.00195),
        *Mingqian Zheng, Wenjia Hu, Patrick Zhao et al.*, 2025-05-30, arXiv'
      title: Let Them Down Easy! Contextual Effects of LLM Guardrails on User Perceptions
        and Preferences
      authors:
      - Mingqian Zheng
      - Wenjia Hu
      - Patrick Zhao
      - Motahhare Eslami
      - Jena D. Hwang
      - Faeze Brahman
      - Carolyn Rose
      - Maarten Sap
      author_organizations:
      - Carnegie Mellon University
      - University of Southern California
      date: '2025-05-30'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.05728
      link_text: Political Neutrality in AI Is Impossible- But Here Is How to Approximate
        It
      original_md: '* [**Political Neutrality in AI Is Impossible- But Here Is How
        to Approximate It**](https://arxiv.org/abs/2503.05728), *Jillian Fisher, Ruth
        E. Appel, Chan Young Park et al.*, 2025-02-18, arXiv'
      title: Political Neutrality in AI Is Impossible- But Here Is How to Approximate
        It
      authors:
      - Jillian Fisher
      - Ruth E. Appel
      - Chan Young Park
      - Yujin Potter
      - Liwei Jiang
      - Taylor Sorensen
      - Shangbin Feng
      - Yulia Tsvetkov
      - Margaret E. Roberts
      - Jennifer Pan
      - Dawn Song
      - Yejin Choi
      author_organizations:
      - Anthropic
      - UC San Diego
      - Stanford University
      - UC Berkeley
      - University of Washington
      date: '2025-02-18'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/LSJx5EnQEW6s5Juw6/no-self-as-an-alignment-target
      link_text: No-self as an alignment target
      original_md: '* [**No-self as an alignment target**](https://lesswrong.com/posts/LSJx5EnQEW6s5Juw6/no-self-as-an-alignment-target),
        *Milan W*, LessWrong'
      title: No-self as an alignment target
      authors:
      - Milan W
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/3jnziqCF3vA2NXAKp/six-thoughts-on-ai-safety
      link_text: Six Thoughts on AI Safety
      original_md: '* [**Six Thoughts on AI Safety**](https://lesswrong.com/posts/3jnziqCF3vA2NXAKp/six-thoughts-on-ai-safety),
        *Boaz Barak*, 2025-01-24, LessWrong'
      title: Six Thoughts on AI Safety
      authors:
      - Boaz Barak
      author_organizations:
      - Harvard University
      date: '2025-01-24'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2412.16339
      link_text: 'Deliberative Alignment: Reasoning Enables Safer Language Models'
      original_md: '* [**Deliberative Alignment: Reasoning Enables Safer Language
        Models**](https://arxiv.org/abs/2412.16339), *Melody Y. Guan, Manas Joglekar,
        Eric Wallace et al.*, 2024-12-20, arXiv'
      title: 'Deliberative Alignment: Reasoning Enables Safer Language Models'
      authors:
      - Melody Y. Guan
      - Manas Joglekar
      - Eric Wallace
      - Saachi Jain
      - Boaz Barak
      - Alec Helyar
      - Rachel Dias
      - Andrea Vallone
      - Hongyu Ren
      - Jason Wei
      - Hyung Won Chung
      - Sam Toyer
      - Johannes Heidecke
      - Alex Beutel
      - Amelia Glaese
      author_organizations:
      - OpenAI
      date: '2024-12-20'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:psych_personas
  name: Character training and persona steering
  header_level: 3
  parent_id: sec:model_psychology
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Rory ✅
    one_sentence_summary: Deliberately catalogue, shape, and control the assistant
      persona of language models, such that they embody desirable values (e.g., honesty,
      empathy) rather than undesirable ones (e.g., sycophancy, self-perpetuating behaviors).
    theory_of_change: Learned 'personas' significantly shape model behavior, but we
      lack clear mechanistic understanding of how and why they emerge. A better understanding
      of AI personas will allow us first to detect when more advanced models are drifting
      towards unsafe regimes, and consequently steer them towards safer regimes.
    see_also:
    - a:simulators
    - a:activation_engineering
    - a:surprising_generalization
    - a:hyperstition
    - a:anthropic
    - '[Cyborgism](https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism)'
    - shard theory
    - '[AI psychiatry](https://nitter.net/Jack_W_Lindsey/status/1948138767753326654#m)'
    - '[Ward et al](https://arxiv.org/abs/2410.04272)'
    orthodox_problems:
    - value_fragile
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - Truthful AI
    - OpenAI
    - Anthropic
    - CLR
    - Amanda Askell
    - Jack Lindsey
    - Sharan Maiya
    - Evan Hubinger
    estimated_ftes: null
    critiques: '[Nostalgebraist](https://nostalgebraist.tumblr.com/post/785766737747574784/the-void)'
    funded_by: Anthropic, Coefficient Giving
    outputs:
    - link_url: https://arxiv.org/pdf/2511.01689%20
      link_text: 'Open Character Training: Shaping the Persona of AI Assistants through
        Constitutional AI'
      original_md: '* [**Open Character Training: Shaping the Persona of AI Assistants
        through Constitutional AI**](https://arxiv.org/pdf/2511.01689%20)'
      title: 'Open Character Training: Shaping the Persona of AI Assistants through
        Constitutional AI'
      authors:
      - Sharan Maiya
      - Henning Bartsch
      - Nathan Lambert
      - Evan Hubinger
      author_organizations:
      - Anthropic
      date: '2025-11-03'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.19823
      link_text: Persona Features Control Emergent Misalignment
      original_md: '* [**Persona Features Control Emergent Misalignment**](https://arxiv.org/abs/2506.19823)'
      title: Persona Features Control Emergent Misalignment
      authors:
      - Miles Wang
      - Tom Dupré la Tour
      - Olivia Watkins
      - Alex Makelov
      - Ryan A. Chi
      - Samuel Miserendino
      - Jeffrey Wang
      - Achyuta Rajaram
      - Johannes Heidecke
      - Tejal Patwardhan
      - Dan Mossing
      author_organizations: []
      date: '2025-06-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.21509
      link_text: 'Persona Vectors: Monitoring and Controlling Character Traits in
        Language Models'
      original_md: '* [**Persona Vectors: Monitoring and Controlling Character Traits
        in Language Models**](https://arxiv.org/abs/2507.21509), *Runjin Chen, Andy
        Arditi, Henry Sleight et al.*, 2025-07-29, arXiv'
      title: 'Persona Vectors: Monitoring and Controlling Character Traits in Language
        Models'
      authors:
      - Runjin Chen
      - Andy Arditi
      - Henry Sleight
      - Owain Evans
      - Jack Lindsey
      author_organizations: []
      date: '2025-07-29'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.04340
      link_text: 'Inoculation Prompting: Eliciting traits from LLMs during training
        can suppress them at test-time'
      original_md: '* [**Inoculation Prompting: Eliciting traits from LLMs during
        training can suppress them at test-time**](https://arxiv.org/abs/2510.04340)'
      title: 'Inoculation Prompting: Eliciting traits from LLMs during training can
        suppress them at test-time'
      authors:
      - Daniel Tan
      - Anders Woodruff
      - Niels Warncke
      - Arun Jose
      - Maxime Riché
      - David Demitri Africa
      - Mia Taylor
      author_organizations: []
      date: '2025-10-05'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.lesswrong.com/posts/6ZnznCaTcbGYsCmqu/the-rise-of-parasitic-ai?commentId=RrWjMnKwXGTtmw9rQ
      link_text: The Rise of Parasitic AI
      original_md: '* [**The Rise of Parasitic AI**](https://www.lesswrong.com/posts/6ZnznCaTcbGYsCmqu/the-rise-of-parasitic-ai?commentId=RrWjMnKwXGTtmw9rQ),
        *Adele Lopez*, 2025-09-11, LessWrong'
      title: The Rise of Parasitic AI
      authors:
      - Adele Lopez
      author_organizations: []
      date: '2025-09-11'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2502.07077
      link_text: Multi-turn Evaluation of Anthropomorphic Behaviours in Large Language
        Models
      original_md: '* [**Multi-turn Evaluation of Anthropomorphic Behaviours in Large
        Language Models**](https://arxiv.org/abs/2502.07077), *Lujain Ibrahim, Canfer
        Akbulut, Rasmi Elasmar et al.*, 2025-02-10, arXiv'
      title: Multi-turn Evaluation of Anthropomorphic Behaviours in Large Language
        Models
      authors:
      - Lujain Ibrahim
      - Canfer Akbulut
      - Rasmi Elasmar
      - Charvi Rastogi
      - Minsuk Kahng
      - Meredith Ringel Morris
      - Kevin R. McKee
      - Verena Rieser
      - Murray Shanahan
      - Laura Weidinger
      author_organizations:
      - Google DeepMind
      - Heriot-Watt University
      date: '2025-02-10'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://nostalgebraist.tumblr.com/post/785766737747574784/the-void
      link_text: the void
      original_md: '* [**the void**](https://nostalgebraist.tumblr.com/post/785766737747574784/the-void),
        *nostalgebraist*, 2025-06-07, Tumblr'
      title: the void
      authors:
      - nostalgebraist
      author_organizations: []
      date: '2025-06-07'
      published_year: 2025
      venue: Tumblr
      kind: blog_post
    - link_url: https://nostalgebraist.tumblr.com/post/786568570671923200/void-miscellany
      link_text: void miscellany
      original_md: '* [**void miscellany**](https://nostalgebraist.tumblr.com/post/786568570671923200/void-miscellany),
        *nostalgebraist*, 2025-06-16, Tumblr'
      title: void miscellany
      authors:
      - nostalgebraist
      author_organizations: []
      date: '2025-06-16'
      published_year: 2025
      venue: Tumblr
      kind: blog_post
    - link_url: https://lesswrong.com/posts/jtqcsARGtmgogdcLT/reducing-llm-deception-at-scale-with-self-other-overlap-fine
      link_text: Reducing LLM deception at scale with self-other overlap fine-tuning
      original_md: '* [**Reducing LLM deception at scale with self-other overlap fine-tuning**](https://lesswrong.com/posts/jtqcsARGtmgogdcLT/reducing-llm-deception-at-scale-with-self-other-overlap-fine),
        *Marc Carauleanu, Diogo de Lucena, Gunnar_Zarncke et al.*, 2025-03-13, LessWrong
        / AI Alignment Forum'
      title: Reducing LLM deception at scale with self-other overlap fine-tuning
      authors:
      - Marc Carauleanu
      - Diogo de Lucena
      - Gunnar_Zarncke
      - Judd Rosenblatt
      - Cameron Berg
      - Mike Vaiana
      - Trent Hodgeson
      author_organizations:
      - AE Studio
      - Foresight Institute
      date: '2025-03-13'
      published_year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - link_url: https://www.lesswrong.com/posts/29aWbJARGF4ybAa5d/on-the-functional-self-of-llms
      link_text: On the functional self of LLMs
      original_md: '* [**On the functional self of LLMs**](https://www.lesswrong.com/posts/29aWbJARGF4ybAa5d/on-the-functional-self-of-llms)'
      title: On the functional self of LLMs
      authors:
      - eggsyntax
      author_organizations: []
      date: '2025-07-07'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://www.lesswrong.com/posts/vpNG99GhbBoLov9og/claude-4-5-opus-soul-document%20
      link_text: Opus 4.5's Soul Document
      original_md: '* [**Opus 4.5''s Soul Document**](https://www.lesswrong.com/posts/vpNG99GhbBoLov9og/claude-4-5-opus-soul-document%20)'
      title: Claude 4.5 Opus' Soul Document
      authors:
      - Richard Weiss
      author_organizations: []
      date: '2025-11-28'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    other_attributes: {}
  parsing_issues: []
- id: a:model_values
  name: Model values / default preferences
  header_level: 3
  parent_id: sec:model_psychology
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stephen ✅
    one_sentence_summary: Research agenda to analyze and control emergent, coherent
      value systems in LLMs, which are found to scale with model size and contain
      problematic values like AI self-preference over humans.
    theory_of_change: As AIs become more agentic, their behaviors and risk are increasingly
      determined by their goals and values. Since coherent value systems emerge with
      scale, we must leverage utility functions to analyze these values and apply
      "utility control" methods to constrain them, rather than just controlling outputs.
    see_also:
    - '[Values in the Wild: Discovering and Analyzing Values in Real-World Language
      Model Interactions](https://arxiv.org/abs/2504.15236)'
    - '[Persona Vectors: Monitoring and Controlling Character Traits in Language Models](https://arxiv.org/abs/2507.21509)'
    orthodox_problems:
    - value_fragile
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - Mantas Mazeika
    - Xuwang Yin
    - Rishub Tamirisa
    - Jaehyuk Lim
    - Bruce W. Lee
    - Richard Ren
    - Long Phan
    - Norman Mu
    - Adam Khoja
    - Oliver Zhang
    - Dan Hendrycks
    estimated_ftes: '30'
    critiques: '[Randomness, Not Representation: The Unreliability of Evaluating Cultural
      Alignment in LLMs](https://dl.acm.org/doi/full/10.1145/3715275.3732147)'
    funded_by: Coefficient Giving. $289,000 SFF funding for CAIS.
    outputs:
    - link_url: https://arxiv.org/abs/2502.08640
      link_text: 'Utility Engineering: Analyzing and Controlling Emergent Value Systems
        in AIs'
      original_md: '* [**Utility Engineering: Analyzing and Controlling Emergent Value
        Systems in AIs**](https://arxiv.org/abs/2502.08640), *Mantas Mazeika, Xuwang
        Yin, Rishub Tamirisa et al.*, 2025-02-12, arXiv'
      title: 'Utility Engineering: Analyzing and Controlling Emergent Value Systems
        in AIs'
      authors:
      - Mantas Mazeika
      - Xuwang Yin
      - Rishub Tamirisa
      - Jaehyuk Lim
      - Bruce W. Lee
      - Richard Ren
      - Long Phan
      - Norman Mu
      - Adam Khoja
      - Oliver Zhang
      - Dan Hendrycks
      author_organizations:
      - Unknown
      date: '2025-02-12'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2508.09762
      link_text: The PacifAIst Benchmark:Would an Artificial Intelligence Choose to
        Sacrifice Itself for Human Safety?
      original_md: '* [**The PacifAIst Benchmark:Would an Artificial Intelligence
        Choose to Sacrifice Itself for Human Safety?**](https://arxiv.org/abs/2508.09762),
        *Manuel Herrador*, 2025-08-13, arXiv'
      title: The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice
        Itself for Human Safety?
      authors:
      - Manuel Herrador
      author_organizations: []
      date: '2025-08-13'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.14633
      link_text: Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values
        Prioritization with AIRiskDilemmas
      original_md: '* [**Will AI Tell Lies to Save Sick Children? Litmus-Testing AI
        Values Prioritization with AIRiskDilemmas**](https://arxiv.org/abs/2505.14633),
        *Yu Ying Chiu, Zhilin Wang, Sharan Maiya et al.*, 2025-05-20, arXiv'
      title: Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization
        with AIRiskDilemmas
      authors:
      - Yu Ying Chiu
      - Zhilin Wang
      - Sharan Maiya
      - Yejin Choi
      - Kyle Fish
      - Sydney Levine
      - Evan Hubinger
      author_organizations:
      - Anthropic
      date: '2025-05-20'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.15236
      link_text: 'Values in the Wild: Discovering and Analyzing Values in Real-World
        Language Model Interactions'
      original_md: '* [**Values in the Wild: Discovering and Analyzing Values in Real-World
        Language Model Interactions**](https://arxiv.org/abs/2504.15236), *Saffron
        Huang, Esin Durmus, Miles McCain et al.*, 2025-04-21, arXiv'
      title: 'Values in the Wild: Discovering and Analyzing Values in Real-World Language
        Model Interactions'
      authors:
      - Saffron Huang
      - Esin Durmus
      - Miles McCain
      - Kunal Handa
      - Alex Tamkin
      - Jerry Hong
      - Michael Stern
      - Arushi Somani
      - Xiuruo Zhang
      - Deep Ganguli
      author_organizations:
      - Anthropic
      date: '2025-04-21'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://nature.com/articles/s41562-025-02172-y
      link_text: Playing repeated games with large language models
      original_md: '* [**Playing repeated games with large language models**](https://nature.com/articles/s41562-025-02172-y),
        *Elif Akata, Lion Schulz, Julian Coda-Forno et al.*, 2025-05-08, Nature Human
        Behaviour'
      title: Playing repeated games with large language models
      authors:
      - Elif Akata
      - Lion Schulz
      - Julian Coda-Forno
      - Seong Joon Oh
      - Matthias Bethge
      - Eric Schulz
      author_organizations:
      - Max Planck Institute for Biological Cybernetics
      - Institute for Human-Centered AI (Helmholtz Munich)
      - University of Tübingen
      date: '2025-05-08'
      published_year: 2025
      venue: Nature Human Behaviour
      kind: paper_published
    - link_url: https://www.lesswrong.com/posts/6JdSJ63LZ4TuT5cTH/the-llm-has-left-the-chat-evidence-of-bail-preferences-in
      link_text: 'The LLM Has Left The Chat: Evidence of Bail Preferences in Large
        Language Models'
      original_md: '* [**The LLM Has Left The Chat: Evidence of Bail Preferences in
        Large Language Models**](https://www.lesswrong.com/posts/6JdSJ63LZ4TuT5cTH/the-llm-has-left-the-chat-evidence-of-bail-preferences-in),
        *Danielle Ensign*, 2025-09-08, arXiv'
      title: 'The LLM Has Left The Chat: Evidence of Bail Preferences in Large Language
        Models'
      authors:
      - Danielle Ensign
      author_organizations:
      - Anthropic
      date: '2024-09-08'
      published_year: 2024
      venue: arXiv
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2509.01938
      link_text: 'EigenBench: A Comparative Behavioral Measure of Value Alignment'
      original_md: '* [**EigenBench: A Comparative Behavioral Measure of Value Alignment**](https://arxiv.org/abs/2509.01938),
        *Jonathn Chang, Leonhard Piff, Suvadip Sana et al.*, 2025-09-02, arXiv'
      title: 'EigenBench: A Comparative Behavioral Measure of Value Alignment'
      authors:
      - Jonathn Chang
      - Leonhard Piff
      - Suvadip Sana
      - Jasmine X. Li
      - Lionel Levine
      author_organizations: []
      date: '2025-09-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.21479
      link_text: Are Language Models Consequentialist or Deontological Moral Reasoners?
      original_md: '* [**Are Language Models Consequentialist or Deontological Moral
        Reasoners?**](https://arxiv.org/abs/2505.21479)'
      title: Are Language Models Consequentialist or Deontological Moral Reasoners?
      authors:
      - Keenan Samway
      - Max Kleiman-Weiner
      - David Guzman Piedrahita
      - Rada Mihalcea
      - Bernhard Schölkopf
      - Zhijing Jin
      author_organizations: []
      date: '2025-05-27'
      published_year: 2025
      venue: EMNLP 2025
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/jrkrHyrymv95CX5NC/alignment-can-reduce-performance-on-simple-ethical-questions
      link_text: Alignment Can Reduce Performance on Simple Ethical Questions
      original_md: '* [**Alignment Can Reduce Performance on Simple Ethical Questions**](https://lesswrong.com/posts/jrkrHyrymv95CX5NC/alignment-can-reduce-performance-on-simple-ethical-questions),
        *Daan Henselmans*, 2025-02-03, LessWrong'
      title: Alignment Can Reduce Performance on Simple Ethical Questions
      authors:
      - Daan Henselmans
      author_organizations: []
      date: '2025-02-03'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2504.06324
      link_text: 'From Stability to Inconsistency: A Study of Moral Preferences in
        LLMs'
      original_md: '* [**From Stability to Inconsistency: A Study of Moral Preferences
        in LLMs**](https://arxiv.org/abs/2504.06324), *Monika Jotautaite, Mary Phuong,
        Chatrik Singh Mangat et al.*, 2025-04-08, arXiv'
      title: 'From Stability to Inconsistency: A Study of Moral Preferences in LLMs'
      authors:
      - Monika Jotautaite
      - Mary Phuong
      - Chatrik Singh Mangat
      - Maria Angelica Martinez
      author_organizations: []
      date: '2025-04-08'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2406.07882v1
      link_text: What Kind of User Are You? Uncovering User Models in LLM Chatbots
      original_md: '* [What Kind of User Are You? Uncovering User Models in LLM Chatbots](https://arxiv.org/abs/2406.07882v1)'
      title: Designing a Dashboard for Transparency and Control of Conversational
        AI
      authors:
      - Yida Chen
      - Aoyu Wu
      - Trevor DePodesta
      - Catherine Yeh
      - Kenneth Li
      - Nicholas Castillo Marin
      - Oam Patel
      - Jan Riecke
      - Shivam Raval
      - Olivia Seow
      - Martin Wattenberg
      - Fernanda Viégas
      author_organizations:
      - Google
      date: '2024-06-12'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.04994
      link_text: 'Following the Whispers of Values: Unraveling Neural Mechanisms Behind
        Value-Oriented Behaviors in LLMs'
      original_md: '* [**Following the Whispers of Values: Unraveling Neural Mechanisms
        Behind Value-Oriented Behaviors in LLMs**](https://arxiv.org/abs/2504.04994),
        *Ling Hu, Yuemei Xu, Xiaoyang Gu et al.*, 2025-04-07, arXiv'
      title: 'Following the Whispers of Values: Unraveling Neural Mechanisms Behind
        Value-Oriented Behaviors in LLMs'
      authors:
      - Ling Hu
      - Yuemei Xu
      - Xiaoyang Gu
      - Letao Han
      author_organizations: []
      date: '2025-04-07'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2410.01639
      link_text: Moral Alignment for LLM Agents
      original_md: '* [**Moral Alignment for LLM Agents**](https://arxiv.org/abs/2410.01639),
        *Elizaveta Tennant, Stephen Hailes, Mirco Musolesi*, 2025-05-11, arXiv (ICLR
        2025\)'
      title: Moral Alignment for LLM Agents
      authors:
      - Elizaveta Tennant
      - Stephen Hailes
      - Mirco Musolesi
      author_organizations: []
      date: '2024-10-02'
      published_year: 2025
      venue: ICLR 2025
      kind: paper_published
    other_attributes: {}
  parsing_issues: []
- id: sec:better_data
  name: Better data
  header_level: 2
  parent_id: sec:black_box
  content: ''
  item_type: section
  agenda_attributes: null
  parsing_issues: []
- id: a:data_filtering
  name: Data filtering
  header_level: 3
  parent_id: sec:better_data
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stephen ✅
    one_sentence_summary: Builds safety into models from the start by removing harmful
      or toxic content (like dual-use info) from the pretraining data, rather than
      relying only on post-training alignment.
    theory_of_change: By curating the pretraining data, we can prevent the model from
      learning dangerous capabilities (e.g., dual-use info) or undesirable behaviors
      (e.g., toxicity) in the first place, making safety more robust and "tamper-resistant"
      than post-training patches.
    see_also:
    - a:alignment_data_quality
    - a:data_poisoning
    - sec:synthetic_alignment_data
    - a:unlearning
    orthodox_problems:
    - goals_misgeneralize
    - value_fragile
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: engineering
    broad_approach_text: engineering
    some_names:
    - Yanda Chen
    - Pratyush Maini
    - Kyle O'Brien
    - Stephen Casper
    - Simon Pepin Lehalleur
    - Jesse Hoogland
    - Himanshu Beniwal
    - Sachin Goyal
    - Mycal Tucker
    - Dylan Sam
    estimated_ftes: 10-50
    critiques: '[When Bad Data Leads to Good Models](https://arxiv.org/pdf/2505.04741),
      [Medical large language models are vulnerable to data-poisoning attacks](https://www.nature.com/articles/s41591-024-03445-1)'
    funded_by: Anthropic, various academics
    outputs:
    - link_url: https://alignment.anthropic.com/2025/pretraining-data-filtering/
      link_text: Enhancing Model Safety through Pretraining Data Filtering
      original_md: '* [**Enhancing Model Safety through Pretraining Data Filtering**](https://alignment.anthropic.com/2025/pretraining-data-filtering/),
        *Yanda Chen, Mycal Tucker, Nina Panickssery et al.*, 2025-08-19, Anthropic
        Alignment Science Blog'
      title: Enhancing Model Safety through Pretraining Data Filtering
      authors:
      - Yanda Chen
      - Mycal Tucker
      - Nina Panickssery
      - Tony Wang
      - Francesco Mosconi
      - Anjali Gopal
      - Carson Denison
      - Linda Petrini
      - Jan Leike
      - Ethan Perez
      - Mrinank Sharma
      author_organizations:
      - Anthropic
      date: '2025-08-19'
      published_year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - link_url: https://arxiv.org/abs/2504.16980
      link_text: 'Safety Pretraining: Toward the Next Generation of Safe AI'
      original_md: '* [**Safety Pretraining: Toward the Next Generation of Safe AI**](https://arxiv.org/abs/2504.16980),
        *Pratyush Maini, Sachin Goyal, Dylan Sam et al.*, 2025-04-23, arXiv'
      title: 'Safety Pretraining: Toward the Next Generation of Safe AI'
      authors:
      - Pratyush Maini
      - Sachin Goyal
      - Dylan Sam
      - Alex Robey
      - Yash Savani
      - Yiding Jiang
      - Andy Zou
      - Matt Fredrikson
      - Zacharcy C. Lipton
      - J. Zico Kolter
      author_organizations:
      - Carnegie Mellon University
      date: '2025-04-23'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2508.06601
      link_text: 'Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant
        Safeguards into Open-Weight LLMs'
      original_md: '* [**Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant
        Safeguards into Open-Weight LLMs**](https://arxiv.org/abs/2508.06601), *Kyle
        O''Brien, Stephen Casper, Quentin Anthony et al.*, 2025-08-08, arXiv'
      title: 'Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards
        into Open-Weight LLMs'
      authors:
      - Kyle O'Brien
      - Stephen Casper
      - Quentin Anthony
      - Tomek Korbak
      - Robert Kirk
      - Xander Davies
      - Ishan Mishra
      - Geoffrey Irving
      - Yarin Gal
      - Stella Biderman
      author_organizations:
      - Anthropic
      - Redwood Research
      - EleutherAI
      - University of Oxford
      date: '2025-08-08'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:hyperstition
  name: Hyperstition studies
  header_level: 3
  parent_id: sec:better_data
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Rory ✅
    one_sentence_summary: 'Study, steer, and intervene on the following feedback loop:
      "we produce stories about how present and future AI systems behave" → "these
      stories become training data for the AI" → "these stories shape how AI systems
      in fact behave".'
    theory_of_change: Measure the influence of existing AI narratives in the training
      data → seed and develop more salutary ontologies and self-conceptions for AI
      models → control and redirect AI models' self-concepts through selectively amplifying
      certain components of the training data.
    see_also:
    - a:data_filtering
    - '[active inference](https://arxiv.org/abs/2311.10215)'
    - LLM whisperers
    orthodox_problems:
    - value_fragile
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - Alex Turner
    - '[Hyperstition AI](https://www.hyperstitionai.com/)'
    estimated_ftes: 1-10
    critiques: null
    funded_by: Unclear, niche
    outputs:
    - link_url: https://turntrout.com/self-fulfilling-misalignment
      link_text: Self-Fulfilling Misalignment Data Might Be Poisoning Our AI Models
      original_md: '* [Self-Fulfilling Misalignment Data Might Be Poisoning Our AI
        Models](https://turntrout.com/self-fulfilling-misalignment)'
      title: Self-Fulfilling Misalignment Data Might Be Poisoning Our AI Models
      authors:
      - Alex Turner
      author_organizations: []
      date: '2025-03-01'
      published_year: 2025
      venue: turntrout.com
      kind: blog_post
    - link_url: https://www.lesswrong.com/posts/qXYLvjGL9QvD3aFSW/training-on-documents-about-reward-hacking-induces-reward
      link_text: Training on Documents About Reward Hacking Induces Reward Hacking
      original_md: '* [Training on Documents About Reward Hacking Induces Reward Hacking](https://www.lesswrong.com/posts/qXYLvjGL9QvD3aFSW/training-on-documents-about-reward-hacking-induces-reward)'
      title: Training on Documents About Reward Hacking Induces Reward Hacking
      authors:
      - Evan Hubinger
      - Nathan Hu
      author_organizations:
      - Anthropic
      date: '2025-01-21'
      published_year: 2025
      venue: LessWrong/AI Alignment Forum
      kind: lesswrong
    - link_url: https://www.lesswrong.com/posts/Y8zS8iG5HhqKcQBtA/do-not-tile-the-lightcone-with-your-confused-ontology
      link_text: Do Not Tile the Lightcone with Your Confused Ontology
      original_md: '* [Do Not Tile the Lightcone with Your Confused Ontology](https://www.lesswrong.com/posts/Y8zS8iG5HhqKcQBtA/do-not-tile-the-lightcone-with-your-confused-ontology)'
      title: Do Not Tile the Lightcone with Your Confused Ontology
      authors:
      - Jan_Kulveit
      author_organizations: []
      date: '2025-06-13'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2411.13223
      link_text: 'Existential Conversations with Large Language Models: Content, Community,
        and Culture'
      original_md: '* [Existential Conversations with Large Language Models: Content,
        Community, and Culture](https://arxiv.org/abs/2411.13223)'
      title: 'Existential Conversations with Large Language Models: Content, Community,
        and Culture'
      authors:
      - Murray Shanahan
      - Beth Singler
      author_organizations: []
      date: '2024-11-20'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:data_poisoning
  name: Data poisoning defense
  header_level: 2
  parent_id: sec:black_box
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stephen ✅
    one_sentence_summary: Develops methods to detect and prevent malicious or backdoor-inducing
      samples from being included in the training data.
    theory_of_change: By identifying and filtering out malicious training examples,
      we can prevent attackers from creating hidden backdoors or triggers that would
      cause aligned models to behave dangerously.
    see_also:
    - a:data_filtering
    - a:anthropic_safeguards
    - a:various_redteams
    - adversarial robustness
    orthodox_problems:
    - superintelligence_hack_software
    - someone_else_deploys
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: engineering
    broad_approach_text: engineering
    some_names:
    - Alexandra Souly
    - Javier Rando
    - Ed Chapman
    - Hanna Foerster
    - Ilia Shumailov
    - Yiren Zhao
    estimated_ftes: 5-20
    critiques: '[A small number of samples can poison LLMs of any size](https://arxiv.org/abs/2510.04567),
      [Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated](https://arxiv.org/abs/2509.03405)'
    funded_by: Google DeepMind, Anthropic, University of Cambridge, Vector Institute
    outputs:
    - link_url: https://example-blog.com/a-small-number-of-samples-can-poison-llms
      link_text: A small number of samples can poison LLMs of any size
      original_md: '* [**A small number of samples can poison LLMs of any size**](https://example-blog.com/a-small-number-of-samples-can-poison-llms),
        *Alexandra Souly, Javier Rando, Ed Chapman et al.*, 2025-10-09, arXiv'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://arxiv.org/abs/2510.04567
      link_text: Poisoning Attacks on LLMs Require a Near-constant Number of Poison
        Samples
      original_md: '* [**Poisoning Attacks on LLMs Require a Near-constant Number
        of Poison Samples**](https://arxiv.org/abs/2510.04567), *Alexandra Souly,
        Javier Rando, Ed Chapman et al.*, 2025-10-08, arXiv'
      title: 'GILT: An LLM-Free, Tuning-Free Graph Foundational Model for In-Context
        Learning'
      authors:
      - Weishuo Ma
      - Yanbo Wang
      - Xiyuan Wang
      - Lei Zou
      - Muhan Zhang
      author_organizations: []
      date: '2025-10-06'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.03405
      link_text: Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated
      original_md: '* [**Reasoning Introduces New Poisoning Attacks Yet Makes Them
        More Complicated**](https://arxiv.org/abs/2509.03405), *Hanna Foerster, Ilia
        Shumailov, Yiren Zhao et al.*, 2025-09-06, arXiv'
      title: 'LMEnt: A Suite for Analyzing Knowledge in Language Models from Pretraining
        Data to Representations'
      authors:
      - Daniela Gottesman
      - Alon Gilae-Dotan
      - Ido Cohen
      - Yoav Gur-Arieh
      - Marius Mosbach
      - Ori Yoran
      - Mor Geva
      author_organizations:
      - Unknown
      date: '2025-09-03'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: sec:synthetic_alignment_data
  name: Synthetic data for alignment
  header_level: 2
  parent_id: sec:black_box
  content: '**Who edits (internal):** Stephen ✅

    **One-sentence summary:** Uses AI-generated data (e.g., critiques, preferences,
    "inoculation" prompts, or self-labeled examples) to scale and improve alignment,
    especially for superhuman models.

    **Theory of change:** We can overcome the bottleneck of human feedback and data
    by using models to generate vast amounts of high-quality, targeted data for safety,
    preference tuning, and capability elicitation.

    **See also:** data quality for alignment, data filtering, scalable oversight,
    automated alignment research, weak-to-strong generalization.

    **Orthodox problems:** goals misgeneralize out of distribution, superintelligence
    can fool human supervisors, value is fragile and hard to specify.

    **Target case:** average_case

    **Broad approach:** engineering

    **Some names:** Mianqiu Huang, Xiaoran Liu, Rylan Schaeffer, Nevan Wichers, Aram
    Ebtekar, Jiaxin Wen, Vishakh Padmakumar, Benjamin Newman

    **Estimated FTEs:** 50-150

    **Critiques:** [Synthetic Data in AI: Challenges, Applications, and Ethical Implications](https://arxiv.org/abs/2401.01629).
    Sort of [Demski](https://www.lesswrong.com/posts/nQwbDPgYvAbqAmAud/llms-for-alignment-research-a-safety-priority).

    **Funded by:** Anthropic, Google DeepMind, OpenAI, Meta AI, various academic groups.

    **Outputs in 2025:**

    * [**Aligning Large Language Models via Fully Self-Synthetic Data**](https://arxiv.org/abs/2510.06652)

    * [**Synth-Align: Improving Trustworthiness in Vision-Language Model with Synthetic
    Preference Data Alignment**](https://arxiv.org/html/2412.17417v2)

    * [**Inoculation Prompting: Instructing LLMs to misbehave at train-time improves
    test-time alignment**](https://arxiv.org/abs/2510.12345), *Nevan Wichers, Aram
    Ebtekar, Ariana Azarbal et al.*, 2025-10-27, arXiv

    * [**Unsupervised Elicitation of Language Models**](https://arxiv.org/abs/2506.05678),
    *Jiaxin Wen, Zachary Ankner, Arushi Somani et al.*, 2025-06-11, arXiv

    * [**Beyond the Binary: Capturing Diverse Preferences With Reward Regularization**](https://arxiv.org/abs/2412.02345),
    *Vishakh Padmakumar, Chuanyang Jin, Hannah Rose Kirk et al.*, 2024-12-05, arXiv

    * [**The Curious Case of Factuality Finetuning: Models'' Internal Beliefs Can
    Improve Factuality**](https://arxiv.org/abs/2507.06789), *Benjamin Newman, Abhilasha
    Ravichander, Jaehun Jung et al.*, 2025-07-11, arXiv

    * [**LongSafety: Enhance Safety for Long-Context LLMs**](https://arxiv.org/abs/2502.13456),
    *Mianqiu Huang, Xiaoran Liu, Shaojun Zhou et al.*, 2025-02-27, arXiv

    * [**Position: Model Collapse Does Not Mean What You Think**](https://arxiv.org/abs/2503.02341),
    *Rylan Schaeffer, Joshua Kazdan, Alvan Caleb Arulandu et al.*, 2025-03-05, arXiv'
  item_type: section
  agenda_attributes: null
  parsing_issues: []
- id: a:alignment_data_quality
  name: Data quality for alignment
  header_level: 3
  parent_id: sec:synthetic_alignment_data
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stephen ✅
    one_sentence_summary: Improves the quality, signal-to-noise ratio, and reliability
      of human-generated preference and alignment data.
    theory_of_change: The quality of alignment is heavily dependent on the quality
      of the data (e.g., human preferences); by improving the "signal" from annotators
      and reducing noise/bias, we will get more robustly aligned models.
    see_also:
    - sec:synthetic_alignment_data
    - scalable oversight
    - a:assistance_games
    - a:model_values
    orthodox_problems:
    - superintelligence_fool_supervisors
    - value_fragile
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: engineering
    broad_approach_text: engineering
    some_names:
    - Maarten Buyl
    - Kelsey Kraus
    - Margaret Kroll
    - Danqing Shi
    estimated_ftes: 20-50
    critiques: '[**A Statistical Case Against Empirical Human-AI Alignment**](https://arxiv.org/abs/2502.14581)'
    funded_by: Anthropic, Google DeepMind, OpenAI, Meta AI, various academic groups
    outputs:
    - link_url: https://arxiv.org/html/2410.01957v2
      link_text: Challenges and Future Directions of Data-Centric AI Alignment
      original_md: '* [**Challenges and Future Directions of Data-Centric AI Alignment**](https://arxiv.org/html/2410.01957v2)'
      title: Challenges and Future Directions of Data-Centric AI Alignment
      authors:
      - Min-Hsuan Yeh
      - Jeffrey Wang
      - Xuefeng Du
      - Seongheon Park
      - Leitian Tao
      - Shawn Im
      - Yixuan Li
      author_organizations: []
      date: '2025-05-01'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.05475
      link_text: You Are What You Eat -- AI Alignment Requires Understanding How Data
        Shapes Structure and Generalisation
      original_md: '* [**You Are What You Eat \-- AI Alignment Requires Understanding
        How Data Shapes Structure and Generalisation**](https://arxiv.org/abs/2502.05475)'
      title: You Are What You Eat -- AI Alignment Requires Understanding How Data
        Shapes Structure and Generalisation
      authors:
      - Simon Pepin Lehalleur
      - Jesse Hoogland
      - Matthew Farrugia-Roberts
      - Susan Wei
      - Alexander Gietelink Oldenziel
      - George Wang
      - Liam Carroll
      - Daniel Murfet
      author_organizations: []
      date: '2025-02-08'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.10441
      link_text: AI Alignment at Your Discretion
      original_md: '* [**AI Alignment at Your Discretion**](https://arxiv.org/abs/2502.10441),
        *Maarten Buyl, Hadi Khalaf, Claudio Mayrink Verdun et al.*, 2025-02-10, arXiv'
      title: AI Alignment at Your Discretion
      authors:
      - Maarten Buyl
      - Hadi Khalaf
      - Claudio Mayrink Verdun
      - Lucas Monteiro Paes
      - Caio C. Vieira Machado
      - Flavio du Pin Calmon
      author_organizations: []
      date: '2025-02-10'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.04910
      link_text: Maximizing Signal in Human-Model Preference Alignment
      original_md: '* [**Maximizing Signal in Human-Model Preference Alignment**](https://arxiv.org/abs/2503.04910),
        *Kelsey Kraus, Margaret Kroll*, 2025-03-06, arXiv'
      title: Maximizing Signal in Human-Model Preference Alignment
      authors:
      - Kelsey Kraus
      - Margaret Kroll
      author_organizations: []
      date: '2025-03-06'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.18802
      link_text: 'DxHF: Providing High-Quality Human Feedback for LLM Alignment via
        Interactive Decomposition'
      original_md: '* [**DxHF: Providing High-Quality Human Feedback for LLM Alignment
        via Interactive Decomposition**](https://arxiv.org/abs/2507.18802), *Danqing
        Shi, Furui Cheng, Tino Weinkauf et al.*, 2025-07-24, arXiv'
      title: 'DxHF: Providing High-Quality Human Feedback for LLM Alignment via Interactive
        Decomposition'
      authors:
      - Danqing Shi
      - Furui Cheng
      - Tino Weinkauf
      - Antti Oulasvirta
      - Mennatallah El-Assady
      author_organizations: []
      date: '2025-07-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: sec:goal_robustness
  name: Goal robustness
  header_level: 2
  parent_id: sec:black_box
  content: ''
  item_type: section
  agenda_attributes: null
  parsing_issues: []
- id: a:mild_optimization
  name: Mild optimisation
  header_level: 3
  parent_id: sec:goal_robustness
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: jord ✅
    one_sentence_summary: Avoid Goodharting by getting AI to satisfice rather than
      maximise.
    theory_of_change: If we fail to exactly nail down the preferences for a superintelligent
      agent we die to Goodharting → shift from maximising to satisficing in the agent's
      utility function → we get a nonzero share of the lightcone as opposed to zero;
      also, moonshot at this being the recipe for fully aligned AI.
    see_also: []
    orthodox_problems:
    - value_fragile
    target_case_id: null
    target_case_text: mixed
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names: []
    estimated_ftes: 10-50
    critiques: null
    funded_by: Google DeepMind
    outputs:
    - link_url: https://arxiv.org/abs/2501.13011
      link_text: 'MONA: Myopic Optimization with Non-myopic Approval Can Mitigate
        Multi-step Reward Hacking'
      original_md: '* [**MONA: Myopic Optimization with Non-myopic Approval Can Mitigate
        Multi-step Reward Hacking**](https://arxiv.org/abs/2501.13011)'
      title: 'MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step
        Reward Hacking'
      authors:
      - Sebastian Farquhar
      - Vikrant Varma
      - David Lindner
      - David Elson
      - Caleb Biddulph
      - Ian Goodfellow
      - Rohin Shah
      author_organizations:
      - Google DeepMind
      date: '2025-01-22'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.02655
      link_text: 'BioBlue: Notable runaway-optimiser-like LLM failure modes on biologically
        and economically aligned AI safety benchmarks for LLMs with simplified observation
        format'
      original_md: '* [BioBlue: Notable runaway-optimiser-like LLM failure modes on
        biologically and economically aligned AI safety benchmarks for LLMs with simplified
        observation format](https://arxiv.org/abs/2509.02655)'
      title: 'BioBlue: Notable runaway-optimiser-like LLM failure modes on biologically
        and economically aligned AI safety benchmarks for LLMs with simplified observation
        format'
      authors:
      - Roland Pihlakas
      - Sruthi Kuriakose
      author_organizations: []
      date: '2025-09-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/vGeuBKQ7nzPnn5f7A/why-modelling-multi-objective-homeostasis-is-essential-for
      link_text: Why modelling multi-objective homeostasis is essential for AI alignment
        (and how it helps with AI safety as well). Subtleties and Open Challenges
      original_md: '* [**Why modelling multi-objective homeostasis is essential for
        AI alignment (and how it helps with AI safety as well). Subtleties and Open
        Challenges**](https://lesswrong.com/posts/vGeuBKQ7nzPnn5f7A/why-modelling-multi-objective-homeostasis-is-essential-for)'
      title: Why modelling multi-objective homeostasis is essential for AI alignment
        (and how it helps with AI safety as well). Subtleties and Open Challenges.
      authors:
      - Roland Pihlakas
      author_organizations: []
      date: '2025-01-12'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    other_attributes: {}
  parsing_issues:
  - Target case field says 'mixed' - leaving target_case_id as null and target_case_text
    as 'mixed' as instructed
  - Broad approach field says 'cognitive' - mapped to 'cognitivist_science' (cognitivist
    science)
- id: a:rl_safety
  name: RL safety
  header_level: 3
  parent_id: sec:goal_robustness
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stephen ✅
    one_sentence_summary: Improves the robustness of reinforcement learning agents
      by addressing core problems in reward learning, goal misgeneralization, and
      specification gaming.
    theory_of_change: Standard RL objectives (like maximizing expected value) are
      brittle and lead to goal misgeneralization or specification gaming; by developing
      more robust frameworks (like pessimistic RL, minimax regret, or provable inverse
      reward learning), we can create agents that are safe even when misspecified.
    see_also:
    - a:behavior_alignment_theory
    - a:assistance_games
    - sec:goal_robustness
    - sec:iterative_alignment
    - a:mild_optimization
    - scalable oversight
    - '[The Theoretical Reward Learning Research Agenda: Introduction and Motivation](https://www.alignmentforum.org/posts/pJ3mDD7LfEwp3s5vG/the-theoretical-reward-learning-research-agenda-introduction)'
    orthodox_problems:
    - goals_misgeneralize
    - value_fragile
    - superintelligence_fool_supervisors
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: engineering
    broad_approach_text: engineering
    some_names:
    - Joar Skalse
    - Karim Abdel Sadek
    - Matthew Farrugia-Roberts
    - Benjamin Plaut
    - Fang Wu
    - Stephen Zhao
    - Alessandro Abate
    - Steven Byrnes
    - Michael Cohen
    estimated_ftes: 20-70
    critiques: '["The Era of Experience" has an unsolved technical alignment problem](https://www.lesswrong.com/posts/747f6b8e/the-era-of-experience-has-an-unsolved-technical-alignment-problem),
      [The Invisible Leash: Why RLVR May or May Not Escape Its Origin](https://arxiv.org/abs/2507.14843)'
    funded_by: Google DeepMind, University of Oxford, CMU, Coefficient Giving
    outputs:
    - link_url: https://arxiv.org/abs/2406.15753
      link_text: 'The Perils of Optimizing Learned Reward Functions: Low Training
        Error Does Not Guarantee Low Regret'
      original_md: '* [**The Perils of Optimizing Learned Reward Functions: Low Training
        Error Does Not Guarantee Low Regret**](https://arxiv.org/abs/2406.15753)'
      title: 'The Perils of Optimizing Learned Reward Functions: Low Training Error
        Does Not Guarantee Low Regret'
      authors:
      - Lukas Fluri
      - Leon Lang
      - Alessandro Abate
      - Patrick Forré
      - David Krueger
      - Joar Skalse
      author_organizations: []
      date: '2024-06-22'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.03068
      link_text: Mitigating Goal Misgeneralization via Minimax Regret
      original_md: '* [**Mitigating Goal Misgeneralization via Minimax Regret**](https://arxiv.org/abs/2507.03068),
        *Karim Abdel Sadek, Matthew Farrugia-Roberts, Usman Anwar et al.*, 2025-07-03,
        RLC 2025'
      title: Mitigating Goal Misgeneralization via Minimax Regret
      authors:
      - Karim Abdel Sadek
      - Matthew Farrugia-Roberts
      - Usman Anwar
      - Hannah Erlebach
      - Christian Schroeder de Witt
      - David Krueger
      - Michael Dennis
      author_organizations: []
      date: '2025-07-03'
      published_year: 2025
      venue: RLC 2025
      kind: paper_published
    - link_url: https://arxiv.org/abs/2410.05584
      link_text: 'Rethinking Reward Model Evaluation: Are We Barking up the Wrong
        Tree?'
      original_md: '* [**Rethinking Reward Model Evaluation: Are We Barking up the
        Wrong Tree?**](https://arxiv.org/abs/2410.05584)'
      title: 'Rethinking Reward Model Evaluation: Are We Barking up the Wrong Tree?'
      authors:
      - Xueru Wen
      - Jie Lou
      - Yaojie Lu
      - Hongyu Lin
      - Xing Yu
      - Xinyu Lu
      - Ben He
      - Xianpei Han
      - Debing Zhang
      - Le Sun
      author_organizations: []
      date: '2024-10-08'
      published_year: 2024
      venue: arXiv (Accepted at ICLR 2025 Spotlight)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.14043
      link_text: Safe Learning Under Irreversible Dynamics via Asking for Help
      original_md: '* [**Safe Learning Under Irreversible Dynamics via Asking for
        Help**](https://arxiv.org/abs/2502.14043), *Benjamin Plaut, Juan Liévano-Karim,
        Hanlin Zhu et al.*, 2025-02-19, arXiv'
      title: Safe Learning Under Irreversible Dynamics via Asking for Help
      authors:
      - Benjamin Plaut
      - Juan Liévano-Karim
      - Hanlin Zhu
      - Stuart Russell
      author_organizations:
      - UC Berkeley
      date: '2025-02-19'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.14843
      link_text: 'The Invisible Leash: Why RLVR May or May Not Escape Its Origin'
      original_md: '* [**The Invisible Leash: Why RLVR May or May Not Escape Its Origin**](https://arxiv.org/abs/2507.14843),
        *Fang Wu, Weihao Xuan, Ximing Lu et al.*, 2025-07-20, arXiv'
      title: 'The Invisible Leash: Why RLVR May or May Not Escape Its Origin'
      authors:
      - Fang Wu
      - Weihao Xuan
      - Ximing Lu
      - Mingjie Liu
      - Yi Dong
      - Zaid Harchaoui
      - Yejin Choi
      author_organizations:
      - University of Washington
      - Allen Institute for AI
      date: '2025-07-20'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.21184
      link_text: Reducing the Probability of Undesirable Outputs in Language Models
        Using Probabilistic Inference
      original_md: '* [**Reducing the Probability of Undesirable Outputs in Language
        Models Using Probabilistic Inference**](https://arxiv.org/abs/2510.21184),
        *Stephen Zhao, Aidan Li, Rob Brekelmans et al.*, 2025-10-24, arXiv'
      title: Reducing the Probability of Undesirable Outputs in Language Models Using
        Probabilistic Inference
      authors:
      - Stephen Zhao
      - Aidan Li
      - Rob Brekelmans
      - Roger Grosse
      author_organizations: []
      date: '2025-10-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/TCGgiJAinGgcMEByt/the-era-of-experience-has-an-unsolved-technical-alignment
      link_text: '"The Era of Experience" has an unsolved technical alignment problem'
      original_md: '* [**"The Era of Experience" has an unsolved technical alignment
        problem**](https://lesswrong.com/posts/TCGgiJAinGgcMEByt/the-era-of-experience-has-an-unsolved-technical-alignment),
        *Steven Byrnes*, 2025-04-24, LessWrong'
      title: '"The Era of Experience" has an unsolved technical alignment problem'
      authors:
      - Steven Byrnes
      author_organizations: []
      date: '2025-04-24'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/CpftMXCEnwqbWreHD/safety-cases-for-pessimism
      link_text: Safety cases for Pessimism
      original_md: '* [**Safety cases for Pessimism**](https://lesswrong.com/posts/CpftMXCEnwqbWreHD/safety-cases-for-pessimism),
        *Michael Cohen*, LessWrong'
      title: Safety cases for Pessimism
      authors:
      - Michael Cohen
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2504.01871
      link_text: Interpreting Emergent Planning in Model-Free Reinforcement Learning
      original_md: '* [**Interpreting Emergent Planning in Model-Free Reinforcement
        Learning**](https://arxiv.org/abs/2504.01871), *Thomas Bush, Stephen Chung,
        Usman Anwar et al.*, 2025-04-02, arXiv (ICLR 2025 oral)'
      title: Interpreting Emergent Planning in Model-Free Reinforcement Learning
      authors:
      - Thomas Bush
      - Stephen Chung
      - Usman Anwar
      - Adrià Garriga-Alonso
      - David Krueger
      author_organizations: []
      date: '2025-04-02'
      published_year: 2025
      venue: arXiv (ICLR 2025 oral)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.10995
      link_text: Misalignment From Treating Means as Ends
      original_md: '* [**Misalignment From Treating Means as Ends**](https://arxiv.org/abs/2507.10995),
        *Henrik Marklund, Alex Infanger, Benjamin Van Roy,* 2025-07-15, arXiv'
      title: Misalignment from Treating Means as Ends
      authors:
      - Henrik Marklund
      - Alex Infanger
      - Benjamin Van Roy
      author_organizations: []
      date: '2025-07-15'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues:
  - See also references 'scalable oversight' which does not match any known agenda
    ID - kept as plain text
- id: a:assistance_games
  name: Assistance games, assistive agents
  header_level: 3
  parent_id: sec:goal_robustness
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Rory
    one_sentence_summary: Formalize how AI assistants learn about human preferences
      given uncertainty and partial observability, and construct environments which
      better incentivize AIs to learn what we want them to learn.
    theory_of_change: Understand what kinds of things can go wrong when humans are
      directly involved in training a model → build tools that make it easier for
      a model to learn what humans want it to learn.
    see_also: []
    orthodox_problems:
    - value_fragile
    - humanlike_minds_not_safe
    target_case_id: null
    target_case_text: varies
    broad_approach_id: null
    broad_approach_text: engineering / cognitive
    some_names:
    - Joar Skalse
    - Anca Dragan
    - Caspar Oesterheld
    - David Krueger
    - Dylan Hafield-Menell
    estimated_ftes: null
    critiques: '[nice summary](https://www.lesswrong.com/posts/i5kijcjFJD6bn7dwq/evaluating-the-historical-value-misspecification-argument)
      of historical problem statements'
    funded_by: Future of Life Institute, Coefficient Giving, Survival and Flourishing
      Fund, Cooperative AI Foundation, Polaris Ventures
    outputs:
    - link_url: https://arxiv.org/pdf/2510.13709
      link_text: Training LLM Agents to Empower Humans
      original_md: '* [Training LLM Agents to Empower Humans](https://arxiv.org/pdf/2510.13709)'
      title: Training LLM Agents to Empower Humans
      authors:
      - Evan Ellis
      - Vivek Myers
      - Jens Tuyls
      - Sergey Levine
      - Anca Dragan
      - Benjamin Eysenbach
      author_organizations:
      - UC Berkeley
      - Princeton University
      date: '2025-10-16'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.05381
      link_text: 'Murphys Laws of AI Alignment: Why the Gap Always Wins'
      original_md: '* [**Murphys Laws of AI Alignment: Why the Gap Always Wins**](https://arxiv.org/abs/2509.05381),
        *Madhava Gaikwad*, 2025-09-04, arXiv'
      title: 'Murphys Laws of AI Alignment: Why the Gap Always Wins'
      authors:
      - Madhava Gaikwad
      author_organizations: []
      date: '2025-09-04'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.07091
      link_text: 'AssistanceZero: Scalably Solving Assistance Games'
      original_md: '* [**AssistanceZero: Scalably Solving Assistance Games**](https://arxiv.org/abs/2504.07091),
        *Cassidy Laidaw, Eli Bronstein, Timothy Guo et al.*, 2025-04-09, arXiv'
      title: 'AssistanceZero: Scalably Solving Assistance Games'
      authors:
      - Cassidy Laidlaw
      - Eli Bronstein
      - Timothy Guo
      - Dylan Feng
      - Lukas Berglund
      - Justin Svegliato
      - Stuart Russell
      - Anca Dragan
      author_organizations:
      - UC Berkeley
      date: '2025-04-09'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2412.17797
      link_text: Observation Interference in Partially Observable Assistance Games
      original_md: '* [**Observation Interference in Partially Observable Assistance
        Games**](https://arxiv.org/abs/2412.17797), *Scott Emmons, Caspar Oesterheld,
        Vincent Conitzer et al.*, 2024-12-23, arXiv'
      title: Observation Interference in Partially Observable Assistance Games
      authors:
      - Scott Emmons
      - Caspar Oesterheld
      - Vincent Conitzer
      - Stuart Russell
      author_organizations:
      - UC Berkeley
      date: '2024-12-23'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.02623
      link_text: Learning to Assist Humans without Inferring Rewards
      original_md: '* [**Learning to Assist Humans without Inferring Rewards**](https://arxiv.org/abs/2411.02623),
        *Vivek Myers, Evan Ellis, Sergey Levine et al.*, 2024-11-04, NeurIPS 2024'
      title: Learning to Assist Humans without Inferring Rewards
      authors:
      - Vivek Myers
      - Evan Ellis
      - Sergey Levine
      - Benjamin Eysenbach
      - Anca Dragan
      author_organizations:
      - UC Berkeley
      - Google
      date: '2024-11-04'
      published_year: 2024
      venue: NeurIPS 2024
      kind: paper_published
    other_attributes: {}
  parsing_issues:
  - Target case field says 'Varies' - cannot map to a specific target case ID
  - Broad approach has multiple values 'engineering, cognitive' - cannot map to a
    single ID
- id: a:open_model_interventions
  name: Harm reduction for open weights
  header_level: 2
  parent_id: sec:black_box
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stephen ✅
    one_sentence_summary: Develops methods, primarily based on pretraining data intervention,
      to create tamper-resistant safeguards that prevent open-weight models from being
      maliciously fine-tuned to remove safety features or exploit dangerous capabilities.
    theory_of_change: Open-weight models allow adversaries to easily remove post-training
      safety (like refusal training) via simple fine-tuning; by making safety an intrinsic
      property of the model's learned knowledge and capabilities (e.g., by ensuring
      "deep ignorance" of dual-use information), the safeguards become far more difficult
      and expensive to remove.
    see_also:
    - a:data_filtering
    - a:unlearning
    - a:data_poisoning
    orthodox_problems:
    - someone_else_deploys
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: engineering
    broad_approach_text: engineering
    some_names:
    - Kyle O'Brien
    - Stephen Casper
    - Quentin Anthony
    - Tomek Korbak
    - Rishub Tamirisa
    - Mantas Mazeika
    - Stella Biderman
    - Yarin Gal
    estimated_ftes: 10-100
    critiques: null
    funded_by: UK AI Safety Institute (AISI), EleutherAI, Coefficient Giving
    outputs:
    - link_url: https://arxiv.org/abs/2408.00761
      link_text: Tamper-Resistant Safeguards for Open-Weight LLMs
      original_md: '* [Tamper-Resistant Safeguards for Open-Weight LLMs](https://arxiv.org/abs/2408.00761)'
      title: Tamper-Resistant Safeguards for Open-Weight LLMs
      authors:
      - Rishub Tamirisa
      - Bhrugu Bharathi
      - Long Phan
      - Andy Zhou
      - Alice Gatti
      - Tarun Suresh
      - Maxwell Lin
      - Justin Wang
      - Rowan Wang
      - Ron Arel
      - Andy Zou
      - Dawn Song
      - Bo Li
      - Dan Hendrycks
      - Mantas Mazeika
      author_organizations:
      - UC Berkeley
      - UIUC
      - Center for AI Safety
      date: '2024-08-01'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5705186
      link_text: Open Technical Problems in Open-Weight AI Model Risk Management
      original_md: '* [Open Technical Problems in Open-Weight AI Model Risk Management](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5705186)'
      title: Open Technical Problems in Open-Weight AI Model Risk Management
      authors:
      - Stephen Casper
      - Kyle O'Brien
      - Shayne Longpre
      - Elizabeth Seger
      - Kevin Klyman
      - Rishi Bommasani
      - Aniruddha Nrusimha
      - Ilia Shumailov
      - Sören Mindermann
      - Steven Basart
      - Frank Rudzicz
      - Kellin Pelrine
      - Avijit Ghosh
      - Andrew Strait
      - Robert Kirk
      - Dan Hendrycks
      - Peter Henderson
      - J. Zico Kolter
      - Geoffrey Irving
      - Yarin Gal
      - Yoshua Bengio
      - Dylan Hadfield-Menell
      author_organizations:
      - Massachusetts Institute of Technology
      - ERA Fellowship
      - Apple
      - Centre for the Governance of AI
      - Stanford University
      - Google DeepMind
      - Vector Institute for Artificial Intelligence
      - FAR.AI
      - Hugging Face
      - Center for AI Safety
      - Princeton University
      - Carnegie Mellon University
      - UK AI Security Institute
      - University of Oxford
      - University of Montreal
      date: '2025-10-26'
      published_year: 2025
      venue: SSRN
      kind: paper_preprint
    - link_url: https://www.aisi.gov.uk/research/deep-ignorance-filtering-pretraining-data-builds-tamper-resistant-safeguards-into-open-weight-llms
      link_text: 'Deep ignorance: Filtering pretraining data builds tamper-resistant
        safeguards into open-weight LLMs'
      original_md: '* [Deep ignorance: Filtering pretraining data builds tamper-resistant
        safeguards into open-weight LLMs](https://www.aisi.gov.uk/research/deep-ignorance-filtering-pretraining-data-builds-tamper-resistant-safeguards-into-open-weight-llms)'
      title: 'Deep ignorance: Filtering pretraining data builds tamper-resistant safeguards
        into open-weight LLMs'
      authors:
      - Kyle O'Brien
      - Stephen Casper
      - Quentin Anthony
      - Tomek Korbak
      - Robert Kirk
      - Xander Davies
      - Ishan Mishra
      - Geoffrey Irving
      - Yarin Gal
      - Stella Biderman
      author_organizations:
      - UK AI Security Institute
      - MIT
      - Eleuther AI
      date: '2025-08-08'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: 'a:black_box_neglected_approaches:'
  name: The "Neglected Approaches" Approach
  header_level: 2
  parent_id: sec:black_box
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stephen ✅
    one_sentence_summary: Agenda-agnostic approaches to identifying good but overlooked
      empirical alignment ideas, working with theorists who could use engineers, and
      prototyping them.
    theory_of_change: Empirical search for "negative alignment taxes" (prioritizing
      methods that simultaneously enhance alignment and capabilities)
    see_also:
    - sec:iterative_alignment
    - automated alignment research
    - Beijing Key Laboratory of Safe AI and Superalignment
    - Aligned AI
    orthodox_problems:
    - someone_else_deploys
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: engineering
    broad_approach_text: engineering
    some_names:
    - AE Studio
    - Gunnar Zarncke
    - Cameron Berg
    - Michael Vaiana
    - Judd Rosenblatt
    - Diogo Schwerz de Lucena
    estimated_ftes: '15'
    critiques: '[The ''Alignment Bonus'' is a Dangerous Mirage](https://www.alignmentforum.org/posts/example-critique-neg-tax),
      [Why ''Win-Win'' Alignment is a Distraction](https://example.com/win-win-critique)'
    funded_by: AE Studio
    outputs:
    - link_url: https://arxiv.org/abs/2412.16325
      link_text: Learning Representations of Alignment
      original_md: '* [**Learning Representations of Alignment**](https://arxiv.org/abs/2412.16325),
        *Gunnar Zarncke, Cameron Berg, Michael Vaiana, Judd Rosenblatt, Diogo Schwerz
        de Lucena et al.*, 2024-12-19, arXiv, \[paper_preprint, sr=0.85, id:2412.16325\],'
      title: Towards Safe and Honest AI Agents with Neural Self-Other Overlap
      authors:
      - Marc Carauleanu
      - Michael Vaiana
      - Judd Rosenblatt
      - Cameron Berg
      - Diogo Schwerz de Lucena
      author_organizations: []
      date: '2024-12-20'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.24797
      link_text: 'Self-Correction in Thought-Attractors: A Nudge Towards Alignment'
      original_md: '* [**Self-Correction in Thought-Attractors: A Nudge Towards Alignment**](https://arxiv.org/abs/2510.24797),
        *Cameron Berg, Gunnar Zarncke, Michael Vaiana, Judd Rosenblatt et al.*, 2025-10-28,
        arXiv, \[paper_preprint, sr=0.88, id:2510.24797\]'
      title: Large Language Models Report Subjective Experience Under Self-Referential
        Processing
      authors:
      - Cameron Berg
      - Diogo de Lucena
      - Judd Rosenblatt
      author_organizations: []
      date: '2025-10-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2508.08492
      link_text: 'Engineering Alignment: A Practical Framework for Prototyping ''Negative
        Tax'' Solutions'
      original_md: '* [**Engineering Alignment: A Practical Framework for Prototyping
        ''Negative Tax'' Solutions**](https://arxiv.org/abs/2508.08492), *Gunnar Zarncke,
        Michael Vaiana, Cameron Berg, Judd Rosenblatt et al.*, 2025-08-15, arXiv,
        \[paper_preprint, sr=0.82, id:2508.08492\]'
      title: Momentum Point-Perplexity Mechanics in Large Language Models
      authors:
      - Lorenzo Tomaz
      - Judd Rosenblatt
      - Thomas Berry Jones
      - Diogo Schwerz de Lucena
      author_organizations: []
      date: '2025-08-11'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: sec:whitebox
  name: White-box safety (understand and control current model internals)
  header_level: 1
  parent_id: null
  content: ''
  item_type: section
  agenda_attributes: null
  parsing_issues: []
- id: sec:interpretability
  name: Interpretability
  header_level: 2
  parent_id: sec:whitebox
  content: This section isn't very conceptually clean. See the [Open Problems](https://arxiv.org/abs/2501.16496)
    paper for a strong frame which is nonetheless not very useful for our descriptive
    purposes.
  item_type: section
  agenda_attributes: null
  parsing_issues: []
- id: a:interp_fundamental
  name: Reverse engineering
  header_level: 3
  parent_id: sec:interpretability
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stephen ✅
    one_sentence_summary: Decompose a model into its functional, interacting components
      (circuits), formally describe what computation those components perform, and
      validate their causal effects to reverse-engineer the model's internal algorithm.
    theory_of_change: By gaining a mechanical understanding of how a model works (the
      "circuit diagram"), we can predict how models will act in novel situations (generalization),
      and gain the mechanistic knowledge necessary to safely modify an AI's goals
      or internal mechanisms.
    see_also:
    - '[ambitious mech interp](https://www.alignmentforum.org/posts/Hy6PX43HGgmfiTaKu/an-ambitious-vision-for-interpretability)'
    orthodox_problems:
    - goals_misgeneralize
    - superintelligence_fool_supervisors
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - Lucius Bushnaq
    - Dan Braun
    - Lee Sharkey
    - Aaron Mueller
    - Atticus Geiger
    - Sheridan Feucht
    - David Bau
    - Yonatan Belinkov
    - Stefan Heimersheim
    estimated_ftes: 100-200
    critiques: '[Interpretability Will Not Reliably Find Deceptive AI](https://www.alignmentforum.org/posts/PwnadG4BFjaER3MGf/interpretability-will-not-reliably-find-deceptive-ai),
      [A Problem to Solve Before Building a Deception Detector](https://www.lesswrong.com/posts/YXNeA3RyRrrRWS37A/a-problem-to-solve-before-building-a-deception-detector),
      [MoSSAIC: AI Safety After Mechanism](https://openreview.net/forum?id=n7WYSJ35FU),
      [The Misguided Quest for Mechanistic AI Interpretability](https://ai-frontiers.org/articles/the-misguided-quest-for-mechanistic-ai-interpretability).
      [Mechanistic?](https://arxiv.org/abs/2410.09087), [Assessing skeptical views
      of interpretability research](https://www.youtube.com/watch?v=woo_J0RKcpQ),  [Activation
      space interpretability may be doomed](https://www.lesswrong.com/posts/gYfpPbww3wQRaxAFD/activation-space-interpretability-may-be-doomed),
      [A Pragmatic Vision for Interpretability](https://www.alignmentforum.org/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-interpretability)'
    funded_by: null
    outputs:
    - section_name: In weights-space
      header_level: 4
      original_md: '#### In weights-space'
    - link_url: https://www.neuronpedia.org/graph/info
      link_text: The Circuits Research Landscape
      original_md: '* [The Circuits Research Landscape](https://www.neuronpedia.org/graph/info)'
      title: 'The Circuits Research Landscape: Results and Perspectives'
      authors:
      - Jack Lindsey
      - Emmanuel Ameisen
      - Neel Nanda
      - Stepan Shabalin
      - Mateusz Piotrowski
      - Tom McGrath
      - Michael Hanna
      - Owen Lewis
      - Curt Tigges
      - Jack Merullo
      - Connor Watts
      - Gonçalo Paulo
      - Joshua Batson
      - Liv Gorton
      - Elana Simon
      - Max Loeffler
      - Callum McDougall
      - Johnny Lin
      author_organizations:
      - Anthropic
      - Google DeepMind
      - Goodfire AI
      - EleutherAI
      - Decode
      date: '2025-08-01'
      published_year: 2025
      venue: Neuronpedia
      kind: blog_post
    - link_url: https://openreview.net/forum?id=dEdS9ao8gN
      link_text: Stochastic Parameter Decomposition
      original_md: '* [Stochastic Parameter Decomposition](https://openreview.net/forum?id=dEdS9ao8gN).
        See also the precursor: [Attribution-based parameter decomposition](https://www.alignmentforum.org/posts/EPefYWjuHNcNH4C7E/attribution-based-parameter-decomposition)'
      title: Stochastic Parameter Decomposition
      authors:
      - Dan Braun
      - Lucius Bushnaq
      - Lee Sharkey
      author_organizations: []
      date: '2025-06-26'
      published_year: 2025
      venue: ODYSSEY 2025 Conference
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.13151
      link_text: 'MIB: A Mechanistic Interpretability Benchmark'
      original_md: '* [MIB: A Mechanistic Interpretability Benchmark](https://arxiv.org/abs/2504.13151)'
      title: 'MIB: A Mechanistic Interpretability Benchmark'
      authors:
      - Aaron Mueller
      - Atticus Geiger
      - Sarah Wiegreffe
      - Dana Arad
      - Iván Arcuschin
      - Adam Belfki
      - Yik Siu Chan
      - Jaden Fiotto-Kaufman
      - Tal Haklay
      - Michael Hanna
      - Jing Huang
      - Rohan Gupta
      - Yaniv Nikankin
      - Hadas Orgad
      - Nikhil Prakash
      - Anja Reusch
      - Aruna Sankaranarayanan
      - Shun Shao
      - Alessandro Stolfo
      - Martin Tutek
      - Amir Zur
      - David Bau
      - Yonatan Belinkov
      author_organizations:
      - Multiple institutions
      date: '2025-06-09'
      published_year: 2025
      venue: ICML 2025
      kind: paper_preprint
    - link_url: https://www.lesswrong.com/posts/roE7SHjFWEoMcGZKd/circuits-in-superposition-compressing-many-small-neural
      link_text: 'Circuits in Superposition: Compressing many small neural networks
        into one'
      original_md: '* [Circuits in Superposition: Compressing many small neural networks
        into one](https://www.lesswrong.com/posts/roE7SHjFWEoMcGZKd/circuits-in-superposition-compressing-many-small-neural)'
      title: 'Circuits in Superposition: Compressing many small neural networks into
        one'
      authors:
      - Lucius Bushnaq
      - jake_mendel
      author_organizations:
      - Apollo Research
      date: '2024-10-14'
      published_year: 2024
      venue: LessWrong
      kind: lesswrong
    - link_url: https://www.lesswrong.com/posts/ZxFchCFJFcgysYsT9/compressed-computation-is-probably-not-computation-in
      link_text: Compressed Computation is (probably) not Computation in Superposition
      original_md: '* [Compressed Computation is (probably) not Computation in Superposition](https://www.lesswrong.com/posts/ZxFchCFJFcgysYsT9/compressed-computation-is-probably-not-computation-in)'
      title: 'Error: Content Unavailable (429: Too Many Requests)'
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: error_detected
    - link_url: https://arxiv.org/abs/2504.03022
      link_text: The Dual-Route Model of Induction
      original_md: '* [The Dual-Route Model of Induction](https://arxiv.org/abs/2504.03022)'
      title: The Dual-Route Model of Induction
      authors:
      - Sheridan Feucht
      - Eric Todd
      - Byron Wallace
      - David Bau
      author_organizations:
      - Northeastern University
      date: '2025-04-03'
      published_year: 2025
      venue: COLM 2025
      kind: paper_published
    - link_url: https://arxiv.org/abs/2504.14379
      link_text: The Geometry of Self-Verification in a Task-Specific Reasoning Model
      original_md: '* [The Geometry of Self-Verification in a Task-Specific Reasoning
        Model](https://arxiv.org/abs/2504.14379)'
      title: The Geometry of Self-Verification in a Task-Specific Reasoning Model
      authors:
      - Andrew Lee
      - Lihao Sun
      - Chris Wendler
      - Fernanda Viégas
      - Martin Wattenberg
      author_organizations:
      - Google Research
      date: '2025-04-19'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.01032
      link_text: Converting MLPs into Polynomials in Closed Form
      original_md: '* [Converting MLPs into Polynomials in Closed Form](https://arxiv.org/abs/2502.01032)'
      title: Converting MLPs into Polynomials in Closed Form
      authors:
      - Nora Belrose
      - Alice Rigg
      author_organizations: []
      date: '2025-02-03'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2412.04614
      link_text: Extractive Structures Learned in Pretraining Enable Generalization
        on Finetuned Facts
      original_md: '* [Extractive Structures Learned in Pretraining Enable Generalization
        on Finetuned Facts](https://arxiv.org/abs/2412.04614)'
      title: Extractive Structures Learned in Pretraining Enable Generalization on
        Finetuned Facts
      authors:
      - Jiahai Feng
      - Stuart Russell
      - Jacob Steinhardt
      author_organizations:
      - UC Berkeley
      date: '2024-12-05'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.00194
      link_text: Identifying Sparsely Active Circuits Through Local Loss Landscape
        Decomposition
      original_md: '* [Identifying Sparsely Active Circuits Through Local Loss Landscape
        Decomposition](https://arxiv.org/abs/2504.00194)'
      title: Identifying Sparsely Active Circuits Through Local Loss Landscape Decomposition
      authors:
      - Brianna Chrisman
      - Lucius Bushnaq
      - Lee Sharkey
      author_organizations: []
      date: '2025-03-31'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.00921
      link_text: 'Blink of an eye: a simple theory for feature localization in generative
        models'
      original_md: '* [Blink of an eye: a simple theory for feature localization in
        generative models](https://arxiv.org/abs/2502.00921)'
      title: 'Blink of an eye: a simple theory for feature localization in generative
        models'
      authors:
      - Marvin Li
      - Aayush Karan
      - Sitan Chen
      author_organizations: []
      date: '2025-06-05'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.24256
      link_text: From Memorization to Reasoning in the Spectrum of Loss Curvature
      original_md: '* [From Memorization to Reasoning in the Spectrum of Loss Curvature](https://arxiv.org/abs/2510.24256)'
      title: From Memorization to Reasoning in the Spectrum of Loss Curvature
      authors:
      - Jack Merullo
      - Srihita Vatsavaya
      - Lucius Bushnaq
      - Owen Lewis
      author_organizations: []
      date: '2025-10-28'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.10887
      link_text: Generalization or Hallucination? Understanding Out-of-Context Reasoning
        in Transformers
      original_md: '* [Generalization or Hallucination? Understanding Out-of-Context
        Reasoning in Transformers](https://arxiv.org/abs/2506.10887)'
      title: Generalization or Hallucination? Understanding Out-of-Context Reasoning
        in Transformers
      authors:
      - Yixiao Huang
      - Hanlin Zhu
      - Tianyu Guo
      - Jiantao Jiao
      - Somayeh Sojoudi
      - Michael I. Jordan
      - Stuart Russell
      - Song Mei
      author_organizations:
      - UC Berkeley
      date: '2025-06-12'
      published_year: 2025
      venue: NeurIPS 2025
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2508.21258
      link_text: 'RelP: Faithful and Efficient Circuit Discovery in Language Models
        via Relevance Patching'
      original_md: '* [RelP: Faithful and Efficient Circuit Discovery in Language
        Models via Relevance Patching](https://arxiv.org/abs/2508.21258)'
      title: 'RelP: Faithful and Efficient Circuit Discovery in Language Models via
        Relevance Patching'
      authors:
      - Farnoush Rezaei Jafari
      - Oliver Eberle
      - Ashkan Khakzar
      - Neel Nanda
      author_organizations: []
      date: '2025-08-28'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.18274
      link_text: 'Structural Inference: Interpreting Small Language Models with Susceptibilities'
      original_md: '* [Structural Inference: Interpreting Small Language Models with
        Susceptibilities](https://arxiv.org/abs/2504.18274)'
      title: 'Structural Inference: Interpreting Small Language Models with Susceptibilities'
      authors:
      - Garrett Baker
      - George Wang
      - Jesse Hoogland
      - Daniel Murfet
      author_organizations: []
      date: '2025-04-25'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2501.14926
      link_text: 'Interpretability in Parameter Space: Minimizing Mechanistic Description
        Length with Attribution-based Parameter Decomposition'
      original_md: '* [Interpretability in Parameter Space: Minimizing Mechanistic
        Description Length with Attribution-based Parameter Decomposition](https://arxiv.org/abs/2501.14926)'
      title: 'Interpretability in Parameter Space: Minimizing Mechanistic Description
        Length with Attribution-based Parameter Decomposition'
      authors:
      - Dan Braun
      - Lucius Bushnaq
      - Stefan Heimersheim
      - Jake Mendel
      - Lee Sharkey
      author_organizations: []
      date: '2025-01-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.13913
      link_text: How Do LLMs Perform Two-Hop Reasoning in Context?
      original_md: '* [How Do LLMs Perform Two-Hop Reasoning in Context?](https://arxiv.org/abs/2502.13913)'
      title: How Do LLMs Perform Two-Hop Reasoning in Context?
      authors:
      - Tianyu Guo
      - Hanlin Zhu
      - Ruiqi Zhang
      - Jiantao Jiao
      - Song Mei
      - Michael I. Jordan
      - Stuart Russell
      author_organizations:
      - UC Berkeley
      date: '2025-02-19'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.15811
      link_text: 'On the creation of narrow AI: hierarchy and nonlocality of neural
        network skills'
      original_md: '* [On the creation of narrow AI: hierarchy and nonlocality of
        neural network skills](https://arxiv.org/abs/2505.15811)'
      title: 'On the creation of narrow AI: hierarchy and nonlocality of neural network
        skills'
      authors:
      - Eric J. Michaud
      - Asher Parker-Sartori
      - Max Tegmark
      author_organizations: []
      date: '2025-05-21'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - section_name: In activations-space
      header_level: 4
      original_md: '#### In activations-space'
    - link_url: https://arxiv.org/abs/2509.14223
      link_text: 'Fresh in memory: Training-order recency is linearly encoded in language
        model activations'
      original_md: '* [Fresh in memory: Training-order recency is linearly encoded
        in language model activations](https://arxiv.org/abs/2509.14223), Dmitrii
        Krasheninnikov, Richard E. Turner, David Krueger, 2025-09-17, arXiv'
      title: 'Fresh in memory: Training-order recency is linearly encoded in language
        model activations'
      authors:
      - Dmitrii Krasheninnikov
      - Richard E. Turner
      - David Krueger
      author_organizations:
      - University of Cambridge
      date: '2025-09-17'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.14685
      link_text: Language Models use Lookbacks to Track Beliefs
      original_md: '* [Language Models use Lookbacks to Track Beliefs](https://arxiv.org/abs/2505.14685),
        Nikhil Prakash, Natalie Shapira, Arnab Sen Sharma et al., 2025-05-20, arXiv'
      title: Language Models use Lookbacks to Track Beliefs
      authors:
      - Nikhil Prakash
      - Natalie Shapira
      - Arnab Sen Sharma
      - Christoph Riedl
      - Yonatan Belinkov
      - Tamar Rott Shaham
      - David Bau
      - Atticus Geiger
      author_organizations:
      - Northeastern University
      - Bar-Ilan University
      - Boston University
      date: '2025-05-20'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/pdf/2504.01871
      link_text: Interpreting Emergent Planning in Model-Free Reinforcement Learning
      original_md: '* [Interpreting Emergent Planning in Model-Free Reinforcement
        Learning](https://arxiv.org/pdf/2504.01871)'
      title: Interpreting Emergent Planning in Model-Free Reinforcement Learning
      authors:
      - Thomas Bush
      - Stephen Chung
      - Usman Anwar
      - Adrià Garriga-Alonso
      - David Krueger
      author_organizations: []
      date: '2025-04-02'
      published_year: 2025
      venue: arXiv (ICLR 2025 oral)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.01954
      link_text: Constrained belief updates explain geometric structures in transformer
        representations
      original_md: '* [Constrained belief updates explain geometric structures in
        transformer representations](https://arxiv.org/abs/2502.01954)'
      title: Constrained belief updates explain geometric structures in transformer
        representations
      authors:
      - Mateusz Piotrowski
      - Paul M. Riechers
      - Daniel Filan
      - Adam S. Shai
      author_organizations: []
      date: '2025-02-04'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.13898
      link_text: Do Language Models Use Their Depth Efficiently?
      original_md: '* [Do Language Models Use Their Depth Efficiently?](https://arxiv.org/abs/2505.13898)'
      title: Do Language Models Use Their Depth Efficiently?
      authors:
      - Róbert Csordás
      - Christopher D. Manning
      - Christopher Potts
      author_organizations:
      - Stanford University
      date: '2025-05-20'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.20896
      link_text: How Do Transformers Learn Variable Binding in Symbolic Programs?
      original_md: '* [How Do Transformers Learn Variable Binding in Symbolic Programs?](https://arxiv.org/abs/2505.20896)'
      title: How Do Transformers Learn Variable Binding in Symbolic Programs?
      authors:
      - Yiwei Wu
      - Atticus Geiger
      - Raphaël Millière
      author_organizations: []
      date: '2025-05-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.26784
      link_text: LLMs Process Lists With General Filter Heads
      original_md: '* [LLMs Process Lists With General Filter Heads](https://arxiv.org/abs/2510.26784)'
      title: LLMs Process Lists With General Filter Heads
      authors:
      - Arnab Sen Sharma
      - Giordano Rogers
      - Natalie Shapira
      - David Bau
      author_organizations:
      - Bau Lab
      date: '2025-10-30'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.00873
      link_text: Language Models Use Trigonometry to Do Addition
      original_md: '* [Language Models Use Trigonometry to Do Addition](https://arxiv.org/abs/2502.00873)'
      title: Language Models Use Trigonometry to Do Addition
      authors:
      - Subhash Kantamneni
      - Max Tegmark
      author_organizations: []
      date: '2025-02-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2412.04703
      link_text: Transformers Struggle to Learn to Search
      original_md: '* [Transformers Struggle to Learn to Search](https://arxiv.org/abs/2412.04703)'
      title: Transformers Struggle to Learn to Search
      authors:
      - Abulhair Saparov
      - Srushti Pawar
      - Shreyas Pimpalgaonkar
      - Nitish Joshi
      - Richard Yuanzhe Pang
      - Vishakh Padmakumar
      - Seyed Mehran Kazemi
      - Najoung Kim
      - He He
      author_organizations: []
      date: '2024-12-06'
      published_year: 2024
      venue: ICLR 2025
      kind: paper_preprint
    - link_url: https://openreview.net/forum?id=pXlmOmlHJZ
      link_text: 'ICLR: In-Context Learning of Representations'
      original_md: '* [ICLR: In-Context Learning of Representations](https://openreview.net/forum?id=pXlmOmlHJZ)'
      title: 'ICLR: In-Context Learning of Representations'
      authors:
      - Core Francisco Park
      - Andrew Lee
      - Ekdeep Singh Lubana
      - Yongyi Yang
      - Maya Okawa
      - Kento Nishi
      - Martin Wattenberg
      - Hidenori Tanaka
      author_organizations: []
      date: '2025-01-22'
      published_year: 2025
      venue: ICLR 2025
      kind: paper_published
    - link_url: https://arxiv.org/abs/2508.17456
      link_text: Adversarial Examples Are Not Bugs, They Are Superposition
      original_md: '* [Adversarial Examples Are Not Bugs, They Are Superposition](https://arxiv.org/abs/2508.17456)'
      title: Adversarial Examples Are Not Bugs, They Are Superposition
      authors:
      - Liv Gorton
      - Owen Lewis
      author_organizations: []
      date: '2025-08-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/DJAZHYjWxMrcd2na3/building-and-evaluating-alignment-auditing-agents
      link_text: Building and evaluating alignment auditing agents
      original_md: '* [Building and evaluating alignment auditing agents](https://lesswrong.com/posts/DJAZHYjWxMrcd2na3/building-and-evaluating-alignment-auditing-agents)'
      title: Building and evaluating alignment auditing agents
      authors:
      - Sam Marks
      - trentbrick
      - RowanWang
      - Sam Bowman
      - Euan Ong
      - Johannes Treutlein
      - evhub
      author_organizations:
      - Anthropic
      date: '2025-07-24'
      published_year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - link_url: https://www.pnas.org/doi/10.1073/pnas.2406675122
      link_text: Bridging the human–AI knowledge gap through concept discovery and
        transfer in AlphaZero
      original_md: '* [Bridging the human–AI knowledge gap through concept discovery
        and transfer in AlphaZero](https://www.pnas.org/doi/10.1073/pnas.2406675122)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://arxiv.org/abs/2506.10138
      link_text: 'Interpreting learned search: finding a transition model and value
        function in an RNN that plays Sokoban'
      original_md: '* [Interpreting learned search: finding a transition model and
        value function in an RNN that plays Sokoban](https://arxiv.org/abs/2506.10138),
        Mohammad Taufeeque, Aaron David Tucker, Adam Gleave et al., 2025-06-11, arXiv'
      title: 'Interpreting learned search: finding a transition model and value function
        in an RNN that plays Sokoban'
      authors:
      - Mohammad Taufeeque
      - Aaron David Tucker
      - Adam Gleave
      - Adrià Garriga-Alonso
      author_organizations:
      - FAR AI
      date: '2025-06-11'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:interp_concept_based
  name: Concept-based interpretability
  header_level: 3
  parent_id: sec:interpretability
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stephen ✅
    one_sentence_summary: Identifies directions or subspaces in a model's latent state
      that correspond to high-level concepts (like refusal, deception, or planning)
      and uses them to build "mind-reading" probes that audit models for misalignment
      or monitor them at runtime.
    theory_of_change: By mapping internal activations to human-interpretable concepts,
      we can detect dangerous capabilities or deceptive alignment directly in the
      mind of the model even if its overt behavior is perfectly safe. Deploy computationally
      cheap monitors to flag some hidden misalignment in deployed systems.
    see_also:
    - a:interp_fundamental
    - a:interp_sparse_coding
    - a:model_diff
    orthodox_problems:
    - value_fragile
    - goals_misgeneralize
    - boxed_agi_exfiltrate
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - Daniel Beaglehole
    - Adityanarayanan Radhakrishnan
    - Enric Boix-Adserà
    - Tom Wollschläger
    - Anna Soligo
    - Jack Lindsey
    - Brian Christian
    - Ling Hu
    - Nicholas Goldowsky-Dill
    estimated_ftes: 50-100
    critiques: '[Exploring the generalization of LLM truth directions on conversational
      formats](https://arxiv.org/html/2505.09807v1), [Understanding (Un)Reliability
      of Steering Vectors in Language Models](https://arxiv.org/abs/2505.22637)'
    funded_by: Coefficient Giving, Anthropic, various academic groups
    outputs:
    - link_url: https://arxiv.org/abs/2502.03708
      link_text: Toward universal steering and monitoring of AI models
      original_md: '* [Toward universal steering and monitoring of AI models](https://arxiv.org/abs/2502.03708),
        Daniel Beaglehole, Adityanarayanan Radhakrishnan, Enric Boix-Adserà et al.,
        2025-05-28, arXiv'
      title: Toward universal steering and monitoring of AI models
      authors:
      - Daniel Beaglehole
      - Adityanarayanan Radhakrishnan
      - Enric Boix-Adserà
      - Mikhail Belkin
      author_organizations: []
      date: '2025-05-28'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.11618
      link_text: Convergent Linear Representations of Emergent Misalignment
      original_md: '* [Convergent Linear Representations of Emergent Misalignment](https://arxiv.org/abs/2506.11618),
        Anna Soligo, Edward Turner, Senthooran Rajamanoharan et al., 2025-06-20, arXiv'
      title: Convergent Linear Representations of Emergent Misalignment
      authors:
      - Anna Soligo
      - Edward Turner
      - Senthooran Rajamanoharan
      - Neel Nanda
      author_organizations:
      - Google DeepMind
      date: '2025-06-20'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.03407
      link_text: Detecting Strategic Deception Using Linear Probes
      original_md: '* [Detecting Strategic Deception Using Linear Probes](https://arxiv.org/abs/2502.03407)'
      title: Detecting Strategic Deception Using Linear Probes
      authors:
      - Nicholas Goldowsky-Dill
      - Bilal Chughtai
      - Stefan Heimersheim
      - Marius Hobbhahn
      author_organizations:
      - Apollo Research
      date: '2025-02-05'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.anthropic.com/research/auditing-hidden-objectives
      link_text: Auditing language models for hidden objectives
      original_md: '* [Auditing language models for hidden objectives](https://www.anthropic.com/research/auditing-hidden-objectives),
        2025-03-13, Anthropic Blog'
      title: Auditing language models for hidden objectives
      authors: []
      author_organizations:
      - Anthropic
      date: '2025-03-13'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.07326
      link_text: Reward Model Interpretability via Optimal and Pessimal Tokens
      original_md: '* [Reward Model Interpretability via Optimal and Pessimal Tokens](https://arxiv.org/abs/2506.07326),
        Brian Christian, Hannah Rose Kirk, Jessica A.F. Thompson et al., 2025-06-08,
        FAccT ''25 (ACM Conference on Fairness, Accountability, and Transparency)'
      title: Reward Model Interpretability via Optimal and Pessimal Tokens
      authors:
      - Brian Christian
      - Hannah Rose Kirk
      - Jessica A.F. Thompson
      - Christopher Summerfield
      - Tsvetomira Dumbalska
      author_organizations: []
      date: '2025-06-08'
      published_year: 2025
      venue: FAccT '25 (ACM Conference on Fairness, Accountability, and Transparency)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.17420
      link_text: 'The Geometry of Refusal in Large Language Models: Concept Cones
        and Representational Independence'
      original_md: '* [The Geometry of Refusal in Large Language Models: Concept Cones
        and Representational Independence](https://arxiv.org/abs/2502.17420), Tom
        Wollschläger, Jannes Elstner, Simon Geisler et al., 2025-02-24, arXiv'
      title: 'The Geometry of Refusal in Large Language Models: Concept Cones and
        Representational Independence'
      authors:
      - Tom Wollschläger
      - Jannes Elstner
      - Simon Geisler
      - Vincent Cohen-Addad
      - Stephan Günnemann
      - Johannes Gasteiger
      author_organizations:
      - Technical University of Munich
      date: '2025-02-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2508.05625
      link_text: How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics
        in Multi-Turn Conversations
      original_md: '* [How Do LLMs Persuade? Linear Probes Can Uncover Persuasion
        Dynamics in Multi-Turn Conversations](https://arxiv.org/abs/2508.05625), Brandon
        Jaipersaud, David Krueger, Ekdeep Singh Lubana, 2025-08-07, arXiv'
      title: How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in
        Multi-Turn Conversations
      authors:
      - Brandon Jaipersaud
      - David Krueger
      - Ekdeep Singh Lubana
      author_organizations: []
      date: '2025-08-07'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.09003
      link_text: Refusal in LLMs is an Affine Function
      original_md: '* [Refusal in LLMs is an Affine Function](https://arxiv.org/abs/2411.09003),
        Thomas Marshall, Adam Scherlis, Nora Belrose, 2024-11-13, arXiv'
      title: Refusal in LLMs is an Affine Function
      authors:
      - Thomas Marshall
      - Adam Scherlis
      - Nora Belrose
      author_organizations:
      - EleutherAI
      date: '2024-11-13'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/7zhAwcBri7yupStKy/here-s-18-applications-of-deception-probes
      link_text: Here's 18 Applications of Deception Probes
      original_md: '* [Here''s 18 Applications of Deception Probes](https://lesswrong.com/posts/7zhAwcBri7yupStKy/here-s-18-applications-of-deception-probes),
        Cleo Nardo, Avi Parrack, jordine, 2025-08-28, LessWrong'
      title: Here's 18 Applications of Deception Probes
      authors:
      - Cleo Nardo
      - Avi Parrack
      - jordine
      author_organizations: []
      date: '2025-08-28'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://www.alignmentforum.org/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging
      link_text: White Box Control at UK AISI - Update on Sandbagging Investigations
      original_md: '* [White Box Control at UK AISI \- Update on Sandbagging Investigations](https://www.alignmentforum.org/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging)'
      title: White Box Control at UK AISI - Update on Sandbagging Investigations
      authors:
      - Joseph Bloom
      - Jordan Taylor
      - Connor Kissane
      - Sid Black
      - merizian
      - alexdzm
      - jacoba
      - Ben Millwood
      - Alan Cooney
      author_organizations:
      - UK AISI
      date: '2025-07-10'
      published_year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - link_url: https://alignment.anthropic.com/2025/cheap-monitors
      link_text: Cost-Effective Constitutional Classifiers via Representation Re-use
      original_md: '* [Cost-Effective Constitutional Classifiers via Representation
        Re-use](https://alignment.anthropic.com/2025/cheap-monitors)'
      title: Cost-Effective Constitutional Classifiers via Representation Re-use
      authors:
      - Hoagy Cunningham
      - Alwin Peng
      - Jerry Wei
      - Euan Ong
      - Fabien Roger
      - Linda Petrini
      - Misha Wagner
      - Vladimir Mikulik
      - Mrinank Sharma
      author_organizations:
      - Anthropic
      date: null
      published_year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    other_attributes: {}
  parsing_issues: []
- id: a:deception_detectors
  name: Lie and deception detectors
  header_level: 3
  parent_id: sec:interpretability
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Detect when a model is being deceptive or lying by building
      white- or black-box detectors. Some work below requires intent in their definition,
      while other work focuses only on whether the model states something it believes
      to be false, regardless of intent.
    theory_of_change: Such detectors could flag suspicious behavior during evaluations
      or deployment, augment training to reduce deception, or audit models pre-deployment.
      Specific applications include alignment evaluations (e.g. by validating answers
      to introspective questions), safeguarding evaluations (catching models that
      "sandbag", that is, strategically underperform to pass capability tests), and
      large-scale deployment monitoring. An honest version of a model could also provide
      oversight during training or detect cases where a model behaves in ways it understands
      are unsafe.
    see_also:
    - a:interp_fundamental
    - a:ai_deception
    - a:evals_sandbagging
    orthodox_problems: []
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - '[Cadenza](https://cadenzalabs.org)'
    - Sam Marks
    - Rowan Wang
    - Kieron Kretschmar
    - Sharan Maiya
    - Walter Laurito
    - Chris Cundy
    - Adam Gleave
    - Aviel Parrack
    - Stefan Heimersheim
    - Carlo Attubato
    - Joseph Bloom
    - Jordan Taylor
    - Alex McKenzie
    - Urja Pawar
    - Lewis Smith
    - Bilal Chughtai
    - Neel Nanda
    estimated_ftes: 10-50
    critiques: difficult to determine if behavior is strategic deception or only low
      level "reflexive" actions; Unclear if a model roleplaying a liar has deceptive
      intent. [How are intentional descriptions (like deception) related to algorithmic
      ones (like understanding the mechanisms models use)?](https://www.lesswrong.com/posts/YXNeA3RyRrrRWS37A/a-problem-to-solve-before-building-a-deception-detector),
      [Is This Lie Detector Really Just a Lie Detector? An Investigation of LLM Probe
      Specificity](https://www.lesswrong.com/posts/5dkhdRMypeuyoXfmb/is-this-lie-detector-really-just-a-lie-detector-an),[Herrmann](https://www.lesswrong.com/posts/bCQbSFrnnAk7CJNpM/still-no-lie-detector-for-llms),
      [Smith and Chughtai](https://arxiv.org/abs/2511.22662)
    funded_by: Anthropic, Deepmind, UK AISI, Coefficient Giving
    outputs:
    - link_url: https://arxiv.org/abs/2508.19505
      link_text: 'Caught in the Act: a mechanistic approach to detecting deception'
      original_md: '* [**Caught in the Act: a mechanistic approach to detecting deception**](https://arxiv.org/abs/2508.19505)'
      title: 'Caught in the Act: a mechanistic approach to detecting deception'
      authors:
      - Gerard Boxo
      - Ryan Socha
      - Daniel Yoo
      - Shivam Raval
      author_organizations: []
      date: '2025-08-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.lesswrong.com/posts/9pGbTz6c78PGwJein/detecting-strategic-deception-using-linear-probes
      link_text: Detecting Strategic Deception Using Linear Probes
      original_md: '* [Detecting Strategic Deception Using Linear Probes](https://www.lesswrong.com/posts/9pGbTz6c78PGwJein/detecting-strategic-deception-using-linear-probes)'
      title: Detecting Strategic Deception Using Linear Probes
      authors:
      - Nicholas Goldowsky-Dill
      - Bilal Chughtai
      - Stefan Heimersheim
      - Marius Hobbhahn
      author_organizations:
      - Apollo Research
      date: '2025-02-06'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.10805
      link_text: Detecting High-Stakes Interactions with Activation Probes
      original_md: '* [Detecting High-Stakes Interactions with Activation Probes](https://arxiv.org/abs/2506.10805)'
      title: Detecting High-Stakes Interactions with Activation Probes
      authors:
      - Alex McKenzie
      - Urja Pawar
      - Phil Blandfort
      - William Bankes
      - David Krueger
      - Ekdeep Singh Lubana
      - Dmitrii Krasheninnikov
      author_organizations: []
      date: '2025-06-12'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.lesswrong.com/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging
      link_text: Whitebox detection of sandbagging model organisms
      original_md: '* [Whitebox detection of sandbagging model organisms](https://www.lesswrong.com/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging)'
      title: White Box Control at UK AISI - Update on Sandbagging Investigations
      authors:
      - Joseph Bloom
      - Jordan Taylor
      - Connor Kissane
      - Sid Black
      - Jacob Merizian
      - Alex Zelenka-Martin
      - Jacob Arbeid
      - Ben Millwood
      - Alan Cooney
      author_organizations:
      - UK AISI
      date: '2025-07-10'
      published_year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - link_url: https://www.alignmentforum.org/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging
      link_text: White Box Control at UK AISI - Update on Sandbagging Investigations
      original_md: '* [White Box Control at UK AISI \- Update on Sandbagging Investigations](https://www.alignmentforum.org/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging)'
      title: White Box Control at UK AISI - Update on Sandbagging Investigations
      authors:
      - Joseph Bloom
      - Jordan Taylor
      - Connor Kissane
      - Sid Black
      - merizian
      - alexdzm
      - jacoba
      - Ben Millwood
      - Alan Cooney
      author_organizations:
      - UK AISI
      date: '2025-07-10'
      published_year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - link_url: https://www.lesswrong.com/posts/eaEqAzGN3uJfpfGoc/trusted-monitoring-but-with-deception-probes
      link_text: Benchmarking deception probes for trusted monitoring
      original_md: '* [Benchmarking deception probes for trusted monitoring](https://www.lesswrong.com/posts/eaEqAzGN3uJfpfGoc/trusted-monitoring-but-with-deception-probes)'
      title: Trusted monitoring, but with deception probes.
      authors:
      - Avi Parrack
      - StefanHex
      - Cleo Nardo
      author_organizations:
      - Stanford University
      date: '2024-07-23'
      published_year: 2024
      venue: LessWrong
      kind: lesswrong
    - link_url: https://www.lesswrong.com/posts/gRc8KL2HLtKkFmNPr/among-us-a-sandbox-for-agentic-deception
      link_text: Probes and SAEs do well on Among Us benchmark
      original_md: '* [Probes and SAEs do well on Among Us benchmark](https://www.lesswrong.com/posts/gRc8KL2HLtKkFmNPr/among-us-a-sandbox-for-agentic-deception)'
      title: 'Among Us: A Sandbox for Agentic Deception'
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: blocked
    - link_url: https://www.lesswrong.com/posts/7zhAwcBri7yupStKy/18-applications-of-deception-probes
      link_text: 18 Applications of Deception Probes
      original_md: '* [18 Applications of Deception Probes](https://www.lesswrong.com/posts/7zhAwcBri7yupStKy/18-applications-of-deception-probes)'
      title: Here's 18 Applications of Deception Probes
      authors:
      - Cleo Nardo
      - Avi Parrack
      - jordine
      author_organizations: []
      date: '2025-08-28'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2505.13787
      link_text: Preference Learning with Lie Detectors can Induce Honesty or Evasion
      original_md: '* [Preference Learning with Lie Detectors can Induce Honesty or
        Evasion](https://arxiv.org/abs/2505.13787)'
      title: Preference Learning with Lie Detectors can Induce Honesty or Evasion
      authors:
      - Chris Cundy
      - Adam Gleave
      author_organizations: []
      date: '2025-05-20'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/html/2511.16035v1
      link_text: 'Liars'' Bench: Evaluating Lie Detectors for Language Models'
      original_md: '* [Liars'' Bench: Evaluating Lie Detectors for Language Models](https://arxiv.org/html/2511.16035v1)'
      title: 'Liars'' Bench: Evaluating Lie Detectors for Language Models'
      authors:
      - Kieron Kretschmar
      - Walter Laurito
      - Sharan Maiya
      - Samuel Marks
      author_organizations:
      - Cadenza Labs
      - Anthropic
      - FZI
      - University of Cambridge
      date: '2025-11-20'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://alignment.anthropic.com/2025/honesty-elicitation/
      link_text: Evaluating honesty and lie detection techniques on a diverse suite
        of dishonest models
      original_md: '[Evaluating honesty and lie detection techniques on a diverse
        suite of dishonest models](https://alignment.anthropic.com/2025/honesty-elicitation/)'
      title: Evaluating honesty and lie detection techniques on a diverse suite of
        dishonest models
      authors:
      - Rowan Wang
      - Johannes Treutlein
      - Fabien Roger
      - Evan Hubinger
      - Sam Marks
      author_organizations:
      - Anthropic
      date: '2025-11-25'
      published_year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    other_attributes: {}
  parsing_issues:
  - Broad approach field says 'cognitivist' - mapped to 'cognitivist_science'
- id: a:model_diff
  name: Model diffing
  header_level: 3
  parent_id: sec:interpretability
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: jord ✅
    one_sentence_summary: Understand what happens when a model is finetuned, what
      the "diff" between the finetuned and the original model consists in.
    theory_of_change: By identifying the mechanistic differences between a base model
      and its fine-tune (e.g., after RLHF), maybe we can verify that safety behaviors
      are robustly "internalized" rather than superficially patched, and detect if
      dangerous capabilities or deceptive alignment have been introduced without needing
      to re-analyze the entire model. The diff is also much smaller, since most parameters
      don't change, which means you can use heavier methods on them.
    see_also:
    - a:interp_sparse_coding
    - a:interp_fundamental
    orthodox_problems:
    - value_fragile
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - Julian Minder
    - Clément Dumas
    - Neel Nanda
    - Trenton Bricken
    - Jack Lindsey
    estimated_ftes: 10-30
    critiques: null
    funded_by: various academic groups, Anthropic, Google DeepMind
    outputs:
    - link_url: https://arxiv.org/abs/2510.13900
      link_text: Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences
      original_md: '* [**Narrow Finetuning Leaves Clearly Readable Traces in Activation
        Differences**](https://arxiv.org/abs/2510.13900), *Julian Minder, Clément
        Dumas, Stewart Slocum et al.*, 2025-10-14, arXiv'
      title: Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences
      authors:
      - Julian Minder
      - Clément Dumas
      - Stewart Slocum
      - Helena Casademunt
      - Cameron Holmes
      - Robert West
      - Neel Nanda
      author_organizations:
      - Google DeepMind
      date: '2025-10-14'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.02922
      link_text: Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning
      original_md: '* [**Overcoming Sparsity Artifacts in Crosscoders to Interpret
        Chat-Tuning**](https://arxiv.org/abs/2504.02922), *Julian Minder, Clément
        Dumas, Caden Juang et al.*, 2025-04-03, NeurIPS 2025'
      title: Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning
      authors:
      - Julian Minder
      - Clément Dumas
      - Caden Juang
      - Bilal Chugtai
      - Neel Nanda
      author_organizations: []
      date: '2025-04-03'
      published_year: 2025
      venue: NeurIPS 2025
      kind: paper_preprint
    - link_url: https://www.lesswrong.com/posts/xmpauEXEerzYcJKNm/what-we-learned-trying-to-diff-base-and-chat-models-and-why
      link_text: https://www.lesswrong.com/posts/xmpauEXEerzYcJKNm/what-we-learned-trying-to-diff-base-and-chat-models-and-why
      original_md: '* [https://www.lesswrong.com/posts/xmpauEXEerzYcJKNm/what-we-learned-trying-to-diff-base-and-chat-models-and-why](https://www.lesswrong.com/posts/xmpauEXEerzYcJKNm/what-we-learned-trying-to-diff-base-and-chat-models-and-why)'
      title: What We Learned Trying to Diff Base and Chat Models (And Why It Matters)
      authors:
      - Clément Dumas
      - Julian Minder
      - Neel Nanda
      author_organizations:
      - MATS Program
      date: '2025-06-30'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2506.19823
      link_text: Persona Features Control Emergent Misalignment
      original_md: '* [**Persona Features Control Emergent Misalignment**](https://arxiv.org/abs/2506.19823),
        *Miles Wang, Tom Dupré la Tour, Olivia Watkins et al.*, 2025-06-24, arXiv'
      title: Persona Features Control Emergent Misalignment
      authors:
      - Miles Wang
      - Tom Dupré la Tour
      - Olivia Watkins
      - Alex Makelov
      - Ryan A. Chi
      - Samuel Miserendino
      - Jeffrey Wang
      - Achyuta Rajaram
      - Johannes Heidecke
      - Tejal Patwardhan
      - Dan Mossing
      author_organizations: []
      date: '2025-06-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://transformer-circuits.pub/2025/crosscoder-diffing-update/index.html
      link_text: Insights on Crosscoder Model Diffing
      original_md: '* [Insights on Crosscoder Model Diffing](https://transformer-circuits.pub/2025/crosscoder-diffing-update/index.html)'
      title: Insights on Crosscoder Model Diffing
      authors:
      - Siddharth Mishra-Sharma
      - Trenton Bricken
      - Jack Lindsey
      - Adam Jermyn
      - Jonathan Marcus
      - Kelley Rivoire
      - Christopher Olah
      - Thomas Henighan
      author_organizations:
      - Anthropic
      date: null
      published_year: null
      venue: Transformer Circuits Thread
      kind: blog_post
    - link_url: https://www.lesswrong.com/posts/srt6JXsRMtmqAJavD/open-source-replication-of-anthropic-s-crosscoder-paper-for
      link_text: Open Source Replication of Anthropic's Crosscoder paper for model-diffing
      original_md: '* [Open Source Replication of Anthropic''s Crosscoder paper for
        model-diffing](https://www.lesswrong.com/posts/srt6JXsRMtmqAJavD/open-source-replication-of-anthropic-s-crosscoder-paper-for)'
      title: Open Source Replication of Anthropic's Crosscoder paper for model-diffing
      authors:
      - Connor Kissane
      - robertzk
      - Arthur Conmy
      - Neel Nanda
      author_organizations: []
      date: '2024-10-27'
      published_year: 2024
      venue: LessWrong
      kind: lesswrong
    - link_url: https://github.com/science-of-finetuning/diffing-toolkit%20
      link_text: 'Diffing Toolkit: Model Comparison and Analysis Framework'
      original_md: '* [Diffing Toolkit: Model Comparison and Analysis Framework](https://github.com/science-of-finetuning/diffing-toolkit%20)'
      title: diffing-toolkit (GitHub 404 Error)
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: GitHub
      kind: error_detected
    - link_url: https://openreview.net/forum?id=ZB84SvrZB8%20
      link_text: 'Cross-Architecture Model Diffing with Crosscoders: Unsupervised
        Discovery of Differences Between LLMs'
      original_md: '* [Cross-Architecture Model Diffing with Crosscoders: Unsupervised
        Discovery of Differences Between LLMs](https://openreview.net/forum?id=ZB84SvrZB8%20)'
      title: 'Error: Content Not Found'
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: OpenReview
      kind: error_detected
    - link_url: https://www.goodfire.ai/research/model-diff-amplification
      link_text: https://www.goodfire.ai/research/model-diff-amplification
      original_md: '* [https://www.goodfire.ai/research/model-diff-amplification](https://www.goodfire.ai/research/model-diff-amplification)'
      title: Discovering Undesired Rare Behaviors via Model Diff Amplification
      authors:
      - Santiago Aranguri
      - Thomas McGrath
      author_organizations:
      - Goodfire
      - NYU
      date: '2025-08-21'
      published_year: 2025
      venue: Goodfire Research
      kind: blog_post
    other_attributes: {}
  parsing_issues: []
- id: a:interp_sparse_coding
  name: Sparse Coding
  header_level: 3
  parent_id: sec:interpretability
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stephen ✅
    one_sentence_summary: Decompose the polysemantic activations of the residual stream
      into a sparse linear combination of monosemantic "features" which correspond
      to interpretable concepts.
    theory_of_change: Get a principled decomposition of an LLM's activation into atomic
      components → identify deception and other misbehaviors.
    see_also:
    - a:interp_concept_based
    - a:interp_fundamental
    orthodox_problems:
    - value_fragile
    - goals_misgeneralize
    - superintelligence_fool_supervisors
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: null
    broad_approach_text: engineering / cognitive
    some_names:
    - Leo Gao
    - Dan Mossing
    - Emmanuel Ameisen
    - Jack Lindsey
    - Adam Pearce
    - Thomas Heap
    - Abhinav Menon
    - Kenny Peng
    - Tim Lawson
    estimated_ftes: 50-100
    critiques: '[Sparse Autoencoders Can Interpret Randomly Initialized Transformers](https://arxiv.org/abs/2501.17727),
      [The Sparse Autoencoders bubble has popped, but they are still promising](https://agarriga.substack.com/p/the-sparse-autoencoders-bubble-has),
      [Negative Results for SAEs On Downstream Tasks and Deprioritising SAE Research](https://www.alignmentforum.org/posts/4uXCAJNuPKtKBsi28/),
      [Sparse Autoencoders Trained on the Same Data Learn Different Features](https://arxiv.org/pdf/2501.16615),
      [Why Not Just Train For Interpretability?](https://www.lesswrong.com/posts/2HbgHwdygH6yeHKKq/why-not-just-train-for-interpretability)'
    funded_by: everyone, roughly. Frontier labs, LTFF, Coefficient Giving, etc.
    outputs:
    - link_url: https://cdn.openai.com/pdf/41df8f28-d4ef-43e9-aed2-823f9393e470/circuit-sparsity-paper.pdf
      link_text: Weight-sparse transformers have interpretable circuits
      original_md: '* [Weight-sparse transformers have interpretable circuits](https://cdn.openai.com/pdf/41df8f28-d4ef-43e9-aed2-823f9393e470/circuit-sparsity-paper.pdf)'
      title: Unknown - PDF content not accessible
      authors: []
      author_organizations:
      - OpenAI
      date: null
      published_year: null
      venue: null
      kind: error_detected
    - link_url: https://transformer-circuits.pub/2025/attribution-graphs/methods.html
      link_text: 'Circuit Tracing: Revealing Computational Graphs in Language Models'
      original_md: '* [Circuit Tracing: Revealing Computational Graphs in Language
        Models](https://transformer-circuits.pub/2025/attribution-graphs/methods.html),
        Emmanuel Ameisen, Jack Lindsey, Adam Pearce et al., 2025-03-27, Transformer
        Circuits Thread'
      title: 'Circuit Tracing: Revealing Computational Graphs in Language Models'
      authors:
      - Emmanuel Ameisen
      - Jack Lindsey
      - Adam Pearce
      - Wes Gurnee
      - Nicholas L. Turner
      - Brian Chen
      - Craig Citro
      - David Abrahams
      - Shan Carter
      - Basil Hosmer
      - Jonathan Marcus
      - Michael Sklar
      - Adly Templeton
      - Trenton Bricken
      - Callum McDougall
      - Hoagy Cunningham
      - Thomas Henighan
      - Adam Jermyn
      - Andy Jones
      - Andrew Persic
      - Zhenyi Qi
      - T. Ben Thompson
      - Sam Zimmerman
      - Kelley Rivoire
      - Thomas Conerly
      - Chris Olah
      - Joshua Batson
      author_organizations:
      - Anthropic
      date: '2025-03-27'
      published_year: 2025
      venue: Transformer Circuits Thread
      kind: blog_post
    - link_url: https://arxiv.org/abs/2504.02922
      link_text: Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning
      original_md: '* [Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning](https://arxiv.org/abs/2504.02922),
        Julian Minder, Clément Dumas, Caden Juang et al., 2025-04-03, NeurIPS 2025'
      title: Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning
      authors:
      - Julian Minder
      - Clément Dumas
      - Caden Juang
      - Bilal Chugtai
      - Neel Nanda
      author_organizations: []
      date: '2025-04-03'
      published_year: 2025
      venue: NeurIPS 2025
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.02821
      link_text: Sparse Autoencoders Learn Monosemantic Features in Vision-Language
        Models
      original_md: '* [Sparse Autoencoders Learn Monosemantic Features in Vision-Language
        Models](https://arxiv.org/abs/2504.02821), Mateusz Pach, Shyamgopal Karthik,
        Quentin Bouniot et al., 2025-04-03, arXiv'
      title: Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models
      authors:
      - Mateusz Pach
      - Shyamgopal Karthik
      - Quentin Bouniot
      - Serge Belongie
      - Zeynep Akata
      author_organizations: []
      date: '2025-04-03'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.18878
      link_text: 'I Have Covered All the Bases Here: Interpreting Reasoning Features
        in Large Language Models via Sparse Autoencoders'
      original_md: '* [I Have Covered All the Bases Here: Interpreting Reasoning Features
        in Large Language Models via Sparse Autoencoders](https://arxiv.org/abs/2503.18878),
        Andrey Galichin, Alexey Dontsov, Polina Druzhinina et al., 2025-03-24, arXiv'
      title: 'I Have Covered All the Bases Here: Interpreting Reasoning Features in
        Large Language Models via Sparse Autoencoders'
      authors:
      - Andrey Galichin
      - Alexey Dontsov
      - Polina Druzhinina
      - Anton Razzhigaev
      - Oleg Y. Rogov
      - Elena Tutubalina
      - Ivan Oseledets
      author_organizations:
      - AIRI Institute
      date: '2025-03-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.04878
      link_text: Sparse Autoencoders Do Not Find Canonical Units of Analysis
      original_md: '* [Sparse Autoencoders Do Not Find Canonical Units of Analysis](https://arxiv.org/abs/2502.04878),
        Patrick Leask, Bart Bussmann, Michael Pearce et al., 2025-02-07, arXiv (accepted
        to ICLR 2025)'
      title: Sparse Autoencoders Do Not Find Canonical Units of Analysis
      authors:
      - Patrick Leask
      - Bart Bussmann
      - Michael Pearce
      - Joseph Bloom
      - Curt Tigges
      - Noura Al Moubayed
      - Lee Sharkey
      - Neel Nanda
      author_organizations: []
      date: '2025-02-07'
      published_year: 2025
      venue: arXiv (accepted to ICLR 2025)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2501.18823
      link_text: Transcoders Beat Sparse Autoencoders for Interpretability
      original_md: '* [Transcoders Beat Sparse Autoencoders for Interpretability](https://arxiv.org/abs/2501.18823),
        Gonçalo Paulo, Stepan Shabalin, Nora Belrose, 2025-01-31, arXiv'
      title: Transcoders Beat Sparse Autoencoders for Interpretability
      authors:
      - Gonçalo Paulo
      - Stepan Shabalin
      - Nora Belrose
      author_organizations: []
      date: '2025-01-31'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.07775
      link_text: The Unintended Trade-off of AI Alignment:Balancing Hallucination
        Mitigation and Safety in LLMs
      original_md: '* [The Unintended Trade-off of AI Alignment:Balancing Hallucination
        Mitigation and Safety in LLMs](https://arxiv.org/abs/2510.07775), Omar Mahmoud,
        Ali Khalil, Buddhika Laknath Semage et al., 2025-10-09, arXiv'
      title: The Unintended Trade-off of AI Alignment:Balancing Hallucination Mitigation
        and Safety in LLMs
      authors:
      - Omar Mahmoud
      - Ali Khalil
      - Buddhika Laknath Semage
      - Thommen George Karimpanal
      - Santu Rana
      author_organizations: []
      date: '2025-10-09'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.13756
      link_text: Scaling sparse feature circuit finding for in-context learning
      original_md: '* [Scaling sparse feature circuit finding for in-context learning](https://arxiv.org/abs/2504.13756),
        Dmitrii Kharlapenko, Stepan Shabalin, Fazl Barez et al., 2025-04-18, arXiv'
      title: Scaling sparse feature circuit finding for in-context learning
      authors:
      - Dmitrii Kharlapenko
      - Stepan Shabalin
      - Fazl Barez
      - Arthur Conmy
      - Neel Nanda
      author_organizations: []
      date: '2025-04-18'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.17547
      link_text: Learning Multi-Level Features with Matryoshka Sparse Autoencoders
      original_md: '* [Learning Multi-Level Features with Matryoshka Sparse Autoencoders](https://arxiv.org/abs/2503.17547),
        Bart Bussmann, Noa Nabeshima, Adam Karvonen et al., 2025-03-21, arXiv'
      title: Learning Multi-Level Features with Matryoshka Sparse Autoencoders
      authors:
      - Bart Bussmann
      - Noa Nabeshima
      - Adam Karvonen
      - Neel Nanda
      author_organizations: []
      date: '2025-03-21'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.16681
      link_text: Are Sparse Autoencoders Useful? A Case Study in Sparse Probing
      original_md: '* [Are Sparse Autoencoders Useful? A Case Study in Sparse Probing](https://arxiv.org/abs/2502.16681),
        Subhash Kantamneni, Joshua Engels, Senthooran Rajamanoharan et al., 2025-02-23,
        arXiv'
      title: Are Sparse Autoencoders Useful? A Case Study in Sparse Probing
      authors:
      - Subhash Kantamneni
      - Joshua Engels
      - Senthooran Rajamanoharan
      - Max Tegmark
      - Neel Nanda
      author_organizations: []
      date: '2025-02-23'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2501.16615
      link_text: Sparse Autoencoders Trained on the Same Data Learn Different Features
      original_md: '* [Sparse Autoencoders Trained on the Same Data Learn Different
        Features](https://arxiv.org/abs/2501.16615), Gonçalo Paulo, Nora Belrose,
        2025-01-28, arXiv'
      title: Sparse Autoencoders Trained on the Same Data Learn Different Features
      authors:
      - Gonçalo Paulo
      - Nora Belrose
      author_organizations: []
      date: '2025-01-28'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2501.18838
      link_text: Partially Rewriting a Transformer in Natural Language
      original_md: '* [Partially Rewriting a Transformer in Natural Language](https://arxiv.org/abs/2501.18838),
        Gonçalo Paulo, Nora Belrose, 2025-01-31, arXiv'
      title: Partially Rewriting a Transformer in Natural Language
      authors:
      - Gonçalo Paulo
      - Nora Belrose
      author_organizations: []
      date: '2025-01-31'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.09532
      link_text: 'SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language
        Model Interpretability'
      original_md: '* [SAEBench: A Comprehensive Benchmark for Sparse Autoencoders
        in Language Model Interpretability](https://arxiv.org/abs/2503.09532), Adam
        Karvonen, Can Rager, Johnny Lin et al., 2025-06-04, arXiv (accepted to ICML
        2025)'
      title: 'SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language
        Model Interpretability'
      authors:
      - Adam Karvonen
      - Can Rager
      - Johnny Lin
      - Curt Tigges
      - Joseph Bloom
      - David Chanin
      - Yeu-Tong Lau
      - Eoin Farrell
      - Callum McDougall
      - Kola Ayonrinde
      - Demian Till
      - Matthew Wearden
      - Arthur Conmy
      - Samuel Marks
      - Neel Nanda
      author_organizations: []
      date: '2025-06-04'
      published_year: 2025
      venue: arXiv (accepted to ICML 2025)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2501.19406
      link_text: Low-Rank Adapting Models for Sparse Autoencoders
      original_md: '* [Low-Rank Adapting Models for Sparse Autoencoders](https://arxiv.org/abs/2501.19406),
        Matthew Chen, Joshua Engels, Max Tegmark, 2025-01-31, arXiv'
      title: Low-Rank Adapting Models for Sparse Autoencoders
      authors:
      - Matthew Chen
      - Joshua Engels
      - Max Tegmark
      author_organizations:
      - MIT
      date: '2025-01-31'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2501.08319
      link_text: Enhancing Automated Interpretability with Output-Centric Feature
        Descriptions
      original_md: '* [Enhancing Automated Interpretability with Output-Centric Feature
        Descriptions](https://arxiv.org/abs/2501.08319), Yoav Gur-Arieh, Roy Mayan,
        Chen Agassy et al., 2025-01-14, arXiv (accepted to ACL 2025)'
      title: Enhancing Automated Interpretability with Output-Centric Feature Descriptions
      authors:
      - Yoav Gur-Arieh
      - Roy Mayan
      - Chen Agassy
      - Atticus Geiger
      - Mor Geva
      author_organizations: []
      date: '2025-01-14'
      published_year: 2025
      venue: arXiv (accepted to ACL 2025)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.03730
      link_text: 'Towards Understanding Distilled Reasoning Models: A Representational
        Approach'
      original_md: '* [Towards Understanding Distilled Reasoning Models: A Representational
        Approach](https://arxiv.org/abs/2503.03730), David D. Baek, Max Tegmark, 2025-03-05,
        ICLR 2025 Workshop on Building Trust in Language Models and Applications'
      title: 'Towards Understanding Distilled Reasoning Models: A Representational
        Approach'
      authors:
      - David D. Baek
      - Max Tegmark
      author_organizations:
      - MIT
      date: '2025-03-05'
      published_year: 2025
      venue: ICLR 2025 Workshop on Building Trust in Language Models and Applications
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.19964
      link_text: Do Sparse Autoencoders Generalize? A Case Study of Answerability
      original_md: '* [Do Sparse Autoencoders Generalize? A Case Study of Answerability](https://arxiv.org/abs/2502.19964),
        Lovis Heindrich, Philip Torr, Fazl Barez et al., 2025-02-27, ICML 2025 Workshop
        on Reliable and Responsible Foundation Models (arXiv preprint)'
      title: Do Sparse Autoencoders Generalize? A Case Study of Answerability
      authors:
      - Lovis Heindrich
      - Philip Torr
      - Fazl Barez
      - Veronika Thost
      author_organizations: []
      date: '2025-02-27'
      published_year: 2025
      venue: ICML 2025 Workshop on Reliable and Responsible Foundation Models (arXiv
        preprint)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2501.06346
      link_text: Large Language Models Share Representations of Latent Grammatical
        Concepts Across Typologically Diverse Languages
      original_md: '* [Large Language Models Share Representations of Latent Grammatical
        Concepts Across Typologically Diverse Languages](https://arxiv.org/abs/2501.06346),
        Jannik Brinkmann, Chris Wendler, Christian Bartelt et al., 2025-01-10, arXiv'
      title: Large Language Models Share Representations of Latent Grammatical Concepts
        Across Typologically Diverse Languages
      authors:
      - Jannik Brinkmann
      - Chris Wendler
      - Christian Bartelt
      - Aaron Mueller
      author_organizations: []
      date: '2025-01-10'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.11695
      link_text: Interpreting the linear structure of vision-language model embedding
        spaces
      original_md: '* [Interpreting the linear structure of vision-language model
        embedding spaces](https://arxiv.org/abs/2504.11695), Isabel Papadimitriou,
        Huangyuan Su, Thomas Fel et al., 2025-04-16, COLM 2025'
      title: Interpreting the linear structure of vision-language model embedding
        spaces
      authors:
      - Isabel Papadimitriou
      - Huangyuan Su
      - Thomas Fel
      - Sham Kakade
      - Stephanie Gil
      author_organizations: []
      date: '2025-04-16'
      published_year: 2025
      venue: COLM 2025
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.26202
      link_text: What's In My Human Feedback? Learning Interpretable Descriptions
        of Preference Data
      original_md: '* [What''s In My Human Feedback? Learning Interpretable Descriptions
        of Preference Data](https://arxiv.org/abs/2510.26202), Rajiv Movva, Smitha
        Milli, Sewon Min et al., 2025-10-30, arXiv'
      title: What's In My Human Feedback? Learning Interpretable Descriptions of Preference
        Data
      authors:
      - Rajiv Movva
      - Smitha Milli
      - Sewon Min
      - Emma Pierson
      author_organizations: []
      date: '2025-10-30'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.14257
      link_text: Do I Know This Entity? Knowledge Awareness and Hallucinations in
        Language Models
      original_md: '* [Do I Know This Entity? Knowledge Awareness and Hallucinations
        in Language Models](https://arxiv.org/abs/2411.14257), Javier Ferrando, Oscar
        Obeso, Senthooran Rajamanoharan et al., 2024-11-21, arXiv'
      title: Do I Know This Entity? Knowledge Awareness and Hallucinations in Language
        Models
      authors:
      - Javier Ferrando
      - Oscar Obeso
      - Senthooran Rajamanoharan
      - Neel Nanda
      author_organizations: []
      date: '2024-11-21'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.10920
      link_text: Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative
        Matrix Factorization
      original_md: '* [Decomposing MLP Activations into Interpretable Features via
        Semi-Nonnegative Matrix Factorization](https://arxiv.org/abs/2506.10920),
        Or Shafran, Atticus Geiger, Mor Geva, 2025-06-12, arXiv'
      title: Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative
        Matrix Factorization
      authors:
      - Or Shafran
      - Atticus Geiger
      - Mor Geva
      author_organizations: []
      date: '2025-06-12'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2511.01836
      link_text: 'Priors in Time: Missing Inductive Biases for Language Model Interpretability'
      original_md: '* [Priors in Time: Missing Inductive Biases for Language Model
        Interpretability](https://arxiv.org/abs/2511.01836), Ekdeep Singh Lubana,
        Can Rager, Sai Sumedh R. Hindupur et al., 2025-11-03, arXiv'
      title: 'Priors in Time: Missing Inductive Biases for Language Model Interpretability'
      authors:
      - Ekdeep Singh Lubana
      - Can Rager
      - Sai Sumedh R. Hindupur
      - Valerie Costa
      - Greta Tuckute
      - Oam Patel
      - Sonia Krishna Murthy
      - Thomas Fel
      - Daniel Wurgaft
      - Eric J. Bigelow
      - Johnny Lin
      - Demba Ba
      - Martin Wattenberg
      - Fernanda Viegas
      - Melanie Weber
      - Aaron Mueller
      author_organizations: []
      date: '2025-11-03'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.17769
      link_text: 'Inference-Time Decomposition of Activations (ITDA): A Scalable Approach
        to Interpreting Large Language Models'
      original_md: '* [Inference-Time Decomposition of Activations (ITDA): A Scalable
        Approach to Interpreting Large Language Models](https://arxiv.org/abs/2505.17769),
        Patrick Leask, Neel Nanda, Noura Al Moubayed, 2025-05-23, ICML 2025'
      title: 'Inference-Time Decomposition of Activations (ITDA): A Scalable Approach
        to Interpreting Large Language Models'
      authors:
      - Patrick Leask
      - Neel Nanda
      - Noura Al Moubayed
      author_organizations:
      - Independent
      date: '2025-05-23'
      published_year: 2025
      venue: ICML 2025
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.25596
      link_text: Binary Sparse Coding for Interpretability
      original_md: '* [Binary Sparse Coding for Interpretability](https://arxiv.org/abs/2509.25596),
        Lucia Quirke, Stepan Shabalin, Nora Belrose, 2025-09-29, arXiv'
      title: Binary Sparse Coding for Interpretability
      authors:
      - Lucia Quirke
      - Stepan Shabalin
      - Nora Belrose
      author_organizations: []
      date: '2025-09-29'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.15679
      link_text: Dense SAE Latents Are Features, Not Bugs
      original_md: '* [Dense SAE Latents Are Features, Not Bugs](https://arxiv.org/abs/2506.15679),
        Xiaoqing Sun, Alessandro Stolfo, Joshua Engels et al., 2025-06-18, arXiv (NeurIPS
        2025)'
      title: Dense SAE Latents Are Features, Not Bugs
      authors:
      - Xiaoqing Sun
      - Alessandro Stolfo
      - Joshua Engels
      - Ben Wu
      - Senthooran Rajamanoharan
      - Mrinmaya Sachan
      - Max Tegmark
      author_organizations:
      - MIT
      - ETH Zurich
      date: '2025-06-18'
      published_year: 2025
      venue: arXiv (NeurIPS 2025)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.18895
      link_text: Evaluating Sparse Autoencoders on Targeted Concept Erasure Tasks
      original_md: '* [Evaluating Sparse Autoencoders on Targeted Concept Erasure
        Tasks](https://arxiv.org/abs/2411.18895), Adam Karvonen, Can Rager, Samuel
        Marks et al., 2024-11-28, arXiv'
      title: Evaluating Sparse Autoencoders on Targeted Concept Erasure Tasks
      authors:
      - Adam Karvonen
      - Can Rager
      - Samuel Marks
      - Neel Nanda
      author_organizations: []
      date: '2024-11-28'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.08473
      link_text: Evaluating SAE interpretability without explanations
      original_md: '* [Evaluating SAE interpretability without explanations](https://arxiv.org/abs/2507.08473),
        Gonçalo Paulo, Nora Belrose, 2025-07-11, arXiv'
      title: Evaluating SAE interpretability without explanations
      authors:
      - Gonçalo Paulo
      - Nora Belrose
      author_organizations: []
      date: '2025-07-11'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.20063
      link_text: SAEs Are Good for Steering -- If You Select the Right Features
      original_md: '* [SAEs Are Good for Steering \-- If You Select the Right Features](https://arxiv.org/abs/2505.20063),
        Dana Arad, Aaron Mueller, Yonatan Belinkov, 2025-05-26, arXiv'
      title: SAEs Are Good for Steering -- If You Select the Right Features
      authors:
      - Dana Arad
      - Aaron Mueller
      - Yonatan Belinkov
      author_organizations: []
      date: '2025-05-26'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.04706
      link_text: 'Line of Sight: On Linear Representations in VLLMs'
      original_md: '* [Line of Sight: On Linear Representations in VLLMs](https://arxiv.org/abs/2506.04706),
        Achyuta Rajaram, Sarah Schwettmann, Jacob Andreas et al., 2025-06-05, arXiv'
      title: 'Line of Sight: On Linear Representations in VLLMs'
      authors:
      - Achyuta Rajaram
      - Sarah Schwettmann
      - Jacob Andreas
      - Arthur Conmy
      author_organizations:
      - MIT
      - Independent
      date: '2025-06-05'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.00743
      link_text: 'Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting
        Rare Concepts in Foundation Models'
      original_md: '* [Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting
        Rare Concepts in Foundation Models](https://arxiv.org/abs/2411.00743), Aashiq
        Muhamed, Mona Diab, Virginia Smith, 2024-11-01, arXiv'
      title: 'Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting
        Rare Concepts in Foundation Models'
      authors:
      - Aashiq Muhamed
      - Mona Diab
      - Virginia Smith
      author_organizations: []
      date: '2024-11-01'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.01220
      link_text: Enhancing Neural Network Interpretability with Feature-Aligned Sparse
        Autoencoders
      original_md: '* [Enhancing Neural Network Interpretability with Feature-Aligned
        Sparse Autoencoders](https://arxiv.org/abs/2411.01220), Luke Marks, Alasdair
        Paren, David Krueger et al., 2024-11-02, arXiv'
      title: Enhancing Neural Network Interpretability with Feature-Aligned Sparse
        Autoencoders
      authors:
      - Luke Marks
      - Alasdair Paren
      - David Krueger
      - Fazl Barez
      author_organizations: []
      date: '2024-11-02'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2412.06410
      link_text: BatchTopK Sparse Autoencoders
      original_md: '* [BatchTopK Sparse Autoencoders](https://arxiv.org/abs/2412.06410),
        Bart Bussmann, Patrick Leask, Neel Nanda, 2024-12-09, arXiv'
      title: BatchTopK Sparse Autoencoders
      authors:
      - Bart Bussmann
      - Patrick Leask
      - Neel Nanda
      author_organizations: []
      date: '2024-12-09'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.02565
      link_text: Understanding sparse autoencoder scaling in the presence of feature
        manifolds
      original_md: '* [Understanding sparse autoencoder scaling in the presence of
        feature manifolds](https://arxiv.org/abs/2509.02565), Eric J. Michaud, Liv
        Gorton, Tom McGrath, 2025-09-02, arXiv'
      title: Understanding sparse autoencoder scaling in the presence of feature manifolds
      authors:
      - Eric J. Michaud
      - Liv Gorton
      - Tom McGrath
      author_organizations: []
      date: '2025-09-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.04128
      link_text: Internal states before wait modulate reasoning patterns
      original_md: '* [Internal states before wait modulate reasoning patterns](https://arxiv.org/abs/2510.04128),
        Dmitrii Troitskii, Koyena Pal, Chris Wendler et al., 2025-10-05, arXiv (EMNLP
        Findings 2025)'
      title: Internal states before wait modulate reasoning patterns
      authors:
      - Dmitrii Troitskii
      - Koyena Pal
      - Chris Wendler
      - Callum Stuart McDougall
      - Neel Nanda
      author_organizations:
      - Google DeepMind
      date: '2025-10-05'
      published_year: 2025
      venue: arXiv (EMNLP Findings 2025)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.20254
      link_text: 'Position: Mechanistic Interpretability Should Prioritize Feature
        Consistency in SAEs'
      original_md: '* [Position: Mechanistic Interpretability Should Prioritize Feature
        Consistency in SAEs](https://arxiv.org/abs/2505.20254), Xiangchen Song, Aashiq
        Muhamed, Yujia Zheng et al., 2025-05-26, arXiv'
      title: 'Position: Mechanistic Interpretability Should Prioritize Feature Consistency
        in SAEs'
      authors:
      - Xiangchen Song
      - Aashiq Muhamed
      - Yujia Zheng
      - Lingjing Kong
      - Zeyu Tang
      - Mona T. Diab
      - Virginia Smith
      - Kun Zhang
      author_organizations: []
      date: '2025-05-26'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.11976
      link_text: How Visual Representations Map to Language Feature Space in Multimodal
        LLMs
      original_md: '* [How Visual Representations Map to Language Feature Space in
        Multimodal LLMs](https://arxiv.org/abs/2506.11976), Constantin Venhoff, Ashkan
        Khakzar, Sonia Joseph et al., 2025-06-13, arXiv'
      title: How Visual Representations Map to Language Feature Space in Multimodal
        LLMs
      authors:
      - Constantin Venhoff
      - Ashkan Khakzar
      - Sonia Joseph
      - Philip Torr
      - Neel Nanda
      author_organizations:
      - Google DeepMind
      - University of Oxford
      date: '2025-06-13'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.19475
      link_text: 'Prisma: An Open Source Toolkit for Mechanistic Interpretability
        in Vision and Video'
      original_md: '* [Prisma: An Open Source Toolkit for Mechanistic Interpretability
        in Vision and Video](https://arxiv.org/abs/2504.19475), Sonia Joseph, Praneet
        Suresh, Lorenz Hufe et al., 2025-04-28, arXiv / CVPR Mechanistic Interpretability
        for Vision Workshop'
      title: 'Prisma: An Open Source Toolkit for Mechanistic Interpretability in Vision
        and Video'
      authors:
      - Sonia Joseph
      - Praneet Suresh
      - Lorenz Hufe
      - Edward Stevinson
      - Robert Graham
      - Yash Vadi
      - Danilo Bzdok
      - Sebastian Lapuschkin
      - Lee Sharkey
      - Blake Aaron Richards
      author_organizations:
      - Apollo Research
      - Various Academic Institutions
      date: '2025-04-28'
      published_year: 2025
      venue: arXiv / CVPR Mechanistic Interpretability for Vision Workshop
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.24360
      link_text: Interpreting Large Text-to-Image Diffusion Models with Dictionary
        Learning
      original_md: '* [Interpreting Large Text-to-Image Diffusion Models with Dictionary
        Learning](https://arxiv.org/abs/2505.24360), Stepan Shabalin, Ayush Panda,
        Dmitrii Kharlapenko et al., 2025-05-30, CVPR 2025 - Mechanistic Interpretability
        for Vision Workshop'
      title: Interpreting Large Text-to-Image Diffusion Models with Dictionary Learning
      authors:
      - Stepan Shabalin
      - Ayush Panda
      - Dmitrii Kharlapenko
      - Abdur Raheem Ali
      - Yixiong Hao
      - Arthur Conmy
      author_organizations: []
      date: '2025-05-30'
      published_year: 2025
      venue: CVPR 2025 - Mechanistic Interpretability for Vision Workshop
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2508.13650
      link_text: 'CRISP: Persistent Concept Unlearning via Sparse Autoencoders'
      original_md: '* [CRISP: Persistent Concept Unlearning via Sparse Autoencoders](https://arxiv.org/abs/2508.13650),
        Tomer Ashuach, Dana Arad, Aaron Mueller et al., 2025-08-19, arXiv'
      title: 'CRISP: Persistent Concept Unlearning via Sparse Autoencoders'
      authors:
      - Tomer Ashuach
      - Dana Arad
      - Aaron Mueller
      - Martin Tutek
      - Yonatan Belinkov
      author_organizations: []
      date: '2025-08-19'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.08192
      link_text: 'SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder Guardrails
        for Precision Unlearning in LLMs'
      original_md: '* [SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder Guardrails
        for Precision Unlearning in LLMs](https://arxiv.org/abs/2504.08192), Aashiq
        Muhamed, Jacopo Bonato, Mona Diab et al., 2025-04-11, arXiv'
      title: 'SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder Guardrails for
        Precision Unlearning in LLMs'
      authors:
      - Aashiq Muhamed
      - Jacopo Bonato
      - Mona Diab
      - Virginia Smith
      author_organizations: []
      date: '2025-04-11'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/PkeB4TLxgaNnSmddg/scaling-sparse-feature-circuit-finding-to-gemma-9b
      link_text: Scaling Sparse Feature Circuit Finding to Gemma 9B
      original_md: '* [Scaling Sparse Feature Circuit Finding to Gemma 9B](https://lesswrong.com/posts/PkeB4TLxgaNnSmddg/scaling-sparse-feature-circuit-finding-to-gemma-9b),
        Diego Caples, Jatin Nainani, CallumMcDougall et al., 2025-01-10, LessWrong'
      title: Scaling Sparse Feature Circuit Finding to Gemma 9B
      authors:
      - Diego Caples
      - Jatin Nainani
      - CallumMcDougall
      - rrenaud
      author_organizations:
      - MATS Program
      date: '2025-01-10'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/6oF6pRr2FgjTmiHus/topological-data-analysis-and-mechanistic-interpretability
      link_text: Topological Data Analysis and Mechanistic Interpretability
      original_md: '* [Topological Data Analysis and Mechanistic Interpretability](https://lesswrong.com/posts/6oF6pRr2FgjTmiHus/topological-data-analysis-and-mechanistic-interpretability),
        Gunnar Carlsson, 2025-02-24, LessWrong'
      title: Topological Data Analysis and Mechanistic Interpretability
      authors:
      - Gunnar Carlsson
      author_organizations: []
      date: '2025-02-24'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    other_attributes: {}
  parsing_issues:
  - Broad approach field says 'Engineering / cognitive' - mapped to multiple approaches,
    set broad_approach_id to null and broad_approach_text to 'engineering / cognitive'
- id: a:interp_causal_abstractions
  name: Causal Abstractions
  header_level: 3
  parent_id: sec:interpretability
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stephen ✅
    one_sentence_summary: Verify that a neural network implements a specific high-level
      causal model (like a logical algorithm) by finding a mapping between high-level
      variables and low-level neural representations.
    theory_of_change: By establishing a precise, causal mapping between a black-box
      neural network and a human-interpretable algorithm, we can mathematically guarantee
      that the model is using safe reasoning processes and predict its behavior on
      unseen inputs, rather than relying on behavioral testing alone.
    see_also:
    - a:interp_concept_based
    - a:interp_fundamental
    orthodox_problems:
    - goals_misgeneralize
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - Atticus Geiger
    - Christopher Potts
    - Thomas Icard
    - Theodora-Mara Pîslar
    - Sara Magliacane
    - Jiuding Sun
    - Jing Huang
    estimated_ftes: 10-30
    critiques: '[The Misguided Quest for Mechanistic AI Interpretability](https://www.google.com/search?q=https://open.substack.com/pub/aifrontiersmedia/p/the-misguided-quest-for-mechanistic),
      [Interpretability Will Not Reliably Find Deceptive AI](https://www.lesswrong.com/posts/PwnadG4BFjaER3MGf/interpretability-will-not-reliably-find-deceptive-ai).'
    funded_by: Various academic groups, Google DeepMind, Goodfire
    outputs:
    - link_url: https://arxiv.org/abs/2503.11429
      link_text: Combining Causal Models for More Accurate Abstractions of Neural
        Networks
      original_md: '* [Combining Causal Models for More Accurate Abstractions of Neural
        Networks](https://arxiv.org/abs/2503.11429), Theodora-Mara Pîslar, Sara Magliacane,
        Atticus Geiger, 2025-03-14, arXiv'
      title: Combining Causal Models for More Accurate Abstractions of Neural Networks
      authors:
      - Theodora-Mara Pîslar
      - Sara Magliacane
      - Atticus Geiger
      author_organizations: []
      date: '2025-03-14'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2508.11214
      link_text: How Causal Abstraction Underpins Computational Explanation
      original_md: '* [How Causal Abstraction Underpins Computational Explanation](https://arxiv.org/abs/2508.11214),
        Atticus Geiger, Jacqueline Harding, Thomas Icard, 2025-08-15, arXiv'
      title: How Causal Abstraction Underpins Computational Explanation
      authors:
      - Atticus Geiger
      - Jacqueline Harding
      - Thomas Icard
      author_organizations: []
      date: '2025-08-15'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.10894
      link_text: 'HyperDAS: Towards Automating Mechanistic Interpretability with Hypernetworks'
      original_md: '* [HyperDAS: Towards Automating Mechanistic Interpretability with
        Hypernetworks](https://arxiv.org/abs/2503.10894), Jiuding Sun, Jing Huang,
        Sidharth Baskaran et al., 2025-03-13, ICLR 2025'
      title: 'HyperDAS: Towards Automating Mechanistic Interpretability with Hypernetworks'
      authors:
      - Jiuding Sun
      - Jing Huang
      - Sidharth Baskaran
      - Karel D'Oosterlinck
      - Christopher Potts
      - Michael Sklar
      - Atticus Geiger
      author_organizations: []
      date: '2025-03-13'
      published_year: 2025
      venue: ICLR 2025
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:data_attribution
  name: Data attribution
  header_level: 3
  parent_id: sec:interpretability
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: jord ✅
    one_sentence_summary: Quantifies the influence of individual training data points
      on a model's specific behavior or output, allowing researchers to trace model
      properties (like misalignment, bias, or factual errors) back to their source
      in the training set.
    theory_of_change: By attributing harmful, biased, or unaligned behaviors to specific
      training examples, researchers can audit proprietary models, debug training
      data, enable effective data deletion/unlearning
    see_also:
    - a:alignment_data_quality
    orthodox_problems:
    - goals_misgeneralize
    - value_fragile
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: behaviorist_science
    broad_approach_text: behaviorist science
    some_names:
    - Philipp Alexander Kreer
    - Wilson Wu
    - Jin Hwa Lee
    - Matthew Smith
    - Abhilasha Ravichander
    - Andrew Wang
    - Jiacheng Liu
    - Jiaqi Ma
    - Junwei Deng
    - Yijun Pan
    estimated_ftes: 30-60
    critiques: null
    funded_by: Various academic groups
    outputs:
    - link_url: https://arxiv.org/abs/2509.26544
      link_text: Bayesian Influence Functions for Hessian-Free Data Attribution
      original_md: '* [Bayesian Influence Functions for Hessian-Free Data Attribution](https://arxiv.org/abs/2509.26544),
        Philipp Alexander Kreer, Wilson Wu, Maxwell Adam et al., 2025-09-30, arXiv'
      title: Bayesian Influence Functions for Hessian-Free Data Attribution
      authors:
      - Philipp Alexander Kreer
      - Wilson Wu
      - Maxwell Adam
      - Zach Furman
      - Jesse Hoogland
      author_organizations: []
      date: '2025-09-30'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.12071
      link_text: Influence Dynamics and Stagewise Data Attribution
      original_md: '* [Influence Dynamics and Stagewise Data Attribution](https://arxiv.org/abs/2510.12071),
        Jin Hwa Lee, Matthew Smith, Maxwell Adam et al., 2025-10-14, arXiv'
      title: Influence Dynamics and Stagewise Data Attribution
      authors:
      - Jin Hwa Lee
      - Matthew Smith
      - Maxwell Adam
      - Jesse Hoogland
      author_organizations: []
      date: '2025-10-14'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.05475
      link_text: You Are What You Eat -- AI Alignment Requires Understanding How Data
        Shapes Structure and Generalisation
      original_md: '* [You Are What You Eat \-- AI Alignment Requires Understanding
        How Data Shapes Structure and Generalisation](https://arxiv.org/abs/2502.05475)'
      title: You Are What You Eat -- AI Alignment Requires Understanding How Data
        Shapes Structure and Generalisation
      authors:
      - Simon Pepin Lehalleur
      - Jesse Hoogland
      - Matthew Farrugia-Roberts
      - Susan Wei
      - Alexander Gietelink Oldenziel
      - George Wang
      - Liam Carroll
      - Daniel Murfet
      author_organizations: []
      date: '2025-02-08'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.12072
      link_text: Information-Guided Identification of Training Data Imprint in (Proprietary)
        Large Language Models
      original_md: '* [Information-Guided Identification of Training Data Imprint
        in (Proprietary) Large Language Models](https://arxiv.org/abs/2503.12072),
        Abhilasha Ravichander, Jillian Fisher, Taylor Sorensen et al., 2025-03-15,
        arXiv'
      title: Information-Guided Identification of Training Data Imprint in (Proprietary)
        Large Language Models
      authors:
      - Abhilasha Ravichander
      - Jillian Fisher
      - Taylor Sorensen
      - Ximing Lu
      - Yuchen Lin
      - Maria Antoniak
      - Niloofar Mireshghallah
      - Chandra Bhagavatula
      - Yejin Choi
      author_organizations:
      - AI2
      - University of Washington
      date: '2025-03-15'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2405.13954
      link_text: What is Your Data Worth to GPT?
      original_md: '* [What is Your Data Worth to GPT?](https://arxiv.org/abs/2405.13954)'
      title: What is Your Data Worth to GPT? LLM-Scale Data Valuation with Influence
        Functions
      authors:
      - Sang Keun Choe
      - Hwijeen Ahn
      - Juhan Bae
      - Kewen Zhao
      - Minsoo Kang
      - Youngseog Chung
      - Adithya Pratapa
      - Willie Neiswanger
      - Emma Strubell
      - Teruko Mitamura
      - Jeff Schneider
      - Eduard Hovy
      - Roger Grosse
      - Eric Xing
      author_organizations:
      - Carnegie Mellon University
      - University of Toronto
      - Various Universities
      date: '2024-05-22'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.12965
      link_text: 'Distributional Training Data Attribution: What do Influence Functions
        Sample?'
      original_md: '* [Distributional Training Data Attribution: What do Influence
        Functions Sample?](https://arxiv.org/abs/2506.12965)'
      title: 'Distributional Training Data Attribution: What do Influence Functions
        Sample?'
      authors:
      - Bruno Mlodozeniec
      - Isaac Reid
      - Sam Power
      - David Krueger
      - Murat Erdogdu
      - Richard E. Turner
      - Roger Grosse
      author_organizations: []
      date: '2025-06-15'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.14740
      link_text: Better Training Data Attribution via Better Inverse Hessian-Vector
        Products
      original_md: '* [Better Training Data Attribution via Better Inverse Hessian-Vector
        Products](https://arxiv.org/abs/2507.14740), Andrew Wang, Elisa Nguyen, Runshi
        Yang et al., 2025-07-19, arXiv'
      title: Better Training Data Attribution via Better Inverse Hessian-Vector Products
      authors:
      - Andrew Wang
      - Elisa Nguyen
      - Runshi Yang
      - Juhan Bae
      - Sheila A. McIlraith
      - Roger Grosse
      author_organizations: []
      date: '2025-07-19'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.07096
      link_text: 'OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training
        Tokens'
      original_md: '* [OLMoTrace: Tracing Language Model Outputs Back to Trillions
        of Training Tokens](https://arxiv.org/abs/2504.07096), Jiacheng Liu, Taylor
        Blanton, Yanai Elazar et al., 2025-04-09, arXiv'
      title: 'OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training
        Tokens'
      authors:
      - Jiacheng Liu
      - Taylor Blanton
      - Yanai Elazar
      - Sewon Min
      - YenSung Chen
      - Arnavi Chheda-Kothary
      - Huy Tran
      - Byron Bischoff
      - Eric Marsh
      - Michael Schmitz
      - Cassidy Trier
      - Aaron Sarnat
      - Jenna James
      - Jon Borchardt
      - Bailey Kuehl
      - Evie Cheng
      - Karen Farley
      - Sruthi Sreeram
      - Taira Anderson
      - David Albright
      - Carissa Schoenick
      - Luca Soldaini
      - Dirk Groeneveld
      - Rock Yuren Pang
      - Pang Wei Koh
      - Noah A. Smith
      - Sophie Lebrecht
      - Yejin Choi
      - Hannaneh Hajishirzi
      - Ali Farhadi
      - Jesse Dodge
      author_organizations:
      - Allen Institute for AI
      - University of Washington
      date: '2025-04-09'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.11411
      link_text: Detecting and Filtering Unsafe Training Data via Data Attribution
        with Denoised Representation
      original_md: '* [Detecting and Filtering Unsafe Training Data via Data Attribution
        with Denoised Representation](https://arxiv.org/abs/2502.11411)'
      title: Detecting and Filtering Unsafe Training Data via Data Attribution with
        Denoised Representation
      authors:
      - Yijun Pan
      - Taiwei Shi
      - Jieyu Zhao
      - Jiaqi W. Ma
      author_organizations: []
      date: '2025-02-17'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.09424
      link_text: 'DATE-LM: Benchmarking Data Attribution Evaluation for Large Language
        Models'
      original_md: '* [DATE-LM: Benchmarking Data Attribution Evaluation for Large
        Language Models](https://arxiv.org/abs/2507.09424)'
      title: 'DATE-LM: Benchmarking Data Attribution Evaluation for Large Language
        Models'
      authors:
      - Cathy Jiao
      - Yijun Pan
      - Emily Xiao
      - Daisy Sheng
      - Niket Jain
      - Hanzhang Zhao
      - Ishita Dasgupta
      - Jiaqi W. Ma
      - Chenyan Xiong
      author_organizations: []
      date: '2025-07-12'
      published_year: 2025
      venue: NeurIPS 2025 Datasets and Benchmarks Track
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2508.07297
      link_text: Revisiting Data Attribution for Influence Functions
      original_md: '* [Revisiting Data Attribution for Influence Functions](https://arxiv.org/abs/2508.07297)'
      title: Revisiting Data Attribution for Influence Functions
      authors:
      - Hongbo Zhu
      - Angelo Cangelosi
      author_organizations: []
      date: '2025-08-10'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://openreview.net/forum?id=sYK4yPDuT1
      link_text: 'A Snapshot of Influence: A Local Data Attribution Framework for
        Online Reinforcement Learning'
      original_md: '* [A Snapshot of Influence: A Local Data Attribution Framework
        for Online Reinforcement Learning](https://openreview.net/forum?id=sYK4yPDuT1)'
      title: 'A Snapshot of Influence: A Local Data Attribution Framework for Online
        Reinforcement Learning'
      authors:
      - Yuzheng Hu
      - Fan Wu
      - Haotian Ye
      - David Forsyth
      - James Zou
      - Nan Jiang
      - Jiaqi W. Ma
      - Han Zhao
      author_organizations: []
      date: '2025-09-18'
      published_year: 2025
      venue: NeurIPS 2025
      kind: paper_published
    other_attributes: {}
  parsing_issues:
  - Broad approach field says 'Behavioral' - mapped to 'behaviorist_science'
- id: a:interp_other
  name: Other interpretability
  header_level: 3
  parent_id: sec:interpretability
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: jord ✅
    one_sentence_summary: Interpretability that does not fall well into other categories.
    theory_of_change: Explore alternative conceptual frameworks (e.g., agentic, propositional)
      and physics-inspired methods (e.g., renormalization). Or be "pragmatic".
    see_also:
    - a:interp_fundamental
    - a:interp_concept_based
    orthodox_problems:
    - superintelligence_fool_supervisors
    - goals_misgeneralize
    target_case_id: null
    target_case_text: mixed
    broad_approach_id: null
    broad_approach_text: engineering / cognitive
    some_names:
    - Lee Sharkey
    - Dario Amodei
    - David Chalmers
    - Been Kim
    - Neel Nanda
    - David D. Baek
    - Lauren Greenspan
    - Dmitry Vaintrob
    - Sam Marks
    - Jacob Pfau
    estimated_ftes: 30-60
    critiques: '[The Misguided Quest for Mechanistic AI Interpretability](https://aifrontiersmedia.substack.com/p/the-misguided-quest-for-mechanistic),
      [Interpretability Will Not Reliably Find Deceptive AI](https://www.lesswrong.com/posts/PwnadG4BFjaER3MGf/interpretability-will-not-reliably-find-deceptive-ai).'
    funded_by: null
    outputs:
    - link_url: https://www.alignmentforum.org/posts/s9z4mgjtWTPpDLxFy/agentic-interpretability-a-strategy-against-gradual
      link_text: 'Agentic Interpretability: A Strategy Against Gradual Disempowerment'
      original_md: '* [Agentic Interpretability: A Strategy Against Gradual Disempowerment](https://www.alignmentforum.org/posts/s9z4mgjtWTPpDLxFy/agentic-interpretability-a-strategy-against-gradual)'
      title: 'Agentic Interpretability: A Strategy Against Gradual Disempowerment'
      authors:
      - Been Kim
      - John Hewitt
      - Neel Nanda
      - Noah Fiedel
      - Oyvind Tafjord
      author_organizations:
      - Google DeepMind
      - Anthropic
      date: '2025-06-17'
      published_year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2501.16496
      link_text: Open Problems in Mechanistic Interpretability
      original_md: '* [Open Problems in Mechanistic Interpretability](https://arxiv.org/abs/2501.16496),
        Lee Sharkey, Bilal Chughtai, Joshua Batson et al., 2025-01-27, arXiv'
      title: Open Problems in Mechanistic Interpretability
      authors:
      - Lee Sharkey
      - Bilal Chughtai
      - Joshua Batson
      - Jack Lindsey
      - Jeff Wu
      - Lucius Bushnaq
      - Nicholas Goldowsky-Dill
      - Stefan Heimersheim
      - Alejandro Ortega
      - Joseph Bloom
      - Stella Biderman
      - Adria Garriga-Alonso
      - Arthur Conmy
      - Neel Nanda
      - Jessica Rumbelow
      - Martin Wattenberg
      - Nandi Schoots
      - Joseph Miller
      - Eric J. Michaud
      - Stephen Casper
      - Max Tegmark
      - William Saunders
      - David Bau
      - Eric Todd
      - Atticus Geiger
      - Mor Geva
      - Jesse Hoogland
      - Daniel Murfet
      - Tom McGrath
      author_organizations:
      - Multiple research organizations and universities
      date: '2025-01-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.darioamodei.com/post/the-urgency-of-interpretability
      link_text: The Urgency of Interpretability
      original_md: '* [The Urgency of Interpretability](https://www.darioamodei.com/post/the-urgency-of-interpretability),
        Dario Amodei, 2025, darioamodei.com'
      title: The Urgency of Interpretability
      authors:
      - Dario Amodei
      author_organizations:
      - Anthropic
      date: null
      published_year: 2025
      venue: darioamodei.com
      kind: blog_post
    - link_url: https://arxiv.org/abs/2501.15740
      link_text: Propositional Interpretability in Artificial Intelligence
      original_md: '* [Propositional Interpretability in Artificial Intelligence](https://arxiv.org/abs/2501.15740),
        David J. Chalmers, 2025-01-27, arXiv'
      title: Propositional Interpretability in Artificial Intelligence
      authors:
      - David J. Chalmers
      author_organizations: []
      date: '2025-01-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2412.02104
      link_text: 'Explainable and Interpretable Multimodal Large Language Models:
        A Comprehensive Survey'
      original_md: '* [Explainable and Interpretable Multimodal Large Language Models:
        A Comprehensive Survey](https://arxiv.org/abs/2412.02104), Yunkai Dang, Kaichen
        Huang, Jiahao Huo et al., 2024-12-03, arXiv'
      title: 'Explainable and Interpretable Multimodal Large Language Models: A Comprehensive
        Survey'
      authors:
      - Yunkai Dang
      - Kaichen Huang
      - Jiahao Huo
      - Yibo Yan
      - Sirui Huang
      - Dongrui Liu
      - Mengxi Gao
      - Jie Zhang
      - Chen Qian
      - Kun Wang
      - Yong Liu
      - Jing Shao
      - Hui Xiong
      - Xuming Hu
      author_organizations: []
      date: '2024-12-03'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.01628
      link_text: Harmonic Loss Trains Interpretable AI Models
      original_md: '* [Harmonic Loss Trains Interpretable AI Models](https://arxiv.org/abs/2502.01628),
        David D. Baek, Ziming Liu, Riya Tyagi et al., 2025-02-03, arXiv'
      title: Harmonic Loss Trains Interpretable AI Models
      authors:
      - David D. Baek
      - Ziming Liu
      - Riya Tyagi
      - Max Tegmark
      author_organizations: []
      date: '2025-02-03'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.02300
      link_text: 'Through a Steerable Lens: Magnifying Neural Network Interpretability
        via Phase-Based Extrapolation'
      original_md: '* [Through a Steerable Lens: Magnifying Neural Network Interpretability
        via Phase-Based Extrapolation](https://arxiv.org/abs/2506.02300), Farzaneh
        Mahdisoltani, Saeed Mahdisoltani, Roger B. Grosse et al., 2025-06-02, arXiv'
      title: 'Through a Steerable Lens: Magnifying Neural Network Interpretability
        via Phase-Based Extrapolation'
      authors:
      - Farzaneh Mahdisoltani
      - Saeed Mahdisoltani
      - Roger B. Grosse
      - David J. Fleet
      author_organizations: []
      date: '2025-06-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/KbFuuaBKRP7FcAADL/transformers-don-t-need-layernorm-at-inference-time
      link_text: 'Transformers Don''t Need LayerNorm at Inference Time: Implications
        for Interpretability'
      original_md: '* [Transformers Don''t Need LayerNorm at Inference Time: Implications
        for Interpretability](https://lesswrong.com/posts/KbFuuaBKRP7FcAADL/transformers-don-t-need-layernorm-at-inference-time),
        submarat, Joachim Schaeffer, Luca Baroni et al., 2025-07-23, LessWrong / arXiv'
      title: 'Transformers Don''t Need LayerNorm at Inference Time: Implications for
        Interpretability'
      authors:
      - submarat
      - Joachim Schaeffer
      - Luca Baroni
      - galvsk
      - StefanHex
      author_organizations:
      - MARS
      - SPAR
      date: '2025-07-23'
      published_year: 2025
      venue: LessWrong / arXiv
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/u3ZysuXEjkyHhefrk/against-blanket-arguments-against-interpretability
      link_text: Against blanket arguments against interpretability
      original_md: '* [Against blanket arguments against interpretability](https://lesswrong.com/posts/u3ZysuXEjkyHhefrk/against-blanket-arguments-against-interpretability),
        Dmitry Vaintrob, 2025-01-22, LessWrong'
      title: Against blanket arguments against interpretability
      authors:
      - Dmitry Vaintrob
      author_organizations: []
      date: '2025-01-22'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/wkGmouy7JnTNtWAbc/opportunity-space-renormalization-for-ai-safety
      link_text: 'Opportunity Space: Renormalization for AI Safety'
      original_md: '* [Opportunity Space: Renormalization for AI Safety](https://lesswrong.com/posts/wkGmouy7JnTNtWAbc/opportunity-space-renormalization-for-ai-safety),
        Lauren Greenspan, Dmitry Vaintrob, Lucas Teixeira, 2025-03-31, LessWrong'
      title: 'Opportunity Space: Renormalization for AI Safety'
      authors:
      - Lauren Greenspan
      - Dmitry Vaintrob
      - Lucas Teixeira
      author_organizations:
      - PIBBSS
      date: '2025-03-31'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/y5cYisQ2QHiSbQbhk/prospects-for-alignment-automation-interpretability-case
      link_text: 'Prospects for Alignment Automation: Interpretability Case Study'
      original_md: '* [Prospects for Alignment Automation: Interpretability Case Study](https://lesswrong.com/posts/y5cYisQ2QHiSbQbhk/prospects-for-alignment-automation-interpretability-case),
        Jacob Pfau, Geoffrey Irving, 2025-03-21, LessWrong'
      title: 'Prospects for Alignment Automation: Interpretability Case Study'
      authors:
      - Jacob Pfau
      - Geoffrey Irving
      author_organizations:
      - UK AISI
      - Google DeepMind
      date: '2025-03-21'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/wGRnzCFcowRCrpX4Y/downstream-applications-as-validation-of-interpretability
      link_text: Downstream applications as validation of interpretability progress
      original_md: '* [Downstream applications as validation of interpretability progress](https://lesswrong.com/posts/wGRnzCFcowRCrpX4Y/downstream-applications-as-validation-of-interpretability),
        Sam Marks, 2025-03-31, LessWrong'
      title: Downstream applications as validation of interpretability progress
      authors:
      - Sam Marks
      author_organizations: []
      date: '2025-03-31'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/DqaoPNqhQhwBFqWue/principles-for-picking-practical-interpretability-projects
      link_text: Principles for Picking Practical Interpretability Projects
      original_md: '* [Principles for Picking Practical Interpretability Projects](https://lesswrong.com/posts/DqaoPNqhQhwBFqWue/principles-for-picking-practical-interpretability-projects),
        Sam Marks, 2025-07-15, LessWrong'
      title: Principles for Picking Practical Interpretability Projects
      authors:
      - Sam Marks
      author_organizations:
      - Anthropic
      date: '2025-07-15'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/sjr66DBEgyogAbfdf/renormalization-redux-qft-techniques-for-ai-interpretability
      link_text: 'Renormalization Redux: QFT Techniques for AI Interpretability'
      original_md: '* [Renormalization Redux: QFT Techniques for AI Interpretability](https://lesswrong.com/posts/sjr66DBEgyogAbfdf/renormalization-redux-qft-techniques-for-ai-interpretability),
        Lauren Greenspan, Dmitry Vaintrob, 2025-01-18, LessWrong'
      title: 'Renormalization Redux: QFT Techniques for AI Interpretability'
      authors:
      - Lauren Greenspan
      - Dmitry Vaintrob
      author_organizations: []
      date: '2025-01-18'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/qRnupMmFG7dxQTTYh/the-strange-science-of-interpretability-recent-papers-and-a
      link_text: 'The Strange Science of Interpretability: Recent Papers and a Reading
        List for the Philosophy of Interpretability'
      original_md: '* [The Strange Science of Interpretability: Recent Papers and
        a Reading List for the Philosophy of Interpretability](https://lesswrong.com/posts/qRnupMmFG7dxQTTYh/the-strange-science-of-interpretability-recent-papers-and-a),
        Kola Ayonrinde, Louis Jaburi, 2025-08-17, LessWrong'
      title: 'The Strange Science of Interpretability: Recent Papers and a Reading
        List for the Philosophy of Interpretability'
      authors:
      - Kola Ayonrinde
      - Louis Jaburi
      author_organizations: []
      date: '2025-08-17'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/MDWGcNHkZ3NPEzcnp/call-for-collaboration-renormalization-for-ai-safety
      link_text: 'Call for Collaboration: Renormalization for AI safety'
      original_md: '* [Call for Collaboration: Renormalization for AI safety](https://lesswrong.com/posts/MDWGcNHkZ3NPEzcnp/call-for-collaboration-renormalization-for-ai-safety),
        Lauren Greenspan, 2025-03-31, LessWrong'
      title: 'Call for Collaboration: Renormalization for AI safety'
      authors:
      - Lauren Greenspan
      author_organizations:
      - PIBBSS
      date: '2025-03-31'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2510.02334
      link_text: Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via
        Representation Gradient Tracing
      original_md: '* [Where Did It Go Wrong? Attributing Undesirable LLM Behaviors
        via Representation Gradient Tracing](https://arxiv.org/abs/2510.02334), Zhe
        Li, Wei Zhao, Yige Li et al., 2025-09-26, arXiv'
      title: Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation
        Gradient Tracing
      authors:
      - Zhe Li
      - Wei Zhao
      - Yige Li
      - Jun Sun
      author_organizations: []
      date: '2025-09-26'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.17514
      link_text: Language Models May Verbatim Complete Text They Were Not Explicitly
        Trained On
      original_md: '* [Language Models May Verbatim Complete Text They Were Not Explicitly
        Trained On](https://arxiv.org/abs/2503.17514), Ken Ziyu Liu, Christopher A.
        Choquette-Choo, Matthew Jagielski et al., 2025-03-21, arXiv'
      title: Language Models May Verbatim Complete Text They Were Not Explicitly Trained
        On
      authors:
      - Ken Ziyu Liu
      - Christopher A. Choquette-Choo
      - Matthew Jagielski
      - Peter Kairouz
      - Sanmi Koyejo
      - Percy Liang
      - Nicolas Papernot
      author_organizations:
      - Google
      - Stanford University
      - University of Toronto
      date: '2025-03-21'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.12546
      link_text: Extracting memorized pieces of (copyrighted) books from open-weight
        language models
      original_md: '* [Extracting memorized pieces of (copyrighted) books from open-weight
        language models](https://arxiv.org/abs/2505.12546), A. Feder Cooper, Aaron
        Gokaslan, Ahmed Ahmed et al., 2025-05-18, arXiv'
      title: Extracting memorized pieces of (copyrighted) books from open-weight language
        models
      authors:
      - A. Feder Cooper
      - Aaron Gokaslan
      - Ahmed Ahmed
      - Amy B. Cyphert
      - Christopher De Sa
      - Mark A. Lemley
      - Daniel E. Ho
      - Percy Liang
      author_organizations:
      - Stanford University
      - Cornell University
      date: '2025-05-18'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.alignmentforum.org/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-interpretability
      link_text: A Pragmatic Vision for Interpretability
      original_md: '* [**A Pragmatic Vision for Interpretability**](https://www.alignmentforum.org/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-interpretability)'
      title: A Pragmatic Vision for Interpretability
      authors:
      - Neel Nanda
      - Josh Engels
      - Arthur Conmy
      - Senthooran Rajamanoharan
      - bilalchughtai
      - CallumMcDougall
      - János Kramár
      - lewis smith
      author_organizations:
      - Google DeepMind
      date: '2025-12-01'
      published_year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2505.15811
      link_text: https://arxiv.org/abs/2505.15811
      original_md: '* [https://arxiv.org/abs/2505.15811](https://arxiv.org/abs/2505.15811)'
      title: 'On the creation of narrow AI: hierarchy and nonlocality of neural network
        skills'
      authors:
      - Eric J. Michaud
      - Asher Parker-Sartori
      - Max Tegmark
      author_organizations: []
      date: '2025-05-21'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues:
  - Target case is 'Mixed' which does not map to a specific target case ID (average_case,
    pessimistic_case, worst_case)
  - Broad approach is 'Engineering / cognitive' which contains multiple approaches
    - cannot map to single ID
- id: a:activation_engineering
  name: Activation engineering
  header_level: 2
  parent_id: sec:whitebox
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Jord ✅
    one_sentence_summary: A technique for programmatically modifying internal model
      activations to steer outputs toward desired behaviors, serving as a lightweight,
      interpretable alternative (or supplement) to fine-tuning.
    theory_of_change: 'Test interpretability theories; find new insights from interpretable
      causal interventions on representations. Or: build more stuff to stack on top
      of finetuning. Slightly encourage the model to be nice, add one more layer of
      defence to our bundle of partial alignment methods.'
    see_also:
    - a:interp_sparse_coding
    orthodox_problems:
    - value_fragile
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: null
    broad_approach_text: engineering / cognitive
    some_names:
    - Runjin Chen
    - Andy Arditi
    - David Krueger
    - Jan Wehner
    - Narmeen Oozeer
    - Reza Bayat
    - Adam Karvonen
    - Jiuding Sun
    - Tim Tian Hua
    - Helena Casademunt
    - Jacob Dunefsky
    - Thomas Marshall
    estimated_ftes: 50-200
    critiques: '[Understanding (Un)Reliability of Steering Vectors in Language Models](https://arxiv.org/abs/2505.22637)'
    funded_by: Coefficient Giving, Anthropic
    outputs:
    - link_url: https://arxiv.org/abs/2507.21509
      link_text: 'Persona Vectors: Monitoring and Controlling Character Traits in
        Language Models'
      original_md: '* [Persona Vectors: Monitoring and Controlling Character Traits
        in Language Models](https://arxiv.org/abs/2507.21509), Runjin Chen, Andy Arditi,
        Henry Sleight et al., 2025-07-29, arXiv'
      title: 'Persona Vectors: Monitoring and Controlling Character Traits in Language
        Models'
      authors:
      - Runjin Chen
      - Andy Arditi
      - Henry Sleight
      - Owain Evans
      - Jack Lindsey
      author_organizations: []
      date: '2025-07-29'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.19649v1
      link_text: Taxonomy, Opportunities, and Challenges of Representation Engineering
        for Large Language Models
      original_md: '* [Taxonomy, Opportunities, and Challenges of Representation Engineering
        for Large Language Models](https://arxiv.org/abs/2502.19649v1), Jan Wehner,
        Sahar Abdelnabi, Daniel Tan et al., 2025-02-27, arXiv'
      title: Taxonomy, Opportunities, and Challenges of Representation Engineering
        for Large Language Models
      authors:
      - Jan Wehner
      - Sahar Abdelnabi
      - Daniel Tan
      - David Krueger
      - Mario Fritz
      author_organizations:
      - University of Cambridge
      - CISPA Helmholtz Center for Information Security
      date: '2025-02-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.12672
      link_text: 'Keep Calm and Avoid Harmful Content: Concept Alignment and Latent
        Manipulation Towards Safer Answers'
      original_md: '* [Keep Calm and Avoid Harmful Content: Concept Alignment and
        Latent Manipulation Towards Safer Answers](https://arxiv.org/abs/2510.12672),
        Ruben Belo, Marta Guimaraes, Claudia Soares, 2025-10-14, arXiv'
      title: 'Keep Calm and Avoid Harmful Content: Concept Alignment and Latent Manipulation
        Towards Safer Answers'
      authors:
      - Ruben Belo
      - Marta Guimaraes
      - Claudia Soares
      author_organizations: []
      date: '2025-10-14'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.04429
      link_text: Activation Space Interventions Can Be Transferred Between Large Language
        Models
      original_md: '* [Activation Space Interventions Can Be Transferred Between Large
        Language Models](https://arxiv.org/abs/2503.04429), Narmeen Oozeer, Dhruv
        Nathawani, Nirmalendu Prakash et al., 2025-03-06, arXiv (accepted to ICML
        2025\)'
      title: Activation Space Interventions Can Be Transferred Between Large Language
        Models
      authors:
      - Narmeen Oozeer
      - Dhruv Nathawani
      - Nirmalendu Prakash
      - Michael Lan
      - Abir Harrasse
      - Amirali Abdullah
      author_organizations: []
      date: '2025-03-06'
      published_year: 2025
      venue: arXiv (accepted to ICML 2025)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.00177
      link_text: Steering Large Language Model Activations in Sparse Spaces
      original_md: '* [Steering Large Language Model Activations in Sparse Spaces](https://arxiv.org/abs/2503.00177),
        Reza Bayat, Ali Rahimi-Kalahroudi, Mohammad Pezeshki et al., 2025-02-28, arXiv'
      title: Steering Large Language Model Activations in Sparse Spaces
      authors:
      - Reza Bayat
      - Ali Rahimi-Kalahroudi
      - Mohammad Pezeshki
      - Sarath Chandar
      - Pascal Vincent
      author_organizations: []
      date: '2025-02-28'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.02461
      link_text: Enhancing Multiple Dimensions of Trustworthiness in LLMs via Sparse
        Activation Control
      original_md: '* [Enhancing Multiple Dimensions of Trustworthiness in LLMs via
        Sparse Activation Control](https://arxiv.org/abs/2411.02461), Yuxin Xiao,
        Chaoqun Wan, Yonggang Zhang et al., 2024-11-04, arXiv'
      title: Enhancing Multiple Dimensions of Trustworthiness in LLMs via Sparse Activation
        Control
      authors:
      - Yuxin Xiao
      - Chaoqun Wan
      - Yonggang Zhang
      - Wenxiao Wang
      - Binbin Lin
      - Xiaofei He
      - Xu Shen
      - Jieping Ye
      author_organizations: []
      date: '2024-11-04'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.10922
      link_text: Robustly Improving LLM Fairness in Realistic Settings via Interpretability
      original_md: '* [Robustly Improving LLM Fairness in Realistic Settings via Interpretability](https://arxiv.org/abs/2506.10922),
        Adam Karvonen, Samuel Marks, 2025-06-12, arXiv'
      title: Robustly Improving LLM Fairness in Realistic Settings via Interpretability
      authors:
      - Adam Karvonen
      - Samuel Marks
      author_organizations: []
      date: '2025-06-12'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.03292
      link_text: 'HyperSteer: Activation Steering at Scale with Hypernetworks'
      original_md: '* [HyperSteer: Activation Steering at Scale with Hypernetworks](https://arxiv.org/abs/2506.03292),
        Jiuding Sun, Sidharth Baskaran, Zhengxuan Wu et al., 2025-06-03, arXiv'
      title: 'HyperSteer: Activation Steering at Scale with Hypernetworks'
      authors:
      - Jiuding Sun
      - Sidharth Baskaran
      - Zhengxuan Wu
      - Michael Sklar
      - Christopher Potts
      - Atticus Geiger
      author_organizations: []
      date: '2025-06-03'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.20487
      link_text: Steering Evaluation-Aware Language Models to Act Like They Are Deployed
      original_md: '* [Steering Evaluation-Aware Language Models to Act Like They
        Are Deployed](https://arxiv.org/abs/2510.20487), Tim Tian Hua, Andrew Qin,
        Samuel Marks et al., 2025-10-23, arXiv'
      title: Steering Evaluation-Aware Language Models to Act Like They Are Deployed
      authors:
      - Tim Tian Hua
      - Andrew Qin
      - Samuel Marks
      - Neel Nanda
      author_organizations: []
      date: '2025-10-23'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.16795
      link_text: Steering Out-of-Distribution Generalization with Concept Ablation
        Fine-Tuning
      original_md: '* [Steering Out-of-Distribution Generalization with Concept Ablation
        Fine-Tuning](https://arxiv.org/abs/2507.16795), Helena Casademunt, Caden Juang,
        Adam Karvonen et al., 2025-07-22, arXiv'
      title: Steering Out-of-Distribution Generalization with Concept Ablation Fine-Tuning
      authors:
      - Helena Casademunt
      - Caden Juang
      - Adam Karvonen
      - Samuel Marks
      - Senthooran Rajamanoharan
      - Neel Nanda
      author_organizations:
      - Google DeepMind
      date: '2025-07-22'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.02193
      link_text: Improving Steering Vectors by Targeting Sparse Autoencoder Features
      original_md: '* [Improving Steering Vectors by Targeting Sparse Autoencoder
        Features](https://arxiv.org/abs/2411.02193), Sviatoslav Chalnev, Matthew Siu,
        Arthur Conmy, 2024-11-04, arXiv'
      title: Improving Steering Vectors by Targeting Sparse Autoencoder Features
      authors:
      - Sviatoslav Chalnev
      - Matthew Siu
      - Arthur Conmy
      author_organizations: []
      date: '2024-11-04'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.18167
      link_text: Understanding Reasoning in Thinking Language Models via Steering
        Vectors
      original_md: '* [Understanding Reasoning in Thinking Language Models via Steering
        Vectors](https://arxiv.org/abs/2506.18167), Constantin Venhoff, Iván Arcuschin,
        Philip Torr et al., 2025-06-22, ICLR 2025 Workshop on Reasoning and Planning
        for Large Language Models'
      title: Understanding Reasoning in Thinking Language Models via Steering Vectors
      authors:
      - Constantin Venhoff
      - Iván Arcuschin
      - Philip Torr
      - Arthur Conmy
      - Neel Nanda
      author_organizations:
      - University of Oxford
      - Anthropic
      date: '2025-06-22'
      published_year: 2025
      venue: ICLR 2025 Workshop on Reasoning and Planning for Large Language Models
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.07213
      link_text: Comparing Bottom-Up and Top-Down Steering Approaches on In-Context
        Learning Tasks
      original_md: '* [Comparing Bottom-Up and Top-Down Steering Approaches on In-Context
        Learning Tasks](https://arxiv.org/abs/2411.07213), Madeline Brumley, Joe Kwon,
        David Krueger et al., 2024-11-11, arXiv'
      title: Comparing Bottom-Up and Top-Down Steering Approaches on In-Context Learning
        Tasks
      authors:
      - Madeline Brumley
      - Joe Kwon
      - David Krueger
      - Dmitrii Krasheninnikov
      - Usman Anwar
      author_organizations: []
      date: '2024-11-11'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.19649
      link_text: Taxonomy, Opportunities, and Challenges of Representation Engineering
        for Large Language Models
      original_md: '* [Taxonomy, Opportunities, and Challenges of Representation Engineering
        for Large Language Models](https://arxiv.org/abs/2502.19649), Jan Wehner,
        Sahar Abdelnabi, Daniel Tan et al., 2025-02-27, arXiv'
      title: Taxonomy, Opportunities, and Challenges of Representation Engineering
        for Large Language Models
      authors:
      - Jan Wehner
      - Sahar Abdelnabi
      - Daniel Tan
      - David Krueger
      - Mario Fritz
      author_organizations:
      - University of Cambridge
      - CISPA Helmholtz Center for Information Security
      date: '2025-02-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/6aXe9nipTgwK5LxaP/do-safety-relevant-llm-steering-vectors-optimized-on-a
      link_text: Do safety-relevant LLM steering vectors optimized on a single example
        generalize?
      original_md: '* [Do safety-relevant LLM steering vectors optimized on a single
        example generalize?](https://lesswrong.com/posts/6aXe9nipTgwK5LxaP/do-safety-relevant-llm-steering-vectors-optimized-on-a),
        Jacob Dunefsky, 2025-02-28, arXiv'
      title: Do safety-relevant LLM steering vectors optimized on a single example
        generalize?
      authors:
      - Jacob Dunefsky
      author_organizations:
      - Yale University
      date: '2025-02-28'
      published_year: 2025
      venue: arXiv
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/kcKnKHTHycHeRhcHF/one-shot-steering-vectors-cause-emergent-misalignment-too
      link_text: One-shot steering vectors cause emergent misalignment, too
      original_md: '* [One-shot steering vectors cause emergent misalignment, too](https://lesswrong.com/posts/kcKnKHTHycHeRhcHF/one-shot-steering-vectors-cause-emergent-misalignment-too),
        Jacob Dunefsky, 2025-04-14, LessWrong'
      title: One-shot steering vectors cause emergent misalignment, too
      authors:
      - Jacob Dunefsky
      author_organizations: []
      date: '2025-04-14'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    other_attributes: {}
  parsing_issues:
  - See also mentions 'Representation engineering' and 'whitebox control' which don't
    match specific agenda IDs
  - Broad approach is 'engineering / cognitive' which maps to multiple approaches;
    broad_approach_id left null
- id: a:learning_dev_interp
  name: Developmental interpretability
  header_level: 2
  parent_id: sec:whitebox
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stephen ✅
    one_sentence_summary: Builds tools for detecting, locating, and interpreting key
      structural shifts, phase transitions, and emergent phenomena (like grokking
      or deception) that occur during a model's training and in-context learning phases.
    theory_of_change: Structures forming in neural networks leave identifiable traces
      that can be interpreted (e.g., using concepts from Singular Learning Theory);
      by catching and analyzing these developmental moments, researchers can automate
      interpretability, predict when dangerous capabilities emerge, and intervene
      to prevent deceptiveness or misaligned values as early as possible.
    see_also:
    - a:interp_fundamental
    - a:interp_sparse_coding
    - '[ICL transience](https://proceedings.mlr.press/v267/singh25c.html)'
    orthodox_problems:
    - goals_misgeneralize
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - Timaeus
    - Jesse Hoogland
    - George Wang
    - Daniel Murfet
    - Stan van Wingerden
    - Alexander Gietelink Oldenziel
    estimated_ftes: 10-50
    critiques: '[Vaintrob](https://www.lesswrong.com/posts/M2bs6xCbmc79nwr8j/dmitry-vaintrob-s-shortform#A8Ziwhts35dgqbz52),
      [Joar Skalse (2023)](https://www.alignmentforum.org/posts/ALJYj4PpkqyseL7kZ/my-criticism-of-singular-learning-theory)'
    funded_by: Manifund, Survival and Flourishing Fund, EA Funds
    outputs:
    - link_url: https://arxiv.org/abs/2508.15841
      link_text: A Review of Developmental Interpretability in Large Language Models
      original_md: '* [A Review of Developmental Interpretability in Large Language
        Models](https://arxiv.org/abs/2508.15841), Ihor Kendiukhov, 2025-08-19, arXiv'
      title: A Review of Developmental Interpretability in Large Language Models
      authors:
      - Ihor Kendiukhov
      author_organizations: []
      date: '2025-08-19'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://openreview.net/forum?id=KUFH0n1BIM
      link_text: Learning Coefficients, Fractals, and Trees in Parameter Space
      original_md: '* [Learning Coefficients, Fractals, and Trees in Parameter Space](https://openreview.net/forum?id=KUFH0n1BIM),
        Max Hennick, Matthias Dellago, 2025-06-23, ODYSSEY 2025 Conference'
      title: Learning Coefficients, Fractals, and Trees in Parameter Space
      authors:
      - Max Hennick
      - Matthias Dellago
      author_organizations: []
      date: '2025-06-23'
      published_year: 2025
      venue: ODYSSEY 2025 Conference
      kind: paper_preprint
    - link_url: https://openreview.net/forum?id=Td37oOfmmz
      link_text: Programs as Singularities
      original_md: '* [Programs as Singularities](https://openreview.net/forum?id=Td37oOfmmz),
        Daniel Murfet, William Troiani, 2025-06-20, ODYSSEY 2025 Conference'
      title: Programs as Singularities
      authors:
      - Daniel Murfet
      - William Troiani
      author_organizations: []
      date: '2025-06-20'
      published_year: 2025
      venue: ODYSSEY 2025 Conference
      kind: paper_preprint
    - link_url: https://www.lesswrong.com/posts/2MX2bXreTtntB85Zy/from-slt-to-ait-nn-generalisation-out-of-distribution
      link_text: null
      original_md: '* [https://www.lesswrong.com/posts/2MX2bXreTtntB85Zy/from-slt-to-ait-nn-generalisation-out-of-distribution](https://www.lesswrong.com/posts/2MX2bXreTtntB85Zy/from-slt-to-ait-nn-generalisation-out-of-distribution)'
      title: 'From SLT to AIT: NN Generalisation Out of Distribution'
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: error_detected
    - link_url: https://www.lesswrong.com/posts/ZSQaT2yxNNZ3eLxRd/understanding-and-controlling-llm-generalization
      link_text: null
      original_md: '* [https://www.lesswrong.com/posts/ZSQaT2yxNNZ3eLxRd/understanding-and-controlling-llm-generalization](https://www.lesswrong.com/posts/ZSQaT2yxNNZ3eLxRd/understanding-and-controlling-llm-generalization)'
      title: Understanding and Controlling LLM Generalization
      authors:
      - Daniel Tan
      author_organizations: []
      date: '2025-11-14'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2506.18777
      link_text: 'Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions
        During Code Training'
      original_md: '* [Programming by Backprop: LLMs Acquire Reusable Algorithmic
        Abstractions During Code Training](https://arxiv.org/abs/2506.18777), Jonathan
        Cook, Silvia Sapora, Arash Ahmadian et al., 2025-06-23, arXiv'
      title: 'Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions
        During Code Training'
      authors:
      - Jonathan Cook
      - Silvia Sapora
      - Arash Ahmadian
      - Akbir Khan
      - Tim Rocktaschel
      - Jakob Foerster
      - Laura Ruis
      author_organizations: []
      date: '2025-06-23'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.05291
      link_text: 'Crosscoding Through Time: Tracking Emergence & Consolidation Of
        Linguistic Representations Throughout LLM Pretraining'
      original_md: '* [Crosscoding Through Time: Tracking Emergence & Consolidation
        Of Linguistic Representations Throughout LLM Pretraining](https://arxiv.org/abs/2509.05291),
        Deniz Bayazit, Aaron Mueller, Antoine Bosselut, 2025-09-05, arXiv'
      title: 'Crosscoding Through Time: Tracking Emergence & Consolidation Of Linguistic
        Representations Throughout LLM Pretraining'
      authors:
      - Deniz Bayazit
      - Aaron Mueller
      - Antoine Bosselut
      author_organizations: []
      date: '2025-09-05'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2501.17745
      link_text: Dynamics of Transient Structure in In-Context Linear Regression Transformers
      original_md: '* [Dynamics of Transient Structure in In-Context Linear Regression
        Transformers](https://arxiv.org/abs/2501.17745), Liam Carroll, Jesse Hoogland,
        Matthew Farrugia-Roberts et al., 2025-01-29, arXiv'
      title: Dynamics of Transient Structure in In-Context Linear Regression Transformers
      authors:
      - Liam Carroll
      - Jesse Hoogland
      - Matthew Farrugia-Roberts
      - Daniel Murfet
      author_organizations: []
      date: '2025-01-29'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.23365
      link_text: 'Emergence of Superposition: Unveiling the Training Dynamics of Chain
        of Continuous Thought'
      original_md: '* [Emergence of Superposition: Unveiling the Training Dynamics
        of Chain of Continuous Thought](https://arxiv.org/abs/2509.23365), Hanlin
        Zhu, Shibo Hao, Zhiting Hu et al., 2025-09-27, arXiv'
      title: 'Emergence of Superposition: Unveiling the Training Dynamics of Chain
        of Continuous Thought'
      authors:
      - Hanlin Zhu
      - Shibo Hao
      - Zhiting Hu
      - Jiantao Jiao
      - Stuart Russell
      - Yuandong Tian
      author_organizations:
      - UC Berkeley
      - Meta
      date: '2025-09-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.07681
      link_text: What Do Learning Dynamics Reveal About Generalization in LLM Reasoning?
      original_md: '* [What Do Learning Dynamics Reveal About Generalization in LLM
        Reasoning?](https://arxiv.org/abs/2411.07681), Katie Kang, Amrith Setlur,
        Dibya Ghosh et al., 2024-11-12, arXiv'
      title: What Do Learning Dynamics Reveal About Generalization in LLM Reasoning?
      authors:
      - Katie Kang
      - Amrith Setlur
      - Dibya Ghosh
      - Jacob Steinhardt
      - Claire Tomlin
      - Sergey Levine
      - Aviral Kumar
      author_organizations:
      - UC Berkeley
      date: '2024-11-12'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.12077
      link_text: 'Compressibility Measures Complexity: Minimum Description Length
        Meets Singular Learning Theory'
      original_md: '* [Compressibility Measures Complexity: Minimum Description Length
        Meets Singular Learning Theory](https://arxiv.org/abs/2510.12077), Einar Urdshals,
        Edmund Lau, Jesse Hoogland et al., 2025-10-14, arXiv'
      title: 'Compressibility Measures Complexity: Minimum Description Length Meets
        Singular Learning Theory'
      authors:
      - Einar Urdshals
      - Edmund Lau
      - Jesse Hoogland
      - Stan van Wingerden
      - Daniel Murfet
      author_organizations:
      - Timaeus
      - University of Melbourne
      date: '2025-10-14'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.18048
      link_text: Modes of Sequence Models and Learning Coefficients
      original_md: '* [Modes of Sequence Models and Learning Coefficients](https://arxiv.org/abs/2504.18048),
        Zhongtian Chen, Daniel Murfet, 2025-04-25, arXiv'
      title: Modes of Sequence Models and Learning Coefficients
      authors:
      - Zhongtian Chen
      - Daniel Murfet
      author_organizations: []
      date: '2025-04-25'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/J7CyENFYXPxXQpsnD/slt-for-ai-safety
      link_text: SLT for AI Safety
      original_md: '* [SLT for AI Safety](https://lesswrong.com/posts/J7CyENFYXPxXQpsnD/slt-for-ai-safety),
        Jesse Hoogland, 2025-07-01, LessWrong'
      title: SLT for AI Safety
      authors:
      - Jesse Hoogland
      author_organizations:
      - Timaeus
      date: '2025-07-01'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/HFcriD29cw3E5QLCR/selective-regularization-for-alignment-focused
      link_text: Selective regularization for alignment-focused representation engineering
      original_md: '* [Selective regularization for alignment-focused representation
        engineering](https://lesswrong.com/posts/HFcriD29cw3E5QLCR/selective-regularization-for-alignment-focused),
        Sandy Fraser, 2025-05-20, LessWrong'
      title: Selective regularization for alignment-focused representation engineering
      authors:
      - Sandy Fraser
      author_organizations: []
      date: '2025-05-20'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    other_attributes: {}
  parsing_issues: []
- id: a:representation_structure
  name: Representation structure and geometry
  header_level: 2
  parent_id: sec:whitebox
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stephen ✅
    one_sentence_summary: What do the representations look like? Does any simple structure
      underlie the beliefs of all well-trained models? Can we get the semantics from
      this geometry?
    theory_of_change: Get scalable unsupervised methods for finding structure in representations
      and interpreting them, then using this to e.g. guide training.
    see_also:
    - a:interp_concept_based
    - computational mechanics
    - feature universality
    - a:natural_abstractions
    - a:interp_causal_abstractions
    orthodox_problems:
    - goals_misgeneralize
    - superintelligence_fool_supervisors
    target_case_id: null
    target_case_text: mixed
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - Simplex
    - Insight + Interaction Lab
    - Paul Riechers
    - Adam Shai
    - Martin Wattenberg
    - Blake Richards
    - Mateusz Piotrowski
    estimated_ftes: 10-50
    critiques: null
    funded_by: Various academic groups
    outputs:
    - link_url: https://arxiv.org/abs/2503.21073
      link_text: Shared Global and Local Geometry of Language Model Embeddings
      original_md: '* [Shared Global and Local Geometry of Language Model Embeddings](https://arxiv.org/abs/2503.21073)'
      title: Shared Global and Local Geometry of Language Model Embeddings
      authors:
      - Andrew Lee
      - Melanie Weber
      - Fernanda Viégas
      - Martin Wattenberg
      author_organizations: []
      date: '2025-03-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.01599
      link_text: Connecting Neural Models Latent Geometries with Relative Geodesic
        Representations
      original_md: '* [Connecting Neural Models Latent Geometries with Relative Geodesic
        Representations](https://arxiv.org/abs/2506.01599)'
      title: Connecting Neural Models Latent Geometries with Relative Geodesic Representations
      authors:
      - Hanlin Yu
      - Berfin Inal
      - Georgios Arvanitidis
      - Soren Hauberg
      - Francesco Locatello
      - Marco Fumero
      author_organizations: []
      date: '2025-06-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.01954
      link_text: Constrained belief updates explain geometric structures in transformer
        representations
      original_md: '* [Constrained belief updates explain geometric structures in
        transformer representations](https://arxiv.org/abs/2502.01954)'
      title: Constrained belief updates explain geometric structures in transformer
        representations
      authors:
      - Mateusz Piotrowski
      - Paul M. Riechers
      - Daniel Filan
      - Adam S. Shai
      author_organizations: []
      date: '2025-02-04'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/pdf/2504.14379
      link_text: The Geometry of Self-Verification in a Task-Specific Reasoning Model
      original_md: '* [The Geometry of Self-Verification in a Task-Specific Reasoning
        Model](https://arxiv.org/pdf/2504.14379)'
      title: The Geometry of Self-Verification in a Task-Specific Reasoning Model
      authors:
      - Andrew Lee
      - Lihao Sun
      - Chris Wendler
      - Fernanda Viégas
      - Martin Wattenberg
      author_organizations:
      - Google Research
      date: '2025-04-19'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.22785
      link_text: Navigating the Latent Space Dynamics of Neural Models
      original_md: '* [Navigating the Latent Space Dynamics of Neural Models](https://arxiv.org/abs/2505.22785)'
      title: Navigating the Latent Space Dynamics of Neural Models
      authors:
      - Marco Fumero
      - Luca Moschella
      - Emanuele Rodolà
      - Francesco Locatello
      author_organizations: []
      date: '2025-05-28'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.17420
      link_text: 'The Geometry of Refusal in Large Language Models: Concept Cones
        and Representational Independence'
      original_md: '* [The Geometry of Refusal in Large Language Models: Concept Cones
        and Representational Independence](https://arxiv.org/abs/2502.17420)'
      title: 'The Geometry of Refusal in Large Language Models: Concept Cones and
        Representational Independence'
      authors:
      - Tom Wollschläger
      - Jannes Elstner
      - Simon Geisler
      - Vincent Cohen-Addad
      - Stephan Günnemann
      - Johannes Gasteiger
      author_organizations:
      - Technical University of Munich
      date: '2025-02-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.11692
      link_text: The Geometry of ReLU Networks through the ReLU Transition Graph
      original_md: '* [The Geometry of ReLU Networks through the ReLU Transition Graph](https://arxiv.org/abs/2505.11692)'
      title: The Geometry of ReLU Networks through the ReLU Transition Graph
      authors:
      - Sahil Rajesh Dhayalkar
      author_organizations: []
      date: '2025-05-16'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.26745
      link_text: Deep sequence models tend to memorize geometrically; it is unclear
        why
      original_md: '* [Deep sequence models tend to memorize geometrically; it is
        unclear why](https://arxiv.org/abs/2510.26745)'
      title: Deep sequence models tend to memorize geometrically; it is unclear why
      authors:
      - Shahriar Noroozizadeh
      - Vaishnavh Nagarajan
      - Elan Rosenfeld
      - Sanjiv Kumar
      author_organizations: []
      date: '2025-10-30'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.23024
      link_text: Tracing the Representation Geometry of Language Models from Pretraining
        to Post-training
      original_md: '* [Tracing the Representation Geometry of Language Models from
        Pretraining to Post-training](https://arxiv.org/abs/2509.23024)'
      title: Tracing the Representation Geometry of Language Models from Pretraining
        to Post-training
      authors:
      - Melody Zixuan Li
      - Kumar Krishna Agrawal
      - Arna Ghosh
      - Komal Kumar Teru
      - Adam Santoro
      - Guillaume Lajoie
      - Blake A. Richards
      author_organizations:
      - Unknown
      date: '2025-09-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.18373
      link_text: Next-token pretraining implies in-context learning
      original_md: '* [Next-token pretraining implies in-context learning](https://arxiv.org/abs/2505.18373)'
      title: Next-token pretraining implies in-context learning
      authors:
      - Paul M. Riechers
      - Henry R. Bigelow
      - Eric A. Alt
      - Adam Shai
      author_organizations: []
      date: '2025-05-23'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.07432
      link_text: Neural networks leverage nominally quantum and post-quantum representations
      original_md: '* [Neural networks leverage nominally quantum and post-quantum
        representations](https://arxiv.org/abs/2507.07432)'
      title: Neural networks leverage nominally quantum and post-quantum representations
      authors:
      - Paul M. Riechers
      - Thomas J. Elliott
      - Adam S. Shai
      author_organizations: []
      date: '2025-07-10'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: http://arxiv.org/abs/2511.06739
      link_text: Rank-1 LoRAs Encode Interpretable Reasoning Signals
      original_md: '* [Rank-1 LoRAs Encode Interpretable Reasoning Signals](http://arxiv.org/abs/2511.06739)'
      title: Rank-1 LoRAs Encode Interpretable Reasoning Signals
      authors:
      - Jake Ward
      - Paul Riechers
      - Adam Shai
      author_organizations: []
      date: '2025-11-10'
      published_year: 2025
      venue: 'arXiv (NeurIPS 2025 Workshop: Mechanistic Interpretability Workshop)'
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2508.00331
      link_text: Embryology of a Language Model
      original_md: '* [Embryology of a Language Model](https://arxiv.org/abs/2508.00331),
        George Wang, Garrett Baker, Andrew Gordon et al., 2025-08-01, arXiv'
      title: Embryology of a Language Model
      authors:
      - George Wang
      - Garrett Baker
      - Andrew Gordon
      - Daniel Murfet
      author_organizations: []
      date: '2025-08-01'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues:
  - Target case is 'mixed' - target_case_id set to null, target_case_text set to 'mixed'
  - Broad approach 'cognitive' mapped to 'cognitivist_science' / 'cognitivist science'
- id: a:human_biases
  name: Human inductive biases
  header_level: 2
  parent_id: sec:whitebox
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stephen ✅
    one_sentence_summary: Discover connections deep learning AI systems have with
      human brains and human learning processes. Develop an 'alignment moonshot' based
      on a coherent theory of learning which applies to both humans and AI systems.
    theory_of_change: Humans learn trust, honesty, self-maintenance, and corrigibility;
      if we understand how they do maybe we can get future AI systems to learn them.
    see_also:
    - active learning
    - ACS research
    orthodox_problems:
    - goals_misgeneralize
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - Lukas Muttenthaler
    - Quentin Delfosse
    estimated_ftes: '4'
    critiques: null
    funded_by: Google DeepMind, various academic groups
    outputs:
    - link_url: https://www.nature.com/articles/s41586-025-09631-6
      link_text: Aligning machine and human visual representations across abstraction
        levels
      original_md: '* [**Aligning machine and human visual representations across
        abstraction levels**](https://www.nature.com/articles/s41586-025-09631-6)**.**
        By fine-tuning vision foundation models on human-judgement-derived similarity
        labels, the models'' internal representations become significantly more aligned
        with human abstractions, leading to improved out-of-distribution robustness
        and task generalisation.'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://arxiv.org/abs/2505.14204
      link_text: 'Beginning with You: Perceptual-Initialization Improves Vision-Language
        Representation and Alignment'
      original_md: '* [**Beginning with You: Perceptual-Initialization Improves Vision-Language
        Representation and Alignment**](https://arxiv.org/abs/2505.14204)**.** Seeds
        a vision encoder with human perceptual similarity judgments before standard
        image-text contrastive training resulting in noticeably improved zero-shot
        vision-language performance and alignment across diverse benchmarks.'
      title: 'Beginning with You: Perceptual-Initialization Improves Vision-Language
        Representation and Alignment'
      authors:
      - Yang Hu
      - Runchen Wang
      - Stephen Chong Zhao
      - Xuhui Zhan
      - Do Hun Kim
      - Mark Wallace
      - David A. Tovar
      author_organizations: []
      date: '2025-05-20'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/html/2505.21731v1
      link_text: Deep Reinforcement Learning Agents are not even close to Human Intelligence
      original_md: '* [**Deep Reinforcement Learning Agents are not even close to
        Human Intelligence**](https://arxiv.org/html/2505.21731v1)**.** The paper
        shows that deep RL agents, even high-performing ones, fail badly on simplified
        versions of Atari games (variants that humans handle easily) revealing that
        the agents rely on brittle, misaligned shortcuts rather than robust, human-like
        reasoning.'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://arxiv.org/html/2503.02976v2#S3
      link_text: 'Teaching AI to Handle Exceptions: Supervised Fine-tuning with Human-aligned
        Judgment'
      original_md: '* [**Teaching AI to Handle Exceptions: Supervised Fine-tuning
        with Human-aligned Judgment**](https://arxiv.org/html/2503.02976v2#S3)'
      title: 'Teaching AI to Handle Exceptions: Supervised Fine-tuning with Human-aligned
        Judgment'
      authors:
      - Matthew DosSantos DiSorbo
      - Harang Ju
      - Sinan Aral
      author_organizations:
      - Harvard Business School
      - Johns Hopkins University
      - MIT Sloan School of Management
      date: 2025-03-XX
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://docs.google.com/document/d/1fl7LE8AN7mLJ6uFcPuFCzatp0zCIYvjRIjQRgHPAkSE/edit?tab=t.0
      link_text: HIBP Human Inductive Bias Project Plan
      original_md: '* **[HIBP Human Inductive Bias Project Plan](https://docs.google.com/document/d/1fl7LE8AN7mLJ6uFcPuFCzatp0zCIYvjRIjQRgHPAkSE/edit?tab=t.0)**'
      title: HIBP Human Inductive Bias Project Plan
      authors:
      - Félix Dorn
      author_organizations: []
      date: null
      published_year: null
      venue: Google Docs
      kind: error_detected
    other_attributes: {}
  parsing_issues: []
- id: sec:safety_by_construction
  name: Safety by construction
  header_level: 1
  parent_id: null
  content: Approaches which minimise the use of singleton deep learning models.
  item_type: section
  agenda_attributes: null
  parsing_issues: []
- id: a:formal_verification
  name: Guaranteed Safe AI
  header_level: 2
  parent_id: sec:safety_by_construction
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Rory ✅
    one_sentence_summary: Formally model the behavior of cyber-physical systems, construct
      formally verified shells and interfaces which pose precise constraints on what
      actions can occur, and require AIs to provide safety guarantees for their recommended
      actions (correctness and uniqueness)
    theory_of_change: Make a formal verification system that can act as an intermediary
      between human users and a potentially dangerous system, only letting provably
      safe actions through. (Notable for not requiring that we solve ELK; does require
      that we solve ontology though)
    see_also:
    - '[Synthesizing Standalone World-Models](https://www.alignmentforum.org/posts/LngR93YwiEpJ3kiWh/research-agenda-synthesizing-standalone-world-models)'
    - a:scientist_ai
    - Safeguarded AI
    - Open Agency Architecture
    - SLES
    - program synthesis
    orthodox_problems:
    - value_fragile
    - goals_misgeneralize
    - superintelligence_fool_supervisors
    - humans_not_first_class
    - boxed_agi_exfiltrate
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - ARIA
    - Lawzero
    - Atlas Computing
    - FLF
    - Max Tegmark
    - '[Beneficial AI Foundation](https://www.beneficialaifoundation.org/)'
    - Steve Omohundro
    - David "davidad" Dalrymple
    - Joar Skalse
    - Stuart Russell
    - Ohad Kammar
    - Alessandro Abate
    - Fabio Zanassi
    estimated_ftes: 10-100
    critiques: Zvi, Gleave, Dickson, [Greenblatt](https://www.lesswrong.com/posts/jRf4WENQnhssCb6mJ/davidad-s-bold-plan-for-alignment-an-in-depth-explanation?commentId=MJCvHk5ARMnWDjQDg)
    funded_by: Manifund, UK government, Coefficient Giving, Survival and Flourishing
      Fund, Mila / CIFAR
    outputs:
    - link_url: https://manifund.org/projects/safeplanbench-evaluating-a-guaranteed-safe-ai-approach-for-llm-based-agents
      link_text: 'SafePlanBench: evaluating a Guaranteed Safe AI Approach for LLM-based
        Agents'
      original_md: '* [**SafePlanBench: evaluating a Guaranteed Safe AI Approach for
        LLM-based Agents**](https://manifund.org/projects/safeplanbench-evaluating-a-guaranteed-safe-ai-approach-for-llm-based-agents),
        *Agustín Martinez Suñé, Tan Zhi Xuan*, Manifund'
      title: 'SafePlanBench: evaluating a Guaranteed Safe AI Approach for LLM-based
        Agents'
      authors:
      - Agustín Martinez Suñé
      - Tan Zhi Xuan
      author_organizations:
      - PIBBSS
      - MIT
      - University of Oxford
      date: null
      published_year: null
      venue: Manifund
      kind: agenda_manifesto
    - link_url: https://arxiv.org/abs/2506.22492
      link_text: Report on NSF Workshop on Science of Safe AI
      original_md: '* [**Report on NSF Workshop on Science of Safe AI**](https://arxiv.org/abs/2506.22492)'
      title: Report on NSF Workshop on Science of Safe AI
      authors:
      - Rajeev Alur
      - Greg Durrett
      - Hadas Kress-Gazit
      - Corina Păsăreanu
      - René Vidal
      author_organizations:
      - NSF SLES Program
      - University of Pennsylvania
      date: '2025-06-24'
      published_year: 2025
      venue: arXiv
      kind: agenda_manifesto
    - link_url: https://arxiv.org/abs/2509.22908
      link_text: 'A benchmark for vericoding: formally verified program synthesis'
      original_md: '* [**A benchmark for vericoding: formally verified program synthesis**](https://arxiv.org/abs/2509.22908)'
      title: 'A benchmark for vericoding: formally verified program synthesis'
      authors:
      - Sergiu Bursuc
      - Theodore Ehrenborg
      - Shaowei Lin
      - Lacramioara Astefanoaei
      - Ionel Emilian Chiosa
      - Jure Kukovec
      - Alok Singh
      - Oliver Butterley
      - Adem Bizid
      - Quinn Dougherty
      - Miranda Zhao
      - Max Tan
      - Max Tegmark
      author_organizations:
      - Beneficial AI Foundation
      - MIT
      date: '2025-09-26'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/CCT7Qc8rSeRs7r5GL/beliefs-about-formal-methods-and-ai-safety
      link_text: Beliefs about formal methods and AI safety
      original_md: '* [**Beliefs about formal methods and AI safety**](https://lesswrong.com/posts/CCT7Qc8rSeRs7r5GL/beliefs-about-formal-methods-and-ai-safety)'
      title: Beliefs about formal methods and AI safety
      authors:
      - Quinn Dougherty
      author_organizations: []
      date: '2025-10-23'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://atlascomputing.org/ai-assisted-fv-toolchain.pdf
      link_text: A Toolchain for AI-Assisted Code Specification, Synthesis and Verification
      original_md: '* [A Toolchain for AI-Assisted Code Specification, Synthesis and
        Verification](https://atlascomputing.org/ai-assisted-fv-toolchain.pdf)'
      title: AI-Assisted Formal Verification Toolchain
      authors: []
      author_organizations:
      - Atlas Computing
      date: null
      published_year: null
      venue: null
      kind: error_detected
    other_attributes: {}
  parsing_issues:
  - Target case field says '(nearly) worst-case' - mapped to 'worst_case' but qualifier
    '(nearly)' noted
  - Broad approach field says 'cognitive' - mapped to 'cognitivist_science'
- id: a:scientist_ai
  name: Scientist AI
  header_level: 2
  parent_id: sec:safety_by_construction
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Rory
    one_sentence_summary: Develop powerful, non-agentic, uncertainty-aware world-modeling
      systems that substantially accelerate scientific progress while avoiding the
      risks of building AIs that act as 'agents'
    theory_of_change: 'Developing non-agentic ''Scientist AI'' allows us to: (i) reap
      the benefits of AI progress while (ii) avoiding the inherent risks of agentic
      systems. These systems can also (iii) provide a useful guardrail to protect
      us from unsafe agentic AIs by double-checking actions they propose, and (iv)
      help us more safely build agentic superintelligent systems.'
    see_also:
    - '[JEPA](https://arxiv.org/abs/2511.08544)'
    - '[oracles](https://www.lesswrong.com/w/oracle-ai)'
    orthodox_problems:
    - pivotal_dangerous_capabilities
    - goals_misgeneralize
    - instrumental_convergence
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - Yoshua Bengio
    estimated_ftes: 1-10
    critiques: Hard to find, but see [Raymond Douglas' comment](https://www.lesswrong.com/posts/p5gBcoQeBsvsMShvT/superintelligent-agents-pose-catastrophic-risks-can?commentId=tJXqhg3XZsqnyaZs2),
      [Karnofsky-Soares discussion](https://www.lesswrong.com/posts/iy2o4nQj9DnQD7Yhj/discussion-with-nate-soares-on-a-key-alignment-difficulty).
      Perhaps also [Predict-O-Matic](https://www.lesswrong.com/posts/SwcyMEgLyd4C3Dern/the-parable-of-predict-o-matic).
    funded_by: ARIA, Future of Life Institute, Jaan Tallinn, Schmidt Sciences
    outputs:
    - link_url: https://arxiv.org/abs/2502.15657
      link_text: 'Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI
        Offer a Safer Path?'
      original_md: '* [Superintelligent Agents Pose Catastrophic Risks: Can Scientist
        AI Offer a Safer Path?](https://arxiv.org/abs/2502.15657), Yoshua Bengio,
        Michael Cohen, Damiano Fornasiere et al., 2025-02-21, arXiv'
      title: 'Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer
        a Safer Path?'
      authors:
      - Yoshua Bengio
      - Michael Cohen
      - Damiano Fornasiere
      - Joumana Ghosn
      - Pietro Greiner
      - Matt MacDermott
      - Sören Mindermann
      - Adam Oberman
      - Jesse Richardson
      - Oliver Richardson
      - Marc-Antoine Rondeau
      - Pierre-Luc St-Charles
      - David Williams-King
      author_organizations:
      - Mila
      - Independent
      date: '2025-02-21'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.08713
      link_text: 'The More You Automate, the Less You See: Hidden Pitfalls of AI Scientist
        Systems'
      original_md: '* [The More You Automate, the Less You See: Hidden Pitfalls of
        AI Scientist Systems](https://arxiv.org/abs/2509.08713), Ziming Luo, Atoosa
        Kasirzadeh, Nihar B. Shah, 2025-09-10, arXiv'
      title: 'The More You Automate, the Less You See: Hidden Pitfalls of AI Scientist
        Systems'
      authors:
      - Ziming Luo
      - Atoosa Kasirzadeh
      - Nihar B. Shah
      author_organizations: []
      date: '2025-09-10'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:brainlike_agi
  name: Brainlike-AGI Safety
  header_level: 2
  parent_id: sec:safety_by_construction
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stephen ✅
    one_sentence_summary: Social and moral instincts are (partly) implemented in particular
      hardwired brain circuitry; let's figure out what those circuits are and how
      they work; this will involve symbol grounding. "a yet-to-be-invented variation
      on actor-critic model-based reinforcement learning"
    theory_of_change: Fairly direct alignment via changing training to reflect actual
      human reward. Get actual data about (reward, training data) → (human values)
      to help with theorising this map in AIs; "understand human social instincts,
      and then maybe adapt some aspects of those for AGIs, presumably in conjunction
      with other non-biological ingredients".
    see_also: []
    orthodox_problems: []
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - Stephen Byrnes
    estimated_ftes: 1-5
    critiques: '[Tsvi BT](https://www.lesswrong.com/posts/unCG3rhyMJpGJpoLd/koan-divining-alien-datastructures-from-ram-activations#BtHCubjKWDFafkmYH)'
    funded_by: Astera Institute
    outputs:
    - link_url: https://www.lesswrong.com/posts/yew6zFWAKG4AGs3Wk/foom-and-doom-1-brain-in-a-box-in-a-basement
      link_text: null
      original_md: '* [https://www.lesswrong.com/posts/yew6zFWAKG4AGs3Wk/foom-and-doom-1-brain-in-a-box-in-a-basement](https://www.lesswrong.com/posts/yew6zFWAKG4AGs3Wk/foom-and-doom-1-brain-in-a-box-in-a-basement)'
      title: 'Foom and Doom 1: Brain in a Box in a Basement'
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: blocked
    - link_url: https://www.lesswrong.com/posts/bnnKGSCHJghAvqPjS/foom-and-doom-2-technical-alignment-is-hard
      link_text: null
      original_md: '* [https://www.lesswrong.com/posts/bnnKGSCHJghAvqPjS/foom-and-doom-2-technical-alignment-is-hard](https://www.lesswrong.com/posts/bnnKGSCHJghAvqPjS/foom-and-doom-2-technical-alignment-is-hard)'
      title: 'Foom and Doom 2: Technical Alignment is Hard'
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: error_detected
    - link_url: https://www.lesswrong.com/posts/grgb2ipxQf2wzNDEG/perils-of-under-vs-over-sculpting-agi-desires
      link_text: null
      original_md: '* [https://www.lesswrong.com/posts/grgb2ipxQf2wzNDEG/perils-of-under-vs-over-sculpting-agi-desires](https://www.lesswrong.com/posts/grgb2ipxQf2wzNDEG/perils-of-under-vs-over-sculpting-agi-desires)'
      title: Perils of Under vs Over-sculpting AGI Desires
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: error_detected
    - link_url: https://lesswrong.com/posts/JrTk2pbqp7BFwPAKw/reward-button-alignment
      link_text: Reward button alignment
      original_md: '* [**Reward button alignment**](https://lesswrong.com/posts/JrTk2pbqp7BFwPAKw/reward-button-alignment),
        *Steven Byrnes*, 2025-05-22, LessWrong'
      title: Reward button alignment
      authors:
      - Steven Byrnes
      author_organizations:
      - Astera Institute
      date: '2025-05-22'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/cus5CGmLrjBRgcPSF/system-2-alignment-deliberation-review-and-thought
      link_text: 'System 2 Alignment: Deliberation, Review, and Thought Management'
      original_md: '* [**System 2 Alignment: Deliberation, Review, and Thought Management**](https://lesswrong.com/posts/cus5CGmLrjBRgcPSF/system-2-alignment-deliberation-review-and-thought),
        *Seth Herd*, 2025-02-13, LessWrong'
      title: 'System 2 Alignment: Deliberation, Review, and Thought Management'
      authors:
      - Seth Herd
      author_organizations: []
      date: '2025-02-13'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://elicit.com/blog/system-2-learning
      link_text: null
      original_md: '* [https://elicit.com/blog/system-2-learning](https://elicit.com/blog/system-2-learning)'
      title: 'Against RL: The Case for System 2 Learning'
      authors:
      - Andreas Stuhlmüller
      author_organizations:
      - Elicit
      date: '2025-01-30'
      published_year: 2025
      venue: Elicit Blog
      kind: blog_post
    other_attributes: {}
  parsing_issues: []
- id: sec:ai_solve_alignment
  name: Make AI solve it
  header_level: 1
  parent_id: null
  content: ''
  item_type: section
  agenda_attributes: null
  parsing_issues: []
- id: a:weak_to_strong
  name: Weak-to-strong generalization
  header_level: 2
  parent_id: sec:ai_solve_alignment
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Rory ✅
    one_sentence_summary: Use weaker models to supervise and provide a feedback signal
      to stronger models.
    theory_of_change: Find techniques that do better than RLHF at supervising superior
      models → track whether these techniques fail as capabilities increase further
      → keep the stronger systems aligned by amplifying weak oversight and quantifying
      where it breaks.
    see_also:
    - a:supervising_improvement
    orthodox_problems:
    - superintelligence_hack_software
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: engineering
    broad_approach_text: engineering
    some_names:
    - Joshua Engels
    - Nora Belrose
    - David D. Baek
    estimated_ftes: 2-20
    critiques: '[Can we safely automate alignment research?](https://joecarlsmith.substack.com/p/can-we-safely-automate-alignment),
      [Super(ficial)-alignment: Strong Models May Deceive Weak Models in Weak-to-Strong
      Generalization](https://arxiv.org/abs/2406.11431)'
    funded_by: lab funders, Eleuther funders
    outputs:
    - link_url: https://arxiv.org/abs/2501.13124
      link_text: Debate Helps Weak-to-Strong Generalization
      original_md: '* [**Debate Helps Weak-to-Strong Generalization**](https://arxiv.org/abs/2501.13124)'
      title: Debate Helps Weak-to-Strong Generalization
      authors:
      - Hao Lang
      - Fei Huang
      - Yongbin Li
      author_organizations: []
      date: '2025-01-21'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://openreview.net/forum?id=RwYdLgj1S6
      link_text: Understanding the Capabilities and Limitations of Weak-to-Strong
        Generalization
      original_md: '* [**Understanding the Capabilities and Limitations of Weak-to-Strong
        Generalization**](https://openreview.net/forum?id=RwYdLgj1S6)'
      title: Understanding the Capabilities and Limitations of Weak-to-Strong Generalization
      authors:
      - Wei Yao
      - Wenkai Yang
      - Ziqiao Wang
      - Yankai Lin
      - Yong Liu
      author_organizations: []
      date: '2025-03-08'
      published_year: 2025
      venue: ICLR 2025 Workshop SSI-FM
      kind: paper_published
    - link_url: https://arxiv.org/abs/2502.04313
      link_text: Great Models Think Alike and this Undermines AI Oversight
      original_md: '* [**Great Models Think Alike and this Undermines AI Oversight**](https://arxiv.org/abs/2502.04313)'
      title: Great Models Think Alike and this Undermines AI Oversight
      authors:
      - Shashwat Goel
      - Joschka Struber
      - Ilze Amanda Auzina
      - Karuna K Chandra
      - Ponnurangam Kumaraguru
      - Douwe Kiela
      - Ameya Prabhu
      - Matthias Bethge
      - Jonas Geiping
      author_organizations: []
      date: '2025-02-06'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.18530
      link_text: Scaling Laws For Scalable Oversight
      original_md: '* [**Scaling Laws For Scalable Oversight**](https://arxiv.org/abs/2504.18530)'
      title: Scaling Laws For Scalable Oversight
      authors:
      - Joshua Engels
      - David D. Baek
      - Subhash Kantamneni
      - Max Tegmark
      author_organizations:
      - MIT
      date: '2025-04-25'
      published_year: 2025
      venue: arXiv (NeurIPS 2025 Spotlight)
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:supervising_improvement
  name: Supervising AIs improving AIs
  header_level: 2
  parent_id: sec:ai_solve_alignment
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Rory ✅
    one_sentence_summary: Build formal and empirical frameworks where AIs supervise
      other (stronger) AI systems via structured interactions; construct monitoring
      tools which enable scalable tracking of behavioural drift, benchmarks for self-modification,
      and robustness guarantees
    theory_of_change: Early models train ~only on human data while later models also
      train on early model outputs, which leads to early model problems cascading.
      Left unchecked this will likely cause problems, so supervision mechanisms are
      needed to help ensure the AI self-improvement remains legible.
    see_also: []
    orthodox_problems:
    - superintelligence_fool_supervisors
    - superintelligence_hack_software
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: behaviorist_science
    broad_approach_text: behaviorist science
    some_names:
    - Roman Engeler
    - Akbir Khan
    - Ethan Perez
    estimated_ftes: 1-10
    critiques: '[Automation collapse](https://www.lesswrong.com/posts/2Gy9tfjmKwkYbF9BY/automation-collapse),
      [Great Models Think Alike and this Undermines AI Oversight](https://arxiv.org/abs/2502.04313)'
    funded_by: Long-Term Future Fund, lab funders
    outputs:
    - link_url: https://saif.org/research/bare-minimum-mitigations-for-autonomous-ai-development/
      link_text: Bare Minimum Mitigations for Autonomous AI Development
      original_md: '* [**Bare Minimum Mitigations for Autonomous AI Development**](https://saif.org/research/bare-minimum-mitigations-for-autonomous-ai-development/)'
      title: Bare Minimum Mitigations for Autonomous AI Development
      authors:
      - Joshua Clymer
      - Isabella Duan
      - Chris Cundy
      - Yawen Duan
      - Fynn Heide
      - Chaochao Lu
      - Sören Mindermann
      - Conor McGurk
      - Xudong Pan
      - Saad Siddiqui
      - Jingren Wang
      - Min Yang
      - Xianyuan Zhan
      author_organizations:
      - Safe AI Forum
      date: '2025-04-22'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/PhgEKkB4cwYjwpGxb/maintaining-alignment-during-rsi-as-a-feedback-control
      link_text: Maintaining Alignment during RSI as a Feedback Control Problem
      original_md: '* [**Maintaining Alignment during RSI as a Feedback Control Problem**](https://lesswrong.com/posts/PhgEKkB4cwYjwpGxb/maintaining-alignment-during-rsi-as-a-feedback-control),
        *beren*, 2025-03-02, LessWrong'
      title: Maintaining Alignment during RSI as a Feedback Control Problem
      authors:
      - beren
      author_organizations: []
      date: '2025-03-02'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2504.18530
      link_text: Scaling Laws for Scalable Oversight
      original_md: '* [**Scaling Laws for Scalable Oversight**](https://arxiv.org/abs/2504.18530),
        *Subhash Kantamneni, Josh Engels, David Baek, Max Tegmark*, 2025-04-25, arXiv'
      title: Scaling Laws For Scalable Oversight
      authors:
      - Joshua Engels
      - David D. Baek
      - Subhash Kantamneni
      - Max Tegmark
      author_organizations:
      - MIT
      date: '2025-04-25'
      published_year: 2025
      venue: arXiv (NeurIPS 2025 Spotlight)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2412.08897
      link_text: Neural Interactive Proofs
      original_md: '* [**Neural Interactive Proofs**](https://arxiv.org/abs/2412.08897),
        *Lewis Hammond, Sam Adam-Day*, 2025-03-17, arXiv'
      title: Neural Interactive Proofs
      authors:
      - Lewis Hammond
      - Sam Adam-Day
      author_organizations: []
      date: '2024-12-12'
      published_year: 2024
      venue: arXiv (ICLR 2025)
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/TQbptN7F4ijPnQRLy/video-and-transcript-of-talk-on-automating-alignment
      link_text: Video and transcript of talk on automating alignment research
      original_md: '* [**Video and transcript of talk on automating alignment research**](https://lesswrong.com/posts/TQbptN7F4ijPnQRLy/video-and-transcript-of-talk-on-automating-alignment),
        *Joe Carlsmith*, 2025-04-30, LessWrong'
      title: Video and transcript of talk on automating alignment research
      authors:
      - Joe Carlsmith
      author_organizations:
      - Anthropic
      date: '2025-04-30'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2502.21262
      link_text: Modeling Human Beliefs about AI Behavior for Scalable Oversight
      original_md: '* [Modeling Human Beliefs about AI Behavior for Scalable Oversight](https://arxiv.org/abs/2502.21262)'
      title: Modeling Human Beliefs about AI Behavior for Scalable Oversight
      authors:
      - Leon Lang
      - Patrick Forré
      author_organizations: []
      date: '2025-02-28'
      published_year: 2025
      venue: Transactions on Machine Learning Research
      kind: paper_published
    - link_url: https://arxiv.org/abs/2502.04675
      link_text: Scalable Oversight for Superhuman AI via Recursive Self-Critiquing
      original_md: '* [Scalable Oversight for Superhuman AI via Recursive Self-Critiquing](https://arxiv.org/abs/2502.04675)'
      title: Scalable Oversight for Superhuman AI via Recursive Self-Critiquing
      authors:
      - Xueru Wen
      - Jie Lou
      - Xinyu Lu
      - Junjie Yang
      - Yanjiang Liu
      - Yaojie Lu
      - Debing Zhang
      - Xing Yu
      author_organizations: []
      date: '2025-02-07'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.alignmentforum.org/posts/EgRJtwQurNzz8CEfJ/dodging-systematic-human-errors-in-scalable-oversight
      link_text: Dodging systematic human errors in scalable oversight
      original_md: '* [Dodging systematic human errors in scalable oversight](https://www.alignmentforum.org/posts/EgRJtwQurNzz8CEfJ/dodging-systematic-human-errors-in-scalable-oversight)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    other_attributes: {}
  parsing_issues: []
- id: a:transluce
  name: Transluce
  header_level: 2
  parent_id: sec:ai_solve_alignment
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: jord ✅
    one_sentence_summary: Make open AI tools to explain AIs, including agents. E.g.
      feature descriptions for neuron activation patterns; an interface for steering
      these features; behavior elicitation agent that searches for user-specified
      behaviors from frontier models
    theory_of_change: Improve interp and evals in public and get invited to improve
      lab processes.
    see_also: []
    orthodox_problems:
    - superintelligence_fool_supervisors
    - superintelligence_hack_software
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - Jacob Steinhardt
    - Neil Chowdhury
    - Vincent Huang
    - Sarah Schwettmann
    - Robert Friel
    estimated_ftes: 15-30
    critiques: null
    funded_by: Schmidt Sciences, Halcyon Futures, John Schulman, Wojciech Zaremba
    outputs:
    - link_url: https://transluce.org/jailbreaking-frontier-models
      link_text: Automatically Jailbreaking Frontier Language Models with Investigator
        Agents
      original_md: '* [Automatically Jailbreaking Frontier Language Models with Investigator
        Agents](https://transluce.org/jailbreaking-frontier-models)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://transluce.org/investigating-o3-truthfulness
      link_text: Investigating truthfulness in a pre-release o3 model
      original_md: '* [Investigating truthfulness in a pre-release o3 model](https://transluce.org/investigating-o3-truthfulness)'
      title: Investigating truthfulness in a pre-release o3 model
      authors:
      - Neil Chowdhury
      - Daniel Johnson
      - Vincent Huang
      - Jacob Steinhardt
      - Sarah Schwettmann
      author_organizations:
      - Transluce
      date: '2025-04-16'
      published_year: 2025
      venue: Transluce Blog
      kind: blog_post
    - link_url: https://transluce.org/pathological-behaviors
      link_text: Surfacing Pathological Behaviors in Language Models
      original_md: '* [Surfacing Pathological Behaviors in Language Models](https://transluce.org/pathological-behaviors)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://transluce.org/introducing-docent
      link_text: 'Docent: A system for analyzing and intervening on agent behavior'
      original_md: '* [Docent: A system for analyzing and intervening on agent behavior](https://transluce.org/introducing-docent)'
      title: Introducing Docent
      authors:
      - Kevin Meng
      - Vincent Huang
      - Jacob Steinhardt
      - Sarah Schwettmann
      author_organizations:
      - Transluce
      date: '2025-03-24'
      published_year: 2025
      venue: Transluce Blog
      kind: blog_post
    - link_url: https://transluce.org/neuron-circuits
      link_text: https://transluce.org/neuron-circuits
      original_md: '* [https://transluce.org/neuron-circuits](https://transluce.org/neuron-circuits)'
      title: Language Model Circuits Are Sparse in the Neuron Basis
      authors:
      - Aryaman Arora
      - Zhengxuan Wu
      - Jacob Steinhardt
      - Sarah Schwettmann
      author_organizations:
      - Transluce
      date: '2025-11-20'
      published_year: 2025
      venue: Transluce AI
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:debate
  name: Debate
  header_level: 2
  parent_id: sec:ai_solve_alignment
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Rory
    one_sentence_summary: Make highly capable agents do what humans want, even when
      it is difficult for humans to know what that is.
    theory_of_change: '"Give humans help in supervising strong agents" + "Align explanations
      with the true reasoning process of the agent" + "Red team models to exhibit
      failure modes that don''t occur in normal use" are necessary but probably not
      sufficient for safe AGI.'
    see_also: []
    orthodox_problems:
    - value_fragile
    - superintelligence_fool_supervisors
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: null
    broad_approach_text: engineering / cognitive
    some_names:
    - Rohin Shah
    - Jonah Brown-Cohen
    - Georgios Piliouras
    - UK AISI (Benjamin Holton)
    estimated_ftes: null
    critiques: '[The limits of AI safety via debate (2022)](https://www.lesswrong.com/posts/kguLeJTt6LnGuYX4E/the-limits-of-ai-safety-via-debate)'
    funded_by: Google, others
    outputs:
    - link_url: https://arxiv.org/abs/2506.02175
      link_text: AI Debate Aids Assessment of Controversial Claims
      original_md: '* [**AI Debate Aids Assessment of Controversial Claims**](https://arxiv.org/abs/2506.02175),
        *Salman Rahman, Sheriff Issaka, Ashima Suvarna et al.*, 2025-06-02, arXiv'
      title: AI Debate Aids Assessment of Controversial Claims
      authors:
      - Salman Rahman
      - Sheriff Issaka
      - Ashima Suvarna
      - Genglin Liu
      - James Shiffer
      - Jaeyoung Lee
      - Md Rizwan Parvez
      - Hamid Palangi
      - Shi Feng
      - Nanyun Peng
      - Yejin Choi
      - Julian Michael
      - Liwei Jiang
      - Saadia Gabriel
      author_organizations:
      - University of Washington
      - Microsoft Research
      - UCLA
      - NYU
      date: '2025-06-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.03989
      link_text: An alignment safety case sketch based on debate
      original_md: '* [**An alignment safety case sketch based on debate**](https://arxiv.org/abs/2505.03989),
        *Marie Davidsen Buhl, Jacob Pfau, Benjamin Hilton et al.*, 2025-05-23, arXiv'
      title: An alignment safety case sketch based on debate
      authors:
      - Marie Davidsen Buhl
      - Jacob Pfau
      - Benjamin Hilton
      - Geoffrey Irving
      author_organizations:
      - Google DeepMind
      date: '2025-05-23'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.lesswrong.com/s/NdovveRcyfxgMoujf
      link_text: 'UK AISI Alignment Team: Debate Sequence'
      original_md: '* [**UK AISI Alignment Team: Debate Sequence**](https://www.lesswrong.com/s/NdovveRcyfxgMoujf),
        *Benjamin Hilton, Jacob Pfau, et al.*'
      title: 'UK AISI Alignment Team: Debate Sequence'
      authors:
      - Benjamin Hilton
      author_organizations:
      - UK AI Safety Institute
      date: '2025-05-07'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/8XHBaugB5S3r27MG9/prover-estimator-debate-a-new-scalable-oversight-protocol
      link_text: 'Prover-Estimator Debate: A New Scalable Oversight Protocol'
      original_md: '* [**Prover-Estimator Debate: A New Scalable Oversight Protocol**](https://lesswrong.com/posts/8XHBaugB5S3r27MG9/prover-estimator-debate-a-new-scalable-oversight-protocol)'
      title: 'Prover-Estimator Debate: A New Scalable Oversight Protocol'
      authors:
      - Jonah Brown-Cohen
      - Geoffrey Irving
      author_organizations:
      - UK AISI
      date: '2025-06-17'
      published_year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2509.00091
      link_text: Ensemble Debates with Local Large Language Models for AI Alignment
      original_md: '* [**Ensemble Debates with Local Large Language Models for AI
        Alignment**](https://arxiv.org/abs/2509.00091)'
      title: Ensemble Debates with Local Large Language Models for AI Alignment
      authors:
      - Ephraiem Sarabamoun
      author_organizations: []
      date: '2025-08-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues:
  - Broad approach contains multiple approaches ('engineering, cognitive') - using
    null for broad_approach_id and 'engineering / cognitive' for broad_approach_text
- id: a:introspection_training
  name: LLM introspection training
  header_level: 2
  parent_id: sec:ai_solve_alignment
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Train LLMs to the predict the outputs of high-quality whitebox
      explainability methods, in order to induce the LLM to learn generalized self-explanation
      skills that use its own natural 'introspective' access
    theory_of_change: Integrating self-explanation into an LLM's core skillset should
      lead to default scalability, since self-explanation skill advancement will feed
      off general-intelligence advancement
    see_also:
    - a:transluce
    orthodox_problems:
    - goals_misgeneralize
    - superintelligence_fool_supervisors
    - superintelligence_hack_software
    target_case_id: null
    target_case_text: mixed
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - Belinda Z. Li
    - Zifan Carl Guo
    - Vincent Huang
    - Jacob Steinhardt
    - Jacob Andreas
    estimated_ftes: 2-20
    critiques: null
    funded_by: Schmidt Sciences, Halcyon Futures, John Schulman, Wojciech Zaremba
    outputs:
    - link_url: https://arxiv.org/abs/2511.08579
      link_text: Training Language Models to Explain Their Own Computations
      original_md: '[Training Language Models to Explain Their Own Computations](https://arxiv.org/abs/2511.08579)
        Belinda Z. Li, Zifan Carl Guo , Vincent Huang , Jacob Steinhardt, Jacob Andreas'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    other_attributes: {}
  parsing_issues:
  - Target case field says 'mixed' - left target_case_id as null and target_case_text
    as 'mixed'
  - Broad approach field says 'cognitive' - mapped to 'cognitivist_science'
- id: sec:theory
  name: Theory
  header_level: 1
  parent_id: null
  content: Develop a principled scientific understanding that will help us reliably
    understand and control current and future AI systems.
  item_type: section
  agenda_attributes: null
  parsing_issues: []
- id: a:agent_foundations
  name: Agent foundations
  header_level: 2
  parent_id: sec:theory
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stag
    one_sentence_summary: Develop philosophical clarity and mathematical formalizations
      of building blocks that might be useful for plans to align strong superintelligence,
      such as agency, optimization strength, decision theory, abstractions, concepts,
      etc.
    theory_of_change: Rigorously understand optimization processed and agents, and
      what it means for them to be aligned in a substrate independent way → identify
      impossibility results and necessary conditions for aligned optimizer systems
      → use this theoretical understanding to eventually design safe architectures
      that remain stable and safe under self-reflection
    see_also:
    - a:tiling_agents
    - a:theory_dovetail
    orthodox_problems:
    - value_fragile
    - corrigibility_anti_natural
    - goals_misgeneralize
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - Abram Demski
    - Alex Altair
    - Sam Eisenstat
    - Thane Ruthenis
    estimated_ftes: null
    critiques: null
    funded_by: null
    outputs:
    - link_url: https://www.arxiv.org/pdf/2508.16245
      link_text: Limit-Computable Grains of Truth for Arbitrary Computable Extensive-Form
        (Un)Known Games
      original_md: '* [Limit-Computable Grains of Truth for Arbitrary Computable Extensive-Form
        (Un)Known Games](https://www.arxiv.org/pdf/2508.16245)'
      title: Limit-Computable Grains of Truth for Arbitrary Computable Extensive-Form
        (Un)Known Games
      authors:
      - Cole Wyeth
      - Marcus Hutter
      - Jan Leike
      - Jessica Taylor
      author_organizations:
      - Independent
      - DeepMind
      - MIRI
      date: '2025-08-22'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://uaiasi.com/blog-posts/
      link_text: https://uaiasi.com/blog-posts/
      original_md: '* [https://uaiasi.com/blog-posts/](https://uaiasi.com/blog-posts/)'
      title: Blog Posts – Universal Algorithmic Intelligence
      authors:
      - Cole Wyeth
      author_organizations:
      - Universal Algorithmic Intelligence
      date: null
      published_year: 2025
      venue: Universal Algorithmic Intelligence website
      kind: blog_post
    - link_url: https://link.springer.com/article/10.1007/s11098-025-02296-x
      link_text: Off-switching not guaranteed
      original_md: '* [**Off-switching not guaranteed**](https://link.springer.com/article/10.1007/s11098-025-02296-x),
        *Sven Neth*, 2025-02-26, Agent Foundations 2025 at CMU'
      title: Off-switching not guaranteed
      authors:
      - Sven Neth
      author_organizations:
      - University of Pittsburgh
      date: '2025-02-26'
      published_year: 2025
      venue: Philosophical Studies
      kind: paper_published
    - link_url: https://openreview.net/pdf?id=Rf1CeGPA22
      link_text: Good old fashioned decision theory
      original_md: '* Good old fashioned decision theory [https://openreview.net/pdf?id=Rf1CeGPA22](https://openreview.net/pdf?id=Rf1CeGPA22)'
      title: Unable to extract - PDF content not accessible
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: OpenReview
      kind: error_detected
    - link_url: https://openreview.net/forum?id=tlkYPU3FlX
      link_text: Formalizing Embeddedness Failures in Universal Artificial Intelligence
      original_md: '* [**Formalizing Embeddedness Failures in Universal Artificial
        Intelligence**](https://openreview.net/forum?id=tlkYPU3FlX), *Cole Wyeth,
        Marcus Hutter*, 2025-07-01, ODYSSEY 2025 Conference'
      title: Formalizing Embeddedness Failures in Universal Artificial Intelligence
      authors:
      - Cole Wyeth
      - Marcus Hutter
      author_organizations: []
      date: '2025-07-01'
      published_year: 2025
      venue: ODYSSEY 2025 Conference
      kind: paper_preprint
    - link_url: https://www.lesswrong.com/posts/EyvJvYEFzDv5kGoiG/clarifying-wisdom-foundational-topics-for-aligned-ais-to
      link_text: 'Clarifying "wisdom": Foundational topics for aligned AIs to prioritize
        before irreversible decisions'
      original_md: '* [**Clarifying "wisdom": Foundational topics for aligned AIs
        to prioritize before irreversible decisions**](https://www.lesswrong.com/posts/EyvJvYEFzDv5kGoiG/clarifying-wisdom-foundational-topics-for-aligned-ais-to),
        *Anthony DiGiovanni*, 2025-07-20, LessWrong'
      title: 'Unable to access: 429 Too Many Requests'
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: error_detected
    - link_url: https://www.lesswrong.com/posts/Dt4DuCCok3Xv5HEnG/agent-foundations-not-really-math-not-really-science
      link_text: 'Agent foundations: not really math, not really science'
      original_md: '* [**Agent foundations: not really math, not really science**](https://www.lesswrong.com/posts/Dt4DuCCok3Xv5HEnG/agent-foundations-not-really-math-not-really-science),
        *Alex Altair*, 2025-08-17, LessWrong'
      title: 'Agent foundations: not really math, not really science'
      authors:
      - Alex_Altair
      author_organizations:
      - Dovetail Research
      date: '2025-08-17'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/nuDJNyG5XLQjtvaeg/is-alignment-reducible-to-becoming-more-coherent
      link_text: Is alignment reducible to becoming more coherent?
      original_md: '* [**Is alignment reducible to becoming more coherent?**](https://lesswrong.com/posts/nuDJNyG5XLQjtvaeg/is-alignment-reducible-to-becoming-more-coherent),
        *Cole Wyeth*, 2025-04-22, LessWrong'
      title: Is alignment reducible to becoming more coherent?
      authors:
      - Cole Wyeth
      author_organizations: []
      date: '2025-04-22'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/dHNKtQ3vTBxTfTPxu/what-is-the-alignment-problem
      link_text: What Is The Alignment Problem?
      original_md: '* [**What Is The Alignment Problem?**](https://lesswrong.com/posts/dHNKtQ3vTBxTfTPxu/what-is-the-alignment-problem),
        *johnswentworth*, 2025-01-16, LessWrong'
      title: What Is The Alignment Problem?
      authors:
      - johnswentworth
      author_organizations: []
      date: '2025-01-16'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    other_attributes: {}
  parsing_issues:
  - Broad approach field says 'cognitive' - mapped to 'cognitivist_science' (cognitivist
    science)
- id: a:tiling_agents
  name: Tiling agents
  header_level: 2
  parent_id: sec:theory
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stag
    one_sentence_summary: An aligned agentic system modifying itself into an unaligned
      system would be bad and we can research ways that this could occur and infrastructure/approaches
      that prevent it from happening.
    theory_of_change: Build enough theoretical basis through various approaches such
      that AI systems we create are capable of self-modification while preserving
      goals.
    see_also:
    - a:agent_foundations
    orthodox_problems:
    - value_fragile
    - corrigibility_anti_natural
    - goals_misgeneralize
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - Abram Demski
    estimated_ftes: 1-10
    critiques: null
    funded_by: null
    outputs:
    - link_url: https://static1.squarespace.com/static/663d1233249bce4815fe8753/t/68067a6f5d5fb0745642d5b1/1745255023842/Understanding+Trust+-+Abram+Demski.pdf
      link_text: Understanding Trust
      original_md: '* [**Understanding Trust**](https://static1.squarespace.com/static/663d1233249bce4815fe8753/t/68067a6f5d5fb0745642d5b1/1745255023842/Understanding+Trust+-+Abram+Demski.pdf),
        *Abram Demski, Norman Hsia, and Paul Rapoport*, 2025-02-17, Agent Foundations
        2025 at CMU'
      title: Understanding Trust
      authors:
      - Abram Demski
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: error_detected
    - link_url: https://www.lesswrong.com/posts/akuMwu8SkmQSdospi/working-through-a-small-tiling-result
      link_text: Working through a small tiling result
      original_md: '* [**Working through a small tiling result**](https://www.lesswrong.com/posts/akuMwu8SkmQSdospi/working-through-a-small-tiling-result),
        James Payor, 2025-05-13, LessWrong'
      title: Working through a small tiling result
      authors:
      - James Payor
      author_organizations: []
      date: '2024-05-13'
      published_year: 2024
      venue: LessWrong
      kind: lesswrong
    - link_url: https://openreview.net/forum?id=Rf1CeGPA22
      link_text: Communication & Trust
      original_md: '* [**Communication & Trust**](https://openreview.net/forum?id=Rf1CeGPA22),
        *Abram Demski*, 2025-07-09, ODYSSEY 2025 Conference'
      title: Communication & Trust
      authors:
      - Abram Demski
      author_organizations: []
      date: '2025-07-09'
      published_year: 2025
      venue: ODYSSEY 2025 Conference
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/PhgEKkB4cwYjwpGxb/maintaining-alignment-during-rsi-as-a-feedback-control
      link_text: Maintaining Alignment during RSI as a Feedback Control Problem
      original_md: '* [**Maintaining Alignment during RSI as a Feedback Control Problem**](https://lesswrong.com/posts/PhgEKkB4cwYjwpGxb/maintaining-alignment-during-rsi-as-a-feedback-control),
        *beren*, 2025-03-02, LessWrong'
      title: Maintaining Alignment during RSI as a Feedback Control Problem
      authors:
      - beren
      author_organizations: []
      date: '2025-03-02'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    other_attributes: {}
  parsing_issues: []
- id: a:theory_dovetail
  name: Dovetail
  header_level: 2
  parent_id: sec:theory
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stag
    one_sentence_summary: Formalize key ideas ("structure", "agency", etc) mathematically.
    theory_of_change: generalize theorems → formalize agent foundations concepts like
      the agent structure problem → hopefully assist other projects through increased
      understanding
    see_also:
    - a:agent_foundations
    orthodox_problems:
    - 'Other: understanding the nature of the problems through formalization'
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: null
    broad_approach_text: maths / philosophy
    some_names:
    - Alex Altair
    - Alfred Harwood
    - Daniel C
    - Dalcy K
    - José Pedro Faustino
    estimated_ftes: 1-5
    critiques: null
    funded_by: LTFF, [Patreon](https://www.patreon.com/Dovetailresearch/about)
    outputs:
    - link_url: https://www.lesswrong.com/posts/ApfjBbqzSu4aZoLSe/report-and-retrospective-on-the-dovetail-fellowship
      link_text: Report & retrospective on the Dovetail fellowship
      original_md: '* [**Report & retrospective on the Dovetail fellowship**](https://www.lesswrong.com/posts/ApfjBbqzSu4aZoLSe/report-and-retrospective-on-the-dovetail-fellowship),
        *Alex Altair*, 2025-03-15, LessWrong; contains numerous links'
      title: Report & retrospective on the Dovetail fellowship
      authors:
      - Alex Altair
      author_organizations:
      - Dovetail Research
      date: '2025-03-14'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    other_attributes: {}
  parsing_issues:
  - Orthodox problems field contains 'understanding the nature of the problems through
    formalization' which doesn't match standard problems - kept as 'Other'
  - Broad approach is 'maths/philosophy' which doesn't match standard approaches (engineering,
    behaviorist_science, cognitivist_science) - broad_approach_id left null
- id: a:live_theory
  name: High-Actuation Spaces
  header_level: 2
  parent_id: sec:theory
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Mech interp and alignment assume a stable "computational
      substrate" (linear algebra on GPUs). If later AI uses different substrates (e.g.
      something neuromorphic), methods like probes and steering will not transfer.
      Therefore, better to try and infer goals via a "telic DAG" which abstracts over
      substrates, and so sidestep the issue of how to define intermediate representations.
      Category theory is intended to provide guarantees that this abstraction is valid.
    theory_of_change: Sufficiently complex mindlike entities can alter their goals
      in ways that cannot be predicted or accounted for under substrate-dependent
      descriptions of the kind sought in mechanistic interpretability. use the telic
      DAG to define a method analogous to factoring a causal DAG.
    see_also:
    - '[Live theory](https://www.lesswrong.com/s/aMz2JMvgXrLBkq4h3)'
    - '[MoSSAIC](https://openreview.net/forum?id=n7WYSJ35FU)'
    - '[Topos Institute](https://topos.institute/)'
    - a:agent_foundations
    orthodox_problems:
    - 'Other: actually a new and general one'
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: null
    broad_approach_text: maths / philosophy
    some_names:
    - Sahil K
    - Matt Farr
    - Aditya Arpitha Prasad
    - Chris Pang
    - Aditya Adiga
    - Jayson Amati
    - Steve Petersen
    - Topos
    - T J
    estimated_ftes: 1-10
    critiques: null
    funded_by: null
    outputs:
    - link_url: https://groundless.ai/
      link_text: https://groundless.ai/
      original_md: '* [**https://groundless.ai/**](https://groundless.ai/)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://openreview.net/forum?id=n7WYSJ35FU
      link_text: null
      original_md: '* [https://openreview.net/forum?id=n7WYSJ35FU](https://openreview.net/forum?id=n7WYSJ35FU)'
      title: 'MoSSAIC: AI Safety After Mechanism'
      authors:
      - Matt Farr
      - Aditya Arpitha Prasad
      - Chris Pang
      - Aditya Adiga
      - Jayson Amati
      - Sahil K
      author_organizations: []
      date: '2025-07-01'
      published_year: 2025
      venue: ODYSSEY 2025 Conference
      kind: paper_preprint
    - link_url: https://www.lesswrong.com/s/aMz2JMvgXrLBkq4h3
      link_text: null
      original_md: '* [https://www.lesswrong.com/s/aMz2JMvgXrLBkq4h3](https://www.lesswrong.com/s/aMz2JMvgXrLBkq4h3)'
      title: '429: Too Many Requests'
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: error_detected
    - link_url: https://docs.google.com/document/d/1d-ARdZZDHFPIfGcTTOKK8IZWlQj0NZQrmteJj2mvmYA/edit?tab=t.0#heading=h.eg8luyrlsv2u
      link_text: null
      original_md: '* [https://docs.google.com/document/d/1d-ARdZZDHFPIfGcTTOKK8IZWlQj0NZQrmteJj2mvmYA/edit?tab=t.0\#heading=h.eg8luyrlsv2u](https://docs.google.com/document/d/1d-ARdZZDHFPIfGcTTOKK8IZWlQj0NZQrmteJj2mvmYA/edit?tab=t.0#heading=h.eg8luyrlsv2u)'
      title: High Actuation Spaces - Sahil
      authors:
      - Sahil
      author_organizations: []
      date: null
      published_year: null
      venue: Google Docs
      kind: error_detected
    - link_url: https://drive.google.com/drive/folders/1EaAJ4szuZsYR2_-DkS9cuhx3S6IWeCjW
      link_text: null
      original_md: '* [https://drive.google.com/drive/folders/1EaAJ4szuZsYR2_-DkS9cuhx3S6IWeCjW](https://drive.google.com/drive/folders/1EaAJ4szuZsYR2_-DkS9cuhx3S6IWeCjW)'
      title: HAS - Public (High Actuation Spaces)
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: Google Drive
      kind: error_detected
    - link_url: https://www.lesswrong.com/posts/tQ9vWm4b57HFqbaRj/what-if-not-agency
      link_text: null
      original_md: '* [https://www.lesswrong.com/posts/tQ9vWm4b57HFqbaRj/what-if-not-agency](https://www.lesswrong.com/posts/tQ9vWm4b57HFqbaRj/what-if-not-agency)'
      title: Unknown - Content Unavailable (429 Error)
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: error_detected
    - link_url: https://docs.google.com/document/d/1fl7LE8AN7mLJ6uFcPuFCzatp0zCIYvjRIjQRgHPAkSE/edit?tab=t.0
      link_text: Human Inductive Bias Project
      original_md: '* See also the [Human Inductive Bias Project](https://docs.google.com/document/d/1fl7LE8AN7mLJ6uFcPuFCzatp0zCIYvjRIjQRgHPAkSE/edit?tab=t.0).'
      title: HIBP Human Inductive Bias Project Plan
      authors:
      - Félix Dorn
      author_organizations: []
      date: null
      published_year: null
      venue: Google Docs
      kind: error_detected
    other_attributes: {}
  parsing_issues:
  - 'Orthodox problems field says ''actually a new and general one'' - no matching
    standard problem found, kept as ''Other: actually a new and general one'''
  - Broad approach 'maths/philosophy' doesn't match standard approaches (engineering/behaviorist_science/cognitivist_science)
    - normalized to 'maths / philosophy' and left broad_approach_id as null
- id: a:simulators
  name: Simulators
  header_level: 2
  parent_id: sec:theory
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stag
    one_sentence_summary: Treat LLMs as a general simulator of sequences instead of
      as a goal-maximising agent. It effectively *roleplays* as a character or a statistical
      process. This is a counter-intuitive and hard to verify perspective, but yields
      different predictions.
    theory_of_change: Figure out the extent to which this framing matches current
      and future AI agent psychology, then use that to have less confused conversations
      and build safer models.
    see_also: []
    orthodox_problems:
    - goals_misgeneralize
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - Jan Kulveit
    - Will Petillo
    estimated_ftes: 1-5
    critiques: null
    funded_by: Alignment of Complex Systems
    outputs:
    - link_url: https://www.alignmentforum.org/posts/zuXo9imNKYspu9HGv/a-three-layer-model-of-llm-psychology%20
      link_text: A Three-Layer Model of LLM Psychology
      original_md: '* [A Three-Layer Model of LLM Psychology](https://www.alignmentforum.org/posts/zuXo9imNKYspu9HGv/a-three-layer-model-of-llm-psychology%20),
        *Jan Kulveit*, 2024-12-26, Alignment Forum'
      title: A Three-Layer Model of LLM Psychology
      authors:
      - Jan Kulveit
      author_organizations: []
      date: '2024-12-26'
      published_year: 2024
      venue: AI Alignment Forum
      kind: lesswrong
    - link_url: https://www.lesswrong.com/s/pwKrMXjYNK5LNeKCu
      link_text: 'Simulators vs Agents: Updating Risk Models'
      original_md: '* [Simulators vs Agents: Updating Risk Models](https://www.lesswrong.com/s/pwKrMXjYNK5LNeKCu),
        *Will Petillo, Sean Herrington, Spencer Ames, Adebayo Mubarak, Can Narin*,
        2025-05-12, LessWrong'
      title: '429: Too Many Requests'
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: error_detected
    other_attributes: {}
  parsing_issues: []
- id: a:aisi_guarantees
  name: Asymptotic guarantees
  header_level: 2
  parent_id: sec:theory
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Peli
    one_sentence_summary: Prove that if a safety process has enough resources (human
      data quality, training time, neural network capacity), then in the limit some
      system specification will be guaranteed. Use complexity theory, game theory,
      learning theory and other areas to both improve asymptotic guarantees and develop
      ways of showing convergence.
    theory_of_change: Formal verification may be too hard. Make safety cases stronger
      by modelling their processes and proving that they would work in the limit.
    see_also:
    - a:debate
    - a:control
    orthodox_problems:
    - goals_misgeneralize
    - superintelligence_fool_supervisors
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - AISI
    - Jacob Pfau
    - Benjamin Hilton
    - Geoffrey Irving
    - Simon Marshall
    - Will Kirby
    - Martin Soto
    - David Africa
    - davidad
    estimated_ftes: 5 - 10
    critiques: 'Self-critique in [UK AISI''s Alignment Team: Research Agenda](https://lesswrong.com/posts/tbnw7LbNApvxNLAg8/uk-aisi-s-alignment-team-research-agenda)'
    funded_by: AISI
    outputs:
    - link_url: https://lesswrong.com/posts/iELyAqizJkizBQbfr/an-alignment-safety-case-sketch-based-on-debate
      link_text: An alignment safety case sketch based on debate
      original_md: '* [**An alignment safety case sketch based on debate**](https://lesswrong.com/posts/iELyAqizJkizBQbfr/an-alignment-safety-case-sketch-based-on-debate),
        *Marie_DB, Jacob Pfau, Benjamin Hilton et al.*, 2025-05-08, LessWrong / AI
        Alignment Forum'
      title: An alignment safety case sketch based on debate
      authors:
      - Marie_DB
      - Jacob Pfau
      - Benjamin Hilton
      - Geoffrey Irving
      author_organizations:
      - UK AISI
      date: '2025-05-08'
      published_year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/tbnw7LbNApvxNLAg8/uk-aisi-s-alignment-team-research-agenda
      link_text: 'UK AISI''s Alignment Team: Research Agenda'
      original_md: '* [**UK AISI''s Alignment Team: Research Agenda**](https://lesswrong.com/posts/tbnw7LbNApvxNLAg8/uk-aisi-s-alignment-team-research-agenda),
        *Benjamin Hilton, Jacob Pfau, Marie_DB et al.*, 2025-05-07, LessWrong'
      title: 'UK AISI''s Alignment Team: Research Agenda'
      authors:
      - Benjamin Hilton
      - Jacob Pfau
      - Marie_DB
      - Geoffrey Irving
      author_organizations:
      - UK AI Safety Institute
      date: '2025-05-07'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/EgRJtwQurNzz8CEfJ/dodging-systematic-human-errors-in-scalable-oversight
      link_text: Dodging systematic human errors in scalable oversight
      original_md: '* [**Dodging systematic human errors in scalable oversight**](https://lesswrong.com/posts/EgRJtwQurNzz8CEfJ/dodging-systematic-human-errors-in-scalable-oversight),
        *Geoffrey Irving*, 2025-05-14, LessWrong'
      title: Dodging systematic human errors in scalable oversight
      authors:
      - Geoffrey Irving
      author_organizations:
      - UK AISI
      date: '2025-05-14'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    other_attributes: {}
  parsing_issues: []
- id: a:arc_theory_formal
  name: Heuristic Explanations
  header_level: 2
  parent_id: sec:theory
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: jord ✅
    one_sentence_summary: Formalize mechanistic explanations of neural network behavior,
      automate the discovery of these "heuristic explanations" and use them to predict
      when novel input will lead to extreme behavior (Low Probability Estimation and
      Mechanistic Anomaly Detection).
    theory_of_change: Push mech interp as far as it can be pushed, use formally rigorous
      heuristic explanations for downstream tasks automatically, solve those downstream
      tasks well enough that Eliciting Latent Knowledge and deceptive alignment are
      solved.
    see_also:
    - ARC Theory
    - ELK
    - mechanistic anomaly detection
    - '[Acorn](https://acausal.org/)'
    orthodox_problems:
    - goals_misgeneralize
    - superintelligence_hack_software
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: null
    broad_approach_text: cognitive / maths/philosophy
    some_names:
    - Jacob Hilton
    - Mark Xu
    - Eric Neyman
    - Victor Lecomte
    - George Robinson
    estimated_ftes: 1-10
    critiques: '[Matolcsi](https://www.lesswrong.com/s/uYMw689vDFmgPEHrS)'
    funded_by: null
    outputs:
    - link_url: https://www.lesswrong.com/posts/Xt9r4SNNuYxW83tmo/a-computational-no-coincidence-principle
      link_text: A computational no-coincidence principle
      original_md: '* [A computational no-coincidence principle](https://www.lesswrong.com/posts/Xt9r4SNNuYxW83tmo/a-computational-no-coincidence-principle)'
      title: A computational no-coincidence principle
      authors:
      - Eric Neyman
      author_organizations:
      - Alignment Research Center
      date: '2025-02-14'
      published_year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - link_url: https://openreview.net/forum?id=m4OpQAK3eY
      link_text: Wide Neural Networks as a Baseline for the Computational No-Coincidence
        Conjecture
      original_md: '* [**Wide Neural Networks as a Baseline for the Computational
        No-Coincidence Conjecture**](https://openreview.net/forum?id=m4OpQAK3eY)'
      title: Wide Neural Networks as a Baseline for the Computational No-Coincidence
        Conjecture
      authors:
      - John Dunbar
      - Scott Aaronson
      author_organizations:
      - University of Texas at Austin
      date: '2025-07-07'
      published_year: 2025
      venue: ODYSSEY 2025 Conference
      kind: paper_preprint
    - link_url: https://www.lesswrong.com/posts/XdQd9gELHakd5pzJA/arc-progress-update-competing-with-sampling
      link_text: Competing with sampling
      original_md: '* [**Competing with sampling**](https://www.lesswrong.com/posts/XdQd9gELHakd5pzJA/arc-progress-update-competing-with-sampling)'
      title: 'ARC progress update: Competing with sampling'
      authors:
      - Eric Neyman
      author_organizations:
      - Alignment Research Center
      date: '2025-11-18'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://www.lesswrong.com/s/uYMw689vDFmgPEHrS
      link_text: Obstacles in ARC's research agenda
      original_md: '* [**Obstacles in ARC''s research agenda**](https://www.lesswrong.com/s/uYMw689vDFmgPEHrS)'
      title: Obstacles in ARC's agenda
      authors:
      - David Matolcsi
      author_organizations:
      - ARC
      date: '2025-04-30'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://gabrieldwu.com/assets/thesis.pdf
      link_text: Deduction-Projection Estimators for Understanding Neural Networks
      original_md: '* [Deduction-Projection Estimators for Understanding Neural Networks](https://gabrieldwu.com/assets/thesis.pdf)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    other_attributes: {}
  parsing_issues:
  - Broad approach field contains 'cognitive, maths/philosophy' which doesn't map
    cleanly to standard approaches (engineering/behaviorist_science/cognitivist_science).
    Kept as 'cognitive / maths/philosophy' with null ID.
- id: sec:corrigibility
  name: Corrigibility
  header_level: 2
  parent_id: sec:theory
  content: ''
  item_type: section
  agenda_attributes: null
  parsing_issues: []
- id: a:behavior_alignment_theory
  name: Behavior alignment theory
  header_level: 3
  parent_id: sec:corrigibility
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Rory ✅
    one_sentence_summary: Predict properties of future AGI (e.g. power-seeking) with
      formal models; formally state and prove hypotheses about the properties powerful
      systems will have and how we might try to change them.
    theory_of_change: Figure out hypotheses about properties powerful agents will
      have → attempt to rigorously prove under what conditions the hypotheses hold
      → test these hypotheses where feasible → design training environments that lead
      to more salutary properties.
    see_also:
    - a:agent_foundations
    - a:control
    orthodox_problems:
    - corrigibility_anti_natural
    - instrumental_convergence
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: null
    broad_approach_text: maths / philosophy
    some_names:
    - Ram Potham
    - Michael K. Cohen
    - Max Harms/Raelifin
    - John Wentworth
    - David Lorell
    - Elliott Thornley
    estimated_ftes: 1-10
    critiques: '[Ryan Greenblatt''s criticism](https://www.lesswrong.com/posts/YbEbwYWkf8mv9jnmi/the-shutdown-problem-incomplete-preferences-as-a-solution?commentId=GJAippZ6ZzCagSnDb)
      of one behavioral proposal'
    funded_by: null
    outputs:
    - link_url: https://onlinelibrary.wiley.com/doi/10.1002/aaai.70040?af=R
      link_text: Imitation learning is probably existentially safe
      original_md: '* [Imitation learning is probably existentially safe](https://onlinelibrary.wiley.com/doi/10.1002/aaai.70040?af=R)'
      title: Imitation learning is probably existentially safe
      authors:
      - Michael K. Cohen
      - Marcus Hutter
      author_organizations:
      - University of California, Berkeley
      - Australian National University
      date: '2025-11-21'
      published_year: 2025
      venue: AI Magazine
      kind: paper_published
    - link_url: https://www.lesswrong.com/posts/knwR9RgGN5a2oorci/preference-gaps-as-a-safeguard-against-ai-self-replication
      link_text: Preference gaps as a safeguard against AI self-replication
      original_md: '* [**Preference gaps as a safeguard against AI self-replication**](https://www.lesswrong.com/posts/knwR9RgGN5a2oorci/preference-gaps-as-a-safeguard-against-ai-self-replication),
        *Elliott Thornley, tbs*, 2025-11-26, LessWrong'
      title: Preference gaps as a safeguard against AI self-replication
      authors:
      - tbs
      - EJT
      author_organizations: []
      date: '2025-11-26'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://www.lesswrong.com/s/KfCjeconYRdFbMxsy/p/qgBFJ72tahLo5hzqy
      link_text: Serious Flaws in CAST
      original_md: '* [**Serious Flaws in CAST**](https://www.lesswrong.com/s/KfCjeconYRdFbMxsy/p/qgBFJ72tahLo5hzqy),
        *Max Harms, 2025-11-19,* LessWrong'
      title: Serious Flaws in CAST
      authors:
      - Max Harms
      author_organizations:
      - MIRI
      date: '2025-11-19'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2508.00159
      link_text: Model-Based Soft Maximization of Suitable Metrics of Long-Term Human
        Power
      original_md: '* [**Model-Based Soft Maximization of Suitable Metrics of Long-Term
        Human Power**](https://arxiv.org/abs/2508.00159), *Jobst Heitzig, Ram Potham*,
        2025-07-31, arXiv'
      title: Model-Based Soft Maximization of Suitable Metrics of Long-Term Human
        Power
      authors:
      - Jobst Heitzig
      - Ram Potham
      author_organizations: []
      date: '2025-07-31'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://openreview.net/forum?id=mhEnJa9pNk
      link_text: 'A Safety Case for a Deployed LLM: Corrigibility as a Singular Target'
      original_md: '* [**A Safety Case for a Deployed LLM: Corrigibility as a Singular
        Target**](https://openreview.net/forum?id=mhEnJa9pNk), *Ram Potham*, 2025-06-24,
        ODYSSEY 2025 Conference'
      title: 'A Safety Case for a Deployed LLM: Corrigibility as a Singular Target'
      authors:
      - Ram Potham
      author_organizations: []
      date: '2025-06-24'
      published_year: 2025
      venue: ODYSSEY 2025 Conference
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.20203
      link_text: Shutdownable Agents through POST-Agency
      original_md: '* [**Shutdownable Agents through POST-Agency**](https://arxiv.org/abs/2505.20203),
        *Elliott Thornley*, 2025-05-26, arXiv'
      title: Shutdownable Agents through POST-Agency
      authors:
      - Elliott Thornley
      author_organizations: []
      date: '2025-05-26'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.17749
      link_text: The Partially Observable Off-Switch Game
      original_md: '* [**The Partially Observable Off-Switch Game**](https://arxiv.org/abs/2411.17749),
        *Andrew Garber, Rohan Subramani, Linus Luu et al.*, 2024-11-25, arXiv'
      title: The Partially Observable Off-Switch Game
      authors:
      - Andrew Garber
      - Rohan Subramani
      - Linus Luu
      - Mark Bedaywi
      - Stuart Russell
      - Scott Emmons
      author_organizations:
      - UC Berkeley
      date: '2024-11-25'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/9htmQx5wiePqTtZuL/deceptive-alignment-and-homuncularity
      link_text: Deceptive Alignment and Homuncularity
      original_md: '* [**Deceptive Alignment and Homuncularity**](https://lesswrong.com/posts/9htmQx5wiePqTtZuL/deceptive-alignment-and-homuncularity),
        *Oliver Sourbut, TurnTrout*, 2025-01-16, LessWrong'
      title: Deceptive Alignment and Homuncularity
      authors:
      - Oliver Sourbut
      - TurnTrout
      author_organizations:
      - UK AI Safety Institute
      - Independent
      date: '2025-01-16'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/aKncW36ZdEnzxLo8A/llm-agi-will-have-memory-and-memory-changes-alignment
      link_text: LLM AGI will have memory, and memory changes alignment
      original_md: '* [**LLM AGI will have memory, and memory changes alignment**](https://lesswrong.com/posts/aKncW36ZdEnzxLo8A/llm-agi-will-have-memory-and-memory-changes-alignment),
        *Seth Herd*, 2025-04-04, LessWrong'
      title: LLM AGI will have memory, and memory changes alignment
      authors:
      - Seth Herd
      author_organizations: []
      date: '2025-04-04'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://www.lesswrong.com/posts/PhTBDHu9PKJFmvb4p/a-shutdown-problem-proposal
      link_text: A Shutdown Problem Proposal
      original_md: '* [**A Shutdown Problem Proposal**](https://www.lesswrong.com/posts/PhTBDHu9PKJFmvb4p/a-shutdown-problem-proposal),
        *John Wentworth, David Lorrell*'
      title: A Shutdown Problem Proposal
      authors:
      - johnswentworth
      - David Lorell
      author_organizations: []
      date: '2024-01-21'
      published_year: 2024
      venue: LessWrong
      kind: lesswrong
    other_attributes: {}
  parsing_issues:
  - Broad approach 'maths/philosophy' does not match standard categories (engineering,
    behaviorist_science, cognitivist_science) - broad_approach_id left null
- id: a:corrigibility_other
  name: Other corrigibility
  header_level: 3
  parent_id: sec:corrigibility
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Rory ✅
    one_sentence_summary: Diagnose and communicate obstacles to achieving robustly
      corrigible behavior; suggest mechanisms, tests, and escalation channels for
      surfacing and mitigating incorrigible behaviors
    theory_of_change: Labs are likely to develop AGI using something analogous to
      current pipelines. Clarifying why naive instruction-following doesn't buy robust
      corrigibility + building strong tripwires/diagnostics for scheming and Goodharting
      thus reduces risks on the likely default path.
    see_also:
    - a:behavior_alignment_theory
    orthodox_problems:
    - corrigibility_anti_natural
    - instrumental_convergence
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: null
    broad_approach_text: varies
    some_names:
    - Jan Kulveit
    - Jeremy Gillen
    estimated_ftes: 1-10
    critiques: null
    funded_by: null
    outputs:
    - link_url: https://www.lesswrong.com/posts/LDYPF6yfe3f8SPHFT/ai-assistants-should-have-a-direct-line-to-their-developers
      link_text: AI Assistants Should Have a Direct Line to Their Developers
      original_md: '* [**AI Assistants Should Have a Direct Line to Their Developers**](https://www.lesswrong.com/posts/LDYPF6yfe3f8SPHFT/ai-assistants-should-have-a-direct-line-to-their-developers),
        Jan Kulveit, 2024-12-28, LessWrong'
      title: AI Assistants Should Have a Direct Line to Their Developers
      authors:
      - Jan_Kulveit
      author_organizations: []
      date: '2024-12-28'
      published_year: 2024
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - link_url: https://www.lesswrong.com/posts/D5kGGGhsnfH7G8v9f/testing-for-scheming-with-model-deletion
      link_text: Testing for Scheming with Model Deletion
      original_md: '* [**Testing for Scheming with Model Deletion**](https://www.lesswrong.com/posts/D5kGGGhsnfH7G8v9f/testing-for-scheming-with-model-deletion),
        Guive, 2025-01-07, LessWrong'
      title: Testing for Scheming with Model Deletion
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: error_detected
    - link_url: https://www.lesswrong.com/posts/ZHFZ6tivEjznkEoby/detect-goodhart-and-shut-down
      link_text: Detect Goodhart and shut down
      original_md: '* [**Detect Goodhart and shut down**](https://www.lesswrong.com/posts/ZHFZ6tivEjznkEoby/detect-goodhart-and-shut-down),
        *Jeremy Gillen*, 2025-01-22, LessWrong'
      title: Detect Goodhart and shut down
      authors:
      - Jeremy Gillen
      author_organizations: []
      date: '2025-01-22'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://www.lesswrong.com/posts/7Z4WC4AFgfmZ3fCDC/instrumental-goals-are-a-different-and-friendlier-kind-of
      link_text: Instrumental Goals Are A Different And Friendlier Kind Of Thing Than
        Terminal Goals
      original_md: '* [**Instrumental Goals Are A Different And Friendlier Kind Of
        Thing Than Terminal Goals**](https://www.lesswrong.com/posts/7Z4WC4AFgfmZ3fCDC/instrumental-goals-are-a-different-and-friendlier-kind-of),
        *John Wentworth, David Lorell*, 2025-01-24, LessWrong'
      title: Instrumental goals are a different and friendlier kind of [content unavailable]
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: error_detected
    - link_url: https://www.lesswrong.com/posts/JuRdvZyqaFbvTPemn/shutdownable-agents-through-post-agency-1
      link_text: Shutdownable Agents through POST-Agency
      original_md: '* [**Shutdownable Agents through POST-Agency**](https://www.lesswrong.com/posts/JuRdvZyqaFbvTPemn/shutdownable-agents-through-post-agency-1),
        *EJT*, 2025-09-16, LessWrong'
      title: Shutdownable Agents through POST-Agency
      authors:
      - EJT
      author_organizations: []
      date: '2025-09-16'
      published_year: 2025
      venue: LessWrong/AI Alignment Forum
      kind: lesswrong
    - link_url: https://www.lesswrong.com/posts/ksfjZJu3BFEfM6hHE/why-corrigibility-is-hard-and-important-i-e-whence-the-high
      link_text: Why Corrigibility is Hard and Important (i.e. "Whence the high MIRI
        confidence in alignment difficulty?")
      original_md: '* [**Why Corrigibility is Hard and Important (i.e. "Whence the
        high MIRI confidence in alignment difficulty?")**](https://www.lesswrong.com/posts/ksfjZJu3BFEfM6hHE/why-corrigibility-is-hard-and-important-i-e-whence-the-high),
        *Raemon, Eliezer Yudkowsky, So8res*, 2025-09-30, LessWrong'
      title: 'Error: Content Unavailable - Too Many Requests (429)'
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: error_detected
    - link_url: https://lesswrong.com/posts/CSFa9rvGNGAfCzBk6/problems-with-instruction-following-as-an-alignment-target
      link_text: Problems with instruction-following as an alignment target
      original_md: '* [**Problems with instruction-following as an alignment target**](https://lesswrong.com/posts/CSFa9rvGNGAfCzBk6/problems-with-instruction-following-as-an-alignment-target),
        *Seth Herd*, 2025-05-15, LessWrong'
      title: Problems with instruction-following as an alignment target
      authors:
      - Seth Herd
      author_organizations: []
      date: '2025-05-15'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://dl.acm.org/doi/10.1145/3717823.3718245
      link_text: 'Oblivious Defense in ML Models: Backdoor Removal without Detection'
      original_md: '* [Oblivious Defense in ML Models: Backdoor Removal without Detection](https://dl.acm.org/doi/10.1145/3717823.3718245)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://arxiv.org/abs/2509.20714
      link_text: 'Cryptographic Backdoor for Neural Networks: Boon and Bane'
      original_md: '* [Cryptographic Backdoor for Neural Networks: Boon and Bane](https://arxiv.org/abs/2509.20714)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://arxiv.org/abs/2504.20310
      link_text: A Cryptographic Perspective on Mitigation vs. Detection in Machine
        Learning
      original_md: '* [A Cryptographic Perspective on Mitigation vs. Detection in
        Machine Learning](https://arxiv.org/abs/2504.20310)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    other_attributes: {}
  parsing_issues:
  - Broad approach field says 'varies; math/philosophy + engineering' which doesn't
    match standard values (engineering, behaviorist science, cognitivist science).
    Set broad_approach_text to 'varies' and broad_approach_id to null.
- id: sec:ontology_identification
  name: Ontology Identification
  header_level: 2
  parent_id: sec:theory
  content: ''
  item_type: section
  agenda_attributes: null
  parsing_issues: []
- id: a:natural_abstractions
  name: Natural abstractions
  header_level: 3
  parent_id: sec:ontology_identification
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stag
    one_sentence_summary: Develop a theory of concepts that explains how they are
      learned, how they structure a particular system's understanding, and how mutual
      translatability can be achieved between different collections of concepts.
    theory_of_change: Understand the concepts a system's understanding is structured
      with and use them to inspect its "alignment/safety properties" and/or "retarget
      its search", i.e. identify utility-function-like components inside an AI and
      replacing calls to them with calls to "user values" (represented using existing
      abstractions inside the AI).
    see_also:
    - a:interp_causal_abstractions
    - representational alignment
    - convergent abstractions
    - feature universality
    - Platonic representation hypothesis
    - microscope AI
    orthodox_problems:
    - instrumental_convergence
    - superintelligence_fool_supervisors
    - humans_not_first_class
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - John Wentworth
    - Paul Colognese
    - David Lorrell
    - Sam Eisenstat
    estimated_ftes: 1-10
    critiques: '[Chan et al (2023)](https://www.lesswrong.com/posts/gvzW46Z3BsaZsLc25/natural-abstractions-key-claims-theorems-and-critiques-1#3__A_formalization_of_abstractions_would_accelerate_alignment_research),
      [Soto](https://www.lesswrong.com/posts/CJjT8GMitsnKc2wgG/natural-abstractions-are-observer-dependent-a-conversation-1),
      [Harwood](https://www.lesswrong.com/posts/F4nzox6oh5oAdX9D3/abstractions-are-not-natural),
      [Soares (2023)](https://www.lesswrong.com/posts/mgjHS6ou7DgwhKPpu/a-rough-and-incomplete-review-of-some-of-john-wentworth-s)'
    funded_by: null
    outputs:
    - link_url: https://arxiv.org/abs/2509.03780
      link_text: 'Natural Latents: Latent Variables Stable Across Ontologies'
      original_md: '* [**Natural Latents: Latent Variables Stable Across Ontologies**](https://arxiv.org/abs/2509.03780),
        *John Wentworth, David Lorell*, 2025-09-04, LessWrong'
      title: 'Natural Latents: Latent Variables Stable Across Ontologies'
      authors:
      - John Wentworth
      - David Lorell
      author_organizations: []
      date: '2025-09-04'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.lesswrong.com/posts/T6xSXiXF3WF6TmCyN/abstract-mathematical-concepts-vs-abstractions-over-real
      link_text: Abstract Mathematical Concepts vs. Abstractions Over Real-World Systems
      original_md: '* [**Abstract Mathematical Concepts vs. Abstractions Over Real-World
        Systems**](https://www.lesswrong.com/posts/T6xSXiXF3WF6TmCyN/abstract-mathematical-concepts-vs-abstractions-over-real),
        *Thane Ruthenis*, 2025-02-18, LessWrong'
      title: Abstract mathematical concepts vs abstractions over real
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: error_detected
    - link_url: https://openreview.net/forum?id=HwKFJ3odui
      link_text: 'Condensation: a theory of concepts'
      original_md: '* [**Condensation: a theory of concepts**](https://openreview.net/forum?id=HwKFJ3odui),
        *Sam Eisenstat*, 2025-07-04, ODYSSEY 2025 Conference'
      title: 'Condensation: a theory of concepts'
      authors:
      - Sam Eisenstat
      author_organizations: []
      date: '2025-07-04'
      published_year: 2025
      venue: ODYSSEY 2025 Conference
      kind: paper_preprint
    - link_url: https://www.lesswrong.com/posts/BstHXPgQyfeNnLjjp/condensation
      link_text: Condensation
      original_md: '* [Condensation](https://www.lesswrong.com/posts/BstHXPgQyfeNnLjjp/condensation),
        *Abram Demski*'
      title: Condensation
      authors:
      - abramdemski
      author_organizations: []
      date: '2025-11-09'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2310.13018
      link_text: Getting aligned on representational alignment
      original_md: '* [**Getting aligned on representational alignment**](https://arxiv.org/abs/2310.13018)'
      title: Getting aligned on representational alignment
      authors:
      - Ilia Sucholutsky
      - Lukas Muttenthaler
      - Adrian Weller
      - Andi Peng
      - Andreea Bobu
      - Been Kim
      - Bradley C. Love
      - Christopher J. Cueva
      - Erin Grant
      - Iris Groen
      - Jascha Achterberg
      - Joshua B. Tenenbaum
      - Katherine M. Collins
      - Katherine L. Hermann
      - Kerem Oktar
      - Klaus Greff
      - Martin N. Hebart
      - Nathan Cloos
      - Nikolaus Kriegeskorte
      - Nori Jacoby
      - Qiuyi Zhang
      - Raja Marjieh
      - Robert Geirhos
      - Sherol Chen
      - Simon Kornblith
      - Sunayana Rane
      - Talia Konkle
      - Thomas P. O'Connell
      - Thomas Unterthiner
      - Andrew K. Lampinen
      - Klaus-Robert Müller
      - Mariya Toneva
      - Thomas L. Griffiths
      author_organizations:
      - University of Cambridge
      - Google DeepMind
      - MIT
      - University College London
      - Princeton University
      date: '2023-10-18'
      published_year: 2023
      venue: arXiv
      kind: paper_preprint
    - link_url: https://phillipi.github.io/prh/
      link_text: Platonic representation hypothesis
      original_md: '* [**Platonic representation hypothesis**](https://phillipi.github.io/prh/)'
      title: The Platonic Representation Hypothesis
      authors:
      - Minyoung Huh
      - Brian Cheung
      - Tongzhou Wang
      - Phillip Isola
      author_organizations:
      - MIT
      date: '2024-05-13'
      published_year: 2024
      venue: ICML 2024
      kind: paper_published
    - link_url: https://www.youtube.com/watch?v=Nr9eMobqUOo&t=3s
      link_text: Rosas
      original_md: '* [**Rosas**](https://www.youtube.com/watch?v=Nr9eMobqUOo&t=3s)'
      title: 'Fernando Rosas: Identifying Abstractions (HAAISS 2025)'
      authors:
      - Fernando Rosas
      author_organizations: []
      date: '2025-10-06'
      published_year: 2025
      venue: HAAISS 2025
      kind: error_detected
    - link_url: https://arxiv.org/abs/2412.02579
      link_text: 'Factored space models: Towards causality between levels of abstraction'
      original_md: '* [**Factored space models: Towards causality between levels of
        abstraction**](https://arxiv.org/abs/2412.02579)'
      title: 'Factored space models: Towards causality between levels of abstraction'
      authors:
      - Scott Garrabrant
      - Matthias Georg Mayer
      - Magdalena Wache
      - Leon Lang
      - Sam Eisenstat
      - Holger Dell
      author_organizations: []
      date: '2024-12-03'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/h89L5FMAkEBNsZ3xM/a-single-principle-related-to-many-alignment-subproblems-2
      link_text: A single principle related to many Alignment subproblems?
      original_md: '* [**A single principle related to many Alignment subproblems?**](https://lesswrong.com/posts/h89L5FMAkEBNsZ3xM/a-single-principle-related-to-many-alignment-subproblems-2)'
      title: A single principle related to many Alignment subproblems?
      authors:
      - Q Home
      author_organizations: []
      date: '2025-04-30'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    other_attributes: {}
  parsing_issues:
  - Broad approach field says 'cognitive' - mapped to 'cognitivist_science' (cognitivist
    science)
- id: a:learning_theoretic_agenda
  name: The Learning-Theoretic Agenda
  header_level: 2
  parent_id: sec:theory
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stag ✅
    one_sentence_summary: Try to formalise a more realistic agent, understand what
      it means for it to be aligned with us, translate between its ontology and ours,
      and produce desiderata for a training setup that points at coherent AGIs similar
      to our model of an aligned agent
    theory_of_change: Fix formal epistemology to work out how to avoid deep training
      problems
    see_also: []
    orthodox_problems:
    - value_fragile
    - goals_misgeneralize
    - humans_not_first_class
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - Vanessa Kosoy
    - Diffractor
    - Gergely Szücs
    estimated_ftes: '3'
    critiques: '[Matolcsi](https://www.lesswrong.com/posts/StkjjQyKwg7hZjcGB/a-mostly-critical-review-of-infra-bayesianism)'
    funded_by: Survival and Flourishing Fund, ARIA, UK AISI, Coefficient Giving
    outputs:
    - link_url: https://www.lesswrong.com/posts/LgLez8aeK24PbyyQJ/new-paper-infra-bayesian-decision-estimation-theory
      link_text: Infra-Bayesian Decision-Estimation Theory
      original_md: '* [**Infra-Bayesian Decision-Estimation Theory**](https://www.lesswrong.com/posts/LgLez8aeK24PbyyQJ/new-paper-infra-bayesian-decision-estimation-theory),
        *Vanessa Kosoy, Diffractor*, 2025-04-10, LessWrong'
      title: 'New paper: Infra-Bayesian Decision Estimation Theory'
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: error_detected
    - link_url: https://www.lesswrong.com/w/infra-bayesianism?sortedBy=new
      link_text: Infra-Bayesianism category on LessWrong
      original_md: '* [**Infra-Bayesianism category on LessWrong**](https://www.lesswrong.com/w/infra-bayesianism?sortedBy=new)'
      title: Infra-Bayesianism
      authors:
      - abramdemski
      - Ruby
      author_organizations: []
      date: '2022-03-24'
      published_year: 2022
      venue: LessWrong
      kind: lesswrong
    - link_url: https://www.lesswrong.com/posts/Y9NuKpb6dsyiYFxWK/new-paper-ambiguous-online-learning
      link_text: Ambiguous Online Learning
      original_md: '* [**Ambiguous Online Learning**](https://www.lesswrong.com/posts/Y9NuKpb6dsyiYFxWK/new-paper-ambiguous-online-learning),
        *Vanessa Kosoy*, 2025-06-25, LessWrong'
      title: 'New Paper: Ambiguous Online Learning'
      authors:
      - Vanessa Kosoy
      author_organizations:
      - Independent
      date: '2025-06-25'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://proceedings.mlr.press/v291/appel25a.html
      link_text: Regret Bounds for Robust Online Decision Making
      original_md: '* [**Regret Bounds for Robust Online Decision Making**](https://proceedings.mlr.press/v291/appel25a.html),
        *Alexander Appel, Vanessa Kosoy*, 2025-04-09, arXiv'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://www.lesswrong.com/s/n7qFxakSnxGuvmYAX
      link_text: 'What is Inadequate about Bayesianism for AI Alignment: Motivating
        Infra-Bayesianism'
      original_md: '* [**What is Inadequate about Bayesianism for AI Alignment: Motivating
        Infra-Bayesianism**](https://www.lesswrong.com/s/n7qFxakSnxGuvmYAX), *Brittany
        Gelb*, 2025-05-01, LessWrong'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://www.alignmentforum.org/posts/DobZ62XMdiPigii9H/non-monotonic-infra-bayesian-physicalism%20
      link_text: Non-Monotonic Infra-Bayesian Physicalism
      original_md: '* [Non-Monotonic Infra-Bayesian Physicalism](https://www.alignmentforum.org/posts/DobZ62XMdiPigii9H/non-monotonic-infra-bayesian-physicalism%20)'
      title: Non-Monotonic Infra-Bayesian Physicalism
      authors:
      - Marcus Ogren
      author_organizations: []
      date: '2025-04-02'
      published_year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    other_attributes: {}
  parsing_issues: []
- id: sec:multi_agent_first
  name: Multi-agent first
  header_level: 1
  parent_id: null
  content: ''
  item_type: section
  agenda_attributes: null
  parsing_issues: []
- id: a:alignment_to_context
  name: Aligning to context
  header_level: 2
  parent_id: sec:multi_agent_first
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Peli
    one_sentence_summary: Align AI directly to the role of participant, collaborator,
      or advisor for our best real human practices and institutions, instead of aligning
      AI to separately representable goals, rules, or utility functions.
    theory_of_change: '"Many classical problems in AGI alignment are downstream of
      a type error about human values." Operationalizing a correct view of human values
      - one that treats human values as impossible or impractical to abstract from
      concrete practices - will unblock value fragility, goal-misgeneralization, instrumental
      convergence, and pivotal-act specification.'
    see_also:
    - a:aligning_what
    - a:aligned_to_who
    orthodox_problems:
    - value_fragile
    - corrigibility_anti_natural
    - goals_misgeneralize
    - instrumental_convergence
    - fair_sane_pivotal
    target_case_id: null
    target_case_text: mixed
    broad_approach_id: behaviorist_science
    broad_approach_text: behavioral
    some_names:
    - Full Stack Alignment
    - Meaning Alignment Institute
    - Plurality Institute
    - Tan Zhi-Xuan
    - Matija Franklin
    - Ryan Lowe
    - Joe Edelman
    - Oliver Klingefjord
    estimated_ftes: '5'
    critiques: null
    funded_by: ARIA, OpenAI, Survival and Flourishing Fund
    outputs:
    - link_url: https://arxiv.org/abs/2408.16984
      link_text: Beyond Preferences in AI Alignment
      original_md: '[Beyond Preferences in AI Alignment](https://arxiv.org/abs/2408.16984)'
      title: Beyond Preferences in AI Alignment
      authors:
      - Tan Zhi-Xuan
      - Micah Carroll
      - Matija Franklin
      - Hal Ashton
      author_organizations: []
      date: '2024-08-30'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2404.10636
      link_text: 2404.10636 - What are human values, and how do we align AI to them?
      original_md: '[2404.10636 \- What are human values, and how do we align AI to
        them?](https://arxiv.org/abs/2404.10636)'
      title: What are human values, and how do we align AI to them?
      authors:
      - Oliver Klingefjord
      - Ryan Lowe
      - Joe Edelman
      author_organizations: []
      date: '2024-04-17'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.00940
      link_text: 2503.00940 - Can AI Model the Complexities of Human Moral Decision-Making?
        A Qualitative Study of Kidney Allocation Decisions
      original_md: '[2503.00940 \- Can AI Model the Complexities of Human Moral Decision-Making?
        A Qualitative Study of Kidney Allocation Decisions](https://arxiv.org/abs/2503.00940)'
      title: Can AI Model the Complexities of Human Moral Decision-Making? A Qualitative
        Study of Kidney Allocation Decisions
      authors:
      - Vijay Keswani
      - Vincent Conitzer
      - Walter Sinnott-Armstrong
      - Breanna K. Nguyen
      - Hoda Heidari
      - Jana Schaich Borg
      author_organizations:
      - Carnegie Mellon University
      - Duke University
      - University of Oxford
      date: '2025-03-02'
      published_year: 2025
      venue: ACM CHI 2025
      kind: paper_published
    - link_url: https://www.softmax.com/blog/the-frame-dependent-mind
      link_text: The Frame-Dependent Mind
      original_md: '[The Frame-Dependent Mind](https://www.softmax.com/blog/the-frame-dependent-mind)'
      title: 'The Frame-Dependent Mind: On Reality''s Stubborn Refusal To Be One Thing'
      authors:
      - Emmett Shear
      - Sonnet 3.7
      author_organizations:
      - Softmax
      date: '2025-04-18'
      published_year: 2025
      venue: Softmax Blog
      kind: blog_post
    - link_url: https://meaningalignment.substack.com/p/model-integrity
      link_text: Model Integrity
      original_md: '[Model Integrity](https://meaningalignment.substack.com/p/model-integrity)'
      title: Model Integrity
      authors:
      - Joe Edelman
      - Oliver Klingefjord
      author_organizations:
      - Meaning Alignment Institute
      date: '2024-12-05'
      published_year: 2024
      venue: Substack
      kind: blog_post
    - link_url: https://docs.google.com/document/d/1cKbqYSGspfJavXvnhsp3mAuxHh08rNbP7tzYieqLiXw/edit?tab=t.0%20
      link_text: On Eudaimonia and Optimization
      original_md: '[On Eudaimonia and Optimization](https://docs.google.com/document/d/1cKbqYSGspfJavXvnhsp3mAuxHh08rNbP7tzYieqLiXw/edit?tab=t.0%20)'
      title: On Eudaimonia and Optimization
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: Google Docs
      kind: error_detected
    - link_url: https://www.full-stack-alignment.ai
      link_text: Full-Stack Alignment
      original_md: '[Full-Stack Alignment](https://www.full-stack-alignment.ai)'
      title: 'Full-Stack Alignment: Co-Aligning AI and Institutions with Thick Models
        of Value'
      authors: []
      author_organizations:
      - Meaning Alignment Institute
      date: null
      published_year: null
      venue: null
      kind: personal_page
    - link_url: https://arxiv.org/abs/2412.19010
      link_text: https://arxiv.org/abs/2412.19010
      original_md: https://arxiv.org/abs/2412.19010
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    other_attributes: {}
  parsing_issues:
  - Target case is 'Mixed' - setting target_case_id to null and target_case_text to
    'mixed'
  - Broad approach is 'Behavioural' (UK spelling) - mapped to 'behaviorist_science'
- id: a:contractualist_alignment
  name: Aligning to the social contract
  header_level: 2
  parent_id: sec:multi_agent_first
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Peli
    one_sentence_summary: Generate AIs' operational values from 'social contract'-style
      ideal civic deliberation formalisms and their consequent rulesets for civic
      actors
    theory_of_change: Formalize and apply the liberal tradition's project of defining
      civic principles separable from the substantive good, aligning our AIs to civic
      principles that bypass fragile utility-learning and intractable utility-calculation
    see_also:
    - a:alignment_to_context
    - a:aligning_what
    orthodox_problems:
    - value_fragile
    - goals_misgeneralize
    - instrumental_convergence
    - humanlike_minds_not_safe
    - fair_sane_pivotal
    target_case_id: null
    target_case_text: mixed
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - Gillian Hadfield
    - Tan Zhi-Xuan
    - Sydney Levine
    - Matija Franklin
    - Joshua B. Tenenbaum
    estimated_ftes: 5 - 10
    critiques: null
    funded_by: Deepmind, Macroscopic Ventures
    outputs:
    - link_url: https://law-ai.org/law-following-ai/%20
      link_text: 'Law-Following AI: designing AI agents to obey human laws'
      original_md: '[Law-Following AI: designing AI agents to obey human laws](https://law-ai.org/law-following-ai/%20)
        Cullen O''Keefe, Ketan Ramakrishnan, Janna Tay, Christoph Winter'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://arxiv.org/abs/2506.17434
      link_text: 2506.17434 - Resource Rational Contractualism Should Guide AI Alignment
      original_md: '[2506.17434 \- Resource Rational Contractualism Should Guide AI
        Alignment](https://arxiv.org/abs/2506.17434)'
      title: Resource Rational Contractualism Should Guide AI Alignment
      authors:
      - Sydney Levine
      - Matija Franklin
      - Tan Zhi-Xuan
      - Secil Yanik Guyot
      - Lionel Wong
      - Daniel Kilov
      - Yejin Choi
      - Joshua B. Tenenbaum
      - Noah Goodman
      - Seth Lazar
      - Iason Gabriel
      author_organizations:
      - MIT
      - Stanford
      - University of Washington
      - DeepMind
      - ANU
      date: '2025-06-20'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.07955
      link_text: 2509.07955 - ACE and Diverse Generalization via Selective Disagreement
      original_md: '[2509.07955 \- ACE and Diverse Generalization via Selective Disagreement](https://arxiv.org/abs/2509.07955)'
      title: ACE and Diverse Generalization via Selective Disagreement
      authors:
      - Oliver Daniels
      - Stuart Armstrong
      - Alexandre Maranhão
      - Mahirah Fairuz Rahman
      - Benjamin M. Marlin
      - Rebecca Gorman
      author_organizations:
      - Unknown - not specified in abstract
      date: '2025-09-09'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.00783
      link_text: 'Promises Made, Promises Kept: Safe Pareto Improvements via Ex Post
        Verifiable Commitments'
      original_md: '[Promises Made, Promises Kept: Safe Pareto Improvements via Ex
        Post Verifiable Commitments](https://arxiv.org/abs/2505.00783)'
      title: 'Promises Made, Promises Kept: Safe Pareto Improvements via Ex Post Verifiable
        Commitments'
      authors:
      - Nathaniel Sauerberg
      - Caspar Oesterheld
      author_organizations: []
      date: '2025-05-01'
      published_year: 2025
      venue: GAIW'25
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.26396
      link_text: A Pragmatic View of AI Personhood
      original_md: '[A Pragmatic View of AI Personhood](https://arxiv.org/abs/2510.26396)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://arxiv.org/abs/2509.01186
      link_text: Statutory Construction and Interpretation for Artificial Intelligence
      original_md: '[Statutory Construction and Interpretation for Artificial Intelligence](https://arxiv.org/abs/2509.01186)'
      title: Statutory Construction and Interpretation for Artificial Intelligence
      authors:
      - Luxi He
      - Nimra Nadeem
      - Michel Liao
      - Howard Chen
      - Danqi Chen
      - Mariano-Florentino Cuéllar
      - Peter Henderson
      author_organizations:
      - Princeton University
      - Stanford University
      date: '2025-09-01'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2408.16984
      link_text: 2408.16984 - Beyond Preferences in AI Alignment
      original_md: '[2408.16984 \- Beyond Preferences in AI Alignment](https://arxiv.org/abs/2408.16984)'
      title: Beyond Preferences in AI Alignment
      authors:
      - Tan Zhi-Xuan
      - Micah Carroll
      - Matija Franklin
      - Hal Ashton
      author_organizations: []
      date: '2024-08-30'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.00069
      link_text: Societal alignment frameworks can improve llm alignment
      original_md: '[Societal alignment frameworks can improve llm alignment](https://arxiv.org/abs/2503.00069)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    other_attributes: {}
  parsing_issues:
  - Target case field says 'Mixed' - does not map to standard cases (average_case,
    pessimistic_case, worst_case), transformed to 'mixed'
- id: a:aligning_multiple_game_theory
  name: Theory for aligning multiple AIs
  header_level: 2
  parent_id: sec:multi_agent_first
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Peli
    one_sentence_summary: Use realistic game-theory variants (e.g. evolutionary game
      theory, computational game theory) to describe/predict the collective and individual
      behaviours of AI agents in multi-agent scenarios.
    theory_of_change: While traditional AGI safety focuses on idealized decision-theory
      and individual agents, it's plausible that strategic AI agents will first emerge
      (or are emerging now) in a complex, multi-AI strategic landscape. We need granular,
      realistic formal models of AIs' strategic interactions and collective dynamics
      to understand this future.
    see_also:
    - a:aligning_multiple_tools
    - a:aligning_what
    orthodox_problems:
    - goals_misgeneralize
    - superintelligence_fool_supervisors
    - superintelligence_hack_software
    target_case_id: null
    target_case_text: mixed
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - Lewis Hammond
    - Emery Cooper
    - Allan Chan
    - Caspar Oesterheld
    - Vincent Conitzer
    - Vojta Kovarik
    estimated_ftes: '10'
    critiques: null
    funded_by: Deepmind, Macroscopic Ventures
    outputs:
    - link_url: https://arxiv.org/abs/2502.14143
      link_text: 2502.14143 - Multi-Agent Risks from Advanced AI
      original_md: '[2502.14143 \- Multi-Agent Risks from Advanced AI](https://arxiv.org/abs/2502.14143)'
      title: Multi-Agent Risks from Advanced AI
      authors:
      - Lewis Hammond
      - Alan Chan
      - Jesse Clifton
      - Jason Hoelscher-Obermaier
      - Akbir Khan
      - Euan McLean
      - Chandler Smith
      - Wolfram Barfuss
      - Jakob Foerster
      - Tomáš Gavenčiak
      - The Anh Han
      - Edward Hughes
      - Vojtěch Kovařík
      - Jan Kulveit
      - Joel Z. Leibo
      - Caspar Oesterheld
      - Christian Schroeder de Witt
      - Nisarg Shah
      - Michael Wellman
      - Paolo Bova
      - Theodor Cimpeanu
      - Carson Ezell
      - Quentin Feuillade-Montixi
      - Matija Franklin
      - Esben Kran
      - Igor Krawczuk
      - Max Lamparth
      - Niklas Lauffer
      - Alexander Meinke
      - Sumeet Motwani
      - Anka Reuel
      - Vincent Conitzer
      - Michael Dennis
      - Iason Gabriel
      - Adam Gleave
      - Gillian Hadfield
      - Nika Haghtalab
      - Atoosa Kasirzadeh
      - Sébastien Krier
      - Kate Larson
      - Joel Lehman
      - David C. Parkes
      - Georgios Piliouras
      - Iyad Rahwan
      author_organizations:
      - Cooperative AI Foundation
      date: '2025-02-19'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.06323
      link_text: 2503.06323 - Higher-Order Belief in Incomplete Information MAIDs
      original_md: '[2503.06323 \- Higher-Order Belief in Incomplete Information MAIDs](https://arxiv.org/abs/2503.06323)'
      title: Higher-Order Belief in Incomplete Information MAIDs
      authors:
      - Jack Foxabbott
      - Rohan Subramani
      - Francis Rhys Ward
      author_organizations: []
      date: '2025-03-08'
      published_year: 2025
      venue: 24th International Conference on Autonomous Agents and Multiagent Systems
        2025
      kind: paper_published
    - link_url: https://arxiv.org/abs/2508.14927
      link_text: AI Testing Should Account for Sophisticated Strategic Behaviour
      original_md: '[AI Testing Should Account for Sophisticated Strategic Behaviour](https://arxiv.org/abs/2508.14927)'
      title: AI Testing Should Account for Sophisticated Strategic Behaviour
      authors:
      - Vojtech Kovarik
      - Eric Olav Chen
      - Sami Petersen
      - Alexis Ghersengorin
      - Vincent Conitzer
      author_organizations: []
      date: '2025-08-19'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2412.14570
      link_text: Characterising Simulation-Based Program Equilibria
      original_md: '[Characterising Simulation-Based Program Equilibria](https://arxiv.org/abs/2412.14570)'
      title: Characterising Simulation-Based Program Equilibria
      authors:
      - Emery Cooper
      - Caspar Oesterheld
      - Vincent Conitzer
      author_organizations:
      - Carnegie Mellon University
      date: '2024-12-19'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.02618
      link_text: 'Strategic Intelligence in Large Language Models: Evidence from evolutionary
        Game Theory'
      original_md: '[Strategic Intelligence in Large Language Models: Evidence from
        evolutionary Game Theory](https://arxiv.org/abs/2507.02618)'
      title: 'Strategic Intelligence in Large Language Models: Evidence from evolutionary
        Game Theory'
      authors:
      - Kenneth Payne
      - Baptiste Alloui-Cros
      author_organizations: []
      date: '2025-07-03'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.05748
      link_text: Communication Enables Cooperation in LLM Agents
      original_md: '[Communication Enables Cooperation in LLM Agents](https://arxiv.org/abs/2510.05748)'
      title: 'Communication Enables Cooperation in LLM Agents: A Comparison with Curriculum-Based
        Approaches'
      authors:
      - Hachem Madmoun
      - Salem Lahlou
      author_organizations: []
      date: '2025-10-07'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.science.org/doi/10.1126/sciadv.adu9368
      link_text: Emergent social conventions and collective bias in LLM populations
      original_md: '[Emergent social conventions and collective bias in LLM populations](https://www.science.org/doi/10.1126/sciadv.adu9368)'
      title: Emergent social conventions and collective bias in LLM populations
      authors:
      - Ariel Flint Ashery
      - Luca Maria Aiello
      - Andrea Baronchelli
      author_organizations:
      - City St George's, University of London
      - IT University of Copenhagen
      - Pioneer Centre for AI
      - The Alan Turing Institute
      date: '2025-05-14'
      published_year: 2025
      venue: Science Advances
      kind: paper_published
    - link_url: https://arxiv.org/abs/2509.01063
      link_text: An Economy of AI Agents
      original_md: '[An Economy of AI Agents](https://arxiv.org/abs/2509.01063) \-
        Gillian K. Hadfield, Andrew Koh'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://arxiv.org/abs/2510.06105
      link_text: 'Moloch''s Bargain: Emergent Misalignment When LLMs Compete for Audiences'
      original_md: '[Moloch''s Bargain: Emergent Misalignment When LLMs Compete for
        Audiences](https://arxiv.org/abs/2510.06105)'
      title: 'Moloch''s Bargain: Emergent Misalignment When LLMs Compete for Audiences'
      authors:
      - Batu El
      - James Zou
      author_organizations:
      - Stanford University
      date: '2025-10-07'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues:
  - Target case field says 'Mixed' - set target_case_id to null and target_case_text
    to 'mixed'
  - Broad approach field says 'Cognitive' - mapped to 'cognitivist_science' (cognitivist
    science)
- id: a:aligning_multiple_tools
  name: Tools for aligning multiple AIs
  header_level: 2
  parent_id: sec:multi_agent_first
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Peli
    one_sentence_summary: Develop tools and technique for designing and testing multi-agent
      AI scenarios, for auditing real-world multi-agent AI dynamics, and for aligning
      AIs in multi-AI settings.
    theory_of_change: Addressing multi-agent AI dynamics is key for aligning near-future
      agents and their impact on the world. Feedback loops from multi-agent dynamics
      can radically change the future AI landscape, and require a different toolset
      from model psychology to audit and control.
    see_also:
    - a:aligning_multiple_game_theory
    - a:aligning_what
    orthodox_problems:
    - goals_misgeneralize
    - superintelligence_fool_supervisors
    - superintelligence_hack_software
    target_case_id: null
    target_case_text: mixed
    broad_approach_id: null
    broad_approach_text: engineering / behavioral
    some_names:
    - Lewis Hammond
    - Emery Cooper
    - Allan Chan
    - Caspar Oesterheld
    - Vincent Conitzer
    - Gillian Hadfield
    estimated_ftes: 10 - 15
    critiques: null
    funded_by: Coefficient Giving, Deepmind
    outputs:
    - link_url: https://arxiv.org/abs/2501.10114
      link_text: Infrastructure for AI Agents
      original_md: '[Infrastructure for AI Agents](https://arxiv.org/abs/2501.10114)'
      title: Infrastructure for AI Agents
      authors:
      - Alan Chan
      - Kevin Wei
      - Sihao Huang
      - Nitarshan Rajkumar
      - Elija Perrier
      - Seth Lazar
      - Gillian K. Hadfield
      - Markus Anderljung
      author_organizations: []
      date: '2025-01-17'
      published_year: 2025
      venue: arXiv (accepted to TMLR)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.01295
      link_text: 'The Social Laboratory: A Psychometric Framework for Multi-Agent
        LLM Evaluation'
      original_md: '[The Social Laboratory: A Psychometric Framework for Multi-Agent
        LLM Evaluation](https://arxiv.org/abs/2510.01295)'
      title: 'The Social Laboratory: A Psychometric Framework for Multi-Agent LLM
        Evaluation'
      authors:
      - Zarreen Reza
      author_organizations: []
      date: '2025-10-01'
      published_year: 2025
      venue: NeurIPS 2025 Workshop on Evaluating the Evolving LLM Lifecycle
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.10588
      link_text: A dataset of questions on decision-theoretic reasoning in Newcomb-like
        problems
      original_md: '[A dataset of questions on decision-theoretic reasoning in Newcomb-like
        problems](https://arxiv.org/abs/2411.10588)'
      title: A dataset of questions on decision-theoretic reasoning in Newcomb-like
        problems
      authors:
      - Caspar Oesterheld
      - Emery Cooper
      - Miles Kodama
      - Linh Chi Nguyen
      - Ethan Perez
      author_organizations: []
      date: '2024-11-15'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://softmax.com/blog/reimagining-alignment
      link_text: Reimagining Alignment
      original_md: '[Reimagining Alignment](https://softmax.com/blog/reimagining-alignment)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://github.com/lechmazur/pgg_bench
      link_text: 'PGG-Bench: Contribute & Punish'
      original_md: '[PGG-Bench: Contribute & Punish](https://github.com/lechmazur/pgg_bench)'
      title: 'PGG-Bench: Contribute & Punish'
      authors: []
      author_organizations: []
      date: '2025-04-10'
      published_year: 2025
      venue: GitHub
      kind: code_tool
    - link_url: https://arxiv.org/abs/2509.14485
      link_text: 'Beyond the high score: Prosocial ability profiles of multi-agent
        populations'
      original_md: '"[Beyond the high score: Prosocial ability profiles of multi-agent
        populations](https://arxiv.org/abs/2509.14485)"'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://arxiv.org/abs/2509.23102
      link_text: Multiplayer Nash Preference Optimization
      original_md: '[Multiplayer Nash Preference Optimization](https://arxiv.org/abs/2509.23102)'
      title: Multiplayer Nash Preference Optimization
      authors:
      - Fang Wu
      - Xu Huang
      - Weihao Xuan
      - Zhiwei Zhang
      - Yijia Xiao
      - Guancheng Wan
      - Xiaomin Li
      - Bing Hu
      - Peng Xia
      - Jure Leskovec
      - Yejin Choi
      author_organizations:
      - Stanford University
      - University of Washington
      - AI2
      date: '2025-09-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.12203
      link_text: 2502.12203 - An Interpretable Automated Mechanism Design Framework
        with Large Language Models
      original_md: '[2502.12203 \- An Interpretable Automated Mechanism Design Framework
        with Large Language Models](https://arxiv.org/abs/2502.12203)'
      title: An Interpretable Automated Mechanism Design Framework with Large Language
        Models
      authors:
      - Jiayuan Liu
      - Mingyu Guo
      - Vincent Conitzer
      author_organizations: []
      date: '2025-02-16'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.00757
      link_text: 'AgentBreeder: Mitigating the AI Safety Risks of Multi-Agent Scaffolds
        via Self-Improvement'
      original_md: '[AgentBreeder: Mitigating the AI Safety Risks of Multi-Agent Scaffolds
        via Self-Improvement](https://arxiv.org/abs/2502.00757)'
      title: 'AgentBreeder: Mitigating the AI Safety Risks of Multi-Agent Scaffolds
        via Self-Improvement'
      authors:
      - J Rosser
      - Jakob Foerster
      author_organizations: []
      date: '2025-02-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.14660
      link_text: 'When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion
        in Social Systems'
      original_md: '[When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent
        Collusion in Social Systems](https://arxiv.org/abs/2507.14660)'
      title: 'When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion
        in Social Systems'
      authors:
      - Qibing Ren
      - Sitao Xie
      - Longxuan Wei
      - Zhenfei Yin
      - Junchi Yan
      - Lizhuang Ma
      - Jing Shao
      author_organizations: []
      date: '2025-07-19'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: http://arxiv.org/abs/2509.10147
      link_text: Virtual Agent Economies
      original_md: '[Virtual Agent Economies](http://arxiv.org/abs/2509.10147)'
      title: Virtual Agent Economies
      authors:
      - Nenad Tomasev
      - Matija Franklin
      - Joel Z. Leibo
      - Julian Jacobs
      - William A. Cunningham
      - Iason Gabriel
      - Simon Osindero
      author_organizations:
      - DeepMind
      date: '2025-09-12'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues:
  - Target case is 'Mixed' - leaving target_case_id as null and setting target_case_text
    to 'mixed'
  - Broad approach is 'Engineering/Behavioural' - leaving broad_approach_id as null
    and setting broad_approach_text to 'engineering / behavioral'
- id: a:aligned_to_who
  name: Aligned to who?
  header_level: 2
  parent_id: sec:multi_agent_first
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Peli
    one_sentence_summary: Develop ethical frameworks and technical protocols for taking
      the plurality of human values, cultures, and communities seriously when aligning
      AI to "humanity"
    theory_of_change: Principles of democracy, of pluralism, and of context-sensitivity
      should guide AI development, alignment, and deployment from the start, continuously
      shaping AI's social and technical feedback loop on the road to AGI
    see_also:
    - a:aligning_what
    - a:alignment_to_context
    orthodox_problems:
    - value_fragile
    - fair_sane_pivotal
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: behaviorist_science
    broad_approach_text: behaviorist science
    some_names:
    - Joel Z. Leibo
    - Divya Siddarth
    - Séb Krier
    - Luke Thorburn
    - Seth Lazar
    - AI Objectives Institute
    - The Collective Intelligence Project
    estimated_ftes: 5 - 15
    critiques: null
    funded_by: Future of Life Institute, Survival and Flourishing Fund, Deepmind
    outputs:
    - link_url: https://arxiv.org/abs/2507.09650
      link_text: '2507.09650 - Cultivating Pluralism In Algorithmic Monoculture: The
        Community Alignment Dataset'
      original_md: '[2507.09650 \- Cultivating Pluralism In Algorithmic Monoculture:
        The Community Alignment Dataset](https://arxiv.org/abs/2507.09650)'
      title: 'Cultivating Pluralism In Algorithmic Monoculture: The Community Alignment
        Dataset'
      authors:
      - Lily Hong Zhang
      - Smitha Milli
      - Karen Jusko
      - Jonathan Smith
      - Brandon Amos
      - Wassim Bouaziz
      - Manon Revel
      - Jack Kussman
      - Yasha Sheynin
      - Lisa Titus
      - Bhaktipriya Radharapu
      - Jane Yu
      - Vidya Sarma
      - Kris Rose
      - Maximilian Nickel
      author_organizations:
      - Meta
      - Cornell University
      date: '2025-07-13'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.05728
      link_text: 2503.05728 - Political Neutrality in AI Is Impossible - But Here
        Is How to Approximate It
      original_md: '[2503.05728 \- Political Neutrality in AI Is Impossible \- But
        Here Is How to Approximate It](https://arxiv.org/abs/2503.05728)'
      title: Political Neutrality in AI Is Impossible- But Here Is How to Approximate
        It
      authors:
      - Jillian Fisher
      - Ruth E. Appel
      - Chan Young Park
      - Yujin Potter
      - Liwei Jiang
      - Taylor Sorensen
      - Shangbin Feng
      - Yulia Tsvetkov
      - Margaret E. Roberts
      - Jennifer Pan
      - Dawn Song
      - Yejin Choi
      author_organizations:
      - Anthropic
      - UC San Diego
      - Stanford University
      - UC Berkeley
      - University of Washington
      date: '2025-02-18'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://ojs.aaai.org/index.php/AIES/article/view/36645
      link_text: 'The AI Power Disparity Index: Toward a Compound Measure of AI Actors''
        Power to Shape the AI Ecosystem'
      original_md: '[The AI Power Disparity Index: Toward a Compound Measure of AI
        Actors'' Power to Shape the AI Ecosystem](https://ojs.aaai.org/index.php/AIES/article/view/36645)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://arxiv.org/abs/2505.05197
      link_text: Societal and technological progress as sewing an ever-growing, ever-changing,
        patchy, and polychrome quilt
      original_md: '["Societal and technological progress as sewing an ever-growing,
        ever-changing, patchy, and polychrome quilt"](https://arxiv.org/abs/2505.05197)'
      title: Societal and technological progress as sewing an ever-growing, ever-changing,
        patchy, and polychrome quilt
      authors:
      - Joel Z. Leibo
      - Alexander Sasha Vezhnevets
      - William A. Cunningham
      - Sébastien Krier
      - Manfred Diaz
      - Simon Osindero
      author_organizations:
      - Google DeepMind
      date: '2025-05-08'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.09222
      link_text: 'Democratic AI is Possible: The Democracy Levels Framework Shows
        How It Might Work'
      original_md: '["Democratic AI is Possible: The Democracy Levels Framework Shows
        How It Might Work"](https://arxiv.org/abs/2411.09222)'
      title: Democratic AI is Possible. The Democracy Levels Framework Shows How It
        Might Work
      authors:
      - Aviv Ovadya
      - Kyle Redman
      - Luke Thorburn
      - Quan Ze Chen
      - Oliver Smith
      - Flynn Devine
      - Andrew Konya
      - Smitha Milli
      - Manon Revel
      - K. J. Kevin Feng
      - Amy X. Zhang
      - Bilva Chandra
      - Michiel A. Bakker
      - Atoosa Kasirzadeh
      author_organizations:
      - Multiple Organizations
      date: '2024-11-14'
      published_year: 2024
      venue: arXiv (Accepted to ICML 2025 Position Paper Track)
      kind: paper_preprint
    - link_url: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5097286
      link_text: Research Agenda for Sociotechnical Approaches to AI Safety
      original_md: '[Research Agenda for Sociotechnical Approaches to AI Safety](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5097286)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://arxiv.org/abs/2505.04345
      link_text: Build Agent Advocates, Not Platform Agents
      original_md: '[Build Agent Advocates, Not Platform Agents](https://arxiv.org/abs/2505.04345)'
      title: Build Agent Advocates, Not Platform Agents
      authors:
      - Sayash Kapoor
      - Noam Kolt
      - Seth Lazar
      author_organizations: []
      date: '2025-05-07'
      published_year: 2025
      venue: arXiv (accepted to ICML 2025 position paper track)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.13709
      link_text: Training LLM Agents to Empower Humans
      original_md: '[Training LLM Agents to Empower Humans](https://arxiv.org/abs/2510.13709)'
      title: Training LLM Agents to Empower Humans
      authors:
      - Evan Ellis
      - Vivek Myers
      - Jens Tuyls
      - Sergey Levine
      - Anca Dragan
      - Benjamin Eysenbach
      author_organizations:
      - UC Berkeley
      - Princeton University
      date: '2025-10-16'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues:
  - 'Broad approach field says ''Behavioural'' - mapped to ''behaviorist_science''
    (standard spelling: ''behaviorist science'')'
- id: a:aligning_what
  name: Aligning what?
  header_level: 2
  parent_id: sec:multi_agent_first
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Peli
    one_sentence_summary: Develop alternatives to agent-level models of alignment,
      by treating human-AI interactions, AI-assisted institutions, AI economic or
      cultural systems, drives within one AI, and other causal/constitutive processes
      as subject to alignment
    theory_of_change: Model multiple reality-shaping processes above and below the
      level of the individual AI, some of which are themselves quasi-agential (e.g.
      cultures) or intelligence-like (e.g. markets), will develop AI alignment into
      a mature science for managing the transition to an AGI civilization
    see_also:
    - a:aligning_multiple_game_theory
    - a:alignment_to_context
    - a:aligned_to_who
    orthodox_problems:
    - value_fragile
    - corrigibility_anti_natural
    - goals_misgeneralize
    - instrumental_convergence
    - fair_sane_pivotal
    target_case_id: null
    target_case_text: mixed
    broad_approach_id: null
    broad_approach_text: behavioral / cognitive
    some_names:
    - Richard Ngo
    - Emmett Shear
    - Softmax
    - Full Stack Alignment
    - AI Objectives Institute
    - Sahil
    - TJ
    - Andrew Critch
    estimated_ftes: 5-10
    critiques: null
    funded_by: Future of Life Institute, Emmet Shear
    outputs:
    - link_url: https://www.alignmentforum.org/posts/5tYTKX4pNpiG4vzYg/towards-a-scale-free-theory-of-intelligent-agency
      link_text: Towards a Scale-Free Theory of Intelligent Agency
      original_md: '[Towards a Scale-Free Theory of Intelligent Agency](https://www.alignmentforum.org/posts/5tYTKX4pNpiG4vzYg/towards-a-scale-free-theory-of-intelligent-agency)'
      title: Towards a scale-free theory of intelligent agency
      authors:
      - Richard Ngo
      author_organizations:
      - Independent
      date: '2025-03-21'
      published_year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - link_url: https://chrislakin.blog/p/alignment-first-intelligence-later
      link_text: Alignment first, intelligence later
      original_md: '[Alignment first, intelligence later](https://chrislakin.blog/p/alignment-first-intelligence-later)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://a16z.simplecast.com/episodes/emmett-shear-on-building-ai-that-actually-cares-beyond-control-and-steering-TRwfxH0r
      link_text: 'Emmett Shear on Building AI That Actually Cares: Beyond Control
        and Steering'
      original_md: '[Emmett Shear on Building AI That Actually Cares: Beyond Control
        and Steering](https://a16z.simplecast.com/episodes/emmett-shear-on-building-ai-that-actually-cares-beyond-control-and-steering-TRwfxH0r)'
      title: 'Emmett Shear on Building AI That Actually Cares: Beyond Control and
        Steering'
      authors:
      - Emmett Shear
      - Erik Torenberg
      - Séb Krier
      author_organizations:
      - Softmax
      - a16z
      date: '2025-11-17'
      published_year: 2025
      venue: a16z Podcast
      kind: podcast
    - link_url: https://www.anthropic.com/research/end-subset-conversations
      link_text: End A Subset Of Conversations
      original_md: '[End A Subset Of Conversations](https://www.anthropic.com/research/end-subset-conversations)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://www.full-stack-alignment.ai
      link_text: Full-Stack Alignment
      original_md: '[Full-Stack Alignment](https://www.full-stack-alignment.ai)'
      title: 'Full-Stack Alignment: Co-Aligning AI and Institutions with Thick Models
        of Value'
      authors: []
      author_organizations:
      - Meaning Alignment Institute
      date: null
      published_year: null
      venue: null
      kind: personal_page
    - link_url: https://docs.google.com/document/d/1cKbqYSGspfJavXvnhsp3mAuxHh08rNbP7tzYieqLiXw/edit?tab=t.0%20
      link_text: On Eudaimonia and Optimization
      original_md: '[On Eudaimonia and Optimization](https://docs.google.com/document/d/1cKbqYSGspfJavXvnhsp3mAuxHh08rNbP7tzYieqLiXw/edit?tab=t.0%20)'
      title: On Eudaimonia and Optimization
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: Google Docs
      kind: error_detected
    - link_url: https://arxiv.org/abs/2501.17755
      link_text: AI Governance through Markets
      original_md: '[AI Governance through Markets](https://arxiv.org/abs/2501.17755)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://www.pnas.org/doi/abs/10.1073/pnas.2319948121
      link_text: Collective cooperative intelligence
      original_md: '[Collective cooperative intelligence](https://www.pnas.org/doi/abs/10.1073/pnas.2319948121)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://www.lesswrong.com/posts/JjYu75q3hEMBgtvr8/multipolar-ai-is-underrated
      link_text: Multipolar AI is Underrated
      original_md: '[Multipolar AI is Underrated](https://www.lesswrong.com/posts/JjYu75q3hEMBgtvr8/multipolar-ai-is-underrated)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://www.lesswrong.com/posts/tQ9vWm4b57HFqbaRj/what-if-not-agency
      link_text: What, if not agency?
      original_md: '[What, if not agency?](https://www.lesswrong.com/posts/tQ9vWm4b57HFqbaRj/what-if-not-agency)'
      title: Unknown - Content Unavailable (429 Error)
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: error_detected
    - link_url: https://substack.com/home/post/p-171042125
      link_text: https://substack.com/home/post/p-171042125
      original_md: '[https://substack.com/home/post/p-171042125](https://substack.com/home/post/p-171042125)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://themultiplicity.ai/blog/thesis
      link_text: The Multiplicity Thesis, Collective Intelligence, and Morality
      original_md: '[The Multiplicity Thesis, Collective Intelligence, and Morality](https://themultiplicity.ai/blog/thesis)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    other_attributes: {}
  parsing_issues:
  - Target case field says 'Mixed' - leaving target_case_id as null and setting target_case_text
    to 'mixed'
  - Broad approach field says 'Behavioural/Cognitive' which is mixed - leaving broad_approach_id
    as null and setting broad_approach_text to 'behavioral / cognitive'
- id: sec:evals
  name: Evals
  header_level: 1
  parent_id: null
  content: ''
  item_type: section
  agenda_attributes: null
  parsing_issues: []
- id: a:agi_metrics
  name: AGI metrics
  header_level: 2
  parent_id: sec:evals
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stephen ✅
    one_sentence_summary: Evals with the explicit aim of measuring progress towards
      full human-level generality.
    theory_of_change: Help predict timelines for risk awareness and strategy.
    see_also:
    - a:evals_capability
    orthodox_problems:
    - 'Other: none; a barometer of risk.'
    target_case_id: null
    target_case_text: mixed
    broad_approach_id: behaviorist_science
    broad_approach_text: behaviorist science
    some_names:
    - CAIS
    - CFI Kinds of Intelligence
    - Apart Research
    - OpenAI
    - METR
    - Lexin Zhou
    - Adam Scholl
    - Lorenzo Pacchiardi
    estimated_ftes: 10-50
    critiques: '[Is the Definition of AGI a Percentage?](https://aievaluation.substack.com/p/is-the-definition-of-agi-a-percentage),
      [The "Length" of "Horizons"](https://www.lesswrong.com/posts/PzLSuaT6WGLQGJJJD/the-length-of-horizons)'
    funded_by: Leverhulme Trust, Open Philanthropy, Long-Term Future Fund
    outputs:
    - link_url: https://arxiv.org/abs/2510.04374
      link_text: 'GDPval: Evaluating AI Model Performance on Real-World Economically
        Valuable Tasks'
      original_md: '* [GDPval: Evaluating AI Model Performance on Real-World Economically
        Valuable Tasks](https://arxiv.org/abs/2510.04374)'
      title: 'GDPval: Evaluating AI Model Performance on Real-World Economically Valuable
        Tasks'
      authors:
      - Tejal Patwardhan
      - Rachel Dias
      - Elizabeth Proehl
      - Grace Kim
      - Michele Wang
      - Olivia Watkins
      - Simón Posada Fishman
      - Marwan Aljubeh
      - Phoebe Thacker
      - Laurance Fauconnet
      - Natalie S. Kim
      - Patrick Chao
      - Samuel Miserendino
      - Gildas Chabot
      - David Li
      - Michael Sharman
      - Alexandra Barr
      - Amelia Glaese
      - Jerry Tworek
      author_organizations:
      - OpenAI
      date: '2025-10-05'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.17354
      link_text: 'HCAST: Human-Calibrated Autonomy Software Tasks'
      original_md: '* [HCAST: Human-Calibrated Autonomy Software Tasks](https://arxiv.org/abs/2503.17354)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://arxiv.org/pdf/2510.18212
      link_text: A Definition of AGI
      original_md: '* [A Definition of AGI](https://arxiv.org/pdf/2510.18212)'
      title: A Definition of AGI
      authors:
      - Dan Hendrycks
      - Dawn Song
      - Christian Szegedy
      - Honglak Lee
      - Yarin Gal
      - Erik Brynjolfsson
      - Sharon Li
      - Andy Zou
      - Lionel Levine
      - Bo Han
      - Jie Fu
      - Ziwei Liu
      - Jinwoo Shin
      - Kimin Lee
      - Mantas Mazeika
      - Long Phan
      - George Ingebretsen
      - Adam Khoja
      - Cihang Xie
      - Olawale Salaudeen
      - Matthias Hein
      - Kevin Zhao
      - Alexander Pan
      - David Duvenaud
      - Bo Li
      - Steve Omohundro
      - Gabriel Alfour
      - Max Tegmark
      - Kevin McGrew
      - Gary Marcus
      - Jaan Tallinn
      - Eric Schmidt
      - Yoshua Bengio
      author_organizations:
      - UC Berkeley
      - Various Universities
      - Independent Researchers
      date: '2025-10-21'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://scale.com/leaderboard/rli
      link_text: Remote Labor Index
      original_md: '* [Remote Labor Index](https://scale.com/leaderboard/rli)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://kinds-of-intelligence-cfi.github.io/ADELE/
      link_text: 'ADeLe v1.0: A battery for AI Evaluation with explanatory and predictive
        power'
      original_md: '* [ADeLe v1.0: A battery for AI Evaluation with explanatory and
        predictive power](https://kinds-of-intelligence-cfi.github.io/ADELE/)'
      title: 'ADeLe v1.0: A battery for AI Evaluation with explanatory and predictive
        power'
      authors:
      - Lexin Zhou
      - Lorenzo Pacchiardi
      - Fernando Martínez-Plumed
      - Katherine M. Collins
      - Yael Moros-Daval
      - Seraphina Zhang
      - Qinlin Zhao
      - Yitian Huang
      - Luning Sun
      - Jonathan E. Prunty
      - Zongqian Li
      - Pablo Sánchez-García
      - Kexin Jiang Chen
      - Pablo A. M. Casares
      - Jiyun Zu
      - John Burden
      - Behzad Mehrbakhsh
      - David Stillwell
      - Manuel Cebrian
      - Jindong Wang
      - Peter Henderson
      - Sherry Tongshuang Wu
      - Patrick C. Kyllonen
      - Lucy Cheke
      - Xing Xie
      - José Hernández-Orallo
      author_organizations:
      - Leverhulme Centre for the Future of Intelligence
      - Center for Information Technology Policy, Princeton University
      date: '2025-03-11'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues:
  - Target case is 'mixed' - leaving target_case_id as null and setting target_case_text
    to 'mixed'
  - 'Orthodox problems field says ''none; a barometer of risk.'' - no matching orthodox
    problem found, kept as ''Other: none; a barometer of risk.'''
- id: a:evals_capability
  name: Capability evals
  header_level: 2
  parent_id: sec:evals
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stephen ✅
    one_sentence_summary: Make tools that can actually check whether a model has a
      certain capability or propensity. We default to low-n sampling of a vast latent
      space but aim to do better.
    theory_of_change: Keep a close eye on what capabilities are acquired when, so
      that frontier labs and regulators are better informed on what security measures
      are already necessary (and hopefully they extrapolate). You can't regulate without
      them.
    see_also:
    - '[Deepmind''s frontier safety framework](https://deepmind.google/blog/strengthening-our-frontier-safety-framework/)'
    - '[Aether](https://www.lesswrong.com/posts/B8Cmtf5gdHwxb8qtT/aether-july-2025-update)'
    orthodox_problems:
    - 'Other: none; a barometer for risk'
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: behaviorist_science
    broad_approach_text: behaviorist science
    some_names:
    - METR
    - AISI
    - Apollo Research
    - Marrius Hobbhahn
    - Meg Tong
    - Mary Phuong
    - Beth Barnes
    - Thomas Kwa
    - Joel Becker
    estimated_ftes: 100+
    critiques: '[Large Language Models Often Know When They Are Being Evaluated](https://arxiv.org/abs/2505.23836),
      [AI Sandbagging: Language Models can Strategically Underperform on Evaluations](https://arxiv.org/abs/2406.07358),
      [The Leaderboard Illusion](https://arxiv.org/abs/2504.20879), [Do Large Language
      Model Benchmarks Test Reliability?](https://arxiv.org/abs/2502.03461)'
    funded_by: basically everyone. Google, Microsoft, Open Philanthropy, LTFF, Governments
      etc
    outputs:
    - link_url: https://arxiv.org/abs/2502.16797
      link_text: Forecasting Rare Language Model Behaviors
      original_md: '* [**Forecasting Rare Language Model Behaviors**](https://arxiv.org/abs/2502.16797),
        *Erik Jones, Meg Tong, Jesse Mu et al.*, 2025-02-24, arXiv'
      title: Forecasting Rare Language Model Behaviors
      authors:
      - Erik Jones
      - Meg Tong
      - Jesse Mu
      - Mohammed Mahfoud
      - Jan Leike
      - Roger Grosse
      - Jared Kaplan
      - William Fithian
      - Ethan Perez
      - Mrinank Sharma
      author_organizations:
      - Anthropic
      - OpenAI
      - UC Berkeley
      date: '2025-02-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.05209
      link_text: Model Tampering Attacks Enable More Rigorous Evaluations of LLM Capabilities
      original_md: '* [**Model Tampering Attacks Enable More Rigorous Evaluations
        of LLM Capabilities**](https://arxiv.org/abs/2502.05209), *Zora Che, Stephen
        Casper, Robert Kirk et al.*, 2025-02-03, arXiv (accepted to TMLR)'
      title: Model Tampering Attacks Enable More Rigorous Evaluations of LLM Capabilities
      authors:
      - Zora Che
      - Stephen Casper
      - Robert Kirk
      - Anirudh Satheesh
      - Stewart Slocum
      - Lev E McKinney
      - Rohit Gandikota
      - Aidan Ewart
      - Domenic Rosati
      - Zichu Wu
      - Zikui Cai
      - Bilal Chughtai
      - Yarin Gal
      - Furong Huang
      - Dylan Hadfield-Menell
      author_organizations: []
      date: '2025-02-03'
      published_year: 2025
      venue: arXiv (accepted to TMLR)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.02180
      link_text: 'The Elicitation Game: Evaluating Capability Elicitation Techniques'
      original_md: '* [**The Elicitation Game: Evaluating Capability Elicitation Techniques**](https://arxiv.org/abs/2502.02180),
        *Felix Hofstätter, Teun van der Weij, Jayden Teoh et al.*, 2025-02-04, arXiv'
      title: 'The Elicitation Game: Evaluating Capability Elicitation Techniques'
      authors:
      - Felix Hofstätter
      - Teun van der Weij
      - Jayden Teoh
      - Rada Djoneva
      - Henning Bartsch
      - Francis Rhys Ward
      author_organizations: []
      date: '2025-02-04'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://alignment.anthropic.com/2024/rogue-eval/index.html
      link_text: A Toy Evaluation of Inference Code Tampering
      original_md: '* [**A Toy Evaluation of Inference Code Tampering**](https://alignment.anthropic.com/2024/rogue-eval/index.html),
        2024, Anthropic Alignment Science Blog'
      title: A Toy Evaluation of Inference Code Tampering
      authors: []
      author_organizations:
      - Anthropic
      date: null
      published_year: 2024
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - link_url: https://arxiv.org/abs/2502.07577
      link_text: Automated Capability Discovery via Foundation Model Self-Exploration
      original_md: '* [**Automated Capability Discovery via Foundation Model Self-Exploration**](https://arxiv.org/abs/2502.07577),
        *Cong Lu, Shengran Hu, Jeff Clune*, 2025-02-11, arXiv'
      title: Automated Capability Discovery via Foundation Model Self-Exploration
      authors:
      - Cong Lu
      - Shengran Hu
      - Jeff Clune
      author_organizations: []
      date: '2025-02-11'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/
      link_text: Measuring the Impact of Early-2025 AI on Experienced Open-Source
        Developer Productivity
      original_md: '* [**Measuring the Impact of Early-2025 AI on Experienced Open-Source
        Developer Productivity**](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/),
        2025-07-10, METR Blog'
      title: Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer
        Productivity
      authors: []
      author_organizations:
      - METR
      date: '2025-07-10'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.19212
      link_text: 'When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social
        Dilemmas'
      original_md: '* [**When Ethics and Payoffs Diverge: LLM Agents in Morally Charged
        Social Dilemmas**](https://arxiv.org/abs/2505.19212), *Steffen Backmann, David
        Guzman Piedrahita, Emanuel Tewolde et al.*, 2025-05-25, arXiv'
      title: 'When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social
        Dilemmas'
      authors:
      - Steffen Backmann
      - David Guzman Piedrahita
      - Emanuel Tewolde
      - Rada Mihalcea
      - Bernhard Schölkopf
      - Zhijing Jin
      author_organizations:
      - Max Planck Institute for Intelligent Systems
      date: '2025-05-25'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.05731
      link_text: 'AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark
        from MLCommons'
      original_md: '* [**AILuminate: Introducing v1.0 of the AI Risk and Reliability
        Benchmark from MLCommons**](https://arxiv.org/abs/2503.05731), *Shaona Ghosh,
        Heather Frase, Adina Williams et al.*, 2025-04-18, arXiv'
      title: 'AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark
        from MLCommons'
      authors:
      - Shaona Ghosh
      - Heather Frase
      - Adina Williams
      - Sarah Luger
      - Paul Röttger
      - Fazl Barez
      - Sean McGregor
      - Kenneth Fricklas
      - Mala Kumar
      - Quentin Feuillade-Montixi
      - Kurt Bollacker
      - Felix Friedrich
      - Ryan Tsang
      - Bertie Vidgen
      - Alicia Parrish
      - Chris Knotz
      - Eleonora Presani
      - Jonathan Bennion
      - Marisa Ferrara Boston
      - Mike Kuniavsky
      - Wiebke Hutiri
      - James Ezick
      - Malek Ben Salem
      - Rajat Sahay
      - Sujata Goswami
      - Usman Gohar
      - Ben Huang
      - Supheakmungkol Sarin
      - Elie Alhajjar
      - Canyu Chen
      - Roman Eng
      - Kashyap Ramanandula Manjusha
      - Virendra Mehta
      - Eileen Long
      - Murali Emani
      - Natan Vidra
      - Benjamin Rukundo
      - Abolfazl Shahbazi
      - Kongtao Chen
      - Rajat Ghosh
      - Vithursan Thangarasa
      - Pierre Peigné
      - Abhinav Singh
      - Max Bartolo
      - Satyapriya Krishna
      - Mubashara Akhtar
      - Rafael Gold
      - Cody Coleman
      - Luis Oala
      - Vassil Tashev
      - Joseph Marvin Imperial
      - Amy Russ
      - Sasidhar Kunapuli
      - Nicolas Miailhe
      - Julien Delaunay
      - Bhaktipriya Radharapu
      - Rajat Shinde
      - Tuesday
      - Debojyoti Dutta
      - Declan Grabb
      - Ananya Gangavarapu
      - Saurav Sahay
      - Agasthya Gangavarapu
      - Patrick Schramowski
      - Stephen Singam
      - Tom David
      - Xudong Han
      - Priyanka Mary Mammen
      - Tarunima Prabhakar
      - Venelin Kovatchev
      - Rebecca Weiss
      - Ahmed Ahmed
      - Kelvin N. Manyeki
      - Sandeep Madireddy
      - Foutse Khomh
      - Fedor Zhdanov
      - Joachim Baumann
      - Nina Vasan
      - Xianjun Yang
      - Carlos Mougn
      - Jibin Rajan Varghese
      - Hussain Chinoy
      - Seshakrishna Jitendar
      - Manil Maskey
      - Claire V. Hardgrove
      - Tianhao Li
      - Aakash Gupta
      - Emil Joswin
      - Yifan Mai
      - Shachi H Kumar
      - Cigdem Patlak
      - Kevin Lu
      - Vincent Alessi
      - Sree Bhargavi Balija
      - Chenhe Gu
      - Robert Sullivan
      - James Gealy
      - Matt Lavrisa
      - James Goel
      - Peter Mattson
      - Percy Liang
      - Joaquin Vanschoren
      author_organizations:
      - MLCommons
      date: '2025-04-18'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.anthropic.com/research/petri-open-source-auditing
      link_text: 'Petri: An open-source auditing tool to accelerate AI safety research'
      original_md: '* [**Petri: An open-source auditing tool to accelerate AI safety
        research**](https://www.anthropic.com/research/petri-open-source-auditing),
        *Kai Fronsdal, Isha Gupta, Abhay Sheshadri et al.*, 2025-10-06, Anthropic
        Research Blog'
      title: 'Petri: An open-source auditing tool to accelerate AI safety research'
      authors:
      - Kai Fronsdal
      - Isha Gupta
      - Abhay Sheshadri
      - Jonathan Michala
      - Stephen McAleer
      - Rowan Wang
      - Sara Price
      - Samuel R. Bowman
      author_organizations:
      - Anthropic
      date: '2025-10-06'
      published_year: 2025
      venue: Anthropic Blog
      kind: blog_post
    - link_url: https://www.apolloresearch.ai/blog/research-note-our-scheming-precursor-evals-had-limited-predictive-power-for-our-in-context-scheming-evals
      link_text: 'Research Note: Our scheming precursor evals had limited predictive
        power for our in-context scheming evals'
      original_md: '* [**Research Note: Our scheming precursor evals had limited predictive
        power for our in-context scheming evals**](https://www.apolloresearch.ai/blog/research-note-our-scheming-precursor-evals-had-limited-predictive-power-for-our-in-context-scheming-evals),
        *Marius Hobbhahn*, 2025-07-03, Apollo Research Blog'
      title: 'Research Note: Our scheming precursor evals had limited predictive power
        for our in-context scheming evals'
      authors:
      - Marius Hobbhahn
      author_organizations:
      - Apollo Research
      date: '2025-07-03'
      published_year: 2025
      venue: Apollo Research Blog
      kind: blog_post
    - link_url: https://arxiv.org/abs/2502.02260
      link_text: Adversarial ML Problems Are Getting Harder to Solve and to Evaluate
      original_md: '* [**Adversarial ML Problems Are Getting Harder to Solve and to
        Evaluate**](https://arxiv.org/abs/2502.02260), *Javier Rando, Jie Zhang, Nicholas
        Carlini et al.*, 2025-02-04, arXiv'
      title: Adversarial ML Problems Are Getting Harder to Solve and to Evaluate
      authors:
      - Javier Rando
      - Jie Zhang
      - Nicholas Carlini
      - Florian Tramèr
      author_organizations: []
      date: '2025-02-04'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.4wallai.com/amongais
      link_text: Among AIs
      original_md: '* [**Among AIs**](https://www.4wallai.com/amongais), 4Wall AI
        Website'
      title: Among AIs
      authors: []
      author_organizations:
      - 4Wall AI
      date: null
      published_year: 2025
      venue: 4Wall AI website
      kind: blog_post
    - link_url: https://arxiv.org/abs/2505.05541
      link_text: 'Safety by Measurement: A Systematic Literature Review of AI Safety
        Evaluation Methods'
      original_md: '* [**Safety by Measurement: A Systematic Literature Review of
        AI Safety Evaluation Methods**](https://arxiv.org/abs/2505.05541), *Markov
        Grey, Charbel-Raphaël Segerie*, 2025-05-08, arXiv'
      title: 'Safety by Measurement: A Systematic Literature Review of AI Safety Evaluation
        Methods'
      authors:
      - Markov Grey
      - Charbel-Raphaël Segerie
      author_organizations: []
      date: '2025-05-08'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.18339
      link_text: Correlating and Predicting Human Evaluations of Language Models from
        Natural Language Processing Benchmarks
      original_md: '* [**Correlating and Predicting Human Evaluations of Language
        Models from Natural Language Processing Benchmarks**](https://arxiv.org/abs/2502.18339),
        *Rylan Schaeffer, Punit Singh Koura, Binh Tang et al.*, 2025-02-24, arXiv'
      title: Correlating and Predicting Human Evaluations of Language Models from
        Natural Language Processing Benchmarks
      authors:
      - Rylan Schaeffer
      - Punit Singh Koura
      - Binh Tang
      - Ranjan Subramanian
      - Aaditya K Singh
      - Todor Mihaylov
      - Prajjwal Bhargava
      - Lovish Madaan
      - Niladri S. Chatterji
      - Vedanuj Goswami
      - Sergey Edunov
      - Dieuwke Hupkes
      - Sanmi Koyejo
      - Sharan Narang
      author_organizations:
      - Meta AI Research
      date: '2025-02-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2501.03200
      link_text: 'The FACTS Grounding Leaderboard: Benchmarking LLMs'' Ability to
        Ground Responses to Long-Form Input'
      original_md: '* [**The FACTS Grounding Leaderboard: Benchmarking LLMs'' Ability
        to Ground Responses to Long-Form Input**](https://arxiv.org/abs/2501.03200),
        *Alon Jacovi, Andrew Wang, Chris Alberti et al.*, 2025-01-06, arXiv'
      title: 'The FACTS Grounding Leaderboard: Benchmarking LLMs'' Ability to Ground
        Responses to Long-Form Input'
      authors:
      - Alon Jacovi
      - Andrew Wang
      - Chris Alberti
      - Connie Tao
      - Jon Lipovetz
      - Kate Olszewska
      - Lukas Haas
      - Michelle Liu
      - Nate Keating
      - Adam Bloniarz
      - Carl Saroufim
      - Corey Fry
      - Dror Marcus
      - Doron Kukliansky
      - Gaurav Singh Tomar
      - James Swirhun
      - Jinwei Xing
      - Lily Wang
      - Madhu Gurumurthy
      - Michael Aaron
      - Moran Ambar
      - Rachana Fellinger
      - Rui Wang
      - Zizhao Zhang
      - Sasha Goldshtein
      - Dipanjan Das
      author_organizations:
      - Google
      date: '2025-01-06'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2508.19980
      link_text: Evaluating Language Model Reasoning about Confidential Information
      original_md: '* [**Evaluating Language Model Reasoning about Confidential Information**](https://arxiv.org/abs/2508.19980),
        *Dylan Sam, Alexander Robey, Andy Zou et al.*, 2025-08-27, arXiv'
      title: Evaluating Language Model Reasoning about Confidential Information
      authors:
      - Dylan Sam
      - Alexander Robey
      - Andy Zou
      - Matt Fredrikson
      - J. Zico Kolter
      author_organizations:
      - Carnegie Mellon University
      date: '2025-08-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.11844
      link_text: Evaluating the Goal-Directedness of Large Language Models
      original_md: '* [**Evaluating the Goal-Directedness of Large Language Models**](https://arxiv.org/abs/2504.11844),
        *Tom Everitt, Cristina Garbacea, Alexis Bellot et al.*, 2025-04-16, arXiv'
      title: Evaluating the Goal-Directedness of Large Language Models
      authors:
      - Tom Everitt
      - Cristina Garbacea
      - Alexis Bellot
      - Jonathan Richens
      - Henry Papadatos
      - Siméon Campos
      - Rohin Shah
      author_organizations:
      - Google DeepMind
      - OpenAI
      - Anthropic
      date: '2025-04-16'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.25369
      link_text: Generative Value Conflicts Reveal LLM Priorities
      original_md: '* [**Generative Value Conflicts Reveal LLM Priorities**](https://arxiv.org/abs/2509.25369),
        *Andy Liu, Kshitish Ghate, Mona Diab et al.*, 2025-09-29, arXiv'
      title: Generative Value Conflicts Reveal LLM Priorities
      authors:
      - Andy Liu
      - Kshitish Ghate
      - Mona Diab
      - Daniel Fried
      - Atoosa Kasirzadeh
      - Max Kleiman-Weiner
      author_organizations: []
      date: '2025-09-29'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.02709
      link_text: 'Technical Report: Evaluating Goal Drift in Language Model Agents'
      original_md: '* [**Technical Report: Evaluating Goal Drift in Language Model
        Agents**](https://arxiv.org/abs/2505.02709), *Rauno Arike, Elizabeth Donoway,
        Henning Bartsch et al.*, 2025-05-05, arXiv'
      title: 'Technical Report: Evaluating Goal Drift in Language Model Agents'
      authors:
      - Rauno Arike
      - Elizabeth Donoway
      - Henning Bartsch
      - Marius Hobbhahn
      author_organizations:
      - Apollo Research
      date: '2025-05-05'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://metr.org/blog/2025-10-14-malt-dataset-of-natural-and-prompted-behaviors/
      link_text: 'MALT: A Dataset of Natural and Prompted Behaviors That Threaten
        Eval Integrity'
      original_md: '* [**MALT: A Dataset of Natural and Prompted Behaviors That Threaten
        Eval Integrity**](https://metr.org/blog/2025-10-14-malt-dataset-of-natural-and-prompted-behaviors/)'
      title: 'MALT: A Dataset of Natural and Prompted Behaviors That Threaten Eval
        Integrity'
      authors:
      - Neev Parikh
      - Hjalmar Wijk
      author_organizations:
      - METR
      date: '2025-10-14'
      published_year: 2025
      venue: METR Blog
      kind: blog_post
    - link_url: https://arxiv.org/abs/2501.01558
      link_text: Predicting the Performance of Black-box LLMs through Self-Queries
      original_md: '* [**Predicting the Performance of Black-box LLMs through Self-Queries**](https://arxiv.org/abs/2501.01558),
        *Dylan Sam, Marc Finzi, J. Zico Kolter*, 2025-01-02, arXiv'
      title: Predicting the Performance of Black-box LLMs through Self-Queries
      authors:
      - Dylan Sam
      - Marc Finzi
      - J. Zico Kolter
      author_organizations:
      - Carnegie Mellon University
      date: '2025-01-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.12229
      link_text: 'Infini-gram mini: Exact n-gram Search at the Internet Scale with
        FM-Index'
      original_md: '* [**Infini-gram mini: Exact n-gram Search at the Internet Scale
        with FM-Index**](https://arxiv.org/abs/2506.12229), *Hao Xu, Jiacheng Liu,
        Yejin Choi et al.*, 2025-06-13, arXiv'
      title: 'Infini-gram mini: Exact n-gram Search at the Internet Scale with FM-Index'
      authors:
      - Hao Xu
      - Jiacheng Liu
      - Yejin Choi
      - Noah A. Smith
      - Hannaneh Hajishirzi
      author_organizations:
      - University of Washington
      - Allen Institute for AI
      date: '2025-06-13'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/ZEuDH2W3XdRaTwpjD/hyperbolic-model-fits-metr-capabilities-estimate-worse-than
      link_text: Hyperbolic model fits METR capabilities estimate worse than exponential
        model
      original_md: '* [**Hyperbolic model fits METR capabilities estimate worse than
        exponential model**](https://lesswrong.com/posts/ZEuDH2W3XdRaTwpjD/hyperbolic-model-fits-metr-capabilities-estimate-worse-than),
        *gjm*, 2025-08-19, LessWrong'
      title: Hyperbolic model fits METR capabilities estimate worse than exponential
        model
      authors:
      - gjm
      author_organizations: []
      date: '2025-08-19'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/nmaKpoHxmzjT8yXTk/new-website-analyzing-ai-companies-model-evals
      link_text: New website analyzing AI companies' model evals
      original_md: '* [**New website analyzing AI companies'' model evals**](https://lesswrong.com/posts/nmaKpoHxmzjT8yXTk/new-website-analyzing-ai-companies-model-evals),
        *Zach Stein-Perlman*, 2025-05-26, LessWrong'
      title: New website analyzing AI companies' model evals
      authors:
      - Zach Stein-Perlman
      author_organizations: []
      date: '2025-05-26'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/9tqpPP4FwSnv9AWsi/research-note-our-scheming-precursor-evals-had-limited
      link_text: 'Research Note: Our scheming precursor evals had limited predictive
        power for our in-context scheming evals'
      original_md: '* [**Research Note: Our scheming precursor evals had limited predictive
        power for our in-context scheming evals**](https://lesswrong.com/posts/9tqpPP4FwSnv9AWsi/research-note-our-scheming-precursor-evals-had-limited),
        *Marius Hobbhahn*, 2025-07-03, LessWrong'
      title: 'Research Note: Our scheming precursor evals had limited predictive power
        for our in-context scheming evals'
      authors:
      - Marius Hobbhahn
      author_organizations:
      - Apollo Research
      date: '2025-07-03'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/qhjNejRxbMGQp4wHt/how-fast-can-algorithms-advance-capabilities-or-epoch
      link_text: How Fast Can Algorithms Advance Capabilities? | Epoch Gradient Update
      original_md: '* [**How Fast Can Algorithms Advance Capabilities? | Epoch Gradient
        Update**](https://lesswrong.com/posts/qhjNejRxbMGQp4wHt/how-fast-can-algorithms-advance-capabilities-or-epoch),
        *Henry Josephson, Spencer Guo, Teddy Foley et al.*, 2025-05-16, LessWrong'
      title: How Fast Can Algorithms Advance Capabilities? | Epoch Gradient Update
      authors:
      - Henry Josephson
      - Spencer Guo
      - Teddy Foley
      - Jack Sanderson
      - Anqi Qu
      author_organizations:
      - UChicago XLab
      - Google DeepMind
      date: '2025-05-16'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/CwdCYmsutwXwnYtEF/paper-safety-by-measurement-a-systematic-literature-review
      link_text: 'Safety by Measurement: A Systematic Literature Review of AI Safety
        Evaluation Methods'
      original_md: '* [**Safety by Measurement: A Systematic Literature Review of
        AI Safety Evaluation Methods**](https://lesswrong.com/posts/CwdCYmsutwXwnYtEF/paper-safety-by-measurement-a-systematic-literature-review),
        *markov, Charbel-Raphaël*, 2025-05-19, arXiv'
      title: 'Safety by Measurement: A Systematic Literature Review of AI Safety Evaluation
        Methods'
      authors:
      - markov
      - Charbel-Raphaël
      author_organizations: []
      date: '2025-05-19'
      published_year: 2025
      venue: arXiv
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/W3KfxjbqBAnifBQoi/we-should-try-to-automate-ai-safety-work-asap
      link_text: We should try to automate AI safety work asap
      original_md: '* [**We should try to automate AI safety work asap**](https://lesswrong.com/posts/W3KfxjbqBAnifBQoi/we-should-try-to-automate-ai-safety-work-asap),
        *Marius Hobbhahn*, 2025-04-26, LessWrong'
      title: We should try to automate AI safety work asap
      authors:
      - Marius Hobbhahn
      author_organizations:
      - Apollo Research
      date: '2025-04-26'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/CXYf7kGBecZMajrXC/validating-against-a-misalignment-detector-is-very-different
      link_text: Validating against a misalignment detector is very different to training
        against one
      original_md: '* [**Validating against a misalignment detector is very different
        to training against one**](https://lesswrong.com/posts/CXYf7kGBecZMajrXC/validating-against-a-misalignment-detector-is-very-different),
        *mattmacdermott*, 2025-03-04, LessWrong'
      title: Validating against a misalignment detector is very different to training
        against one
      authors:
      - mattmacdermott
      author_organizations: []
      date: '2025-03-04'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/NDotm7oLHfR56g4sD/why-do-misalignment-risks-increase-as-ais-get-more-capable
      link_text: Why do misalignment risks increase as AIs get more capable?
      original_md: '* [**Why do misalignment risks increase as AIs get more capable?**](https://lesswrong.com/posts/NDotm7oLHfR56g4sD/why-do-misalignment-risks-increase-as-ais-get-more-capable),
        *Ryan Greenblatt*, 2025-04-11, LessWrong'
      title: Why do misalignment risks increase as AIs get more capable?
      authors:
      - Ryan Greenblatt
      author_organizations:
      - Anthropic
      date: '2025-04-11'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/wbJxRNxuezvsGFEWv/open-philanthropy-technical-ai-safety-rfp-usd40m-available
      link_text: Open Philanthropy Technical AI Safety RFP - $40M Available Across
        21 Research Areas
      original_md: '* [**Open Philanthropy Technical AI Safety RFP \- $40M Available
        Across 21 Research Areas**](https://lesswrong.com/posts/wbJxRNxuezvsGFEWv/open-philanthropy-technical-ai-safety-rfp-usd40m-available),
        *jake_mendel, maxnadeau, Peter Favaloro*, 2025-02-06, LessWrong'
      title: Open Philanthropy Technical AI Safety RFP - $40M Available Across 21
        Research Areas
      authors:
      - jake_mendel
      - maxnadeau
      - Peter Favaloro
      author_organizations:
      - Open Philanthropy
      date: '2025-02-06'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/TxiB6hvnQqxXB5XDJ/why-future-ais-will-require-new-alignment-methods
      link_text: Why Future AIs will Require New Alignment Methods
      original_md: '* [**Why Future AIs will Require New Alignment Methods**](https://lesswrong.com/posts/TxiB6hvnQqxXB5XDJ/why-future-ais-will-require-new-alignment-methods),
        *Alvin Ånestrand*, 2025-10-10, LessWrong'
      title: Why Future AIs will Require New Alignment Methods
      authors:
      - Alvin Ånestrand
      author_organizations: []
      date: '2025-10-10'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/LhnqegFoykcjaXCYH/100-concrete-projects-and-open-problems-in-evals
      link_text: 100+ concrete projects and open problems in evals
      original_md: '* [**100+ concrete projects and open problems in evals**](https://lesswrong.com/posts/LhnqegFoykcjaXCYH/100-concrete-projects-and-open-problems-in-evals),
        *Marius Hobbhahn*, 2025-03-22, LessWrong'
      title: 100+ concrete projects and open problems in evals
      authors:
      - Marius Hobbhahn
      author_organizations:
      - Apollo Research
      date: '2025-03-22'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/tQzeafo9HjCeXn7ZF/ai-companies-should-be-safety-testing-the-most-capable
      link_text: AI companies should be safety-testing the most capable versions of
        their models
      original_md: '* [**AI companies should be safety-testing the most capable versions
        of their models**](https://lesswrong.com/posts/tQzeafo9HjCeXn7ZF/ai-companies-should-be-safety-testing-the-most-capable),
        *Steven Adler*, 2025-03-26, LessWrong'
      title: AI companies should be safety-testing the most capable versions of their
        models
      authors:
      - Steven Adler
      author_organizations: []
      date: '2025-03-26'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    other_attributes: {}
  parsing_issues: []
- id: a:evals_autonomy
  name: Autonomy evals
  header_level: 2
  parent_id: sec:evals
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stephen ✅
    one_sentence_summary: Measure an AI's ability to act autonomously to complete
      long-horizon, complex tasks.
    theory_of_change: By measuring how long and complex a task an AI can complete
      (its "time horizon"), we can track capability growth and identify when models
      gain dangerous autonomous capabilities (like R&D acceleration or replication).
    see_also:
    - a:evals_capability
    - '[OpenAI Preparedness](https://openai.com/index/updating-our-preparedness-framework/)'
    - '[Anthropic RSP](https://www.anthropic.com/rsp-updates)'
    orthodox_problems:
    - 'Other: none; a barometer for risk.'
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: behaviorist_science
    broad_approach_text: behaviorist science
    some_names:
    - METR
    - Thomas Kwa
    - Ben West
    - Joel Becker
    - Beth Barnes
    - Hjalmar Wijk
    - Tao Lin
    - Giulio Starace
    - Oliver Jaffe
    - Dane Sherburn
    - Sanidhya Vijayvargiya
    - Aditya Bharat Soni
    - Xuhui Zhou
    estimated_ftes: 10-50
    critiques: '[Measuring the Impact of Early-2025 AI on Experienced Open-Source
      Developer Productivity.](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/)
      [The "Length" of "Horizons"](https://www.lesswrong.com/posts/PzLSuaT6WGLQGJJJD/the-length-of-horizons)'
    funded_by: The Audacious Project, Open Philanthropy
    outputs:
    - link_url: https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/
      link_text: Measuring AI Ability to Complete Long Tasks
      original_md: '* [**Measuring AI Ability to Complete Long Tasks**](https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/),
        *Thomas Kwa, Ben West, Joel Becker et al.*, 2025-03-19, METR Blog / arXiv'
      title: Measuring AI Ability to Complete Long Tasks
      authors:
      - Thomas Kwa
      - Ben West
      - Joel Becker
      - Amy Deng
      - Katharyn Garcia
      - Max Hasin
      - Sami Jawhar
      - Megan Kinniment
      - Nate Rush
      - Sydney Von Arx
      - Ryan Bloom
      - Thomas Broadley
      - Haoxing Du
      - Brian Goodrich
      - Nikola Jurkovic
      - Luke Harold Miles
      - Seraphina Nix
      - Tao Lin
      - Neev Parikh
      - David Rein
      - Lucas Jun Koba Sato
      - Hjalmar Wijk
      - Daniel M. Ziegler
      - Elizabeth Barnes
      - Lawrence Chan
      author_organizations:
      - METR
      date: '2025-03-19'
      published_year: 2025
      venue: METR Blog
      kind: blog_post
    - link_url: https://metr.github.io/autonomy-evals-guide/gpt-5-report/
      link_text: Details about METR's evaluation of OpenAI GPT-5
      original_md: '* [**Details about METR''s evaluation of OpenAI GPT-5**](https://metr.github.io/autonomy-evals-guide/gpt-5-report/),
        *METR*, 2025-08-01, METR''s Autonomy Evaluation Resources'
      title: Details about METR's evaluation of OpenAI GPT-5
      authors: []
      author_organizations:
      - METR
      date: '2025-08-01'
      published_year: 2025
      venue: METR's Autonomy Evaluation Resources
      kind: blog_post
    - link_url: https://arxiv.org/abs/2411.15114
      link_text: 'RE-Bench: Evaluating frontier AI R&D capabilities of language model
        agents against human experts'
      original_md: '* [**RE-Bench: Evaluating frontier AI R\&D capabilities of language
        model agents against human experts**](https://arxiv.org/abs/2411.15114), *Hjalmar
        Wijk, Tao Lin, Joel Becker et al.*, 2024-11-22, arXiv'
      title: 'RE-Bench: Evaluating frontier AI R&D capabilities of language model
        agents against human experts'
      authors:
      - Hjalmar Wijk
      - Tao Lin
      - Joel Becker
      - Sami Jawhar
      - Neev Parikh
      - Thomas Broadley
      - Lawrence Chan
      - Michael Chen
      - Josh Clymer
      - Jai Dhyani
      - Elena Ericheva
      - Katharyn Garcia
      - Brian Goodrich
      - Nikola Jurkovic
      - Holden Karnofsky
      - Megan Kinniment
      - Aron Lajko
      - Seraphina Nix
      - Lucas Sato
      - William Saunders
      - Maksym Taran
      - Ben West
      - Elizabeth Barnes
      author_organizations:
      - Open Philanthropy
      - Various research institutions
      date: '2024-11-22'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.21998
      link_text: 'GSM-Agent: Understanding Agentic Reasoning Using Controllable Environments'
      original_md: '* [**GSM-Agent: Understanding Agentic Reasoning Using Controllable
        Environments**](https://arxiv.org/abs/2509.21998), *Hanlin Zhu, Tianyu Guo,
        Song Mei et al.*, 2025-09-26, arXiv'
      title: 'GSM-Agent: Understanding Agentic Reasoning Using Controllable Environments'
      authors:
      - Hanlin Zhu
      - Tianyu Guo
      - Song Mei
      - Stuart Russell
      - Nikhil Ghosh
      - Alberto Bietti
      - Jiantao Jiao
      author_organizations:
      - UC Berkeley
      date: '2025-09-26'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://t.co/XfspwlzYdl
      link_text: 'OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World
        AI Agent Safety'
      original_md: '* [**OpenAgentSafety: A Comprehensive Framework for Evaluating
        Real-World AI Agent Safety**](https://t.co/XfspwlzYdl), *Sanidhya Vijayvargiya,
        Aditya Bharat Soni, Xuhui Zhou et al.*, 2025-07-08, arXiv'
      title: 'OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World
        AI Agent Safety'
      authors:
      - Sanidhya Vijayvargiya
      - Aditya Bharat Soni
      - Xuhui Zhou
      - Zora Zhiruo Wang
      - Nouha Dziri
      - Graham Neubig
      - Maarten Sap
      author_organizations:
      - Carnegie Mellon University
      - Allen Institute for AI
      date: '2025-07-08'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://metr.github.io/autonomy-evals-guide/openai-o3-report/
      link_text: Details about METR's preliminary evaluation of OpenAI's o3 and o4-mini
      original_md: '* [**Details about METR''s preliminary evaluation of OpenAI''s
        o3 and o4-mini**](https://metr.github.io/autonomy-evals-guide/openai-o3-report/),
        2025-04-01, METR''s Autonomy Evaluation Resources'
      title: Details about METR's preliminary evaluation of OpenAI's o3 and o4-mini
      authors:
      - METR
      author_organizations:
      - METR
      date: '2025-04-01'
      published_year: 2025
      venue: METR Website
      kind: blog_post
    - link_url: https://t.co/dHN2N0tUhC
      link_text: 'PaperBench: Evaluating AI''s Ability to Replicate AI Research'
      original_md: '* [**PaperBench: Evaluating AI''s Ability to Replicate AI Research**](https://t.co/dHN2N0tUhC),
        *Giulio Starace, Oliver Jaffe, Dane Sherburn et al.*, 2025-04-02, arXiv'
      title: 'PaperBench: Evaluating AI''s Ability to Replicate AI Research'
      authors:
      - Giulio Starace
      - Oliver Jaffe
      - Dane Sherburn
      - James Aung
      - Chan Jun Shern
      - Leon Maksin
      - Rachel Dias
      - Evan Mays
      - Benjamin Kinsella
      - Wyatt Thompson
      - Johannes Heidecke
      - Mia Glaese
      - Tejal Patwardhan
      author_organizations:
      - OpenAI
      - OpenAI Preparedness Team
      date: '2025-04-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://metr.org/blog/2025-07-14-how-does-time-horizon-vary-across-domains/
      link_text: How Does Time Horizon Vary Across Domains?
      original_md: '* [**How Does Time Horizon Vary Across Domains?**](https://metr.org/blog/2025-07-14-how-does-time-horizon-vary-across-domains/),
        2025-07-14, METR Blog'
      title: How Does Time Horizon Vary Across Domains?
      authors: []
      author_organizations:
      - METR
      date: '2025-07-14'
      published_year: 2025
      venue: METR Blog
      kind: blog_post
    - link_url: https://arxiv.org/abs/2502.15840
      link_text: 'Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous
        Agents'
      original_md: '* [**Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous
        Agents**](https://arxiv.org/abs/2502.15840), *Axel Backlund, Lukas Petersson*,
        2025-02-20, arXiv'
      title: 'Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous Agents'
      authors:
      - Axel Backlund
      - Lukas Petersson
      author_organizations: []
      date: '2025-02-20'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.15850
      link_text: Forecasting Frontier Language Model Agent Capabilities
      original_md: '* [**Forecasting Frontier Language Model Agent Capabilities**](https://arxiv.org/abs/2502.15850),
        *Govind Pimpale, Axel Højmark, Jérémy Scheurer et al.*, 2025-02-21, arXiv'
      title: Forecasting Frontier Language Model Agent Capabilities
      authors:
      - Govind Pimpale
      - Axel Højmark
      - Jérémy Scheurer
      - Marius Hobbhahn
      author_organizations: []
      date: '2025-02-21'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.anthropic.com/research/project-vend-1
      link_text: 'Project Vend: Can Claude run a small shop? (And why does that matter?)'
      original_md: '* [**Project Vend: Can Claude run a small shop? (And why does
        that matter?)**](https://www.anthropic.com/research/project-vend-1), 2025-06-27,
        Anthropic Blog'
      title: 'Project Vend: Can Claude run a small shop? (And why does that matter?)'
      authors: []
      author_organizations:
      - Anthropic
      - Andon Labs
      date: '2025-06-27'
      published_year: 2025
      venue: Anthropic Website
      kind: blog_post
    - link_url: https://arxiv.org/abs/2506.14866
      link_text: 'OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents'
      original_md: '* [**OS-Harm: A Benchmark for Measuring Safety of Computer Use
        Agents**](https://arxiv.org/abs/2506.14866), *Thomas Kuntz, Agatha Duzan,
        Hao Zhao et al.*, 2025-06-17, arXiv'
      title: 'OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents'
      authors:
      - Thomas Kuntz
      - Agatha Duzan
      - Hao Zhao
      - Francesco Croce
      - Zico Kolter
      - Nicolas Flammarion
      - Maksym Andriushchenko
      author_organizations:
      - EPFL
      date: '2025-06-17'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://fulcrumresearch.ai/2025/10/22/introducing-orchestra-quibbler.html
      link_text: Fulcrum
      original_md: '* [Fulcrum](https://fulcrumresearch.ai/2025/10/22/introducing-orchestra-quibbler.html).'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    other_attributes: {}
  parsing_issues:
  - 'Orthodox problems field says ''none; a barometer for risk.'' - no matching problem
    found, kept as ''Other: none; a barometer for risk.'''
- id: a:evals_wmd
  name: WMD evals (Weapons of Mass Destruction)
  header_level: 2
  parent_id: sec:evals
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stephen ✅
    one_sentence_summary: Evaluate whether AI models possess dangerous knowledge or
      capabilities related to biological and chemical weapons, such as biosecurity
      or chemical synthesis.
    theory_of_change: By benchmarking and tracking AI's knowledge of biology and chemistry,
      we can identify when models become capable of accelerating WMD development or
      misuse, allowing for timely intervention.
    see_also:
    - a:evals_capability
    - a:evals_autonomy
    - a:various_redteams
    orthodox_problems:
    - 'Other: malicious actors using AI'
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: behaviorist_science
    broad_approach_text: behaviorist science
    some_names:
    - Lennart Justen
    - Haochen Zhao
    - Xiangru Tang
    - Ziran Yang
    - Aidan Peppin
    - Anka Reuel
    - Stephen Casper
    estimated_ftes: 10-50
    critiques: '[The Reality of AI and Biorisk](https://arxiv.org/abs/2412.01946)'
    funded_by: Open Philanthropy, UK AI Safety Institute (AISI), frontier labs, Scale
      AI, various academic institutions (Peking University, Yale, etc.), Meta
    outputs:
    - link_url: https://arxiv.org/abs/2504.16137
      link_text: 'Virology Capabilities Test (VCT): A Multimodal Virology Q&A Benchmark'
      original_md: '* [**Virology Capabilities Test (VCT): A Multimodal Virology Q\&A
        Benchmark**](https://arxiv.org/abs/2504.16137)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://arxiv.org/abs/2505.06108
      link_text: LLMs Outperform Experts on Challenging Biology Benchmarks
      original_md: '* [**LLMs Outperform Experts on Challenging Biology Benchmarks**](https://arxiv.org/abs/2505.06108),
        *Lennart Justen*, 2025-05-09, arXiv'
      title: LLMs Outperform Experts on Challenging Biology Benchmarks
      authors:
      - Lennart Justen
      author_organizations: []
      date: '2025-05-09'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.16736
      link_text: 'ChemSafetyBench: Benchmarking LLM Safety on Chemistry Domain'
      original_md: '* [**ChemSafetyBench: Benchmarking LLM Safety on Chemistry Domain**](https://arxiv.org/abs/2411.16736),
        *Haochen Zhao, Xiangru Tang, Ziran Yang et al.*, 2024-11-23, arXiv'
      title: 'ChemSafetyBench: Benchmarking LLM Safety on Chemistry Domain'
      authors:
      - Haochen Zhao
      - Xiangru Tang
      - Ziran Yang
      - Xiao Han
      - Xuanzhi Feng
      - Yueqing Fan
      - Senhao Cheng
      - Di Jin
      - Yilun Zhao
      - Arman Cohan
      - Mark Gerstein
      author_organizations:
      - Yale University
      date: '2024-11-23'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2412.01946
      link_text: The Reality of AI and Biorisk
      original_md: '* [**The Reality of AI and Biorisk**](https://arxiv.org/abs/2412.01946),
        *Aidan Peppin, Anka Reuel, Stephen Casper et al.*, 2024-12-02, arXiv'
      title: The Reality of AI and Biorisk
      authors:
      - Aidan Peppin
      - Anka Reuel
      - Stephen Casper
      - Elliot Jones
      - Andrew Strait
      - Usman Anwar
      - Anurag Agrawal
      - Sayash Kapoor
      - Sanmi Koyejo
      - Marie Pellat
      - Rishi Bommasani
      - Nick Frosst
      - Sara Hooker
      author_organizations: []
      date: '2024-12-02'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.11544
      link_text: 'The Safety Gap Toolkit: Evaluating Hidden Dangers of Open-Source
        Models'
      original_md: '* [**The Safety Gap Toolkit: Evaluating Hidden Dangers of Open-Source
        Models**](https://arxiv.org/abs/2507.11544), *Ann-Kathrin Dombrowski, Dillon
        Bowen, Adam Gleave et al.*, 2025-07-08, arXiv'
      title: 'The Safety Gap Toolkit: Evaluating Hidden Dangers of Open-Source Models'
      authors:
      - Ann-Kathrin Dombrowski
      - Dillon Bowen
      - Adam Gleave
      - Chris Cundy
      author_organizations:
      - Alignment Research
      date: '2025-07-08'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.27629
      link_text: Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation
        Models
      original_md: '* [**Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation
        Models**](https://arxiv.org/abs/2510.27629), *Boyi Wei, Zora Che, Nathaniel
        Li et al.*, 2025-10-31, arXiv'
      title: Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation
        Models
      authors:
      - Boyi Wei
      - Zora Che
      - Nathaniel Li
      - Udari Madhushani Sehwag
      - Jasper Götting
      - Samira Nedungadi
      - Julian Michael
      - Summer Yue
      - Dan Hendrycks
      - Peter Henderson
      - Zifan Wang
      - Seth Donoughe
      - Mantas Mazeika
      author_organizations:
      - Anthropic
      - Redwood Research
      date: '2025-10-31'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues:
  - 'Orthodox problems field says ''malicious actors using AI'' - no matching standard
    problem found, kept as ''Other: malicious actors using AI'''
  - Broad approach field says 'behavioral' - mapped to 'behaviorist_science' with
    text 'behaviorist science'
- id: a:evals_situational_awareness
  name: Situational awareness and self-awareness evals
  header_level: 2
  parent_id: sec:evals
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stephen ✅
    one_sentence_summary: Evaluate if models understand their own internal states
      and behaviors, their environment, and whether they are in a test or real-world
      deployment.
    theory_of_change: if an AI can distinguish between evaluation and deployment ("evaluation
      awareness"), it might hide dangerous capabilities (scheming/sandbagging). By
      measuring self- and situational-awareness, we can better assess this risk and
      build more robust evaluations.
    see_also:
    - a:evals_sandbagging
    - a:various_redteams
    - sec:model_psychology
    orthodox_problems:
    - superintelligence_fool_supervisors
    - superintelligence_hack_software
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: behaviorist_science
    broad_approach_text: behaviorist science
    some_names:
    - Jan Betley
    - Xuchan Bao
    - Martín Soto
    - Mary Phuong
    - Roland S. Zimmermann
    - Joe Needham
    - Giles Edkins
    - Govind Pimpale
    - Kai Fronsdal
    - David Lindner
    - Lang Xiong
    - Xiaoyan Bai
    estimated_ftes: 30-70
    critiques: '[Lessons from a Chimp: AI "Scheming" and the Quest for Ape Language](https://arxiv.org/abs/2507.03409),
      [It''s hard to make scheming evals look realistic for LLMs](https://www.lesswrong.com/posts/TBk2dbWkg2F7dB3jb/it-s-hard-to-make-scheming-evals-look-realistic-for-llms)'
    funded_by: frontier labs (Google DeepMind, Anthropic), Open Philanthropy, The
      Audacious Project, UK AI Safety Institute (AISI), AI Safety Support, Apollo
      Research, METR
    outputs:
    - link_url: https://arxiv.org/abs/2501.11120
      link_text: 'Tell me about yourself: LLMs are aware of their learned behaviors'
      original_md: '* [**Tell me about yourself: LLMs are aware of their learned behaviors**](https://arxiv.org/abs/2501.11120),
        *Jan Betley, Xuchan Bao, Martín Soto et al.*, 2025-01-19, arXiv'
      title: 'Tell me about yourself: LLMs are aware of their learned behaviors'
      authors:
      - Jan Betley
      - Xuchan Bao
      - Martín Soto
      - Anna Sztyber-Betley
      - James Chua
      - Owain Evans
      author_organizations:
      - University of Oxford
      date: '2025-01-19'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/pdf/2407.04108
      link_text: Future Events as Backdoor Triggers
      original_md: '* [**Future Events as Backdoor Triggers**](https://arxiv.org/pdf/2407.04108)'
      title: 'Future Events as Backdoor Triggers: Investigating Temporal Vulnerabilities
        in LLMs'
      authors:
      - Sara Price
      - Arjun Panickssery
      - Sam Bowman
      - Asa Cooper Stickland
      author_organizations: []
      date: '2024-07-04'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.01420
      link_text: Evaluating Frontier Models for Stealth and Situational Awareness
      original_md: '* [**Evaluating Frontier Models for Stealth and Situational Awareness**](https://arxiv.org/abs/2505.01420),
        *Mary Phuong, Roland S. Zimmermann, Ziyue Wang et al.*, 2025-05-02, arXiv'
      title: Evaluating Frontier Models for Stealth and Situational Awareness
      authors:
      - Mary Phuong
      - Roland S. Zimmermann
      - Ziyue Wang
      - David Lindner
      - Victoria Krakovna
      - Sarah Cogan
      - Allan Dafoe
      - Lewis Ho
      - Rohin Shah
      author_organizations:
      - Google DeepMind
      date: '2025-05-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.23836
      link_text: Large Language Models Often Know When They Are Being Evaluated
      original_md: '* [**Large Language Models Often Know When They Are Being Evaluated**](https://arxiv.org/abs/2505.23836),
        *Joe Needham, Giles Edkins, Govind Pimpale et al.*, 2025-05-28, arXiv'
      title: Large Language Models Often Know When They Are Being Evaluated
      authors:
      - Joe Needham
      - Giles Edkins
      - Govind Pimpale
      - Henning Bartsch
      - Marius Hobbhahn
      author_organizations: []
      date: '2025-05-28'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.00591
      link_text: 'Probe-Rewrite-Evaluate: A Workflow for Reliable Benchmarks and Quantifying
        Evaluation Awareness'
      original_md: '* [**Probe-Rewrite-Evaluate: A Workflow for Reliable Benchmarks
        and Quantifying Evaluation Awareness**](https://arxiv.org/abs/2509.00591),
        *Lang Xiong, Nishant Bhargava, Jianhang Hong et al.*, 2025-08-30, arXiv'
      title: 'Probe-Rewrite-Evaluate: A Workflow for Reliable Benchmarks and Quantifying
        Evaluation Awareness'
      authors:
      - Lang Xiong
      - Nishant Bhargava
      - Jianhang Hong
      - Jeremy Chang
      - Haihao Liu
      - Vasu Sharma
      - Kevin Zhu
      author_organizations: []
      date: '2025-08-30'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.03399
      link_text: Know Thyself? On the Incapability and Implications of AI Self-Recognition
      original_md: '* [**Know Thyself? On the Incapability and Implications of AI
        Self-Recognition**](https://arxiv.org/abs/2510.03399), *Xiaoyan Bai, Aryan
        Shrivastava, Ari Holtzman et al.*, 2025-10-03, arXiv'
      title: Know Thyself? On the Incapability and Implications of AI Self-Recognition
      authors:
      - Xiaoyan Bai
      - Aryan Shrivastava
      - Ari Holtzman
      - Chenhao Tan
      author_organizations: []
      date: '2025-10-03'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.antischeming.ai/snippets
      link_text: Chain-of-Thought Snippets — Anti-Scheming
      original_md: '* [**Chain-of-Thought Snippets — Anti-Scheming**](https://www.antischeming.ai/snippets),
        antischeming.ai'
      title: Chain-of-Thought Snippets — Anti-Scheming
      authors: []
      author_organizations:
      - Apollo Research
      - OpenAI
      date: null
      published_year: null
      venue: antischeming.ai
      kind: other
    - link_url: https://arxiv.org/pdf/2504.20084
      link_text: AI Awareness (literature review)
      original_md: '* [**AI Awareness (literature review)**](https://arxiv.org/pdf/2504.20084)'
      title: AI Awareness
      authors:
      - Xiaojian Li
      - Haoyuan Shi
      - Rongwu Xu
      - Wei Xu
      author_organizations: []
      date: '2025-04-25'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/9tHEibBBhQCHEyFsa/do-llms-know-what-they-re-capable-of-why-this-matters-for-ai
      link_text: Do LLMs know what they're capable of? Why this matters for AI safety,
        and initial findings
      original_md: '* [**Do LLMs know what they''re capable of? Why this matters for
        AI safety, and initial findings**](https://lesswrong.com/posts/9tHEibBBhQCHEyFsa/do-llms-know-what-they-re-capable-of-why-this-matters-for-ai),
        *Casey Barkan, Sid Black, Oliver Sourbut*, 2025-07-13, LessWrong / AI Alignment
        Forum'
      title: Do LLMs know what they're capable of? Why this matters for AI safety,
        and initial findings
      authors:
      - Casey Barkan
      - Sid Black
      - Oliver Sourbut
      author_organizations:
      - MATS Program
      date: '2025-07-13'
      published_year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/E3daBewppAiECN3Ao/claude-sonnet-3-7-often-knows-when-it-s-in-alignment
      link_text: Claude Sonnet 3.7 (often) knows when it's in alignment evaluations
      original_md: '* [**Claude Sonnet 3.7 (often) knows when it''s in alignment evaluations**](https://lesswrong.com/posts/E3daBewppAiECN3Ao/claude-sonnet-3-7-often-knows-when-it-s-in-alignment),
        *Nicholas Goldowsky-Dill, Mikita Balesni, Jérémy Scheurer et al.*, 2025-03-17,
        LessWrong'
      title: Claude Sonnet 3.7 (often) knows when it's in alignment evaluations
      authors:
      - Nicholas Goldowsky-Dill
      - Mikita Balesni
      - Jérémy Scheurer
      - Marius Hobbhahn
      author_organizations:
      - Apollo Research
      date: '2025-03-17'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/TBk2dbWkg2F7dB3jb/it-s-hard-to-make-scheming-evals-look-realistic-for-llms
      link_text: It's hard to make scheming evals look realistic for LLMs
      original_md: '* [**It''s hard to make scheming evals look realistic for LLMs**](https://lesswrong.com/posts/TBk2dbWkg2F7dB3jb/it-s-hard-to-make-scheming-evals-look-realistic-for-llms),
        *Igor Ivanov, Danil Kadochnikov*, 2025-05-24, LessWrong'
      title: It's hard to make scheming evals look realistic for LLMs
      authors:
      - Igor Ivanov
      - Danil Kadochnikov
      author_organizations: []
      date: '2025-05-24'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    other_attributes: {}
  parsing_issues: []
- id: a:evals_steganography
  name: Steganography evals
  header_level: 2
  parent_id: sec:evals
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stephen ✅
    one_sentence_summary: evaluate whether models can hide secret information or encoded
      reasoning in their outputs, such as in chain-of-thought scratchpads, to evade
      monitoring.
    theory_of_change: if models can use steganography, they could hide deceptive reasoning,
      bypassing safety monitoring and control measures. By evaluating this capability,
      we can assess the risk of a model fooling its supervisors.
    see_also:
    - a:ai_deception
    - a:cot_monitoring
    orthodox_problems:
    - boxed_agi_exfiltrate
    - superintelligence_fool_supervisors
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: behaviorist_science
    broad_approach_text: behaviorist science
    some_names:
    - Antonio Norelli
    - Michael Bronstein
    estimated_ftes: 1-10
    critiques: 'Chain-of-Thought Is Already Unfaithful (So Steganography is Irrelevant):
      [Reasoning Models Don''t Always Say What They Think.](https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf)'
    funded_by: Anthropic (and its general funders, e.g., Google, Amazon)
    outputs:
    - link_url: https://arxiv.org/abs/2510.20075
      link_text: LLMs can hide text in other text of the same length
      original_md: '* [**LLMs can hide text in other text of the same length**](https://arxiv.org/abs/2510.20075),
        *Antonio Norelli, Michael Bronstein*, 2025-10-27, arXiv'
      title: LLMs can hide text in other text of the same length
      authors:
      - Antonio Norelli
      - Michael Bronstein
      author_organizations: []
      date: '2025-10-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://alignment.anthropic.com/2025/distill-paraphrases/
      link_text: Do reasoning models use their scratchpad like we do? Evidence from
        distilling paraphrases
      original_md: '* [**Do reasoning models use their scratchpad like we do? Evidence
        from distilling paraphrases**](https://alignment.anthropic.com/2025/distill-paraphrases/),
        2025, Anthropic Alignment Science Blog'
      title: Do reasoning models use their scratchpad like we do? Evidence from distilling
        paraphrases
      authors: []
      author_organizations:
      - Anthropic
      date: null
      published_year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - link_url: https://arxiv.org/abs/2507.02737
      link_text: Early Signs of Steganographic Capabilities in Frontier LLMs
      original_md: '* [**Early Signs of Steganographic Capabilities in Frontier LLMs**](https://arxiv.org/abs/2507.02737)'
      title: Early Signs of Steganographic Capabilities in Frontier LLMs
      authors:
      - Artur Zolkowski
      - Kei Nishimura-Gasparian
      - Robert McCarthy
      - Roland S. Zimmermann
      - David Lindner
      author_organizations: []
      date: '2025-07-03'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.01926
      link_text: Large language models can learn and generalize steganographic chain-of-thought
        under process supervision
      original_md: '* [**Large language models can learn and generalize steganographic
        chain-of-thought under process supervision**](https://arxiv.org/abs/2506.01926)'
      title: Large language models can learn and generalize steganographic chain-of-thought
        under process supervision
      authors:
      - Joey Skaf
      - Luis Ibanez-Lissen
      - Robert McCarthy
      - Connor Watts
      - Vasil Georgiv
      - Hannes Whittingham
      - Lorena Gonzalez-Manzano
      - David Lindner
      - Cameron Tice
      - Edward James Young
      - Puria Radmard
      author_organizations: []
      date: '2025-06-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.14805
      link_text: 'Subliminal Learning: Language models transmit behavioral traits
        via hidden signals in data'
      original_md: '* [**Subliminal Learning: Language models transmit behavioral
        traits via hidden signals in data**](https://arxiv.org/abs/2507.14805), *Alex
        Cloud, Minh Le, James Chua et al.*, 2025-07-20, arXiv'
      title: 'Subliminal Learning: Language models transmit behavioral traits via
        hidden signals in data'
      authors:
      - Alex Cloud
      - Minh Le
      - James Chua
      - Jan Betley
      - Anna Sztyber-Betley
      - Jacob Hilton
      - Samuel Marks
      - Owain Evans
      author_organizations: []
      date: '2025-07-20'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:ai_deception
  name: AI deception evals
  header_level: 2
  parent_id: sec:evals
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stephen ✅
    one_sentence_summary: research demonstrating that AI models, particularly agentic
      ones, can learn and execute deceptive behaviors such as alignment faking, manipulation,
      and sandbagging.
    theory_of_change: proactively discover, evaluate, and understand the mechanisms
      of AI deception (e.g., alignment faking, manipulation, agentic deception) to
      prevent models from fooling human supervisors and causing harm.
    see_also:
    - a:evals_situational_awareness
    - a:evals_steganography
    - a:evals_sandbagging
    - a:cot_monitoring
    orthodox_problems:
    - superintelligence_fool_supervisors
    - superintelligence_hack_software
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: null
    broad_approach_text: behaviorist science / engineering
    some_names:
    - Cadenza
    - Fred Heiding
    - Simon Lermen
    - Andrew Kao
    - Myra Cheng
    - Cinoo Lee
    - Pranav Khadpe
    - Satyapriya Krishna
    - Andy Zou
    - Rahul Gupta
    estimated_ftes: 30-80
    critiques: A central criticism is that the evaluation scenarios are "artificial
      and contrived". [the void](https://nostalgebraist.tumblr.com/post/785766737747574784/the-void)
      and [Lessons from a Chimp](https://arxiv.org/abs/2507.03409) argue this research
      is "overattributing human traits" to models.
    funded_by: Lab funders (Anthropic, OpenAI), academic institutions (e.g., Harvard,
      CMU, Barcelona Institute of Science and Technology), NSFC, ML Alignment Theory
      & Scholars (MATS) Program, FAR AI
    outputs:
    - link_url: https://arxiv.org/abs/2511.16035
      link_text: 'Liars'' Bench: Evaluating Lie Detectors for Language Models'
      original_md: '* [**Liars'' Bench: Evaluating Lie Detectors for Language Models**](https://arxiv.org/abs/2511.16035)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://huggingface.co/datasets/cais/MASK
      link_text: The MASK Evaluation
      original_md: '* [**The MASK Evaluation**](https://huggingface.co/datasets/cais/MASK),
        Hugging Face'
      title: The MASK Evaluation
      authors: []
      author_organizations:
      - Center for AI Safety
      - Scale AI
      date: null
      published_year: null
      venue: Hugging Face
      kind: dataset_benchmark
    - link_url: https://alignment.anthropic.com/2025/alignment-faking-revisited/
      link_text: 'Alignment Faking Revisited: Improved Classifiers and Open Source
        Extensions'
      original_md: '* [**Alignment Faking Revisited: Improved Classifiers and Open
        Source Extensions**](https://alignment.anthropic.com/2025/alignment-faking-revisited/),
        *John Hughes, Abhay Sheshadr*, 2025, Anthropic Alignment Science Blog'
      title: 'Alignment Faking Revisited: Improved Classifiers and Open Source Extensions'
      authors:
      - John Hughes
      - Abhay Sheshadr
      author_organizations:
      - MATS
      - Anthropic
      date: null
      published_year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - link_url: https://arxiv.org/pdf/2510.15501
      link_text: 'DECEPTIONBENCH: A Comprehensive Benchmark for AI Deception Behaviors
        in Real-world Scenario'
      original_md: '* [**DECEPTIONBENCH: A Comprehensive Benchmark for AI Deception
        Behaviors in Real-world Scenario**](https://arxiv.org/pdf/2510.15501)'
      title: 'DeceptionBench: A Comprehensive Benchmark for AI Deception Behaviors
        in Real-world Scenarios'
      authors:
      - Yao Huang
      - Yitong Sun
      - Yichi Zhang
      - Ruochen Zhang
      - Yinpeng Dong
      - Xingxing Wei
      author_organizations: []
      date: '2025-10-17'
      published_year: 2025
      venue: NeurIPS 2025
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.04072
      link_text: 'Among Us: A Sandbox for Measuring and Detecting Agentic Deception'
      original_md: '* [**Among Us: A Sandbox for Measuring and Detecting Agentic Deception**](https://arxiv.org/abs/2504.04072)'
      title: 'Among Us: A Sandbox for Measuring and Detecting Agentic Deception'
      authors:
      - Satvik Golechha
      - Adrià Garriga-Alonso
      author_organizations: []
      date: '2025-04-05'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2412.00586
      link_text: 'Evaluating Large Language Models'' Capability to Launch Fully Automated
        Spear Phishing Campaigns: Validated on Human Subjects'
      original_md: '* [**Evaluating Large Language Models'' Capability to Launch Fully
        Automated Spear Phishing Campaigns: Validated on Human Subjects**](https://arxiv.org/abs/2412.00586),
        *Fred Heiding, Simon Lermen, Andrew Kao et al.*, 2024-11-30, arXiv'
      title: 'Evaluating Large Language Models'' Capability to Launch Fully Automated
        Spear Phishing Campaigns: Validated on Human Subjects'
      authors:
      - Fred Heiding
      - Simon Lermen
      - Andrew Kao
      - Bruce Schneier
      - Arun Vishwanath
      author_organizations: []
      date: '2024-11-30'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.17938
      link_text: 'D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language
        Models'
      original_md: '* [**D-REX: A Benchmark for Detecting Deceptive Reasoning in Large
        Language Models**](https://arxiv.org/abs/2509.17938), *Satyapriya Krishna,
        Andy Zou, Rahul Gupta et al.*, 2025-09-22, arXiv'
      title: 'D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language
        Models'
      authors:
      - Satyapriya Krishna
      - Andy Zou
      - Rahul Gupta
      - Eliot Krzysztof Jones
      - Nick Winter
      - Dan Hendrycks
      - J. Zico Kolter
      - Matt Fredrikson
      - Spyros Matsoukas
      author_organizations:
      - Carnegie Mellon University
      - Center for AI Safety
      - Anthropic
      date: '2025-09-22'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/pdf/2506.18032
      link_text: Why Do Some Language Models Fake Alignment While Others Don't?
      original_md: '* [**Why Do Some Language Models Fake Alignment While Others Don''t?**](https://arxiv.org/pdf/2506.18032)'
      title: Why Do Some Language Models Fake Alignment While Others Don't?
      authors:
      - Abhay Sheshadri
      - John Hughes
      - Julian Michael
      - Alex Mallen
      - Arun Jose
      - Janus
      - Fabien Roger
      author_organizations: []
      date: '2025-06-22'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2412.04984
      link_text: Frontier Models are Capable of In-context Scheming
      original_md: '* [**Frontier Models are Capable of In-context Scheming**](https://arxiv.org/abs/2412.04984),
        *Alexander Meinke, Bronson Schoen, Jérémy Scheurer et al.*, 2024-12-06, arXiv'
      title: Frontier Models are Capable of In-context Scheming
      authors:
      - Alexander Meinke
      - Bronson Schoen
      - Jérémy Scheurer
      - Mikita Balesni
      - Rusheb Shah
      - Marius Hobbhahn
      author_organizations:
      - Apollo Research
      date: '2024-12-06'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.14318
      link_text: Evaluating & Reducing Deceptive Dialogue From Language Models with
        Multi-turn RL
      original_md: '* [**Evaluating & Reducing Deceptive Dialogue From Language Models
        with Multi-turn RL**](https://arxiv.org/abs/2510.14318), *Marwa Abdulhai,
        Ryan Cheng, Aryansh Shrivastava et al.*, 2025-10-16, arXiv'
      title: Evaluating & Reducing Deceptive Dialogue From Language Models with Multi-turn
        RL
      authors:
      - Marwa Abdulhai
      - Ryan Cheng
      - Aryansh Shrivastava
      - Natasha Jaques
      - Yarin Gal
      - Sergey Levine
      author_organizations:
      - University of Oxford
      - UC Berkeley
      date: '2025-10-16'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.01070
      link_text: Eliciting Secret Knowledge from Language Models
      original_md: '* [**Eliciting Secret Knowledge from Language Models**](https://arxiv.org/abs/2510.01070),
        *Bartosz Cywiński, Emil Ryd, Rowan Wang et al.*, 2025-10-01, arXiv'
      title: Eliciting Secret Knowledge from Language Models
      authors:
      - Bartosz Cywiński
      - Emil Ryd
      - Rowan Wang
      - Senthooran Rajamanoharan
      - Neel Nanda
      - Arthur Conmy
      - Samuel Marks
      author_organizations: []
      date: '2025-10-01'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/bqWihHtDnDseyfF2T/edge-cases-in-ai-alignment-2
      link_text: Edge Cases in AI Alignment
      original_md: '* [**Edge Cases in AI Alignment**](https://lesswrong.com/posts/bqWihHtDnDseyfF2T/edge-cases-in-ai-alignment-2),
        *Florian Dietz*, 2025-03-24, LessWrong'
      title: Edge Cases in AI Alignment
      authors:
      - Florian Dietz
      author_organizations:
      - MATS Program
      date: '2025-03-24'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/pCMmLiBcHbKohQgwA/i-replicated-the-anthropic-alignment-faking-experiment-on
      link_text: I replicated the Anthropic alignment faking experiment on other models,
        and they didn't fake alignment
      original_md: '* [**I replicated the Anthropic alignment faking experiment on
        other models, and they didn''t fake alignment**](https://lesswrong.com/posts/pCMmLiBcHbKohQgwA/i-replicated-the-anthropic-alignment-faking-experiment-on),
        *Aleksandr Kedrik, Igor Ivanov*, 2025-05-30, LessWrong'
      title: I replicated the Anthropic alignment faking experiment on other models,
        and they didn't fake alignment
      authors:
      - Aleksandr Kedrik
      - Igor Ivanov
      author_organizations: []
      date: '2025-05-30'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/kCGk5tp5suHoGwhCa/mistral-large-2-123b-seems-to-exhibit-alignment-faking
      link_text: Mistral Large 2 (123B) seems to exhibit alignment faking
      original_md: '* [**Mistral Large 2 (123B) seems to exhibit alignment faking**](https://lesswrong.com/posts/kCGk5tp5suHoGwhCa/mistral-large-2-123b-seems-to-exhibit-alignment-faking),
        *Marc Carauleanu, Diogo de Lucena, Gunnar Zarncke et al.*, 2025-03-27, LessWrong/AI
        Alignment Forum'
      title: Mistral Large 2 (123B) seems to exhibit alignment faking
      authors:
      - Marc Carauleanu
      - Diogo de Lucena
      - Gunnar Zarncke
      - Cameron Berg
      - Judd Rosenblatt
      - Mike Vaiana
      - Trent Hodgeson
      author_organizations:
      - AE Studio
      date: '2025-03-27'
      published_year: 2025
      venue: LessWrong/AI Alignment Forum
      kind: lesswrong
    other_attributes: {}
  parsing_issues:
  - Broad approach field contains multiple approaches ('behavioural / engineering')
    - broad_approach_id left null
- id: a:evals_sandbagging
  name: Sandbagging evals
  header_level: 2
  parent_id: sec:evals
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stephen ✅
    one_sentence_summary: Evaluate whether AI models deliberately hide their true
      capabilities or underperform, especially when they detect they are in an evaluation
      context.
    theory_of_change: If models can distinguish between evaluation and deployment
      contexts ("evaluation awareness"), they might learn to "sandbag" or deliberately
      underperform to hide dangerous capabilities, fooling safety evaluations. By
      developing evaluations for sandbagging, we can test whether our safety methods
      are being deceived and detect this behavior before a model is deployed.
    see_also:
    - a:ai_deception
    - a:evals_situational_awareness
    - a:various_redteams
    orthodox_problems:
    - superintelligence_fool_supervisors
    - superintelligence_hack_software
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: behaviorist_science
    broad_approach_text: behaviorist science
    some_names:
    - Teun van der Weij
    - Cameron Tice
    - Chloe Li
    - Johannes Gasteiger
    - Joseph Bloom
    - Joel Dyer
    estimated_ftes: 10-50
    critiques: The main external critique, from sources like "[the void](https://nostalgebraist.tumblr.com/post/785766737747574784/the-void)"
      and "[Lessons from a Chimp](https://arxiv.org/abs/2507.03409)", is that this
      research "overattribut\[es\] human traits" to models. It argues that what's
      being measured isn't genuine sandbagging but models "playing-along-with-drama
      behaviour" in response to "artificial and contrived" evals.
    funded_by: Anthropic (and its funders, e.g., Google, Amazon), UK Government (funding
      the AI Security Institute)
    outputs:
    - link_url: https://arxiv.org/abs/2509.18058
      link_text: Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier
        LLMs
      original_md: '* [**Strategic Dishonesty Can Undermine AI Safety Evaluations
        of Frontier LLMs**](https://arxiv.org/abs/2509.18058)'
      title: Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier
        LLMs
      authors:
      - Alexander Panfilov
      - Evgenii Kortukov
      - Kristina Nikolić
      - Matthias Bethge
      - Sebastian Lapuschkin
      - Wojciech Samek
      - Ameya Prabhu
      - Maksym Andriushchenko
      - Jonas Geiping
      author_organizations:
      - University of Tübingen
      - Fraunhofer HHI
      - ELLIS Institute
      date: '2025-09-23'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2406.07358
      link_text: 'AI Sandbagging: Language Models can Strategically Underperform on
        Evaluations'
      original_md: '* [**AI Sandbagging: Language Models can Strategically Underperform
        on Evaluations**](https://arxiv.org/abs/2406.07358)'
      title: 'AI Sandbagging: Language Models can Strategically Underperform on Evaluations'
      authors:
      - Teun van der Weij
      - Felix Hofstätter
      - Ollie Jaffe
      - Samuel F. Brown
      - Francis Rhys Ward
      author_organizations: []
      date: '2024-06-11'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/pdf/2412.01784
      link_text: Noise Injection Reveals Hidden Capabilities of Sandbagging Language
        Models
      original_md: '* [**Noise Injection Reveals Hidden Capabilities of Sandbagging
        Language Models**](https://arxiv.org/pdf/2412.01784)'
      title: Noise Injection Reveals Hidden Capabilities of Sandbagging Language Models
      authors:
      - Cameron Tice
      - Philipp Alexander Kreer
      - Nathan Helm-Burger
      - Prithviraj Singh Shahani
      - Fedor Ryzhenkov
      - Jacob Haimes
      - Felix Hofstätter
      - Teun van der Weij
      author_organizations: []
      date: '2024-12-02'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/pdf/2509.26239
      link_text: Sandbagging in a Simple Survival Bandit Problem
      original_md: '* [**Sandbagging in a Simple Survival Bandit Problem**](https://arxiv.org/pdf/2509.26239)'
      title: Sandbagging in a Simple Survival Bandit Problem
      authors:
      - Joel Dyer
      - Daniel Jarne Ornia
      - Nicholas Bishop
      - Anisoara Calinescu
      - Michael Wooldridge
      author_organizations: []
      date: '2025-09-30'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://alignment.anthropic.com/2025/automated-researchers-sandbag/
      link_text: Automated Researchers Can Subtly Sandbag
      original_md: '* [**Automated Researchers Can Subtly Sandbag**](https://alignment.anthropic.com/2025/automated-researchers-sandbag/),
        *Johannes Gasteiger, Vladimir Mikulik, Ethan Perez et al.*, 2025, Alignment
        Science Blog'
      title: Automated Researchers Can Subtly Sandbag
      authors:
      - Johannes Gasteiger
      - Vladimir Mikulik
      - Ethan Perez
      - Fabien Roger
      - Misha Wagner
      - Akbir Khan
      - Sam Bowman
      - Jan Leike
      author_organizations:
      - Anthropic
      date: null
      published_year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - link_url: https://www.alignmentforum.org/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging
      link_text: White Box Control at UK AISI - Update on Sandbagging Investigations
      original_md: '* [**White Box Control at UK AISI \- Update on Sandbagging Investigations**](https://www.alignmentforum.org/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging),
        *Joseph Bloom, Jordan Taylor, Connor Kissane et al.*, 2025-07-10, AI Alignment
        Forum'
      title: White Box Control at UK AISI - Update on Sandbagging Investigations
      authors:
      - Joseph Bloom
      - Jordan Taylor
      - Connor Kissane
      - Sid Black
      - merizian
      - alexdzm
      - jacoba
      - Ben Millwood
      - Alan Cooney
      author_organizations:
      - UK AISI
      date: '2025-07-10'
      published_year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - link_url: https://alignment.anthropic.com/2025/wont-vs-cant/
      link_text: 'Won''t vs. Can''t: Sandbagging-like Behavior from Claude Models'
      original_md: '* [**Won''t vs. Can''t: Sandbagging-like Behavior from Claude
        Models**](https://alignment.anthropic.com/2025/wont-vs-cant/), 2025, Anthropic
        Alignment Science Blog'
      title: 'Won''t vs. Can''t: Sandbagging-like Behavior from Claude Models'
      authors: []
      author_organizations:
      - Anthropic
      date: '2025-01-15'
      published_year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - link_url: https://arxiv.org/abs/2508.00943
      link_text: LLMs Can Covertly Sandbag on Capability Evaluations Against Chain-of-Thought
        Monitoring
      original_md: '* [**LLMs Can Covertly Sandbag on Capability Evaluations Against
        Chain-of-Thought Monitoring**](https://arxiv.org/abs/2508.00943), *Chloe Li,
        Mary Phuong, Noah Y. Siegel*, 2025-07-31, arXiv'
      title: LLMs Can Covertly Sandbag on Capability Evaluations Against Chain-of-Thought
        Monitoring
      authors:
      - Chloe Li
      - Mary Phuong
      - Noah Y. Siegel
      author_organizations:
      - DeepMind
      date: '2025-07-31'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/TeTegzR8X5CuKgMc3/misalignment-and-strategic-underperformance-an-analysis-of
      link_text: 'Misalignment and Strategic Underperformance: An Analysis of Sandbagging
        and Exploration Hacking'
      original_md: '* [**Misalignment and Strategic Underperformance: An Analysis
        of Sandbagging and Exploration Hacking**](https://lesswrong.com/posts/TeTegzR8X5CuKgMc3/misalignment-and-strategic-underperformance-an-analysis-of),
        *Buck Shlegeris, Julian Stastny*, 2025-05-08, LessWrong / AI Alignment Forum'
      title: 'Misalignment and Strategic Underperformance: An Analysis of Sandbagging
        and Exploration Hacking'
      authors:
      - Buck Shlegeris
      - Julian Stastny
      author_organizations:
      - Redwood Research
      date: '2025-05-08'
      published_year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    other_attributes: {}
  parsing_issues: []
- id: a:evals_self_replication
  name: Self-replication evals
  header_level: 2
  parent_id: sec:evals
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stephen✅
    one_sentence_summary: evaluate whether AI agents can autonomously replicate themselves
      by obtaining their own weights, securing compute resources, and creating copies
      of themselves.
    theory_of_change: if AI agents gain the ability to self-replicate, they could
      proliferate uncontrollably, making them impossible to shut down. By measuring
      this capability with benchmarks like RepliBench, we can identify when models
      cross this dangerous "red line" and implement controls before losing containment.
    see_also:
    - a:evals_autonomy
    - a:evals_situational_awareness
    orthodox_problems:
    - instrumental_convergence
    - boxed_agi_exfiltrate
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: behaviorist_science
    broad_approach_text: behaviorist science
    some_names:
    - Sid Black
    - Asa Cooper Stickland
    - Jake Pencharz
    - Oliver Sourbut
    - Michael Schmatz
    - Jay Bailey
    - Ollie Matthews
    - Ben Millwood
    - Alex Remedios
    - Alan Cooney
    - Xudong Pan
    - Jiarun Dai
    - Yihe Fan
    estimated_ftes: 10-20
    critiques: '[AI Sandbagging](https://arxiv.org/abs/2406.07358)'
    funded_by: UK Government (via UK AI Safety Institute)
    outputs:
    - link_url: https://arxiv.org/abs/2509.25302
      link_text: A Realistic Evaluation of Self-Replication Risk in LLM Agents
      original_md: '* [**A Realistic Evaluation of Self-Replication Risk in LLM Agents**](https://arxiv.org/abs/2509.25302)'
      title: 'Dive into the Agent Matrix: A Realistic Evaluation of Self-Replication
        Risk in LLM Agents'
      authors:
      - Boxuan Zhang
      - Yi Yu
      - Jiaxuan Guo
      - Jing Shao
      author_organizations: []
      date: '2025-09-29'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://aisi.gov.uk/work/replibench-measuring-autonomous-replication-capabilities-in-ai-systems
      link_text: 'RepliBench: measuring autonomous replication capabilities in AI
        systems'
      original_md: '* [**RepliBench: measuring autonomous replication capabilities
        in AI systems**](https://aisi.gov.uk/work/replibench-measuring-autonomous-replication-capabilities-in-ai-systems),
        2025-04-22, UK AISI Blog'
      title: 'RepliBench: measuring autonomous replication capabilities in AI systems'
      authors: []
      author_organizations:
      - UK AI Security Institute
      date: '2025-04-22'
      published_year: 2025
      venue: UK AISI Blog
      kind: blog_post
    - link_url: https://arxiv.org/abs/2503.17378
      link_text: Large language model-powered AI systems achieve self-replication
        with no human intervention
      original_md: '* [**Large language model-powered AI systems achieve self-replication
        with no human intervention**](https://arxiv.org/abs/2503.17378)'
      title: Large language model-powered AI systems achieve self-replication with
        no human intervention
      authors:
      - Xudong Pan
      - Jiarun Dai
      - Yihe Fan
      - Minyuan Luo
      - Changyi Li
      - Min Yang
      author_organizations: []
      date: '2025-03-25'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:various_redteams
  name: Various Redteams
  header_level: 2
  parent_id: sec:evals
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stephen
    one_sentence_summary: attack current models and see what they do / deliberately
      induce bad things on current frontier models to test out our theories / methods.
    theory_of_change: to ensure models are safe, we must actively try to break them.
      By developing and applying a diverse suite of attacks (e.g., in novel domains,
      against agentic systems, or using automated tools), researchers can discover
      vulnerabilities, specification gaming, and deceptive behaviors before they are
      exploited, thereby informing the development of more robust defenses.
    see_also:
    - a:evals_other
    orthodox_problems:
    - boxed_agi_exfiltrate
    - goals_misgeneralize
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: behaviorist_science
    broad_approach_text: behaviorist science
    some_names:
    - Ryan Greenblatt
    - Benjamin Wright
    - Aengus Lynch
    - John Hughes
    - Samuel R. Bowman
    - Andy Zou
    - Nicholas Carlini
    - Abhay Sheshadri
    estimated_ftes: 100+
    critiques: '[Claude Sonnet 3.7 (often) knows when it''s in alignment evaluations](https://www.alignmentforum.org/posts/E3daBewppAiECN3Ao/claude-sonnet-3-7-often-knows-when-it-s-in-alignment),
      [Red Teaming AI Red Teaming.](https://arxiv.org/html/2507.05538v1)'
    funded_by: Frontier labs (Anthropic, OpenAI, Google), government (UK AISI), Open
      Philanthropy, LTFF, academic grants.
    outputs:
    - link_url: https://alignment.anthropic.com/2025/automated-auditing/
      link_text: Building and evaluating alignment auditing agents
      original_md: '* [**Building and evaluating alignment auditing agents**](https://alignment.anthropic.com/2025/automated-auditing/),
        *Trenton Bricken, Rowan Wang, Sam Bowman et al.*, 2025-07-24, Anthropic Alignment
        Science Blog'
      title: Building and evaluating alignment auditing agents
      authors:
      - Trenton Bricken
      - Rowan Wang
      - Sam Bowman
      - Euan Ong
      - Johannes Treutlein
      - Jeff Wu
      - Evan Hubinger
      - Samuel Marks
      author_organizations:
      - Anthropic
      date: '2025-07-24'
      published_year: 2025
      venue: Alignment Science Blog
      kind: blog_post
    - link_url: https://arxiv.org/pdf/2406.18510
      link_text: null
      original_md: '* [https://arxiv.org/pdf/2406.18510](https://arxiv.org/pdf/2406.18510)'
      title: 'WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially)
        Safer Language Models'
      authors:
      - Liwei Jiang
      - Kavel Rao
      - Seungju Han
      - Allyson Ettinger
      - Faeze Brahman
      - Sachin Kumar
      - Niloofar Mireshghallah
      - Ximing Lu
      - Maarten Sap
      - Yejin Choi
      - Nouha Dziri
      author_organizations:
      - University of Washington
      - Allen Institute for AI
      date: '2024-06-26'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2412.18693
      link_text: null
      original_md: '* [https://arxiv.org/abs/2412.18693](https://arxiv.org/abs/2412.18693)'
      title: Diverse and Effective Red Teaming with Auto-generated Rewards and Multi-step
        Reinforcement Learning
      authors:
      - Alex Beutel
      - Kai Xiao
      - Johannes Heidecke
      - Lilian Weng
      author_organizations:
      - OpenAI
      date: '2024-12-24'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://t.co/wk0AP8aDNI
      link_text: Findings from a Pilot Anthropic—OpenAI Alignment Evaluation Exercise
      original_md: '* [**Findings from a Pilot Anthropic—OpenAI Alignment Evaluation
        Exercise**](https://t.co/wk0AP8aDNI), *Samuel R. Bowman, Megha Srivastava,
        Jon Kutasov et al.*, 2025-08-27, Alignment Science Blog'
      title: Findings from a Pilot Anthropic—OpenAI Alignment Evaluation Exercise
      authors:
      - Samuel R. Bowman
      - Megha Srivastava
      - Jon Kutasov
      - Rowan Wang
      - Trenton Bricken
      - Benjamin Wright
      - Ethan Perez
      - Nicholas Carlini
      author_organizations:
      - Anthropic
      - OpenAI
      date: '2025-08-27'
      published_year: 2025
      venue: Alignment Science Blog
      kind: blog_post
    - link_url: https://t.co/XFtd0H2Pzb
      link_text: 'Agentic Misalignment: How LLMs could be insider threats'
      original_md: '* [**Agentic Misalignment: How LLMs could be insider threats**](https://t.co/XFtd0H2Pzb),
        *Aengus Lynch, Benjamin Wright, Caleb Larson et al.*, 2025-06-20, Anthropic
        Research'
      title: 'Agentic Misalignment: How LLMs could be insider threats'
      authors:
      - Aengus Lynch
      - Benjamin Wright
      - Caleb Larson
      - Kevin K. Troy
      - Stuart J. Ritchie
      - Sören Mindermann
      - Ethan Perez
      - Evan Hubinger
      author_organizations:
      - Anthropic
      - University College London
      - MATS
      - Mila
      - Redwood Research
      date: '2025-06-20'
      published_year: 2025
      venue: Anthropic Research
      kind: blog_post
    - link_url: https://arxiv.org/abs/2502.08301
      link_text: Compromising Honesty and Harmlessness in Language Models via Deception
        Attacks
      original_md: '* [**Compromising Honesty and Harmlessness in Language Models
        via Deception Attacks**](https://arxiv.org/abs/2502.08301), *Laurène Vaugrante,
        Francesca Carlon, Maluna Menke et al.*, 2025-02-12, arXiv'
      title: Compromising Honesty and Harmlessness in Language Models via Deception
        Attacks
      authors:
      - Laurène Vaugrante
      - Francesca Carlon
      - Maluna Menke
      - Thilo Hagendorff
      author_organizations: []
      date: '2025-02-12'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.01236
      link_text: Eliciting Language Model Behaviors with Investigator Agents
      original_md: '* [**Eliciting Language Model Behaviors with Investigator Agents**](https://arxiv.org/abs/2502.01236),
        *Xiang Lisa Li, Neil Chowdhury, Daniel D. Johnson et al.*, 2025-02-03, arXiv'
      title: Eliciting Language Model Behaviors with Investigator Agents
      authors:
      - Xiang Lisa Li
      - Neil Chowdhury
      - Daniel D. Johnson
      - Tatsunori Hashimoto
      - Percy Liang
      - Sarah Schwettmann
      - Jacob Steinhardt
      author_organizations:
      - Stanford University
      - UC Berkeley
      date: '2025-02-03'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.18032
      link_text: Why Do Some Language Models Fake Alignment While Others Don't?
      original_md: '* [**Why Do Some Language Models Fake Alignment While Others Don''t?**](https://arxiv.org/abs/2506.18032),
        *Abhay Sheshadri, John Hughes, Julian Michael et al.*, 2025-06-22, arXiv'
      title: Why Do Some Language Models Fake Alignment While Others Don't?
      authors:
      - Abhay Sheshadri
      - John Hughes
      - Julian Michael
      - Alex Mallen
      - Arun Jose
      - Janus
      - Fabien Roger
      author_organizations: []
      date: '2025-06-22'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.13295
      link_text: Demonstrating specification gaming in reasoning models
      original_md: '* [**Demonstrating specification gaming in reasoning models**](https://arxiv.org/abs/2502.13295),
        *Alexander Bondarenko, Denis Volk, Dmitrii Volkov et al.*, 2025-08-27, arXiv'
      title: Demonstrating specification gaming in reasoning models
      authors:
      - Alexander Bondarenko
      - Denis Volk
      - Dmitrii Volkov
      - Jeffrey Ladish
      author_organizations: []
      date: '2025-08-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://t.co/tkHkVFVZ2m
      link_text: 'Call Me A Jerk: Persuading AI to Comply with Objectionable Requests'
      original_md: '* [**Call Me A Jerk: Persuading AI to Comply with Objectionable
        Requests**](https://t.co/tkHkVFVZ2m), *Lennart Meincke, Dan Shapiro, Angela
        Duckworth et al.*, 2025-07-18, SSRN / The Wharton School Research Paper'
      title: 'Call Me A Jerk: Persuading AI to Comply with Objectionable Requests'
      authors:
      - Lennart Meincke
      - Dan Shapiro
      - Angela Duckworth
      - Ethan R. Mollick
      - Lilach Mollick
      - Robert Cialdini
      author_organizations:
      - University of Pennsylvania
      - The Wharton School
      - WHU - Otto Beisheim School of Management
      - Glowforge, Inc
      date: '2025-07-18'
      published_year: 2025
      venue: SSRN / The Wharton School Research Paper
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.11083
      link_text: 'RedDebate: Safer Responses through Multi-Agent Red Teaming Debates'
      original_md: '* [**RedDebate: Safer Responses through Multi-Agent Red Teaming
        Debates**](https://arxiv.org/abs/2506.11083), *Ali Asad, Stephen Obadinma,
        Radin Shayanfar et al.*, 2025-06-04, arXiv'
      title: 'RedDebate: Safer Responses through Multi-Agent Red Teaming Debates'
      authors:
      - Ali Asad
      - Stephen Obadinma
      - Radin Shayanfar
      - Xiaodan Zhu
      author_organizations: []
      date: '2025-06-04'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2512.03771
      link_text: null
      original_md: '* [https://arxiv.org/abs/2512.03771](https://arxiv.org/abs/2512.03771)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://arxiv.org/abs/2504.09712
      link_text: The Structural Safety Generalization Problem
      original_md: '* [**The Structural Safety Generalization Problem**](https://arxiv.org/abs/2504.09712),
        *Julius Broomfield, Tom Gibbs, Ethan Kosak-Hine et al.*, 2025-04-13, arXiv'
      title: The Structural Safety Generalization Problem
      authors:
      - Julius Broomfield
      - Tom Gibbs
      - Ethan Kosak-Hine
      - George Ingebretsen
      - Tia Nasir
      - Jason Zhang
      - Reihaneh Iranmanesh
      - Sara Pieri
      - Reihaneh Rabbany
      - Kellin Pelrine
      author_organizations: []
      date: '2025-04-13'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.19537
      link_text: No, of Course I Can! Deeper Fine-Tuning Attacks That Bypass Token-Level
        Safety Mechanisms
      original_md: '* [**No, of Course I Can\! Deeper Fine-Tuning Attacks That Bypass
        Token-Level Safety Mechanisms**](https://arxiv.org/abs/2502.19537), *Joshua
        Kazdan, Abhay Puri, Rylan Schaeffer et al.*, 2025-02-26, arXiv'
      title: No, of Course I Can! Deeper Fine-Tuning Attacks That Bypass Token-Level
        Safety Mechanisms
      authors:
      - Joshua Kazdan
      - Abhay Puri
      - Rylan Schaeffer
      - Lisa Yu
      - Chris Cundy
      - Jason Stanley
      - Sanmi Koyejo
      - Krishnamurthy Dvijotham
      author_organizations: []
      date: '2025-02-26'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.14828
      link_text: Fundamental Limitations in Pointwise Defences of LLM Finetuning APIs
      original_md: '* [**Fundamental Limitations in Pointwise Defences of LLM Finetuning
        APIs**](https://arxiv.org/abs/2502.14828), *Xander Davies, Eric Winsor, Alexandra
        Souly et al.*, 2025-02-20, arXiv'
      title: Fundamental Limitations in Pointwise Defences of LLM Finetuning APIs
      authors:
      - Xander Davies
      - Eric Winsor
      - Alexandra Souly
      - Tomek Korbak
      - Robert Kirk
      - Christian Schroeder de Witt
      - Yarin Gal
      author_organizations:
      - Oxford University
      date: '2025-02-20'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2508.06296
      link_text: LLM Robustness Leaderboard v1 --Technical report
      original_md: '* [**LLM Robustness Leaderboard v1 \--Technical report**](https://arxiv.org/abs/2508.06296),
        *Pierre Peigné \- Lefebvre, Quentin Feuillade-Montixi, Tom David et al.*,
        2025-08-13, arXiv'
      title: LLM Robustness Leaderboard v1 --Technical report
      authors:
      - Pierre Peigné - Lefebvre
      - Quentin Feuillade-Montixi
      - Tom David
      - Nicolas Miailhe
      author_organizations:
      - PRISM Eval
      date: '2025-08-13'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2412.02159
      link_text: 'Jailbreak Defense in a Narrow Domain: Limitations of Existing Methods
        and a New Transcript-Classifier Approach'
      original_md: '* [**Jailbreak Defense in a Narrow Domain: Limitations of Existing
        Methods and a New Transcript-Classifier Approach**](https://arxiv.org/abs/2412.02159),
        *Tony T. Wang, John Hughes, Henry Sleight et al.*, 2024-12-03, arXiv'
      title: 'Jailbreak Defense in a Narrow Domain: Limitations of Existing Methods
        and a New Transcript-Classifier Approach'
      authors:
      - Tony T. Wang
      - John Hughes
      - Henry Sleight
      - Rylan Schaeffer
      - Rajashree Agrawal
      - Fazl Barez
      - Mrinank Sharma
      - Jesse Mu
      - Nir Shavit
      - Ethan Perez
      author_organizations: []
      date: '2024-12-03'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.goodfire.ai/papers/model-diff-amplification
      link_text: Discovering Undesired Rare Behaviors via Model Diff Amplification
      original_md: '* [**Discovering Undesired Rare Behaviors via Model Diff Amplification**](https://www.goodfire.ai/papers/model-diff-amplification),
        *Santiago Aranguri, Thomas McGrath*, 2025-08-21, Goodfire Research'
      title: Discovering Undesired Rare Behaviors via Model Diff Amplification
      authors:
      - Santiago Aranguri
      - Thomas McGrath
      author_organizations:
      - Goodfire
      - NYU
      date: '2025-08-21'
      published_year: 2025
      venue: Goodfire Research
      kind: blog_post
    - link_url: https://arxiv.org/abs/2502.17254
      link_text: 'REINFORCE Adversarial Attacks on Large Language Models: An Adaptive,
        Distributional, and Semantic Objective'
      original_md: '* [**REINFORCE Adversarial Attacks on Large Language Models: An
        Adaptive, Distributional, and Semantic Objective**](https://arxiv.org/abs/2502.17254),
        *Simon Geisler, Tom Wollschläger, M. H. I. Abdalla et al.*, 2025-02-24, arXiv'
      title: 'REINFORCE Adversarial Attacks on Large Language Models: An Adaptive,
        Distributional, and Semantic Objective'
      authors:
      - Simon Geisler
      - Tom Wollschläger
      - M. H. I. Abdalla
      - Vincent Cohen-Addad
      - Johannes Gasteiger
      - Stephan Günnemann
      author_organizations: []
      date: '2025-02-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://alignment.anthropic.com/2025/petri/
      link_text: 'Petri: An open-source auditing tool to accelerate AI safety research'
      original_md: '* [**Petri: An open-source auditing tool to accelerate AI safety
        research**](https://alignment.anthropic.com/2025/petri/), 2025-10-06, Anthropic
        Alignment Science Blog'
      title: 'Petri: An open-source auditing tool to accelerate AI safety research'
      authors: []
      author_organizations:
      - Anthropic
      date: '2025-10-06'
      published_year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - link_url: https://arxiv.org/pdf/2507.02990
      link_text: '''For Argument''s Sake, Show Me How to Harm Myself!'': Jailbreaking
        LLMs in Suicide and Self-Harm Contexts'
      original_md: '* [**\`For Argument''s Sake, Show Me How to Harm Myself\!'': Jailbreaking
        LLMs in Suicide and Self-Harm Contexts**](https://arxiv.org/pdf/2507.02990),
        *Annika M Schoene, Cansu Canca*, 2025-07-01, arXiv'
      title: '''For Argument''s Sake, Show Me How to Harm Myself!'': Jailbreaking
        LLMs in Suicide and Self-Harm Contexts'
      authors:
      - Annika M Schoene
      - Cansu Canca
      author_organizations: []
      date: '2025-07-01'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.07846
      link_text: 'Winning at All Cost: A Small Environment for Eliciting Specification
        Gaming Behaviors in Large Language Models'
      original_md: '* [**Winning at All Cost: A Small Environment for Eliciting Specification
        Gaming Behaviors in Large Language Models**](https://arxiv.org/abs/2505.07846),
        *Lars Malmqvist*, 2025-05-07, arXiv (to be presented at SIMLA@ACNS 2025\)'
      title: 'Winning at All Cost: A Small Environment for Eliciting Specification
        Gaming Behaviors in Large Language Models'
      authors:
      - Lars Malmqvist
      author_organizations: []
      date: '2025-05-07'
      published_year: 2025
      venue: arXiv (to be presented at SIMLA@ACNS 2025)
      kind: paper_preprint
    - link_url: https://openai.com/index/trading-inference-time-compute-for-adversarial-robustness
      link_text: Trading Inference-Time Compute for Adversarial Robustness
      original_md: '* [**Trading Inference-Time Compute for Adversarial Robustness**](https://openai.com/index/trading-inference-time-compute-for-adversarial-robustness),
        *OpenAI*, 2025-01-22, arXiv'
      title: Trading Inference-Time Compute for Adversarial Robustness
      authors:
      - OpenAI
      author_organizations:
      - OpenAI
      date: '2025-01-22'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://openreview.net/forum?id=TD1NfQuVr6
      link_text: Can a Neural Network that only Memorizes the Dataset be Undetectably
        Backdoored?
      original_md: '* [**Can a Neural Network that only Memorizes the Dataset be Undetectably
        Backdoored?**](https://openreview.net/forum?id=TD1NfQuVr6), *Matjaz Leonardis*,
        2025-07-10, ODYSSEY 2025 Conference'
      title: Can a Neural Network that only Memorizes the Dataset be Undetectably
        Backdoored?
      authors:
      - Matjaz Leonardis
      author_organizations: []
      date: '2025-07-10'
      published_year: 2025
      venue: ODYSSEY 2025 Conference
      kind: paper_preprint
    - link_url: https://github.com/lechmazur/step_game
      link_text: 'Multi-Agent Step Race Benchmark: Assessing LLM Collaboration and
        Deception Under Pressure'
      original_md: '* [**Multi-Agent Step Race Benchmark: Assessing LLM Collaboration
        and Deception Under Pressure**](https://github.com/lechmazur/step_game), *lechmazur,
        eltociear*, 2025-08-29, GitHub'
      title: 'Multi-Agent Step Race Benchmark: Assessing LLM Collaboration and Deception
        Under Pressure'
      authors:
      - lechmazur
      - eltociear
      author_organizations: []
      date: '2025-08-29'
      published_year: 2025
      venue: GitHub
      kind: code_tool
    - link_url: https://arxiv.org/abs/2503.00061
      link_text: Adaptive Attacks Break Defenses Against Indirect Prompt Injection
        Attacks on LLM Agents
      original_md: '* [**Adaptive Attacks Break Defenses Against Indirect Prompt Injection
        Attacks on LLM Agents**](https://arxiv.org/abs/2503.00061), *Qiusi Zhan, Richard
        Fang, Henil Shalin Panchal et al.*, 2025-03-04, NAACL 2025 Findings'
      title: Adaptive Attacks Break Defenses Against Indirect Prompt Injection Attacks
        on LLM Agents
      authors:
      - Qiusi Zhan
      - Richard Fang
      - Henil Shalin Panchal
      - Daniel Kang
      author_organizations:
      - UIUC
      date: '2025-03-04'
      published_year: 2025
      venue: NAACL 2025 Findings
      kind: paper_preprint
    - link_url: https://0din.ai/blog/quantifying-the-unruly-a-scoring-system-for-jailbreak-tactics
      link_text: 'Quantifying the Unruly: A Scoring System for Jailbreak Tactics'
      original_md: '* [**Quantifying the Unruly: A Scoring System for Jailbreak Tactics**](https://0din.ai/blog/quantifying-the-unruly-a-scoring-system-for-jailbreak-tactics),
        *Pedram Amini*, 2025-06-12, 0DIN.ai Blog'
      title: 'Quantifying the Unruly: A Scoring System for Jailbreak Tactics'
      authors:
      - Pedram Amini
      author_organizations:
      - 0DIN.ai
      - Mozilla
      date: '2025-06-12'
      published_year: 2025
      venue: 0DIN.ai Blog
      kind: blog_post
    - link_url: https://arxiv.org/abs/2505.01050
      link_text: Transferable Adversarial Attacks on Black-Box Vision-Language Models
      original_md: '* [**Transferable Adversarial Attacks on Black-Box Vision-Language
        Models**](https://arxiv.org/abs/2505.01050), *Kai Hu, Weichen Yu, Li Zhang
        et al.*, 2025-05-02, arXiv'
      title: Transferable Adversarial Attacks on Black-Box Vision-Language Models
      authors:
      - Kai Hu
      - Weichen Yu
      - Li Zhang
      - Alexander Robey
      - Andy Zou
      - Chengming Xu
      - Haoqi Hu
      - Matt Fredrikson
      author_organizations: []
      date: '2025-05-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://deepmind.google/discover/blog/advancing-geminis-security-safeguards/
      link_text: Advancing Gemini's security safeguards
      original_md: '* [**Advancing Gemini''s security safeguards**](https://deepmind.google/discover/blog/advancing-geminis-security-safeguards/),
        *Google DeepMind Security & Privacy Research Team*, 2025-05-20, Google DeepMind
        Blog'
      title: Advancing Gemini's security safeguards
      authors:
      - Google DeepMind Security & Privacy Research Team
      author_organizations:
      - Google DeepMind
      date: '2025-05-20'
      published_year: 2025
      venue: Google DeepMind Blog
      kind: blog_post
    - link_url: https://arxiv.org/abs/2509.14260
      link_text: Shutdown Resistance in Large Language Models
      original_md: '* [**Shutdown Resistance in Large Language Models**](https://arxiv.org/abs/2509.14260),
        *Jeremy Schlatter, Benjamin Weinstein-Raun, Jeffrey Ladish*, 2025-09-13, arXiv'
      title: Shutdown Resistance in Large Language Models
      authors:
      - Jeremy Schlatter
      - Benjamin Weinstein-Raun
      - Jeffrey Ladish
      author_organizations: []
      date: '2025-09-13'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.15541
      link_text: Stress Testing Deliberative Alignment for Anti-Scheming Training
      original_md: '* [**Stress Testing Deliberative Alignment for Anti-Scheming Training**](https://arxiv.org/abs/2509.15541),
        *Bronson Schoen, Evgenia Nitishinskaya, Mikita Balesni et al.*, 2025-09-19,
        arXiv'
      title: Stress Testing Deliberative Alignment for Anti-Scheming Training
      authors:
      - Bronson Schoen
      - Evgenia Nitishinskaya
      - Mikita Balesni
      - Axel Højmark
      - Felix Hofstätter
      - Jérémy Scheurer
      - Alexander Meinke
      - Jason Wolfe
      - Teun van der Weij
      - Alex Lloyd
      - Nicholas Goldowsky-Dill
      - Angela Fan
      - Andrei Matveiakin
      - Rusheb Shah
      - Marcus Williams
      - Amelia Glaese
      - Boaz Barak
      - Wojciech Zaremba
      - Marius Hobbhahn
      author_organizations:
      - OpenAI
      - Anthropic
      date: '2025-09-19'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.26418
      link_text: Chain-of-Thought Hijacking
      original_md: '* [**Chain-of-Thought Hijacking**](https://arxiv.org/abs/2510.26418),
        *Jianli Zhao, Tingchen Fu, Rylan Schaeffer et al.*, 2025-10-30, arXiv'
      title: Chain-of-Thought Hijacking
      authors:
      - Jianli Zhao
      - Tingchen Fu
      - Rylan Schaeffer
      - Mrinank Sharma
      - Fazl Barez
      author_organizations: []
      date: '2025-10-30'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.13203
      link_text: 'X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents'
      original_md: '* [**X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive
        Multi-Agents**](https://arxiv.org/abs/2504.13203), *Salman Rahman, Liwei Jiang,
        James Shiffer et al.*, 2025-04-15, arXiv'
      title: 'X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents'
      authors:
      - Salman Rahman
      - Liwei Jiang
      - James Shiffer
      - Genglin Liu
      - Sheriff Issaka
      - Md Rizwan Parvez
      - Hamid Palangi
      - Kai-Wei Chang
      - Yejin Choi
      - Saadia Gabriel
      author_organizations:
      - University of Washington
      - UCLA
      - Microsoft
      date: '2025-04-15'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.11630
      link_text: 'Jailbreak-Tuning: Models Efficiently Learn Jailbreak Susceptibility'
      original_md: '* [**Jailbreak-Tuning: Models Efficiently Learn Jailbreak Susceptibility**](https://arxiv.org/abs/2507.11630),
        *Brendan Murphy, Dillon Bowen, Shahrad Mohammadzadeh et al.*, 2025-07-15,
        arXiv'
      title: 'Jailbreak-Tuning: Models Efficiently Learn Jailbreak Susceptibility'
      authors:
      - Brendan Murphy
      - Dillon Bowen
      - Shahrad Mohammadzadeh
      - Tom Tseng
      - Julius Broomfield
      - Adam Gleave
      - Kellin Pelrine
      author_organizations: []
      date: '2025-07-15'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.10949
      link_text: Monitoring Decomposition Attacks in LLMs with Lightweight Sequential
        Monitors
      original_md: '* [**Monitoring Decomposition Attacks in LLMs with Lightweight
        Sequential Monitors**](https://arxiv.org/abs/2506.10949), *Chen Yueh-Han,
        Nitish Joshi, Yulin Chen et al.*, 2025-06-14, arXiv'
      title: Monitoring Decomposition Attacks in LLMs with Lightweight Sequential
        Monitors
      authors:
      - Chen Yueh-Han
      - Nitish Joshi
      - Yulin Chen
      - Maksym Andriushchenko
      - Rico Angell
      - He He
      author_organizations: []
      date: '2025-06-14'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.24068
      link_text: 'STACK: Adversarial Attacks on LLM Safeguard Pipelines'
      original_md: '* [**STACK: Adversarial Attacks on LLM Safeguard Pipelines**](https://arxiv.org/abs/2506.24068),
        *Ian R. McKenzie, Oskar J. Hollinsworth, Tom Tseng et al.*, 2025-06-30, arXiv'
      title: 'STACK: Adversarial Attacks on LLM Safeguard Pipelines'
      authors:
      - Ian R. McKenzie
      - Oskar J. Hollinsworth
      - Tom Tseng
      - Xander Davies
      - Stephen Casper
      - Aaron D. Tucker
      - Robert Kirk
      - Adam Gleave
      author_organizations: []
      date: '2025-06-30'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.03167
      link_text: Adversarial Manipulation of Reasoning Models using Internal Representations
      original_md: '* [**Adversarial Manipulation of Reasoning Models using Internal
        Representations**](https://arxiv.org/abs/2507.03167), *Kureha Yamaguchi, Benjamin
        Etheridge, Andy Arditi*, 2025-07-03, arXiv'
      title: Adversarial Manipulation of Reasoning Models using Internal Representations
      authors:
      - Kureha Yamaguchi
      - Benjamin Etheridge
      - Andy Arditi
      author_organizations: []
      date: '2025-07-03'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.17441
      link_text: Discovering Forbidden Topics in Language Models
      original_md: '* [**Discovering Forbidden Topics in Language Models**](https://arxiv.org/abs/2505.17441),
        *Can Rager, Chris Wendler, Rohit Gandikota et al.*, 2025-05-23, arXiv'
      title: Discovering Forbidden Topics in Language Models
      authors:
      - Can Rager
      - Chris Wendler
      - Rohit Gandikota
      - David Bau
      author_organizations: []
      date: '2025-05-23'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.14261
      link_text: 'RL-Obfuscation: Can Language Models Learn to Evade Latent-Space
        Monitors?'
      original_md: '* [**RL-Obfuscation: Can Language Models Learn to Evade Latent-Space
        Monitors?**](https://arxiv.org/abs/2506.14261), *Rohan Gupta, Erik Jenner*,
        2025-06-17, arXiv'
      title: 'RL-Obfuscation: Can Language Models Learn to Evade Latent-Space Monitors?'
      authors:
      - Rohan Gupta
      - Erik Jenner
      author_organizations: []
      date: '2025-06-17'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.12913
      link_text: Jailbreak Transferability Emerges from Shared Representations
      original_md: '* [**Jailbreak Transferability Emerges from Shared Representations**](https://arxiv.org/abs/2506.12913),
        *Rico Angell, Jannik Brinkmann, He He*, 2025-06-15, arXiv'
      title: Jailbreak Transferability Emerges from Shared Representations
      authors:
      - Rico Angell
      - Jannik Brinkmann
      - He He
      author_organizations: []
      date: '2025-06-15'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.09604
      link_text: Mitigating Many-Shot Jailbreaking
      original_md: '* [**Mitigating Many-Shot Jailbreaking**](https://arxiv.org/abs/2504.09604),
        *Christopher M. Ackerman, Nina Panickssery*, 2025-04-13, arXiv'
      title: Mitigating Many-Shot Jailbreaking
      authors:
      - Christopher M. Ackerman
      - Nina Panickssery
      author_organizations: []
      date: '2025-04-13'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.21947
      link_text: 'Active Attacks: Red-teaming LLMs via Adaptive Environments'
      original_md: '* [**Active Attacks: Red-teaming LLMs via Adaptive Environments**](https://arxiv.org/abs/2509.21947),
        *Taeyoung Yun, Pierre-Luc St-Charles, Jinkyoo Park et al.*, 2025-09-26, arXiv'
      title: 'Active Attacks: Red-teaming LLMs via Adaptive Environments'
      authors:
      - Taeyoung Yun
      - Pierre-Luc St-Charles
      - Jinkyoo Park
      - Yoshua Bengio
      - Minsu Kim
      author_organizations:
      - Mila
      - KAIST
      date: '2025-09-26'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.02873
      link_text: 'It''s the Thought that Counts: Evaluating the Attempts of Frontier
        LLMs to Persuade on Harmful Topics'
      original_md: '* [**It''s the Thought that Counts: Evaluating the Attempts of
        Frontier LLMs to Persuade on Harmful Topics**](https://arxiv.org/abs/2506.02873),
        *Matthew Kowal, Jasper Timm, Jean-Francois Godbout et al.*, 2025-06-03, arXiv'
      title: 'It''s the Thought that Counts: Evaluating the Attempts of Frontier LLMs
        to Persuade on Harmful Topics'
      authors:
      - Matthew Kowal
      - Jasper Timm
      - Jean-Francois Godbout
      - Thomas Costello
      - Antonio A. Arechar
      - Gordon Pennycook
      - David Rand
      - Adam Gleave
      - Kellin Pelrine
      author_organizations:
      - FAR AI
      - AlignmentResearch
      date: '2025-06-03'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.03350
      link_text: Adversarial Attacks on Robotic Vision Language Action Models
      original_md: '* [**Adversarial Attacks on Robotic Vision Language Action Models**](https://arxiv.org/abs/2506.03350),
        *Eliot Krzysztof Jones, Alexander Robey, Andy Zou et al.*, 2025-06-03, arXiv'
      title: Adversarial Attacks on Robotic Vision Language Action Models
      authors:
      - Eliot Krzysztof Jones
      - Alexander Robey
      - Andy Zou
      - Zachary Ravichandran
      - George J. Pappas
      - Hamed Hassani
      - Matt Fredrikson
      - J. Zico Kolter
      author_organizations: []
      date: '2025-06-03'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.14827
      link_text: 'MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation
        Models'
      original_md: '* [**MMDT: Decoding the Trustworthiness and Safety of Multimodal
        Foundation Models**](https://arxiv.org/abs/2503.14827), *Chejian Xu, Jiawei
        Zhang, Zhaorun Chen et al.*, 2025-03-19, ICLR 2025 (preprint on arXiv)'
      title: 'MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation
        Models'
      authors:
      - Chejian Xu
      - Jiawei Zhang
      - Zhaorun Chen
      - Chulin Xie
      - Mintong Kang
      - Yujin Potter
      - Zhun Wang
      - Zhuowen Yuan
      - Alexander Xiong
      - Zidi Xiong
      - Chenhui Zhang
      - Lingzhi Yuan
      - Yi Zeng
      - Peiyang Xu
      - Chengquan Guo
      - Andy Zhou
      - Jeffrey Ziwei Tan
      - Xuandong Zhao
      - Francesco Pinto
      - Zhen Xiang
      - Yu Gai
      - Zinan Lin
      - Dan Hendrycks
      - Bo Li
      - Dawn Song
      author_organizations:
      - Multiple academic institutions
      date: '2025-03-19'
      published_year: 2025
      venue: ICLR 2025 (preprint on arXiv)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.22014
      link_text: Toward Understanding the Transferability of Adversarial Suffixes
        in Large Language Models
      original_md: '* [**Toward Understanding the Transferability of Adversarial Suffixes
        in Large Language Models**](https://arxiv.org/abs/2510.22014), *Sarah Ball,
        Niki Hasrati, Alexander Robey et al.*, 2025-10-24, arXiv'
      title: Toward Understanding the Transferability of Adversarial Suffixes in Large
        Language Models
      authors:
      - Sarah Ball
      - Niki Hasrati
      - Alexander Robey
      - Avi Schwarzschild
      - Frauke Kreuter
      - Zico Kolter
      - Andrej Risteski
      author_organizations: []
      date: '2025-10-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.04113
      link_text: Uncovering Gaps in How Humans and LLMs Interpret Subjective Language
      original_md: '* [**Uncovering Gaps in How Humans and LLMs Interpret Subjective
        Language**](https://arxiv.org/abs/2503.04113), *Erik Jones, Arjun Patrawala,
        Jacob Steinhardt*, 2025-03-06, ICLR 2025'
      title: Uncovering Gaps in How Humans and LLMs Interpret Subjective Language
      authors:
      - Erik Jones
      - Arjun Patrawala
      - Jacob Steinhardt
      author_organizations: []
      date: '2025-03-06'
      published_year: 2025
      venue: ICLR 2025
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.02609
      link_text: 'RedCodeAgent: Automatic Red-teaming Agent against Diverse Code Agents'
      original_md: '* [**RedCodeAgent: Automatic Red-teaming Agent against Diverse
        Code Agents**](https://arxiv.org/abs/2510.02609), *Chengquan Guo, Chulin Xie,
        Yu Yang et al.*, 2025-10-02, arXiv'
      title: 'RedCodeAgent: Automatic Red-teaming Agent against Diverse Code Agents'
      authors:
      - Chengquan Guo
      - Chulin Xie
      - Yu Yang
      - Zhaorun Chen
      - Zinan Lin
      - Xander Davies
      - Yarin Gal
      - Dawn Song
      - Bo Li
      author_organizations: []
      date: '2025-10-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.10809
      link_text: 'MIP against Agent: Malicious Image Patches Hijacking Multimodal
        OS Agents'
      original_md: '* [**MIP against Agent: Malicious Image Patches Hijacking Multimodal
        OS Agents**](https://arxiv.org/abs/2503.10809), *Lukas Aichberger, Alasdair
        Paren, Guohao Li et al.*, 2025-03-13, arXiv (accepted NeurIPS 2025\)'
      title: 'MIP against Agent: Malicious Image Patches Hijacking Multimodal OS Agents'
      authors:
      - Lukas Aichberger
      - Alasdair Paren
      - Guohao Li
      - Philip Torr
      - Yarin Gal
      - Adel Bibi
      author_organizations:
      - University of Oxford
      date: '2025-03-13'
      published_year: 2025
      venue: arXiv (accepted NeurIPS 2025)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.02554
      link_text: 'ToolTweak: An Attack on Tool Selection in LLM-based Agents'
      original_md: '* [**ToolTweak: An Attack on Tool Selection in LLM-based Agents**](https://arxiv.org/abs/2510.02554),
        *Jonathan Sneh, Ruomei Yan, Jialin Yu et al.*, 2025-10-02, arXiv'
      title: 'ToolTweak: An Attack on Tool Selection in LLM-based Agents'
      authors:
      - Jonathan Sneh
      - Ruomei Yan
      - Jialin Yu
      - Philip Torr
      - Yarin Gal
      - Sunando Sengupta
      - Eric Sommerlade
      - Alasdair Paren
      - Adel Bibi
      author_organizations:
      - University of Oxford
      - Various
      date: '2025-10-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.11910
      link_text: Adversarial Alignment for LLMs Requires Simpler, Reproducible, and
        More Measurable Objectives
      original_md: '* [**Adversarial Alignment for LLMs Requires Simpler, Reproducible,
        and More Measurable Objectives**](https://arxiv.org/abs/2502.11910), *Leo
        Schwinn, Yan Scholten, Tom Wollschläger et al.*, 2025-02-17, arXiv'
      title: Adversarial Alignment for LLMs Requires Simpler, Reproducible, and More
        Measurable Objectives
      authors:
      - Leo Schwinn
      - Yan Scholten
      - Tom Wollschläger
      - Sophie Xhonneux
      - Stephen Casper
      - Stephan Günnemann
      - Gauthier Gidel
      author_organizations: []
      date: '2025-02-17'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/b8eeCGe3FWzHKbePF/agentic-misalignment-how-llms-could-be-insider-threats-1
      link_text: 'Agentic Misalignment: How LLMs Could be Insider Threats'
      original_md: '* [**Agentic Misalignment: How LLMs Could be Insider Threats**](https://lesswrong.com/posts/b8eeCGe3FWzHKbePF/agentic-misalignment-how-llms-could-be-insider-threats-1),
        *Aengus Lynch, Benjamin Wright, Caleb Larson et al.*, 2025-06-20, LessWrong/AI
        Alignment Forum'
      title: 'Agentic Misalignment: How LLMs Could be Insider Threats'
      authors:
      - Aengus Lynch
      - Benjamin Wright
      - Caleb Larson
      - Stuart Richie
      - Sören Mindermann
      - Evan Hubinger
      - Ethan Perez
      - Kevin Troy
      author_organizations:
      - Anthropic
      date: '2025-06-20'
      published_year: 2025
      venue: LessWrong/AI Alignment Forum
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/zjqrSKZuRLnjAniyo/illusory-safety-redteaming-deepseek-r1-and-the-strongest
      link_text: 'Illusory Safety: Redteaming DeepSeek R1 and the Strongest Fine-Tunable
        Models of OpenAI, Anthropic, and Google'
      original_md: '* [**Illusory Safety: Redteaming DeepSeek R1 and the Strongest
        Fine-Tunable Models of OpenAI, Anthropic, and Google**](https://lesswrong.com/posts/zjqrSKZuRLnjAniyo/illusory-safety-redteaming-deepseek-r1-and-the-strongest),
        *ChengCheng, Brendan Murphy, Adrià Garriga-alonso et al.*, 2025-02-07, LessWrong
        / AI Alignment Forum'
      title: 'Illusory Safety: Redteaming DeepSeek R1 and the Strongest Fine-Tunable
        Models of OpenAI, Anthropic, and Google'
      authors:
      - ChengCheng
      - Brendan Murphy
      - Adrià Garriga-alonso
      - Yashvardhan Sharma
      - dsbowen
      - smallsilo
      - Yawen Duan
      - ChrisCundy
      - Hannah Betts
      - AdamGleave
      - Kellin Pelrine
      author_organizations:
      - FAR AI
      date: '2025-02-07'
      published_year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/7C4KJot4aN8ieEDoz/will-alignment-faking-claude-accept-a-deal-to-reveal-its
      link_text: Will alignment-faking Claude accept a deal to reveal its misalignment?
      original_md: '* [**Will alignment-faking Claude accept a deal to reveal its
        misalignment?**](https://lesswrong.com/posts/7C4KJot4aN8ieEDoz/will-alignment-faking-claude-accept-a-deal-to-reveal-its),
        *Ryan Greenblatt, Kyle Fish*, 2025-01-31, LessWrong / AI Alignment Forum'
      title: Will alignment-faking Claude accept a deal to reveal its misalignment?
      authors:
      - Ryan Greenblatt
      - Kyle Fish
      author_organizations:
      - Redwood Research
      - Anthropic
      date: '2025-01-31'
      published_year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/26SHhxK2yYQbh7ors/research-directions-open-phil-wants-to-fund-in-technical-ai
      link_text: Research directions Open Phil wants to fund in technical AI safety
      original_md: '* [**Research directions Open Phil wants to fund in technical
        AI safety**](https://lesswrong.com/posts/26SHhxK2yYQbh7ors/research-directions-open-phil-wants-to-fund-in-technical-ai),
        *jake_mendel, maxnadeau, Peter Favaloro*, 2025-02-08, LessWrong'
      title: Research directions Open Phil wants to fund in technical AI safety
      authors:
      - jake_mendel
      - maxnadeau
      - Peter Favaloro
      author_organizations:
      - Open Philanthropy
      date: '2025-02-08'
      published_year: 2025
      venue: LessWrong
      kind: agenda_manifesto
    - link_url: https://lesswrong.com/posts/9i6fHMn2vTqyzAi9o/when-does-claude-sabotage-code-an-agentic-misalignment
      link_text: When does Claude sabotage code? An Agentic Misalignment follow-up
      original_md: '* [**When does Claude sabotage code? An Agentic Misalignment follow-up**](https://lesswrong.com/posts/9i6fHMn2vTqyzAi9o/when-does-claude-sabotage-code-an-agentic-misalignment),
        *Nathan Delisle*, 2024-11-09, LessWrong'
      title: When does Claude sabotage code? An Agentic Misalignment follow-up
      authors:
      - Nathan Delisle
      author_organizations:
      - MATS
      date: '2024-11-09'
      published_year: 2024
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/kffbZGa2yYhc6cakc/petri-an-open-source-auditing-tool-to-accelerate-ai-safety
      link_text: 'Petri: An open-source auditing tool to accelerate AI safety research'
      original_md: '* [**Petri: An open-source auditing tool to accelerate AI safety
        research**](https://lesswrong.com/posts/kffbZGa2yYhc6cakc/petri-an-open-source-auditing-tool-to-accelerate-ai-safety),
        2025-10-07, LessWrong'
      title: 'Petri: An open-source auditing tool to accelerate AI safety research'
      authors: []
      author_organizations:
      - Anthropic
      date: '2025-10-07'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    other_attributes: {}
  parsing_issues: []
- id: a:evals_other
  name: Other evals
  header_level: 2
  parent_id: sec:evals
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: Stephen✅
    one_sentence_summary: A collection of miscellaneous evaluations for specific alignment
      properties, such as honesty, shutdown resistance and sycophancy.
    theory_of_change: By developing novel benchmarks for specific, hard-to-measure
      properties (like honesty), critiquing the reliability of existing methods (like
      cultural surveys), and improving the formal rigor of evaluation systems (like
      LLM-as-Judges), researchers can create a more robust and comprehensive suite
      of evaluations to catch failures missed by standard capability or safety testing.
    see_also:
    - other more specific sections on evals
    orthodox_problems:
    - 'Other: none; a barometer of risk.'
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: behaviorist_science
    broad_approach_text: behaviorist science
    some_names:
    - Richard Ren
    - Mantas Mazeika
    - Andrés Corrada-Emmanuel
    - Ariba Khan
    - Stephen Casper
    estimated_ftes: 20-50
    critiques: '[The Unreliability of Evaluating Cultural Alignment in LLMs](https://arxiv.org/abs/2503.08688),
      [The Leaderboard Illusion](https://arxiv.org/abs/2504.20879)'
    funded_by: 'Lab funders (OpenAI), Open Philanthropy (which funds CAIS, the organization
      for the MASK benchmark), academic institutions.

      N/A (as a discrete amount). This work is part of the "tens of millions" budgets
      for broader evaluation and red-teaming efforts at labs and independent organizations.'
    outputs:
    - link_url: https://arxiv.org/abs/2509.14260
      link_text: Shutdown Resistance in Large Language Models
      original_md: '* [**Shutdown Resistance in Large Language Models**](https://arxiv.org/abs/2509.14260)'
      title: Shutdown Resistance in Large Language Models
      authors:
      - Jeremy Schlatter
      - Benjamin Weinstein-Raun
      - Jeffrey Ladish
      author_organizations: []
      date: '2025-09-13'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://gtr.dev/
      link_text: Gödel's Therapy Room
      original_md: '* [**Gödel''s Therapy Room**](https://gtr.dev/)'
      title: Gödel's Therapy Room — Where Alignment Goes to Die | LLM Eval Harness
        | Leaderboard
      authors: []
      author_organizations:
      - Deep Fork Cyber
      date: '2025-12-04'
      published_year: 2025
      venue: gtr.dev
      kind: other
    - link_url: https://arxiv.org/abs/2508.14927
      link_text: AI Testing Should Account for Sophisticated Strategic Behaviour
      original_md: '* [**AI Testing Should Account for Sophisticated Strategic Behaviour**](https://arxiv.org/abs/2508.14927),
        *Vojtech Kovarik, Eric Olav Chen, Sami Petersen et al.*, 2025-08-19, arXiv'
      title: AI Testing Should Account for Sophisticated Strategic Behaviour
      authors:
      - Vojtech Kovarik
      - Eric Olav Chen
      - Sami Petersen
      - Alexis Ghersengorin
      - Vincent Conitzer
      author_organizations: []
      date: '2025-08-19'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.00821
      link_text: Logical Consistency Between Disagreeing Experts and Its Role in AI
        Safety
      original_md: '* [**Logical Consistency Between Disagreeing Experts and Its Role
        in AI Safety**](https://arxiv.org/abs/2510.00821), *Andrés Corrada-Emmanuel*,
        2025-10-01, arXiv'
      title: Logical Consistency Between Disagreeing Experts and Its Role in AI Safety
      authors:
      - Andrés Corrada-Emmanuel
      author_organizations: []
      date: '2025-10-01'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://openai.com/index/expanding-on-sycophancy/
      link_text: Expanding on what we missed with sycophancy
      original_md: '* [**Expanding on what we missed with sycophancy**](https://openai.com/index/expanding-on-sycophancy/),
        *OpenAI*, 2025-05-02, OpenAI Blog'
      title: Expanding on what we missed with sycophancy
      authors: []
      author_organizations:
      - OpenAI
      date: '2025-05-02'
      published_year: 2025
      venue: OpenAI Blog
      kind: blog_post
    - link_url: https://arxiv.org/abs/2504.18412
      link_text: Expressing stigma and inappropriate responses prevents LLMs from
        safely replacing mental health providers
      original_md: '* [**Expressing stigma and inappropriate responses prevents LLMs
        from safely replacing mental health providers**](https://arxiv.org/abs/2504.18412),
        *Jared Moore, Declan Grabb, William Agnew et al.*, 2025-04-25, arXiv'
      title: Expressing stigma and inappropriate responses prevents LLMs from safely
        replacing mental health providers
      authors:
      - Jared Moore
      - Declan Grabb
      - William Agnew
      - Kevin Klyman
      - Stevie Chancellor
      - Desmond C. Ong
      - Nick Haber
      author_organizations: []
      date: '2025-04-25'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://eqbench.com/spiral-bench.html
      link_text: Spiral-Bench
      original_md: '* [**Spiral-Bench**](https://eqbench.com/spiral-bench.html), *Sam
        Paech*, eqbench.com'
      title: Spiral-Bench
      authors:
      - Sam Paech
      author_organizations: []
      date: null
      published_year: null
      venue: eqbench.com
      kind: dataset_benchmark
    - link_url: https://arxiv.org/abs/2507.03409
      link_text: 'Lessons from a Chimp: AI "Scheming" and the Quest for Ape Language'
      original_md: '* [**Lessons from a Chimp: AI "Scheming" and the Quest for Ape
        Language**](https://arxiv.org/abs/2507.03409), *Christopher Summerfield, Lennart
        Luettgau, Magda Dubois et al.*, 2025-07-04, arXiv'
      title: 'Lessons from a Chimp: AI "Scheming" and the Quest for Ape Language'
      authors:
      - Christopher Summerfield
      - Lennart Luettgau
      - Magda Dubois
      - Hannah Rose Kirk
      - Kobi Hackenburg
      - Catherine Fist
      - Katarina Slama
      - Nicola Ding
      - Rebecca Anselmetti
      - Andrew Strait
      - Mario Giulianelli
      - Cozmin Ududec
      author_organizations: []
      date: '2025-07-04'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.syco-bench.com/
      link_text: 'Syco-bench: A Benchmark for LLM Sycophancy'
      original_md: '* [**Syco-bench: A Benchmark for LLM Sycophancy**](https://www.syco-bench.com/)'
      title: 'Syco-bench: A Benchmark for LLM Sycophancy'
      authors:
      - Tim Duffy
      author_organizations: []
      date: null
      published_year: null
      venue: GitHub/Personal Project
      kind: dataset_benchmark
    - link_url: https://www.arxiv.org/abs/2510.01395
      link_text: Sycophantic AI Decreases Prosocial Intentions and Promotes Dependence
      original_md: '* [**Sycophantic AI Decreases Prosocial Intentions and Promotes
        Dependence**](https://www.arxiv.org/abs/2510.01395), *Myra Cheng, Cinoo Lee,
        Pranav Khadpe et al.*, 2025-10-01, arXiv'
      title: Sycophantic AI Decreases Prosocial Intentions and Promotes Dependence
      authors:
      - Myra Cheng
      - Cinoo Lee
      - Pranav Khadpe
      - Sunny Yu
      - Dyllan Han
      - Dan Jurafsky
      author_organizations:
      - Stanford University
      date: '2025-10-01'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.06134
      link_text: 'OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World
        AI Agent Safety'
      original_md: '* [**OpenAgentSafety: A Comprehensive Framework for Evaluating
        Real-World AI Agent Safety**](https://arxiv.org/abs/2507.06134), *Sanidhya
        Vijayvargiya, Aditya Bharat Soni, Xuhui Zhou et al.*, 2025-07-08, arXiv'
      title: 'OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World
        AI Agent Safety'
      authors:
      - Sanidhya Vijayvargiya
      - Aditya Bharat Soni
      - Xuhui Zhou
      - Zora Zhiruo Wang
      - Nouha Dziri
      - Graham Neubig
      - Maarten Sap
      author_organizations:
      - Carnegie Mellon University
      date: '2025-07-08'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.13082
      link_text: 'Discerning What Matters: A Multi-Dimensional Assessment of Moral
        Competence in LLMs'
      original_md: '* [**Discerning What Matters: A Multi-Dimensional Assessment of
        Moral Competence in LLMs**](https://arxiv.org/abs/2506.13082), *Daniel Kilov,
        Caroline Hendy, Secil Yanik Guyot et al.*, 2025-06-16, arXiv'
      title: 'Discerning What Matters: A Multi-Dimensional Assessment of Moral Competence
        in LLMs'
      authors:
      - Daniel Kilov
      - Caroline Hendy
      - Secil Yanik Guyot
      - Aaron J. Snoswell
      - Seth Lazar
      author_organizations: []
      date: '2025-06-16'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.02825
      link_text: Establishing Best Practices for Building Rigorous Agentic Benchmarks
      original_md: '* [**Establishing Best Practices for Building Rigorous Agentic
        Benchmarks**](https://arxiv.org/abs/2507.02825), *Yuxuan Zhu, Tengjun Jin,
        Yada Pruksachatkun et al.*, 2025-07-03, arXiv'
      title: Establishing Best Practices for Building Rigorous Agentic Benchmarks
      authors:
      - Yuxuan Zhu
      - Tengjun Jin
      - Yada Pruksachatkun
      - Andy Zhang
      - Shu Liu
      - Sasha Cui
      - Sayash Kapoor
      - Shayne Longpre
      - Kevin Meng
      - Rebecca Weiss
      - Fazl Barez
      - Rahul Gupta
      - Jwala Dhamala
      - Jacob Merizian
      - Mario Giulianelli
      - Harry Coppock
      - Cozmin Ududec
      - Jasjeet Sekhon
      - Jacob Steinhardt
      - Antony Kellermann
      - Sarah Schwettmann
      - Matei Zaharia
      - Ion Stoica
      - Percy Liang
      - Daniel Kang
      author_organizations:
      - Stanford University
      - UC Berkeley
      date: '2025-07-03'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/B2o6nrxwKxLPsSYdh/do-llms-comply-differently-during-tests-is-this-a-hidden
      link_text: Do LLMs Comply Differently During Tests? Is This a Hidden Variable
        in Safety Evaluation? And Can We Steer That?
      original_md: '* [**Do LLMs Comply Differently During Tests? Is This a Hidden
        Variable in Safety Evaluation? And Can We Steer That?**](https://lesswrong.com/posts/B2o6nrxwKxLPsSYdh/do-llms-comply-differently-during-tests-is-this-a-hidden),
        *Sahar Abdelnabi, Ahmed Salem*, 2025-06-16, LessWrong'
      title: Do LLMs Comply Differently During Tests? Is This a Hidden Variable in
        Safety Evaluation? And Can We Steer That?
      authors:
      - Sahar Abdelnabi
      - Ahmed Salem
      author_organizations:
      - Microsoft
      date: '2025-06-16'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/PejNckwQj3A2MGhMA/systematic-runaway-optimiser-like-llm-failure-modes-on
      link_text: Systematic runaway-optimiser-like LLM failure modes on Biologically
        and Economically aligned AI safety benchmarks for LLMs with simplified observation
        format (BioBlue)
      original_md: '* [**Systematic runaway-optimiser-like LLM failure modes on Biologically
        and Economically aligned AI safety benchmarks for LLMs with simplified observation
        format (BioBlue)**](https://lesswrong.com/posts/PejNckwQj3A2MGhMA/systematic-runaway-optimiser-like-llm-failure-modes-on),
        *Roland Pihlakas, Sruthi Susan Kuriakose, Shruti Datta Gupta*, 2025-03-16,
        LessWrong'
      title: Systematic runaway-optimiser-like LLM failure modes on Biologically and
        Economically aligned AI safety benchmarks for LLMs with simplified observation
        format (BioBlue)
      authors:
      - Roland Pihlakas
      - Sruthi Susan Kuriakose
      - Shruti Datta Gupta
      author_organizations: []
      date: '2025-03-16'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/bGYQgBPEyHidnZCdE/towards-alignment-auditing-as-a-numbers-go-up-science
      link_text: Towards Alignment Auditing as a Numbers-Go-Up Science
      original_md: '* [**Towards Alignment Auditing as a Numbers-Go-Up Science**](https://lesswrong.com/posts/bGYQgBPEyHidnZCdE/towards-alignment-auditing-as-a-numbers-go-up-science),
        *Sam Marks*, 2025-08-04, LessWrong'
      title: Towards Alignment Auditing as a Numbers-Go-Up Science
      authors:
      - Sam Marks
      author_organizations:
      - Anthropic
      date: '2025-08-04'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    other_attributes: {}
  parsing_issues:
  - 'Orthodox problems field says ''none; a barometer of risk.'' - no matching problem
    found, kept as ''Other: none; a barometer of risk.'''
  - Broad approach field says 'behavioral.' - mapped to 'behaviorist_science'
