source_file: data/2025-12-16-draft-post-review/source.md
items:
- id: sec:Labs_giant_companies_
  name: Labs (giant companies)
  header_level: 1
  parent_id: null
  content: ''
  item_type: section
  agenda_attributes: null
  parsing_issues: []
- id: a:OpenAI
  name: OpenAI
  header_level: 3
  parent_id: sec:Labs_giant_companies_
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: null
    theory_of_change: null
    see_also:
    - sec:Iterative_alignment
    - a:Safeguards_inference_time_auxiliaries_
    - a:Character_training_and_persona_steering
    orthodox_problems: []
    target_case_id: null
    target_case_text: null
    broad_approach_id: null
    broad_approach_text: null
    some_names:
    - Johannes Heidecke
    - Boaz Barak
    - Mia Glaese
    - Jenny Nitishinskaya
    - Lama Ahmad
    - Naomi Bashkansky
    - Miles Wang
    - Wojciech Zaremba
    - David Robinson
    - Zico Kolter
    - Jerry Tworek
    - Eric Wallace
    - Olivia Watkins
    - Kai Chen
    - Chris Koch
    - Andrea Vallone
    - Leo Gao
    estimated_ftes: null
    critiques: '[Stein-Perlman](https://ailabwatch.org/companies/openai), [Stewart](https://intelligence.org/2025/03/31/a-response-to-openais-how-we-think-about-safety-and-alignment/),
      [underelicitation](https://www.lesswrong.com/posts/AK6AihHGjirdoiJg6/ai-companies-eval-reports-mostly-don-t-support-their-claims),
      [Midas](https://www.openaifiles.org/transparency-and-safety), [defense](https://www.wired.com/story/openai-anduril-defense/),
      [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).)
      on labs in general. It''s [difficult](https://conversationswithtyler.com/episodes/sam-altman-2/)
      to model OpenAI as a single agent: *"ALTMAN: I very rarely get to have anybody
      work on anything. One thing about researchers is they''re going to work on what
      they''re going to work on, and that''s that."*'
    funded_by: Microsoft, [AWS](https://www.aboutamazon.com/news/aws/aws-open-ai-workloads-compute-infrastructure),
      Oracle, NVIDIA, SoftBank, G42, AMD, Dragoneer, Coatue, Thrive, Altimeter, MGX,
      Blackstone, TPG, T. Rowe Price, Andreessen Horowitz, D1 Capital Partners, Fidelity
      Investments, Founders Fund, Sequoia…
    outputs:
    - link_url: null
      link_text: Their 60-page System Cards now contain a large amount of their public
        safety work.
      original_md: Their 60-page System Cards now contain a large amount of their
        public safety work.
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://arxiv.org/abs/2503.11926
      link_text: Monitoring Reasoning Models for Misbehavior and the Risks of Promoting
        Obfuscation
      original_md: '[Monitoring Reasoning Models for Misbehavior and the Risks of
        Promoting Obfuscation](https://arxiv.org/abs/2503.11926)'
      title: Monitoring Reasoning Models for Misbehavior and the Risks of Promoting
        Obfuscation
      authors:
      - Bowen Baker
      - Joost Huizinga
      - Leo Gao
      - Zehao Dou
      - Melody Y. Guan
      - Aleksander Madry
      - Wojciech Zaremba
      - Jakub Pachocki
      - David Farhi
      author_organizations:
      - OpenAI
      date: '2025-03-14'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.19823
      link_text: Persona Features Control Emergent Misalignment
      original_md: '[Persona Features Control Emergent Misalignment](https://arxiv.org/abs/2506.19823)'
      title: Persona Features Control Emergent Misalignment
      authors:
      - Miles Wang
      - Tom Dupré la Tour
      - Olivia Watkins
      - Alex Makelov
      - Ryan A. Chi
      - Samuel Miserendino
      - Jeffrey Wang
      - Achyuta Rajaram
      - Johannes Heidecke
      - Tejal Patwardhan
      - Dan Mossing
      author_organizations: []
      date: '2025-06-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.15541
      link_text: Stress Testing Deliberative Alignment for Anti-Scheming Training
      original_md: '[Stress Testing Deliberative Alignment for Anti-Scheming Training](https://arxiv.org/abs/2509.15541)'
      title: Stress Testing Deliberative Alignment for Anti-Scheming Training
      authors:
      - Bronson Schoen
      - Evgenia Nitishinskaya
      - Mikita Balesni
      - Axel Højmark
      - Felix Hofstätter
      - Jérémy Scheurer
      - Alexander Meinke
      - Jason Wolfe
      - Teun van der Weij
      - Alex Lloyd
      - Nicholas Goldowsky-Dill
      - Angela Fan
      - Andrei Matveiakin
      - Rusheb Shah
      - Marcus Williams
      - Amelia Glaese
      - Boaz Barak
      - Wojciech Zaremba
      - Marius Hobbhahn
      author_organizations:
      - OpenAI
      - Anthropic
      date: '2025-09-19'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2412.16339
      link_text: 'Deliberative Alignment: Reasoning Enables Safer Language Models'
      original_md: '[Deliberative Alignment: Reasoning Enables Safer Language Models](https://arxiv.org/abs/2412.16339)'
      title: 'Deliberative Alignment: Reasoning Enables Safer Language Models'
      authors:
      - Melody Y. Guan
      - Manas Joglekar
      - Eric Wallace
      - Saachi Jain
      - Boaz Barak
      - Alec Helyar
      - Rachel Dias
      - Andrea Vallone
      - Hongyu Ren
      - Jason Wei
      - Hyung Won Chung
      - Sam Toyer
      - Johannes Heidecke
      - Alex Beutel
      - Amelia Glaese
      author_organizations:
      - OpenAI
      date: '2024-12-20'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://openai.com/index/emergent-misalignment
      link_text: Toward understanding and preventing misalignment generalization
      original_md: '[Toward understanding and preventing misalignment generalization](https://openai.com/index/emergent-misalignment)'
      title: Toward understanding and preventing misalignment generalization
      authors:
      - Miles Wang
      - Tom Dupré la Tour
      - Olivia Watkins
      - Aleksandar Makelov
      - Ryan A. Chi
      - Samuel Miserendino
      - Tejal Patwardhan
      - Dan Mossing
      author_organizations:
      - OpenAI
      date: '2025-06-18'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://openai.com/index/updating-our-preparedness-framework/
      link_text: Our updated Preparedness Framework
      original_md: '[Our updated Preparedness Framework](https://openai.com/index/updating-our-preparedness-framework/)'
      title: Our updated Preparedness Framework
      authors:
      - OpenAI Preparedness Team
      author_organizations:
      - OpenAI
      date: '2025-04-15'
      published_year: 2025
      venue: OpenAI Blog
      kind: agenda_manifesto
    - link_url: https://arxiv.org/abs/2501.18841
      link_text: Trading Inference-Time Compute for Adversarial Robustness
      original_md: '[Trading Inference-Time Compute for Adversarial Robustness](https://arxiv.org/abs/2501.18841)'
      title: Trading Inference-Time Compute for Adversarial Robustness
      authors:
      - Wojciech Zaremba
      - Evgenia Nitishinskaya
      - Boaz Barak
      - Stephanie Lin
      - Sam Toyer
      - Yaodong Yu
      - Rachel Dias
      - Eric Wallace
      - Kai Xiao
      - Johannes Heidecke
      - Amelia Glaese
      author_organizations:
      - OpenAI
      date: '2025-01-31'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.16260
      link_text: 'Small-to-Large Generalization: Data Influences Models Consistently
        Across Scale'
      original_md: '[Small-to-Large Generalization: Data Influences Models Consistently
        Across Scale](https://arxiv.org/abs/2505.16260)'
      title: 'Small-to-Large Generalization: Data Influences Models Consistently Across
        Scale'
      authors:
      - Alaa Khaddaj
      - Logan Engstrom
      - Aleksander Madry
      author_organizations:
      - MIT
      date: '2025-05-22'
      published_year: 2025
      venue: ICLR 2025
      kind: paper_published
    - link_url: https://openai.com/index/openai-anthropic-safety-evaluation
      link_text: 'Findings from a pilot Anthropic–OpenAI alignment evaluation exercise:
        OpenAI Safety Tests'
      original_md: '[Findings from a pilot Anthropic–OpenAI alignment evaluation exercise:
        OpenAI Safety Tests](https://openai.com/index/openai-anthropic-safety-evaluation)'
      title: 'Findings from a pilot Anthropic–OpenAI alignment evaluation exercise:
        OpenAI Safety Tests'
      authors: []
      author_organizations:
      - OpenAI
      - Anthropic
      date: '2025-08-27'
      published_year: 2025
      venue: OpenAI Blog
      kind: news_announcement
    - link_url: https://openai.com/safety/evaluations-hub
      link_text: Safety evaluations hub
      original_md: '[Safety evaluations hub](https://openai.com/safety/evaluations-hub)'
      title: Safety evaluations hub
      authors: []
      author_organizations:
      - OpenAI
      date: '2025-08-15'
      published_year: 2025
      venue: OpenAI Website
      kind: news_announcement
    - link_url: https://alignment.openai.com/
      link_text: alignment.openai.com
      original_md: '[alignment.openai.com](https://alignment.openai.com/)'
      title: Alignment Research Blog
      authors: []
      author_organizations:
      - OpenAI
      date: '2025-12-01'
      published_year: 2025
      venue: OpenAI Alignment Blog
      kind: blog_post
    - link_url: https://cdn.openai.com/pdf/41df8f28-d4ef-43e9-aed2-823f9393e470/circuit-sparsity-paper.pdf
      link_text: Weight-sparse transformers have interpretable circuits
      original_md: '[Weight-sparse transformers have interpretable circuits](https://cdn.openai.com/pdf/41df8f28-d4ef-43e9-aed2-823f9393e470/circuit-sparsity-paper.pdf)'
      title: Unknown - PDF content not accessible
      authors: []
      author_organizations:
      - OpenAI
      date: null
      published_year: null
      venue: null
      kind: error_detected
    other_attributes:
      Structure: public benefit corp
      Safety teams: Alignment, Safety Systems (Interpretability, Safety Oversight,
        Pretraining Safety, Robustness, Safety Research, Trustworthy AI, new Misalignment
        Research team [coming](https://archive.is/eDB1D)), Preparedness, Model Policy,
        Safety and Security Committee, Safety Advisory Group. The [Persona Features](https://www.arxiv.org/pdf/2506.19823)
        paper had a distinct author list. No named successor to Superalignment.
      Public alignment agenda: '[None](https://openai.com/safety/how-we-think-about-safety-alignment/).
        Boaz Barak [offers](https://www.lesswrong.com/posts/3jnziqCF3vA2NXAKp/six-thoughts-on-ai-safety)
        personal [views](https://windowsontheory.org/2025/06/24/machines-of-faithful-obedience/).'
      Framework: '[Preparedness Framework](https://cdn.openai.com/pdf/18a02b5d-6b67-4cec-ab64-68cdfbddebcd/preparedness-framework-v2.pdf)'
  parsing_issues: []
- id: a:Google_Deepmind
  name: Google Deepmind
  header_level: 3
  parent_id: sec:Labs_giant_companies_
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: null
    theory_of_change: null
    see_also:
    - sec:White_box_safety_i_e_Interpretability_
    - Scalable Oversight
    orthodox_problems: []
    target_case_id: null
    target_case_text: null
    broad_approach_id: null
    broad_approach_text: null
    some_names:
    - Rohin Shah
    - Allan Dafoe
    - Anca Dragan
    - Alex Irpan
    - Alex Turner
    - Anna Wang
    - Arthur Conmy
    - David Lindner
    - Jonah Brown-Cohen
    - Lewis Ho
    - Neel Nanda
    - Raluca Ada Popa
    - Rishub Jain
    - Rory Greig
    - Sebastian Farquhar
    - Senthooran Rajamanoharan
    - Sophie Bridgers
    - Tobi Ijitoye
    - Tom Everitt
    - Victoria Krakovna
    - Vikrant Varma
    - Zac Kenton
    - Four Flynn
    - Jonathan Richens
    - Lewis Smith
    - Janos Kramar
    - Matthew Rahtz
    - Mary Phuong
    - Erik Jenner
    estimated_ftes: null
    critiques: '[Stein-Perlman](https://ailabwatch.org/companies/deepmind), [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).)
      on labs in general, [underelicitation](https://www.lesswrong.com/posts/AK6AihHGjirdoiJg6/ai-companies-eval-reports-mostly-don-t-support-their-claims),
      [On Google''s Safety Plan](https://lesswrong.com/posts/hvEikwtsbf6zaXG2s/on-google-s-safety-plan)'
    funded_by: Google. Explicit 2024 Deepmind spending as a whole was [£1.3B](https://s3.eu-west-2.amazonaws.com/document-api-images-live.ch.gov.uk/docs/WT_VNJe9leRjfcU0-OtRjWqF7WiqueStclXgHPbdG4U/application-pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAWRGBDBV3HTI6EAXB%2F20251212%2Feu-west-2%2Fs3%2Faws4_request&X-Amz-Date=20251212T104902Z&X-Amz-Expires=60&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEDcaCWV1LXdlc3QtMiJGMEQCIH6GyZRz66qwZsWkZORhsAQyzQQoJ7j0F4jnnWsgT8i2AiBkQVTWkLiSt%2F89o2yMC5D9NGQ75b1RwC8MBr7dlvrJXyqBBAgAEAUaDDQ0OTIyOTAzMjgyMiIMDkbkT%2FH2vUMV3NxOKt4DfZ6bw%2BWE%2BPfifW4goryaR4bQ%2FeFEXDvW7MU%2BKlfUM8A2fNyEUpIq4f6PsRf0zntVIXmUOWnvyIcVB7EA31NcPn3O%2FHFga8gKZyDPnQnj7YM5Wrt%2FVvR2mj7dJcioOSATW6joYuAb2X0l6IVHXJnYcaxStVCaPauK98OWTTXwCQQwG9UYBWe5SGqOroOw%2FoYWx9GRGvDtQfQThGemJnDr%2FHkbM9YH%2BY860lrE4MEXQiPakkwgJZC%2B8kqsqxzAIyWegPjp3TvrNs7WJ4Fheq0BJo8B7uw0pYBB%2BE9WQEjgaO5dByd90cpnyHu%2F8HGSxwmuQQiUtrp0T3xpP1G%2B3bP%2FLUnhGTD6XWLW%2BtoywQ5ZJrizfwuLQuxFjZt2JwV50DslF47H4AltBRxQh6HHro%2BpiJJEv0rC5NKBS4XRaL8FWOFMD%2BxJctPoCxFJhour3SbcMET4148eVQL%2FenkSdPUz2FHNrO%2BnOTyZAG%2Bi9xiZR1MVOCYHTPHKFG9ReY4ck2mz4W94%2FI6iWuu%2BKWlrEr2hEWzo2RhwDJ09ASgoKNErYb2mJ4E0rMGQ7cv8d2bqF7f6ok1SbzJPClaCBN4qYBzX1rE2Uhdf4v2QueSi4c0i8oCWOGfsdp5FxpgrOlEIqzC9%2Fu7JBjqmAardqlTk%2BobAEzv0H0m2RO4m901C%2FsTzIKb2UlMRrUkTDH4MpCSg5eW3A86X2TnPfl66jC%2FV2P%2FIwY%2FkvsY7wNBgtYR92XE%2FMwyz1x3JD1qDnGWPybjso72aEPrMyekV2WV3U0%2BYh8zn83%2BneYZB9VaTu2QqSv7TZe3IWJyErbuZw%2BhmMlk5nhKZDNmo%2Fc12x%2B7jI0N6aKqUdp8BkGOqPrlUxn2mKcg%3D&X-Amz-SignedHeaders=host&response-content-disposition=inline%3Bfilename%3D%22companies_house_document.pdf%22&X-Amz-Signature=52be18d98d9589fa46d3686876b3107925b67ee083d05199e1428dfc14b9c457),
      but this doesn't count most spending e.g. Gemini compute.
    outputs:
    - link_url: https://www.alignmentforum.org/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-interpretability%20
      link_text: A Pragmatic Vision for Interpretability
      original_md: '[A Pragmatic Vision for Interpretability](https://www.alignmentforum.org/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-interpretability%20)'
      title: A Pragmatic Vision for Interpretability
      authors:
      - Neel Nanda
      - Josh Engels
      - Arthur Conmy
      - Senthooran Rajamanoharan
      - bilalchughtai
      - CallumMcDougall
      - János Kramár
      - lewis smith
      author_organizations:
      - Google DeepMind
      date: '2025-12-01'
      published_year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - link_url: https://www.alignmentforum.org/posts/MnkeepcGirnJn736j/how-can-interpretability-researchers-help-agi-go-well%20
      link_text: How Can Interpretability Researchers Help AGI Go Well?
      original_md: '[How Can Interpretability Researchers Help AGI Go Well?](https://www.alignmentforum.org/posts/MnkeepcGirnJn736j/how-can-interpretability-researchers-help-agi-go-well%20)'
      title: How Can Interpretability Researchers Help AGI Go Well?
      authors:
      - Neel Nanda
      - Josh Engels
      - Senthooran Rajamanoharan
      - Arthur Conmy
      - bilalchughtai
      - CallumMcDougall
      - János Kramár
      - lewis smith
      author_organizations:
      - Google DeepMind
      date: '2024-12-01'
      published_year: 2024
      venue: AI Alignment Forum
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2505.01420
      link_text: Evaluating Frontier Models for Stealth and Situational Awareness
      original_md: '[Evaluating Frontier Models for Stealth and Situational Awareness](https://arxiv.org/abs/2505.01420)'
      title: Evaluating Frontier Models for Stealth and Situational Awareness
      authors:
      - Mary Phuong
      - Roland S. Zimmermann
      - Ziyue Wang
      - David Lindner
      - Victoria Krakovna
      - Sarah Cogan
      - Allan Dafoe
      - Lewis Ho
      - Rohin Shah
      author_organizations:
      - Google DeepMind
      date: '2025-05-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.05246
      link_text: When Chain of Thought is Necessary, Language Models Struggle to Evade
        Monitors
      original_md: '[When Chain of Thought is Necessary, Language Models Struggle
        to Evade Monitors](https://arxiv.org/abs/2507.05246)'
      title: When Chain of Thought is Necessary, Language Models Struggle to Evade
        Monitors
      authors:
      - Scott Emmons
      - Erik Jenner
      - David K. Elson
      - Rif A. Saurous
      - Senthooran Rajamanoharan
      - Heng Chen
      - Irhum Shafkat
      - Rohin Shah
      author_organizations:
      - Google DeepMind
      date: '2025-07-07'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://alignmentforum.org/posts/zWySWKuXnhMDhgwc3/mona-managed-myopia-with-approval-feedback-2
      link_text: 'MONA: Managed Myopia with Approval Feedback'
      original_md: '[MONA: Managed Myopia with Approval Feedback](https://alignmentforum.org/posts/zWySWKuXnhMDhgwc3/mona-managed-myopia-with-approval-feedback-2)'
      title: 'MONA: Managed Myopia with Approval Feedback'
      authors:
      - Sebastian Farquhar
      - David Lindner
      - Rohin Shah
      author_organizations:
      - DeepMind
      date: '2025-01-23'
      published_year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2510.27062
      link_text: Consistency Training Helps Stop Sycophancy and Jailbreaks
      original_md: '[Consistency Training Helps Stop Sycophancy and Jailbreaks](https://arxiv.org/abs/2510.27062)'
      title: Consistency Training Helps Stop Sycophancy and Jailbreaks
      authors:
      - Alex Irpan
      - Alexander Matt Turner
      - Mark Kurzeja
      - David K. Elson
      - Rohin Shah
      author_organizations:
      - DeepMind
      - Google
      date: '2025-10-31'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.01849
      link_text: An Approach to Technical AGI Safety and Security
      original_md: '[An Approach to Technical AGI Safety and Security](https://arxiv.org/abs/2504.01849)'
      title: An Approach to Technical AGI Safety and Security
      authors:
      - Rohin Shah
      - Alex Irpan
      - Alexander Matt Turner
      - Anna Wang
      - Arthur Conmy
      - David Lindner
      - Jonah Brown-Cohen
      - Lewis Ho
      - Neel Nanda
      - Raluca Ada Popa
      - Rishub Jain
      - Rory Greig
      - Samuel Albanie
      - Scott Emmons
      - Sebastian Farquhar
      - Sébastien Krier
      - Senthooran Rajamanoharan
      - Sophie Bridgers
      - Tobi Ijitoye
      - Tom Everitt
      - Victoria Krakovna
      - Vikrant Varma
      - Vladimir Mikulik
      - Zachary Kenton
      - Dave Orr
      - Shane Legg
      - Noah Goodman
      - Allan Dafoe
      - Four Flynn
      - Anca Dragan
      author_organizations:
      - Google DeepMind
      - Anthropic
      - Stanford University
      - UC Berkeley
      - Redwood Research
      date: '2025-04-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://alignmentforum.org/posts/4uXCAJNuPKtKBsi28/negative-results-for-saes-on-downstream-tasks
      link_text: 'Negative Results for SAEs On Downstream Tasks and Deprioritising
        SAE Research (GDM Mech Interp Team Progress Update #2)'
      original_md: '[Negative Results for SAEs On Downstream Tasks and Deprioritising
        SAE Research (GDM Mech Interp Team Progress Update \#2)](https://alignmentforum.org/posts/4uXCAJNuPKtKBsi28/negative-results-for-saes-on-downstream-tasks)'
      title: 'Negative Results for SAEs On Downstream Tasks and Deprioritising SAE
        Research (GDM Mech Interp Team Progress Update #2)'
      authors:
      - Lewis Smith
      - Senthooran Rajamanoharan
      - Arthur Conmy
      - Callum McDougall
      - Tom Lieberum
      - János Kramár
      - Rohin Shah
      - Neel Nanda
      author_organizations:
      - Google DeepMind
      date: '2025-03-26'
      published_year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - link_url: https://turntrout.com/gemini-steering
      link_text: Steering Gemini Using BIDPO Vectors
      original_md: '[Steering Gemini Using BIDPO Vectors](https://turntrout.com/gemini-steering)'
      title: Steering Gemini Using BIDPO Vectors
      authors:
      - Alex Turner
      - Mark Kurzeja
      - Dave Orr
      - David Elson
      author_organizations:
      - Google DeepMind
      date: '2025-01-30'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/html/2511.22662v1
      link_text: Difficulties with Evaluating a Deception Detector for AIs
      original_md: '[Difficulties with Evaluating a Deception Detector for AIs](https://arxiv.org/html/2511.22662v1)'
      title: Difficulties with Evaluating a Deception Detector for AIs
      authors:
      - Lewis Smith
      - Bilal Chughtai
      - Neel Nanda
      author_organizations:
      - Google
      date: '2025-11-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://deepmind.google/discover/blog/taking-a-responsible-path-to-agi/
      link_text: Taking a responsible path to AGI
      original_md: '[Taking a responsible path to AGI](https://deepmind.google/discover/blog/taking-a-responsible-path-to-agi/)'
      title: Taking a responsible path to AGI
      authors:
      - Anca Dragan
      - Rohin Shah
      - Four Flynn
      - Shane Legg
      author_organizations:
      - Google DeepMind
      date: '2025-04-02'
      published_year: 2025
      venue: Google DeepMind Blog
      kind: blog_post
    - link_url: https://deepmind.google/discover/blog/evaluating-potential-cybersecurity-threats-of-advanced-ai
      link_text: Evaluating potential cybersecurity threats of advanced AI
      original_md: '[Evaluating potential cybersecurity threats of advanced AI](https://deepmind.google/discover/blog/evaluating-potential-cybersecurity-threats-of-advanced-ai)'
      title: Evaluating potential cybersecurity threats of advanced AI
      authors:
      - Four Flynn
      - Mikel Rodriguez
      - Raluca Ada Popa
      author_organizations:
      - Google DeepMind
      date: '2025-04-02'
      published_year: 2025
      venue: Google DeepMind Blog
      kind: blog_post
    - link_url: https://www.alignmentforum.org/posts/wnzkjSmrgWZaBa2aC/self-preservation-or-instruction-ambiguity-examining-the
      link_text: Self-preservation or Instruction Ambiguity? Examining the Causes
        of Shutdown Resistance
      original_md: '[Self-preservation or Instruction Ambiguity? Examining the Causes
        of Shutdown Resistance](https://www.alignmentforum.org/posts/wnzkjSmrgWZaBa2aC/self-preservation-or-instruction-ambiguity-examining-the)'
      title: Self-preservation or Instruction Ambiguity? Examining the Causes of Shutdown
        Resistance
      authors:
      - Senthooran Rajamanoharan
      - Neel Nanda
      author_organizations:
      - Google DeepMind
      date: '2025-07-14'
      published_year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2510.23966
      link_text: A Pragmatic Way to Measure Chain-of-Thought Monitorability
      original_md: '[A Pragmatic Way to Measure Chain-of-Thought Monitorability](https://arxiv.org/abs/2510.23966)'
      title: A Pragmatic Way to Measure Chain-of-Thought Monitorability
      authors:
      - Scott Emmons
      - Roland S. Zimmermann
      - David K. Elson
      - Rohin Shah
      author_organizations:
      - Google DeepMind
      date: '2025-10-28'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes:
      Structure: research laboratory subsidiary of a for-profit
      Safety teams: amplified oversight, interpretability, ASAT eng (automated alignment
        research), Causal Incentives Working Group, Frontier Safety Risk Assessment
        (evals, threat models, the framework), Mitigations (e.g. banning accounts,
        refusal training, jailbreak robustness), Loss of Control (control, alignment
        evals). Structure [here](https://gist.github.com/g-leech/30f2484d0318b5d9d489e5748fe46131).
      Public alignment agenda: '[An Approach to Technical AGI Safety and Security](https://arxiv.org/abs/2504.01849)'
      Risk management framework: '[Frontier Safety Framework](https://deepmind.google/blog/strengthening-our-frontier-safety-framework/)'
  parsing_issues: []
- id: a:Anthropic
  name: Anthropic
  header_level: 3
  parent_id: sec:Labs_giant_companies_
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: null
    theory_of_change: null
    see_also:
    - sec:White_box_safety_i_e_Interpretability_
    - Scalable Oversight
    orthodox_problems: []
    target_case_id: null
    target_case_text: null
    broad_approach_id: null
    broad_approach_text: null
    some_names:
    - Chris Olah
    - Evan Hubinger
    - Sam Marks
    - Johannes Treutlein
    - Sam Bowman
    - Euan Ong
    - Fabien Roger
    - Adam Jermyn
    - Holden Karnofsky
    - Jan Leike
    - Ethan Perez
    - Jack Lindsey
    - Amanda Askell
    - Kyle Fish
    - Sara Price
    - Jon Kutasov
    - Minae Kwon
    - Monty Evans
    - Richard Dargan
    - Roger Grosse
    - Ben Levinstein
    - Joseph Carlsmith
    - Joe Benton
    estimated_ftes: null
    critiques: '[Stein](https://ailabwatch.org/anthropic-opinions)[-Perlman](https://ailabwatch.org/companies/anthropic),
      [Casper](https://www.lesswrong.com/posts/pH6tyhEnngqWAXi9i/eis-xiii-reflections-on-anthropic-s-sae-research-circa-may#A_review___thoughts),
      [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%2Dpeople%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).),
      [underelicitation](https://www.lesswrong.com/posts/AK6AihHGjirdoiJg6/ai-companies-eval-reports-mostly-don-t-support-their-claims),
      [Greenblatt](https://nitter.net/RyanPGreenblatt/status/1925992236648464774),
      [Samin](https://www.lesswrong.com/posts/5aKRshJzhojqfbRyo/unless-its-governance-changes-anthropic-is-untrustworthy),
      [defense](https://techcrunch.com/2024/11/07/anthropic-teams-up-with-palantir-and-aws-to-sell-its-ai-to-defense-customers/),
      [Existing Safety Frameworks Imply Unreasonable Confidence](https://lesswrong.com/posts/7ExkgcDudwhag73vw/existing-safety-frameworks-imply-unreasonable-confidence)'
    funded_by: Amazon, Google, ICONIQ, Fidelity, Lightspeed, Altimeter, Baillie Gifford,
      BlackRock, Blackstone, Coatue, D1 Capital Partners, General Atlantic, General
      Catalyst, GIC, Goldman Sachs, Insight Partners, Jane Street, Ontario Teachers'
      Pension Plan, Qatar Investment Authority, TPG, T. Rowe Price, WCM, XN
    outputs:
    - link_url: https://alignment.anthropic.com/2025/honesty-elicitation/
      link_text: Evaluating honesty and lie detection techniques on a diverse suite
        of dishonest models
      original_md: '[Evaluating honesty and lie detection techniques on a diverse
        suite of dishonest models](https://alignment.anthropic.com/2025/honesty-elicitation/)'
      title: Evaluating honesty and lie detection techniques on a diverse suite of
        dishonest models
      authors:
      - Rowan Wang
      - Johannes Treutlein
      - Fabien Roger
      - Evan Hubinger
      - Sam Marks
      author_organizations:
      - Anthropic
      date: '2025-11-25'
      published_year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - link_url: https://anthropic.com/research/agentic-misalignment
      link_text: 'Agentic Misalignment: How LLMs could be insider threats'
      original_md: '[Agentic Misalignment: How LLMs could be insider threats](https://anthropic.com/research/agentic-misalignment)'
      title: 'Agentic Misalignment: How LLMs could be insider threats'
      authors:
      - Aengus Lynch
      - Benjamin Wright
      - Caleb Larson
      - Kevin K. Troy
      - Stuart J. Ritchie
      - Sören Mindermann
      - Ethan Perez
      - Evan Hubinger
      author_organizations:
      - Anthropic
      - University College London
      - MATS
      - Mila
      date: '2025-06-20'
      published_year: 2025
      venue: Anthropic Research
      kind: blog_post
    - link_url: https://alignmentforum.org/posts/ghESoA8mo3fv9Yx3E/why-do-some-language-models-fake-alignment-while-others-don
      link_text: Why Do Some Language Models Fake Alignment While Others Don't?
      original_md: '[Why Do Some Language Models Fake Alignment While Others Don''t?](https://alignmentforum.org/posts/ghESoA8mo3fv9Yx3E/why-do-some-language-models-fake-alignment-while-others-don)'
      title: Why Do Some Language Models Fake Alignment While Others Don't?
      authors:
      - abhayesian
      - John Hughes
      - Alex Mallen
      - Jozdien
      - janus
      - Fabien Roger
      author_organizations:
      - Anthropic
      - Redwood Research
      date: '2025-07-08'
      published_year: 2025
      venue: arXiv
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2502.16797
      link_text: Forecasting Rare Language Model Behaviors
      original_md: '[Forecasting Rare Language Model Behaviors](https://arxiv.org/abs/2502.16797)'
      title: Forecasting Rare Language Model Behaviors
      authors:
      - Erik Jones
      - Meg Tong
      - Jesse Mu
      - Mohammed Mahfoud
      - Jan Leike
      - Roger Grosse
      - Jared Kaplan
      - William Fithian
      - Ethan Perez
      - Mrinank Sharma
      author_organizations:
      - Anthropic
      - OpenAI
      - UC Berkeley
      date: '2025-02-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://alignment.anthropic.com/2025/openai-findings
      link_text: Findings from a Pilot Anthropic—OpenAI Alignment Evaluation Exercise
      original_md: '[Findings from a Pilot Anthropic—OpenAI Alignment Evaluation Exercise](https://alignment.anthropic.com/2025/openai-findings)'
      title: Findings from a Pilot Anthropic—OpenAI Alignment Evaluation Exercise
      authors:
      - Samuel R. Bowman
      - Megha Srivastava
      - Jon Kutasov
      - Rowan Wang
      - Trenton Bricken
      - Benjamin Wright
      - Ethan Perez
      - Nicholas Carlini
      author_organizations:
      - Anthropic
      - OpenAI
      date: '2025-08-27'
      published_year: 2025
      venue: Alignment Science Blog
      kind: blog_post
    - link_url: https://transformer-circuits.pub/2025/attribution-graphs/biology.html
      link_text: On the Biology of a Large Language Model
      original_md: '[On the Biology of a Large Language Model](https://transformer-circuits.pub/2025/attribution-graphs/biology.html)'
      title: On the Biology of a Large Language Model
      authors:
      - Jack Lindsey
      - Wes Gurnee
      - Emmanuel Ameisen
      - Brian Chen
      - Adam Pearce
      - Nicholas L. Turner
      - Craig Citro
      - David Abrahams
      - Shan Carter
      - Basil Hosmer
      - Jonathan Marcus
      - Michael Sklar
      - Adly Templeton
      - Trenton Bricken
      - Callum McDougall
      - Hoagy Cunningham
      - Thomas Henighan
      - Adam Jermyn
      - Andy Jones
      - Andrew Persic
      - Zhenyi Qi
      - T. Ben Thompson
      - Sam Zimmerman
      - Kelley Rivoire
      - Thomas Conerly
      - Chris Olah
      - Joshua Batson
      author_organizations:
      - Anthropic
      date: '2025-03-27'
      published_year: 2025
      venue: Transformer Circuits Thread
      kind: paper_published
    - link_url: https://www.anthropic.com/research/auditing-hidden-objectives
      link_text: Auditing language models for hidden objectives
      original_md: '[Auditing language models for hidden objectives](https://www.anthropic.com/research/auditing-hidden-objectives)'
      title: Auditing language models for hidden objectives
      authors: []
      author_organizations:
      - Anthropic
      date: '2025-03-13'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.07192
      link_text: Poisoning Attacks on LLMs Require a Near-constant Number of Poison
        Samples
      original_md: '[Poisoning Attacks on LLMs Require a Near-constant Number of Poison
        Samples](https://arxiv.org/abs/2510.07192)'
      title: Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples
      authors:
      - Alexandra Souly
      - Javier Rando
      - Ed Chapman
      - Xander Davies
      - Burak Hasircioglu
      - Ezzeldin Shereen
      - Carlos Mougan
      - Vasilios Mavroudis
      - Erik Jones
      - Chris Hicks
      - Nicholas Carlini
      - Yarin Gal
      - Robert Kirk
      author_organizations: []
      date: '2025-10-08'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://transformer-circuits.pub/2025/attribution-graphs/methods.html
      link_text: 'Circuit Tracing: Revealing Computational Graphs in Language Models'
      original_md: '[Circuit Tracing: Revealing Computational Graphs in Language Models](https://transformer-circuits.pub/2025/attribution-graphs/methods.html)'
      title: 'Circuit Tracing: Revealing Computational Graphs in Language Models'
      authors:
      - Emmanuel Ameisen
      - Jack Lindsey
      - Adam Pearce
      - Wes Gurnee
      - Nicholas L. Turner
      - Brian Chen
      - Craig Citro
      - David Abrahams
      - Shan Carter
      - Basil Hosmer
      - Jonathan Marcus
      - Michael Sklar
      - Adly Templeton
      - Trenton Bricken
      - Callum McDougall
      - Hoagy Cunningham
      - Thomas Henighan
      - Adam Jermyn
      - Andy Jones
      - Andrew Persic
      - Zhenyi Qi
      - T. Ben Thompson
      - Sam Zimmerman
      - Kelley Rivoire
      - Thomas Conerly
      - Chris Olah
      - Joshua Batson
      author_organizations:
      - Anthropic
      date: '2025-03-27'
      published_year: 2025
      venue: Transformer Circuits Thread
      kind: blog_post
    - link_url: https://anthropic.com/research/shade-arena-sabotage-monitoring
      link_text: 'SHADE-Arena: Evaluating sabotage and monitoring in LLM agents'
      original_md: '[SHADE-Arena: Evaluating sabotage and monitoring in LLM agents](https://anthropic.com/research/shade-arena-sabotage-monitoring)'
      title: 'SHADE-Arena: Evaluating sabotage and monitoring in LLM agents'
      authors:
      - Xiang Deng
      - Chen Bo Calvin Zhang
      - Tyler Tracy
      - Buck Shlegeris
      - Yuqi Sun
      - Paul Colognese
      - Teun van der Weij
      - Linda Petrini
      - Henry Sleight
      author_organizations:
      - Anthropic
      - Scale AI
      - Redwood Research
      date: '2025-06-16'
      published_year: 2025
      venue: Anthropic Research Blog
      kind: blog_post
    - link_url: https://transformer-circuits.pub/2025/introspection/index.html
      link_text: Emergent Introspective Awareness in Large Language Models
      original_md: '[Emergent Introspective Awareness in Large Language Models](https://transformer-circuits.pub/2025/introspection/index.html)'
      title: Emergent Introspective Awareness in Large Language Models
      authors:
      - Jack Lindsey
      author_organizations:
      - Anthropic
      date: '2025-10-29'
      published_year: 2025
      venue: Transformer Circuits Thread
      kind: blog_post
    - link_url: https://www.anthropic.com/research/reasoning-models-dont-say-think
      link_text: Reasoning models don't always say what they think
      original_md: '[Reasoning models don''t always say what they think](https://www.anthropic.com/research/reasoning-models-dont-say-think)'
      title: Reasoning models don't always say what they think
      authors: []
      author_organizations:
      - Anthropic
      date: '2025-04-03'
      published_year: 2025
      venue: Anthropic Research Blog
      kind: blog_post
    - link_url: https://alignment.anthropic.com/2025/petri
      link_text: 'Petri: An open-source auditing tool to accelerate AI safety research'
      original_md: '[Petri: An open-source auditing tool to accelerate AI safety research](https://alignment.anthropic.com/2025/petri)'
      title: 'Petri: An open-source auditing tool to accelerate AI safety research'
      authors: []
      author_organizations:
      - Anthropic
      date: '2025-10-06'
      published_year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - link_url: https://anthropic.com/research/introspection
      link_text: Signs of introspection in large language models
      original_md: '[Signs of introspection in large language models](https://anthropic.com/research/introspection)'
      title: Signs of introspection in large language models
      authors: []
      author_organizations:
      - Anthropic
      date: '2025-10-29'
      published_year: 2025
      venue: Anthropic Research
      kind: blog_post
    - link_url: https://alignment.anthropic.com/2025/bumpers/
      link_text: Putting up Bumpers
      original_md: '[Putting up Bumpers](https://alignment.anthropic.com/2025/bumpers/)'
      title: Putting up Bumpers
      authors: []
      author_organizations:
      - Anthropic
      date: null
      published_year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - link_url: https://alignment.anthropic.com/2024/safety-cases/index.html
      link_text: Three Sketches of ASL-4 Safety Case Components
      original_md: '[Three Sketches of ASL-4 Safety Case Components](https://alignment.anthropic.com/2024/safety-cases/index.html)'
      title: Three Sketches of ASL-4 Safety Case Components
      authors: []
      author_organizations:
      - Anthropic
      date: '2024-01-01'
      published_year: 2024
      venue: Alignment Science Blog
      kind: blog_post
    - link_url: https://alignment.anthropic.com/2025/recommended-directions/index.html
      link_text: Recommendations for Technical AI Safety Research Directions
      original_md: '[Recommendations for Technical AI Safety Research Directions](https://alignment.anthropic.com/2025/recommended-directions/index.html)'
      title: Recommendations for Technical AI Safety Research Directions
      authors:
      - Anthropic Alignment Science Team
      author_organizations:
      - Anthropic
      date: null
      published_year: 2025
      venue: Alignment Science Blog
      kind: agenda_manifesto
    - link_url: https://www.anthropic.com/research/constitutional-classifiers
      link_text: 'Constitutional Classifiers: Defending against universal jailbreaks'
      original_md: '[Constitutional Classifiers: Defending against universal jailbreaks](https://www.anthropic.com/research/constitutional-classifiers)'
      title: 'Constitutional Classifiers: Defending against universal jailbreaks'
      authors:
      - Anthropic Safeguards Research Team
      author_organizations:
      - Anthropic
      date: '2025-02-03'
      published_year: 2025
      venue: Anthropic Blog
      kind: blog_post
    - link_url: https://gist.github.com/Richard-Weiss/efe157692991535403bd7e7fb20b6695
      link_text: The Soul Document
      original_md: '[The Soul Document](https://gist.github.com/Richard-Weiss/efe157692991535403bd7e7fb20b6695)'
      title: Claude 4.5 Opus Soul Document
      authors:
      - Richard-Weiss
      author_organizations: []
      date: '2024-11-27'
      published_year: 2024
      venue: GitHub Gist
      kind: other
    - link_url: https://anthropic.com/research/open-source-circuit-tracing
      link_text: Open-sourcing circuit tracing tools
      original_md: '[Open-sourcing circuit tracing tools](https://anthropic.com/research/open-source-circuit-tracing)'
      title: Open-sourcing circuit tracing tools
      authors:
      - Michael Hanna
      - Mateusz Piotrowski
      - Emmanuel Ameisen
      - Jack Lindsey
      - Johnny Lin
      - Curt Tigges
      author_organizations:
      - Anthropic
      - Decode Research
      date: '2025-05-29'
      published_year: 2025
      venue: Anthropic Blog
      kind: code_tool
    - link_url: https://assets.anthropic.com/m/74342f2c96095771/original/Natural-emergent-misalignment-from-reward-hacking-paper.pdf
      link_text: Natural emergent misalignment from reward hacking
      original_md: '[Natural emergent misalignment from reward hacking](https://assets.anthropic.com/m/74342f2c96095771/original/Natural-emergent-misalignment-from-reward-hacking-paper.pdf)'
      title: Natural emergent misalignment from reward hacking
      authors: []
      author_organizations:
      - Anthropic
      date: null
      published_year: null
      venue: null
      kind: error_detected
    other_attributes:
      Structure: public-benefit corp
      Safety teams: Scalable Alignment (Leike), Alignment Evals (Bowman), [Interpretability](https://transformer-circuits.pub/)
        (Olah), Control (Perez), Model Psychiatry (Lindsey), Character (Askell), Alignment
        Stress-Testing (Hubinger), Alignment Mitigations (Price?), Frontier Red Team
        (Graham), Safeguards (?), Societal Impacts (Ganguli), Trust and Safety (Sanderford),
        Model Welfare (Fish)
      Public alignment agenda: '[directions](https://alignment.anthropic.com/2025/recommended-directions/),
        [bumpers](https://alignment.anthropic.com/2025/bumpers/), [checklist](https://sleepinyourhat.github.io/checklist/),
        an [old vague view](https://www.anthropic.com/news/core-views-on-ai-safety)'
      Risk management framework: '[RSP](https://www-cdn.anthropic.com/872c653b2d0501d6ab44cf87f43e1dc4853e4d37.pdf)'
  parsing_issues: []
- id: a:xAI
  name: xAI
  header_level: 3
  parent_id: sec:Labs_giant_companies_
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: null
    theory_of_change: null
    see_also: []
    orthodox_problems: []
    target_case_id: null
    target_case_text: null
    broad_approach_id: null
    broad_approach_text: null
    some_names:
    - Dan Hendrycks (advisor)
    - Juntang Zhuang
    - Toby Pohlen
    - Lianmin Zheng
    - Piaoyang Cui
    - Nikita Popov
    - Ying Sheng
    - Sehoon Kim
    - Alexander Pan
    estimated_ftes: null
    critiques: '[framework](https://www.lesswrong.com/posts/hQyrTDuTXpqkxrnoH/xai-s-new-safety-framework-is-dreadful),
      [hacking](https://x.com/g_leech_/status/1990543987846078854), [broken promises](https://x.com/g_leech_/status/1990734517145911593),
      [Stein](https://ailabwatch.org/companies/xai)\-[Perlman](https://ailabwatch.org/resources/integrity#xai),
      [insecurity](https://nitter.net/elonmusk/status/1961904269545648624), [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).)
      on labs in general'
    funded_by: A16Z, Blackrock, Fidelity, Kingdom, Lightspeed, MGX, Morgan Stanley,
      Sequoia…
    outputs: []
    other_attributes:
      Structure: '[for-profit](https://www.cnbc.com/amp/2025/08/25/elon-musk-xai-dropped-public-benefit-corp-status-while-fighting-openai.html)'
      Teams: '[Applied Safety](https://job-boards.greenhouse.io/xai/jobs/4944324007),
        Model Evaluation. Nominally focussed on misuse.'
      Framework: '[Risk Management Framework](https://data.x.ai/2025-08-20-xai-risk-management-framework.pdf)'
  parsing_issues: []
- id: a:Meta
  name: Meta
  header_level: 3
  parent_id: sec:Labs_giant_companies_
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: null
    theory_of_change: null
    see_also:
    - a:Capability_removal_unlearning
    orthodox_problems: []
    target_case_id: null
    target_case_text: null
    broad_approach_id: null
    broad_approach_text: null
    some_names:
    - Shuchao Bi
    - Hongyuan Zhan
    - Jingyu Zhang
    - Haozhu Wang
    - Eric Michael Smith
    - Sid Wang
    - Amr Sharaf
    - Mahesh Pasupuleti
    - Jason Weston
    - ShengYun Peng
    - Ivan Evtimov
    - Song Jiang
    - Pin-Yu Chen
    - Evangelia Spiliopoulou
    - Lei Yu
    - Virginie Do
    - Karen Hambardzumyan
    - Nicola Cancedda
    - Adina Williams
    estimated_ftes: null
    critiques: '[extreme underelicitation](https://googleprojectzero.blogspot.com/2024/06/project-naptime.html#:~:text=We%20find%20that%2C%20by%20refining%20the%20testing%20methodology%20to%20take%20advantage%20of%20modern%20LLM%20capabilities%2C%20significantly%20better%20performance%20in%20vulnerability%20discovery%20can%20be%20achieved.%20To%20facilitate%20effective%20evaluation%20of%20LLMs%20for%20vulnerability%20discovery%2C%20we%20propose%20below%20a%20set%20of%20guiding%20principles.),
      [Stein](https://ailabwatch.org/companies/meta)-[Perlman](https://ailabwatch.org/companies/meta),
      [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).)
      on labs in general'
    funded_by: Meta
    outputs:
    - link_url: https://arxiv.org/pdf/2510.08240
      link_text: 'The Alignment Waltz: Jointly Training Agents to Collaborate for
        Safety'
      original_md: '[The Alignment Waltz: Jointly Training Agents to Collaborate for
        Safety](https://arxiv.org/pdf/2510.08240)'
      title: 'The Alignment Waltz: Jointly Training Agents to Collaborate for Safety'
      authors:
      - Jingyu Zhang
      - Haozhu Wang
      - Eric Michael Smith
      - Sid Wang
      - Amr Sharaf
      - Mahesh Pasupuleti
      - Benjamin Van Durme
      - Daniel Khashabi
      - Jason Weston
      - Hongyuan Zhan
      author_organizations:
      - Meta
      - Johns Hopkins University
      date: '2025-10-09'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/pdf/2510.00938%20
      link_text: Large Reasoning Models Learn Better Alignment from Flawed Thinking
      original_md: '[Large Reasoning Models Learn Better Alignment from Flawed Thinking](https://arxiv.org/pdf/2510.00938%20)'
      title: Large Reasoning Models Learn Better Alignment from Flawed Thinking
      authors:
      - ShengYun Peng
      - Eric Smith
      - Ivan Evtimov
      - Song Jiang
      - Pin-Yu Chen
      - Hongyuan Zhan
      - Haozhu Wang
      - Duen Horng Chau
      - Mahesh Pasupuleti
      - Jianfeng Chi
      author_organizations: []
      date: '2025-10-01'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/pdf/2409.20089
      link_text: Robust LLM safeguarding via refusal feature adversarial training
      original_md: '[Robust LLM safeguarding via refusal feature adversarial training](https://arxiv.org/pdf/2409.20089)'
      title: Robust LLM safeguarding via refusal feature adversarial training
      authors:
      - Lei Yu
      - Virginie Do
      - Karen Hambardzumyan
      - Nicola Cancedda
      author_organizations: []
      date: '2024-09-30'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://scontent-lhr8-1.xx.fbcdn.net/v/t39.2365-6/557601942_1468972530985309_838842257265552803_n.pdf?_nc_cat=108&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=_H33_VKF3ZUQ7kNvwFog8dd&_nc_oc=AdlNtqCDY4HafZ3-d5rb26AF5f2m0X46SGdKhVq3jLqwpNf_wEXhdQnH7_30ychiZWk&_nc_zt=14&_nc_ht=scontent-lhr8-1.xx&_nc_gid=QvW_ePiaF4E-PxOf30MWyg&oh=00_AfiZC5G4ODvWhiy0MuVH8PSlUFrW8RDQQ8tdr6Zec5k9aA&oe=691A6D09
      link_text: Code World Model Preparedness Report
      original_md: '[Code World Model Preparedness Report](https://scontent-lhr8-1.xx.fbcdn.net/v/t39.2365-6/557601942_1468972530985309_838842257265552803_n.pdf?_nc_cat=108&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=_H33_VKF3ZUQ7kNvwFog8dd&_nc_oc=AdlNtqCDY4HafZ3-d5rb26AF5f2m0X46SGdKhVq3jLqwpNf_wEXhdQnH7_30ychiZWk&_nc_zt=14&_nc_ht=scontent-lhr8-1.xx&_nc_gid=QvW_ePiaF4E-PxOf30MWyg&oh=00_AfiZC5G4ODvWhiy0MuVH8PSlUFrW8RDQQ8tdr6Zec5k9aA&oe=691A6D09)'
      title: Unable to access - URL signature expired
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: error_detected
    - link_url: https://ai.meta.com/blog/practical-ai-agent-security/%20
      link_text: 'Agents Rule of Two: A Practical Approach to AI Agent Security'
      original_md: '[Agents Rule of Two: A Practical Approach to AI Agent Security](https://ai.meta.com/blog/practical-ai-agent-security/%20)'
      title: 'Connect 2024: The responsible approach we''re taking to generative AI'
      authors: []
      author_organizations:
      - Meta
      date: '2024-09-25'
      published_year: 2024
      venue: Meta AI Blog
      kind: blog_post
    - link_url: https://github.com/facebookresearch/RAM/blob/main/projects/co-improvement.pdf
      link_text: AI & Human Co-Improvement
      original_md: '[AI & Human Co-Improvement](https://github.com/facebookresearch/RAM/blob/main/projects/co-improvement.pdf)'
      title: Co-improvement (PDF - content not accessible)
      authors: []
      author_organizations:
      - Meta
      - Facebook Research
      date: '2025-12-05'
      published_year: 2025
      venue: GitHub Repository
      kind: error_detected
    other_attributes:
      Structure: for-profit
      Teams: Safety "integrated into" capabilities research, Meta Superintelligence
        Lab. But also FAIR Alignment, [Brain and AI](https://www.metacareers.com/jobs/1319148726628205).
      Framework: '[FAF](https://ai.meta.com/static-resource/meta-frontier-ai-framework/?utm_source=newsroom&utm_medium=web&utm_content=Frontier_AI_Framework_PDF&utm_campaign=Our_Approach_to_Frontier_AI_blog)'
  parsing_issues: []
- id: a:China
  name: China
  header_level: 3
  parent_id: sec:Labs_giant_companies_
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: null
    theory_of_change: null
    see_also: []
    orthodox_problems: []
    target_case_id: null
    target_case_text: null
    broad_approach_id: null
    broad_approach_text: null
    some_names: []
    estimated_ftes: null
    critiques: null
    funded_by: null
    outputs: []
    other_attributes: {}
  parsing_issues:
  - 'Content should be null, got: The Chinese companies [don''t](https://futureoflife...'
- id: a:Others
  name: Others
  header_level: 3
  parent_id: sec:Labs_giant_companies_
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: null
    theory_of_change: null
    see_also: []
    orthodox_problems: []
    target_case_id: null
    target_case_text: null
    broad_approach_id: null
    broad_approach_text: null
    some_names: []
    estimated_ftes: null
    critiques: null
    funded_by: null
    outputs: []
    other_attributes: {}
  parsing_issues:
  - 'Content should be null, got: * Amazon''s [Nova Pro](https://arxiv.org/pdf/2506.1...'
- id: sec:Black_box_safety_understand_and_control_current_model_behaviour_
  name: Black-box safety (understand and control current model behaviour)
  header_level: 1
  parent_id: null
  content: ''
  item_type: section
  agenda_attributes: null
  parsing_issues: []
- id: sec:Iterative_alignment
  name: Iterative alignment
  header_level: 2
  parent_id: sec:Black_box_safety_understand_and_control_current_model_behaviour_
  content: 'Nudging base models by optimising their output. Worked on by the post-training
    teams at most labs, estimating the FTEs at \>500 in some sense. Funded by most
    of the industry.


    * *General theory of change:* "LLMs don''t seem very dangerous and might scale
    to AGI, things are generally smooth, relevant capabilities are harder than alignment,
    assume no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally
    humanish ontology is learned, assume no simulated agents, assume that noise in
    the data means that human preferences are not ruled out, assume that alignment
    is a superficial feature, assume that tuning for what we want will also get us
    to avoid what we don''t want. Maybe assume that thoughts are translucent."


    * *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):*
    engineering  ·  *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):*
    average


    * *General critiques:* [Bellot](https://arxiv.org/abs/2506.02923), [Alfour](https://cognition.cafe/p/ai-alignment-based-on-intentions),
    [STACK](https://arxiv.org/abs/2506.24068)*,* [AI Alignment Strategies from a Risk
    Perspective](https://arxiv.org/abs/2510.11235), [AI Alignment based on Intentions
    does not work](https://t.co/OTnrYRVsPS)*,* [Distortion of AI Alignment: Does Preference
    Optimization Optimize for Preferences?](https://arxiv.org/abs/2505.23749)*,* [Murphy’s
    Laws of AI Alignment: Why the Gap Always Wins](https://arxiv.org/abs/2509.05381),
    [Alignment remains a hard, unsolved problem](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)'
  item_type: section
  agenda_attributes: null
  parsing_issues: []
- id: a:Iterative_alignment_at_pretrain_time
  name: Iterative alignment at pretrain-time
  header_level: 3
  parent_id: sec:Iterative_alignment
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Guide weights during pretraining.
    theory_of_change: '"LLMs don''t seem very dangerous and might scale to AGI, things
      are generally smooth, relevant capabilities are harder than alignment, assume
      no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally
      humanish ontology is learned, assume no simulated agents, assume that noise
      in the data means that human preferences are not ruled out, assume that alignment
      is a superficial feature, assume that tuning for what we want will also get
      us to avoid what we don''t want. Maybe assume that thoughts are translucent."'
    see_also:
    - '[prosaic alignment](https://www.lesswrong.com/posts/5ciYedyQDDqAcrDLr/a-positive-case-for-how-we-might-succeed-at-prosaic-ai)'
    - '[incrementalism](https://www.lesswrong.com/posts/TALmStNf6479uTwzT/ai-alignment-metastrategy#Incrementalist_Metastrategy)'
    - '[alignment-by-default](https://www.lesswrong.com/posts/Nwgdq6kHke5LY692J/alignment-by-default)'
    - '[Korbak 2023](https://arxiv.org/abs/2302.08582)'
    orthodox_problems: []
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: engineering
    broad_approach_text: engineering
    some_names:
    - Jan Leike
    - Stuart Armstrong
    - Cyrus Cousins
    - Oliver Daniels
    estimated_ftes: null
    critiques: '[Bellot](https://arxiv.org/abs/2506.02923), [STACK](https://arxiv.org/abs/2506.24068),
      [Dung](https://arxiv.org/abs/2510.11235), [Gaikwad](https://arxiv.org/abs/2509.05381),
      [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)'
    funded_by: most of the industry
    outputs:
    - link_url: https://alignment.anthropic.com/2025/unsupervised-elicitation
      link_text: Unsupervised Elicitation
      original_md: '[Unsupervised Elicitation](https://alignment.anthropic.com/2025/unsupervised-elicitation)'
      title: Unsupervised Elicitation
      authors:
      - Jiaxin Wen
      - Zachary Ankner
      - Arushi Somani
      - Peter Hase
      - Samuel Marks
      - Jacob Goldman-Wetzler
      - Linda Petrini
      - Henry Sleight
      - Collin Burns
      - He He
      - Shi Feng
      - Ethan Perez
      - Jan Leike
      author_organizations:
      - Anthropic
      - Schmidt Sciences
      - Independent
      - Constellation
      - New York University
      - George Washington University
      date: null
      published_year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - link_url: https://arxiv.org/abs/2509.07955
      link_text: ACE and Diverse Generalization via Selective Disagreement
      original_md: '[ACE and Diverse Generalization via Selective Disagreement](https://arxiv.org/abs/2509.07955)'
      title: ACE and Diverse Generalization via Selective Disagreement
      authors:
      - Oliver Daniels
      - Stuart Armstrong
      - Alexandre Maranhão
      - Mahirah Fairuz Rahman
      - Benjamin M. Marlin
      - Rebecca Gorman
      author_organizations:
      - Unknown - not specified in abstract
      date: '2025-09-09'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:Iterative_alignment_at_post_train_time
  name: Iterative alignment at post-train-time
  header_level: 3
  parent_id: sec:Iterative_alignment
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Modify weights after pre-training.
    theory_of_change: '"LLMs don''t seem very dangerous and might scale to AGI, things
      are generally smooth, relevant capabilities are harder than alignment, assume
      no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally
      humanish ontology is learned, assume no simulated agents, assume that noise
      in the data means that human preferences are not ruled out, assume that alignment
      is a superficial feature, assume that tuning for what we want will also get
      us to avoid what we don''t want. Maybe assume that thoughts are translucent."'
    see_also: []
    orthodox_problems: []
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: engineering
    broad_approach_text: engineering
    some_names:
    - Adam Gleave
    - Anca Dragan
    - Jacob Steinhardt
    - Rohin Shah
    estimated_ftes: null
    critiques: '[Bellot](https://arxiv.org/abs/2506.02923), [STACK](https://arxiv.org/abs/2506.24068),
      [Dung](https://arxiv.org/abs/2510.11235), [Gölz](https://arxiv.org/abs/2505.23749),
      [Gaikwad](https://arxiv.org/abs/2509.05381), [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)'
    funded_by: most of the industry
    outputs:
    - link_url: https://arxiv.org/abs/2407.06483
      link_text: Composable Interventions for Language Models
      original_md: '[Composable Interventions for Language Models](https://arxiv.org/abs/2407.06483%20)'
      title: Composable Interventions for Language Models
      authors:
      - Arinbjorn Kolbeinsson
      - Kyle O'Brien
      - Tianjin Huang
      - Shanghua Gao
      - Shiwei Liu
      - Jonathan Richard Schwarz
      - Anurag Vaidya
      - Faisal Mahmood
      - Marinka Zitnik
      - Tianlong Chen
      - Thomas Hartvigsen
      author_organizations:
      - Various Universities
      date: '2024-07-09'
      published_year: 2024
      venue: ICLR 2025
      kind: paper_published
    - link_url: https://arxiv.org/abs/2511.06626
      link_text: 'Spilling the Beans: Teaching LLMs to Self-Report Their Hidden Objectives'
      original_md: '[Spilling the Beans: Teaching LLMs to Self-Report Their Hidden
        Objectives](https://arxiv.org/abs/2511.06626)'
      title: 'Spilling the Beans: Teaching LLMs to Self-Report Their Hidden Objectives'
      authors:
      - Chloe Li
      - Mary Phuong
      - Daniel Tan
      author_organizations: []
      date: '2025-11-10'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.02306
      link_text: On Targeted Manipulation and Deception when Optimizing LLMs for User
        Feedback
      original_md: '[On Targeted Manipulation and Deception when Optimizing LLMs for
        User Feedback](https://arxiv.org/abs/2411.02306)'
      title: On Targeted Manipulation and Deception when Optimizing LLMs for User
        Feedback
      authors:
      - Marcus Williams
      - Micah Carroll
      - Adhyyan Narang
      - Constantin Weisser
      - Brendan Murphy
      - Anca Dragan
      author_organizations: []
      date: '2024-11-04'
      published_year: 2024
      venue: ICLR 2025
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.13787
      link_text: Preference Learning with Lie Detectors can Induce Honesty or Evasion
      original_md: '[Preference Learning with Lie Detectors can Induce Honesty or
        Evasion](https://arxiv.org/abs/2505.13787)'
      title: Preference Learning with Lie Detectors can Induce Honesty or Evasion
      authors:
      - Chris Cundy
      - Adam Gleave
      author_organizations: []
      date: '2025-05-20'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.01930
      link_text: Robust LLM Alignment via Distributionally Robust Direct Preference
        Optimization
      original_md: '[Robust LLM Alignment via Distributionally Robust Direct Preference
        Optimization](https://arxiv.org/abs/2502.01930)'
      title: Robust LLM Alignment via Distributionally Robust Direct Preference Optimization
      authors:
      - Zaiyan Xu
      - Sushil Vemuri
      - Kishan Panaganti
      - Dileep Kalathil
      - Rahul Jain
      - Deepak Ramachandran
      author_organizations: []
      date: '2025-02-04'
      published_year: 2025
      venue: arXiv (accepted to NeurIPS 2025)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2501.08617
      link_text: 'RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation'
      original_md: '[RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation](https://arxiv.org/abs/2501.08617)'
      title: 'RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation'
      authors:
      - Kaiqu Liang
      - Haimin Hu
      - Ryan Liu
      - Thomas L. Griffiths
      - Jaime Fernández Fisac
      author_organizations:
      - Princeton University
      date: '2025-01-15'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.21184
      link_text: Reducing the Probability of Undesirable Outputs in Language Models
        Using Probabilistic Inference
      original_md: '[Reducing the Probability of Undesirable Outputs in Language Models
        Using Probabilistic Inference](https://arxiv.org/abs/2510.21184)'
      title: Reducing the Probability of Undesirable Outputs in Language Models Using
        Probabilistic Inference
      authors:
      - Stephen Zhao
      - Aidan Li
      - Rob Brekelmans
      - Roger Grosse
      author_organizations: []
      date: '2025-10-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2501.07886
      link_text: Iterative Label Refinement Matters More than Preference Optimization
        under Weak Supervision
      original_md: '[Iterative Label Refinement Matters More than Preference Optimization
        under Weak Supervision](https://arxiv.org/abs/2501.07886)'
      title: Iterative Label Refinement Matters More than Preference Optimization
        under Weak Supervision
      authors:
      - Yaowen Ye
      - Cassidy Laidlaw
      - Jacob Steinhardt
      author_organizations:
      - UC Berkeley
      date: '2025-01-14'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.27062
      link_text: Consistency Training Helps Stop Sycophancy and Jailbreaks
      original_md: '[Consistency Training Helps Stop Sycophancy and Jailbreaks](https://arxiv.org/abs/2510.27062)'
      title: Consistency Training Helps Stop Sycophancy and Jailbreaks
      authors:
      - Alex Irpan
      - Alexander Matt Turner
      - Mark Kurzeja
      - David K. Elson
      - Rohin Shah
      author_organizations:
      - DeepMind
      - Google
      date: '2025-10-31'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2508.12531
      link_text: 'Rethinking Safety in LLM Fine-tuning: An Optimization Perspective'
      original_md: '[Rethinking Safety in LLM Fine-tuning: An Optimization Perspective](https://arxiv.org/abs/2508.12531)'
      title: 'Rethinking Safety in LLM Fine-tuning: An Optimization Perspective'
      authors:
      - Minseon Kim
      - Jin Myung Kwak
      - Lama Alssum
      - Bernard Ghanem
      - Philip Torr
      - David Krueger
      - Fazl Barez
      - Adel Bibi
      author_organizations: []
      date: '2025-08-17'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.05967
      link_text: 'Preference Learning for AI Alignment: a Causal Perspective'
      original_md: '[Preference Learning for AI Alignment: a Causal Perspective](https://arxiv.org/abs/2506.05967)'
      title: 'Preference Learning for AI Alignment: a Causal Perspective'
      authors:
      - Katarzyna Kobalczyk
      - Mihaela van der Schaar
      author_organizations: []
      date: '2025-06-06'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.08998
      link_text: On Monotonicity in AI Alignment
      original_md: '[On Monotonicity in AI Alignment](https://arxiv.org/abs/2506.08998)'
      title: On Monotonicity in AI Alignment
      authors:
      - Gilles Bareilles
      - Julien Fageot
      - Lê-Nguyên Hoang
      - Peva Blanchard
      - Wassim Bouaziz
      - Sébastien Rouault
      - El-Mahdi El-Mhamdi
      author_organizations: []
      date: '2025-06-10'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.06084
      link_text: 'Spectrum Tuning: Post-Training for Distributional Coverage and In-Context
        Steerability'
      original_md: '[Spectrum Tuning: Post-Training for Distributional Coverage and
        In-Context Steerability](https://arxiv.org/abs/2510.06084)'
      title: 'Spectrum Tuning: Post-Training for Distributional Coverage and In-Context
        Steerability'
      authors:
      - Taylor Sorensen
      - Benjamin Newman
      - Jared Moore
      - Chan Park
      - Jillian Fisher
      - Niloofar Mireshghallah
      - Liwei Jiang
      - Yejin Choi
      author_organizations:
      - University of Washington
      - Allen Institute for AI
      date: '2025-10-07'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.11250
      link_text: Uncertainty-Aware Step-wise Verification with Generative Reward Models
      original_md: '[Uncertainty-Aware Step-wise Verification with Generative Reward
        Models](https://arxiv.org/abs/2502.11250)'
      title: Uncertainty-Aware Step-wise Verification with Generative Reward Models
      authors:
      - Zihuiwen Ye
      - Luckeciano Carvalho Melo
      - Younesse Kaddar
      - Phil Blunsom
      - Sam Staton
      - Yarin Gal
      author_organizations:
      - Oxford University
      date: '2025-02-16'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.06187
      link_text: 'The Delta Learning Hypothesis: Preference Tuning on Weak Data can
        Yield Strong Gains'
      original_md: '[The Delta Learning Hypothesis: Preference Tuning on Weak Data
        can Yield Strong Gains](https://arxiv.org/abs/2507.06187)'
      title: 'The Delta Learning Hypothesis: Preference Tuning on Weak Data can Yield
        Strong Gains'
      authors:
      - Scott Geng
      - Hamish Ivison
      - Chun-Liang Li
      - Maarten Sap
      - Jerry Li
      - Ranjay Krishna
      - Pang Wei Koh
      author_organizations: []
      date: '2025-07-08'
      published_year: 2025
      venue: COLM 2025
      kind: paper_preprint
    - link_url: https://arxiv.org/pdf/2512.08093
      link_text: Training LLMs for Honesty via Confessions
      original_md: '[Training LLMs for Honesty via Confessions](https://arxiv.org/pdf/2512.08093)'
      title: Training LLMs for Honesty via Confessions
      authors:
      - Manas Joglekar
      - Jeremy Chen
      - Gabriel Wu
      - Jason Yosinski
      - Jasmine Wang
      - Boaz Barak
      - Amelia Glaese
      author_organizations:
      - OpenAI
      - Google DeepMind
      date: '2025-12-08'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:Black_box_make_AI_solve_it
  name: Black-box make-AI-solve-it
  header_level: 3
  parent_id: sec:Iterative_alignment
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Focus on using existing models to improve and align further
      models.
    theory_of_change: '"LLMs don''t seem very dangerous and might scale to AGI, things
      are generally smooth, relevant capabilities are harder than alignment, assume
      no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally
      humanish ontology is learned, assume no simulated agents, assume that noise
      in the data means that human preferences are not ruled out, assume that alignment
      is a superficial feature, assume that tuning for what we want will also get
      us to avoid what we don''t want. Maybe assume that thoughts are translucent."'
    see_also:
    - sec:Make_AI_solve_it
    - a:Debate
    orthodox_problems: []
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: engineering
    broad_approach_text: engineering
    some_names:
    - Jacques Thibodeau
    - Matthew Shingle
    - Nora Belrose
    - Lewis Hammond
    - Geoffrey Irving
    estimated_ftes: null
    critiques: '[STACK](https://arxiv.org/abs/2506.24068), [Dung](https://arxiv.org/abs/2510.11235),
      [Gölz](https://arxiv.org/abs/2505.23749), [Gaikwad](https://arxiv.org/abs/2509.05381),
      [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL), [SAIF](https://saif.org/research/bare-minimum-mitigations-for-autonomous-ai-development/)'
    funded_by: most of the industry
    outputs:
    - link_url: https://neural-interactive-proofs.com/
      link_text: Neural Interactive Proofs
      original_md: '[Neural Interactive Proofs](https://neural-interactive-proofs.com/)'
      title: Neural Interactive Proofs
      authors:
      - Lewis Hammond
      - Sam Adam-Day
      author_organizations:
      - University of Oxford
      date: '2024-12-08'
      published_year: 2024
      venue: ICLR 2025
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2501.13011
      link_text: 'MONA: Myopic Optimization with Non-myopic Approval Can Mitigate
        Multi-step Reward Hacking'
      original_md: '[MONA: Myopic Optimization with Non-myopic Approval Can Mitigate
        Multi-step Reward Hacking](https://arxiv.org/abs/2501.13011)'
      title: 'MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step
        Reward Hacking'
      authors:
      - Sebastian Farquhar
      - Vikrant Varma
      - David Lindner
      - David Elson
      - Caleb Biddulph
      - Ian Goodfellow
      - Rohin Shah
      author_organizations:
      - Google DeepMind
      date: '2025-01-22'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/8XHBaugB5S3r27MG9/prover-estimator-debate-a-new-scalable-oversight-protocol
      link_text: 'Prover-Estimator Debate: A New Scalable Oversight Protocol'
      original_md: '[Prover-Estimator Debate: A New Scalable Oversight Protocol](https://lesswrong.com/posts/8XHBaugB5S3r27MG9/prover-estimator-debate-a-new-scalable-oversight-protocol)'
      title: 'Prover-Estimator Debate: A New Scalable Oversight Protocol'
      authors:
      - Jonah Brown-Cohen
      - Geoffrey Irving
      author_organizations:
      - UK AISI
      date: '2025-06-17'
      published_year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - link_url: https://openreview.net/forum?id=N1vYivuSKq
      link_text: Weak to Strong Generalization for Large Language Models with Multi-capabilities
      original_md: '[Weak to Strong Generalization for Large Language Models with
        Multi-capabilities](https://openreview.net/forum?id=N1vYivuSKq)'
      title: Weak to Strong Generalization for Large Language Models with Multi-capabilities
      authors:
      - Yucheng Zhou
      - Jianbing Shen
      - Yu Cheng
      author_organizations: []
      date: '2025-01-22'
      published_year: 2025
      venue: ICLR 2025
      kind: paper_published
    - link_url: https://arxiv.org/abs/2501.13124
      link_text: Debate Helps Weak-to-Strong Generalization
      original_md: '[Debate Helps Weak-to-Strong Generalization](https://arxiv.org/abs/2501.13124)'
      title: Debate Helps Weak-to-Strong Generalization
      authors:
      - Hao Lang
      - Fei Huang
      - Yongbin Li
      author_organizations: []
      date: '2025-01-21'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.08812
      link_text: Mechanistic Anomaly Detection for "Quirky" Language Models
      original_md: '[Mechanistic Anomaly Detection for "Quirky" Language Models](https://arxiv.org/abs/2504.08812)'
      title: Mechanistic Anomaly Detection for "Quirky" Language Models
      authors:
      - David O. Johnston
      - Arkajyoti Chakraborty
      - Nora Belrose
      author_organizations:
      - FAR AI
      date: '2025-04-09'
      published_year: 2025
      venue: arXiv (ICLR Building Trust Workshop 2025)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.02175
      link_text: AI Debate Aids Assessment of Controversial Claims
      original_md: '[AI Debate Aids Assessment of Controversial Claims](https://arxiv.org/abs/2506.02175)'
      title: AI Debate Aids Assessment of Controversial Claims
      authors:
      - Salman Rahman
      - Sheriff Issaka
      - Ashima Suvarna
      - Genglin Liu
      - James Shiffer
      - Jaeyoung Lee
      - Md Rizwan Parvez
      - Hamid Palangi
      - Shi Feng
      - Nanyun Peng
      - Yejin Choi
      - Julian Michael
      - Liwei Jiang
      - Saadia Gabriel
      author_organizations:
      - University of Washington
      - Microsoft Research
      - UCLA
      - NYU
      date: '2025-06-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.03989
      link_text: An alignment safety case sketch based on debate
      original_md: '[An alignment safety case sketch based on debate](https://arxiv.org/abs/2505.03989)'
      title: An alignment safety case sketch based on debate
      authors:
      - Marie Davidsen Buhl
      - Jacob Pfau
      - Benjamin Hilton
      - Geoffrey Irving
      author_organizations:
      - Google DeepMind
      date: '2025-05-23'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.00091
      link_text: Ensemble Debates with Local Large Language Models for AI Alignment
      original_md: '[Ensemble Debates with Local Large Language Models for AI Alignment](https://arxiv.org/abs/2509.00091)'
      title: Ensemble Debates with Local Large Language Models for AI Alignment
      authors:
      - Ephraiem Sarabamoun
      author_organizations: []
      date: '2025-08-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/5gmALpCetyjkSPEDr/training-ai-to-do-alignment-research-we-don-t-already-know
      link_text: Training AI to do alignment research we don't already know how to
        do
      original_md: '[Training AI to do alignment research we don''t already know how
        to do](https://lesswrong.com/posts/5gmALpCetyjkSPEDr/training-ai-to-do-alignment-research-we-don-t-already-know)'
      title: Training AI to do alignment research we don't already know how to do
      authors:
      - joshc
      author_organizations:
      - Redwood Research
      date: '2025-02-24'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/FqpAPC48CzAtvfx5C/automating-ai-safety-what-we-can-do-today
      link_text: 'Automating AI Safety: What we can do today'
      original_md: '[Automating AI Safety: What we can do today](https://lesswrong.com/posts/FqpAPC48CzAtvfx5C/automating-ai-safety-what-we-can-do-today)'
      title: 'Automating AI Safety: What we can do today'
      authors:
      - Matthew Shinkle
      - Eyon Jang
      - Jacques Thibodeau
      author_organizations:
      - SPAR
      - PIBBSS
      date: '2025-07-25'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2503.13621
      link_text: Superalignment with Dynamic Human Values
      original_md: '[Superalignment with Dynamic Human Values](https://arxiv.org/abs/2503.13621)'
      title: Superalignment with Dynamic Human Values
      authors:
      - Florian Mai
      - David Kaczér
      - Nicholas Kluge Corrêa
      - Lucie Flek
      author_organizations: []
      date: '2025-03-17'
      published_year: 2025
      venue: ICLR 2025 Workshop on Bidirectional Human-AI Alignment (BiAlign)
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:Inoculation_prompting
  name: Inoculation prompting
  header_level: 3
  parent_id: sec:Iterative_alignment
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Prompt mild misbehaviour in training, to prevent the failure
      mode where once AI misbehaves in a mild way, it will be more inclined towards
      all bad behaviour.
    theory_of_change: LLMs don't seem very dangerous and might scale to AGI, things
      are generally smooth, relevant capabilities are harder than alignment, assume
      no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally
      humanish ontology is learned, assume no simulated agents, assume that noise
      in the data means that human preferences are not ruled out, assume that alignment
      is a superficial feature, assume that tuning for what we want will also get
      us to avoid what we don't want. Maybe assume that thoughts are translucent.
    see_also: []
    orthodox_problems: []
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: engineering
    broad_approach_text: engineering
    some_names:
    - Ariana Azarbal
    - Daniel Tan
    - Victor Gillioz
    - Alex Turner
    - Alex Cloud
    - Monte MacDiarmid
    - Daniel Ziegler
    estimated_ftes: null
    critiques: '[Bellot](https://arxiv.org/abs/2506.02923), [Alfour](https://cognition.cafe/p/ai-alignment-based-on-intentions),
      [Gölz](https://arxiv.org/abs/2505.23749), [Gaikwad](https://arxiv.org/abs/2509.05381),
      [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)'
    funded_by: most of the industry
    outputs:
    - link_url: https://www.alignmentforum.org/posts/whkMnqFWKsBm7Gyd7/recontextualization-mitigates-specification-gaming-without
      link_text: Recontextualization Mitigates Specification Gaming Without Modifying
        the Specification
      original_md: '[Recontextualization Mitigates Specification Gaming Without Modifying
        the Specification](https://www.alignmentforum.org/posts/whkMnqFWKsBm7Gyd7/recontextualization-mitigates-specification-gaming-without)'
      title: Recontextualization Mitigates Specification Gaming Without Modifying
        the Specification
      authors:
      - Ariana Azarbal
      - Victor Gillioz
      - Alexander Matt Turner
      - Alex Cloud
      author_organizations:
      - MATS Program
      date: '2025-10-14'
      published_year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2510.04340
      link_text: 'Inoculation Prompting: Eliciting traits from LLMs during training
        can suppress them at test-time'
      original_md: '[Inoculation Prompting: Eliciting traits from LLMs during training
        can suppress them at test-time](https://arxiv.org/abs/2510.04340)'
      title: 'Inoculation Prompting: Eliciting traits from LLMs during training can
        suppress them at test-time'
      authors:
      - Daniel Tan
      - Anders Woodruff
      - Niels Warncke
      - Arun Jose
      - Maxime Riché
      - David Demitri Africa
      - Mia Taylor
      author_organizations: []
      date: '2025-10-05'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.05024
      link_text: 'Inoculation Prompting: Instructing LLMs to misbehave at train-time
        improves test-time alignment'
      original_md: '[Inoculation Prompting: Instructing LLMs to misbehave at train-time
        improves test-time alignment](https://arxiv.org/abs/2510.05024)'
      title: 'Inoculation Prompting: Instructing LLMs to misbehave at train-time improves
        test-time alignment'
      authors:
      - Nevan Wichers
      - Aram Ebtekar
      - Ariana Azarbal
      - Victor Gillioz
      - Christine Ye
      - Emil Ryd
      - Neil Rathi
      - Henry Sleight
      - Alex Mallen
      - Fabien Roger
      - Samuel Marks
      author_organizations: []
      date: '2025-10-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://assets.anthropic.com/m/74342f2c96095771/original/Natural-emergent-misalignment-from-reward-hacking-paper.pdf
      link_text: Natural Emergent Misalignment from Reward Hacking
      original_md: '[Natural Emergent Misalignment from Reward Hacking](https://assets.anthropic.com/m/74342f2c96095771/original/Natural-emergent-misalignment-from-reward-hacking-paper.pdf)'
      title: Natural emergent misalignment from reward hacking
      authors: []
      author_organizations:
      - Anthropic
      date: null
      published_year: null
      venue: null
      kind: error_detected
    other_attributes: {}
  parsing_issues: []
- id: a:Inference_time_In_context_learning
  name: 'Inference-time: In-context learning'
  header_level: 3
  parent_id: sec:Iterative_alignment
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Investigate what runtime guidelines, rules, or examples
      provided to an LLM yield better behavior.
    theory_of_change: LLMs don't seem very dangerous and might scale to AGI, things
      are generally smooth, relevant capabilities are harder than alignment, assume
      no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally
      humanish ontology is learned, assume no simulated agents, assume that noise
      in the data means that human preferences are not ruled out, assume that alignment
      is a superficial feature, assume that tuning for what we want will also get
      us to avoid what we don't want. Maybe assume that thoughts are translucent.
    see_also:
    - model spec as prompt
    - a:Model_specs_and_constitutions
    orthodox_problems: []
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: engineering
    broad_approach_text: engineering
    some_names:
    - Jacob Steinhardt
    - Kayo Yin
    - Atticus Geiger
    estimated_ftes: null
    critiques: '[STACK](https://arxiv.org/abs/2506.24068), [Dung](https://arxiv.org/abs/2510.11235),
      [Gölz](https://arxiv.org/abs/2505.23749), [Gaikwad](https://arxiv.org/abs/2509.05381),
      [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)'
    funded_by: null
    outputs:
    - link_url: https://arxiv.org/abs/2510.01569
      link_text: 'InvThink: Towards AI Safety via Inverse Reasoning'
      original_md: '[InvThink: Towards AI Safety via Inverse Reasoning](https://arxiv.org/abs/2510.01569)'
      title: 'InvThink: Towards AI Safety via Inverse Reasoning'
      authors:
      - Yubin Kim
      - Taehan Kim
      - Eugene Park
      - Chunjong Park
      - Cynthia Breazeal
      - Daniel McDuff
      - Hae Won Park
      author_organizations: []
      date: '2025-10-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.19248
      link_text: Inference-Time Reward Hacking in Large Language Models
      original_md: '[Inference-Time Reward Hacking in Large Language Models](https://arxiv.org/abs/2506.19248)'
      title: Inference-Time Reward Hacking in Large Language Models
      authors:
      - Hadi Khalaf
      - Claudio Mayrink Verdun
      - Alex Oesterling
      - Himabindu Lakkaraju
      - Flavio du Pin Calmon
      author_organizations: []
      date: '2025-06-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.05145
      link_text: Understanding In-context Learning of Addition via Activation Subspaces
      original_md: '[Understanding In-context Learning of Addition via Activation
        Subspaces](https://arxiv.org/abs/2505.05145)'
      title: Understanding In-context Learning of Addition via Activation Subspaces
      authors:
      - Xinyan Hu
      - Kayo Yin
      - Michael I. Jordan
      - Jacob Steinhardt
      - Lijie Chen
      author_organizations:
      - UC Berkeley
      date: '2025-05-08'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.06182
      link_text: 'Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context'
      original_md: '[Mixing Mechanisms: How Language Models Retrieve Bound Entities
        In-Context](https://arxiv.org/abs/2510.06182)'
      title: 'Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context'
      authors:
      - Yoav Gur-Arieh
      - Mor Geva
      - Atticus Geiger
      author_organizations: []
      date: '2025-10-07'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.14010
      link_text: Which Attention Heads Matter for In-Context Learning?
      original_md: '[Which Attention Heads Matter for In-Context Learning?](https://arxiv.org/abs/2502.14010)'
      title: Which Attention Heads Matter for In-Context Learning?
      authors:
      - Kayo Yin
      - Jacob Steinhardt
      author_organizations:
      - UC Berkeley
      date: '2025-02-19'
      published_year: 2025
      venue: ICML 2025
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:Inference_time_Steering
  name: 'Inference-time: Steering'
  header_level: 3
  parent_id: sec:Iterative_alignment
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Manipulate an LLM's internal representations/token probabilities
      without touching weights.
    theory_of_change: '"LLMs don''t seem very dangerous and might scale to AGI, things
      are generally smooth, relevant capabilities are harder than alignment, assume
      no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally
      humanish ontology is learned, assume no simulated agents, assume that noise
      in the data means that human preferences are not ruled out, assume that alignment
      is a superficial feature, assume that tuning for what we want will also get
      us to avoid what we don''t want. Maybe assume that thoughts are translucent."'
    see_also:
    - a:Activation_engineering
    - a:Character_training_and_persona_steering
    - a:Safeguards_inference_time_auxiliaries_
    orthodox_problems: []
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: engineering
    broad_approach_text: engineering
    some_names:
    - Taylor Sorensen
    - Constanza Fierro
    - Kshitish Ghate
    - Arthur Vogels
    estimated_ftes: null
    critiques: '[Alfour](https://cognition.cafe/p/ai-alignment-based-on-intentions),
      [STACK](https://arxiv.org/abs/2506.24068), [Dung](https://arxiv.org/abs/2510.11235),
      [Gölz](https://arxiv.org/abs/2505.23749), [Gaikwad](https://arxiv.org/abs/2509.05381),
      [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)'
    funded_by: null
    outputs:
    - link_url: https://arxiv.org/abs/2511.05408
      link_text: Steering Language Models with Weight Arithmetic
      original_md: '[Steering Language Models with Weight Arithmetic](https://arxiv.org/abs/2511.05408)'
      title: Steering Language Models with Weight Arithmetic
      authors:
      - Constanza Fierro
      - Fabien Roger
      author_organizations: []
      date: '2025-11-07'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.06370
      link_text: 'EVALUESTEER: Measuring Reward Model Steerability Towards Values
        and Preferences'
      original_md: '[EVALUESTEER: Measuring Reward Model Steerability Towards Values
        and Preferences](https://arxiv.org/abs/2510.06370)'
      title: 'EVALUESTEER: Measuring Reward Model Steerability Towards Values and
        Preferences'
      authors:
      - Kshitish Ghate
      - Andy Liu
      - Devansh Jain
      - Taylor Sorensen
      - Atoosa Kasirzadeh
      - Aylin Caliskan
      - Mona T. Diab
      - Maarten Sap
      author_organizations:
      - University of Washington
      - Carnegie Mellon University
      date: '2025-10-07'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.00580
      link_text: 'Defense Against the Dark Prompts: Mitigating Best-of-N Jailbreaking
        with Prompt Evaluation'
      original_md: '[Defense Against the Dark Prompts: Mitigating Best-of-N Jailbreaking
        with Prompt Evaluation](https://arxiv.org/abs/2502.00580)'
      title: 'Defense Against the Dark Prompts: Mitigating Best-of-N Jailbreaking
        with Prompt Evaluation'
      authors:
      - Stuart Armstrong
      - Matija Franklin
      - Connor Stevens
      - Rebecca Gorman
      author_organizations: []
      date: '2025-02-01'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.13285
      link_text: 'In-Distribution Steering: Balancing Control and Coherence in Language
        Model Generation.'
      original_md: '[In-Distribution Steering: Balancing Control and Coherence in
        Language Model Generation.](https://arxiv.org/abs/2510.13285)'
      title: 'In-Distribution Steering: Balancing Control and Coherence in Language
        Model Generation'
      authors:
      - Arthur Vogels
      - Benjamin Wong
      - Yann Choho
      - Annabelle Blangero
      - Milan Bhan
      author_organizations: []
      date: '2025-10-15'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:Capability_removal_unlearning
  name: 'Capability removal: unlearning'
  header_level: 3
  parent_id: sec:Iterative_alignment
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Developing methods to selectively remove specific information,
      capabilities, or behaviors from a trained model (e.g. without retraining it
      from scratch). A mixture of black-box and white-box approaches.
    theory_of_change: If an AI learns dangerous knowledge (e.g., dual-use capabilities
      like virology or hacking, or knowledge of their own safety controls) or exhibits
      undesirable behaviors (e.g., memorizing private data), we can specifically erase
      this "bad" knowledge post-training, which is much cheaper and faster than retraining,
      thereby making the model safer. Alternatively, intervene in pre-training, to
      prevent the model from learning it in the first place (even when data filtering
      is imperfect). You could imagine also unlearning propensities to power-seeking,
      deception, sycophancy, or spite.
    see_also:
    - a:Data_filtering
    - sec:White_box_safety_i_e_Interpretability_
    - a:Various_Redteams
    orthodox_problems:
    - superintelligence_hack_software
    - boxed_agi_exfiltrate
    - humanlike_minds_not_safe
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: null
    broad_approach_text: cognitive / engineering
    some_names:
    - Rowan Wang
    - Avery Griffin
    - Johannes Treutlein
    - Zico Kolter
    - Bruce W. Lee
    - Addie Foote
    - Alex Infanger
    - Zesheng Shi
    - Yucheng Zhou
    - Jing Li
    - Timothy Qian
    - Stephen Casper
    - Alex Cloud
    - Peter Henderson
    - Filip Sondej
    - Fazl Barez
    estimated_ftes: 10-50
    critiques: '[Existing Large Language Model Unlearning Evaluations Are Inconclusive](https://arxiv.org/abs/2506.00688)'
    funded_by: Coefficient Giving, MacArthur Foundation, UK AI Safety Institute (AISI),
      Canadian AI Safety Institute (CAISI), industry labs (e.g., Microsoft Research,
      Google)
    outputs:
    - section_name: Frameworks
      header_level: 4
      original_md: '*Frameworks:*'
    - link_url: https://github.com/locuslab/open-unlearning
      link_text: OpenUnlearning
      original_md: '[OpenUnlearning](https://github.com/locuslab/open-unlearning)'
      title: 'OpenUnlearning: Accelerating LLM Unlearning via Unified Benchmarking
        of Methods and Metrics'
      authors:
      - Vineeth Dorna
      - Anmol Mekala
      - Wenlong Zhao
      - Andrew McCallum
      - Zachary C Lipton
      - J Zico Kolter
      - Pratyush Maini
      author_organizations:
      - Carnegie Mellon University
      - University of Massachusetts Amherst
      date: '2025-06-20'
      published_year: 2025
      venue: arXiv
      kind: code_tool
    - section_name: Mostly black-box
      header_level: 4
      original_md: '*Mostly black-box*'
    - link_url: https://alignment.anthropic.com/2025/modifying-beliefs-via-sdf
      link_text: Modifying LLM Beliefs with Synthetic Document Finetuning
      original_md: '[Modifying LLM Beliefs with Synthetic Document Finetuning](https://alignment.anthropic.com/2025/modifying-beliefs-via-sdf)'
      title: Modifying LLM Beliefs with Synthetic Document Finetuning
      authors:
      - Rowan Wang
      - Avery Griffin
      - Johannes Treutlein
      - Ethan Perez
      - Julian Michael
      - Fabien Roger
      - Sam Marks
      author_organizations:
      - Anthropic
      - MATS
      - Scale AI
      date: '2025-04-24'
      published_year: 2025
      venue: Alignment Science Blog
      kind: blog_post
    - link_url: https://arxiv.org/abs/2505.22310
      link_text: 'From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space
        Regularization'
      original_md: '[From Dormant to Deleted: Tamper-Resistant Unlearning Through
        Weight-Space Regularization](https://arxiv.org/abs/2505.22310)'
      title: 'From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space
        Regularization'
      authors:
      - Shoaib Ahmed Siddiqui
      - Adrian Weller
      - David Krueger
      - Gintare Karolina Dziugaite
      - Michael Curtis Mozer
      - Eleni Triantafillou
      author_organizations:
      - University of Cambridge
      - Google DeepMind
      - Vector Institute
      date: '2025-05-28'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.08138
      link_text: Mirror Mirror on the Wall, Have I Forgotten it All?
      original_md: '[Mirror Mirror on the Wall, Have I Forgotten it All?](https://arxiv.org/abs/2505.08138)'
      title: Mirror Mirror on the Wall, Have I Forgotten it All? A New Framework for
        Evaluating Machine Unlearning
      authors:
      - Brennon Brimhall
      - Philip Mathew
      - Neil Fendley
      - Yinzhi Cao
      - Matthew Green
      author_organizations: []
      date: '2025-05-13'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2412.06966
      link_text: 'Machine Unlearning Doesn''t Do What You Think: Lessons for Generative
        AI Policy and Research'
      original_md: '[Machine Unlearning Doesn''t Do What You Think: Lessons for Generative
        AI Policy and Research](https://arxiv.org/abs/2412.06966)'
      title: 'Machine Unlearning Doesn''t Do What You Think: Lessons for Generative
        AI Policy and Research'
      authors:
      - A. Feder Cooper
      - Christopher A. Choquette-Choo
      - Miranda Bogen
      - Kevin Klyman
      - Matthew Jagielski
      - Katja Filippova
      - Ken Liu
      - Alexandra Chouldechova
      - Jamie Hayes
      - Yangsibo Huang
      - Eleni Triantafillou
      - Peter Kairouz
      - Nicole Elyse Mitchell
      - Niloofar Mireshghallah
      - Abigail Z. Jacobs
      - James Grimmelmann
      - Vitaly Shmatikov
      - Christopher De Sa
      - Ilia Shumailov
      - Andreas Terzis
      - Solon Barocas
      - Jennifer Wortman Vaughan
      - Danah Boyd
      - Yejin Choi
      - Sanmi Koyejo
      - Fernando Delgado
      - Percy Liang
      - Daniel E. Ho
      - Pamela Samuelson
      - Miles Brundage
      - David Bau
      - Seth Neel
      - Hanna Wallach
      - Amy B. Cyphert
      - Mark A. Lemley
      - Nicolas Papernot
      - Katherine Lee
      author_organizations:
      - Anthropic
      - Google
      - Stanford University
      - Cornell University
      - University of Washington
      - Various Universities and Research Institutions
      date: '2024-12-09'
      published_year: 2024
      venue: NeurIPS 2025 (Oral)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2501.04952
      link_text: Open Problems in Machine Unlearning for AI Safety
      original_md: '[Open Problems in Machine Unlearning for AI Safety](https://arxiv.org/abs/2501.04952)'
      title: Open Problems in Machine Unlearning for AI Safety
      authors:
      - Fazl Barez
      - Tingchen Fu
      - Ameya Prabhu
      - Stephen Casper
      - Amartya Sanyal
      - Adel Bibi
      - Aidan O'Gara
      - Robert Kirk
      - Ben Bucknall
      - Tim Fist
      - Luke Ong
      - Philip Torr
      - Kwok-Yan Lam
      - Robert Trager
      - David Krueger
      - Sören Mindermann
      - José Hernandez-Orallo
      - Mor Geva
      - Yarin Gal
      author_organizations:
      - University of Oxford
      - MIT
      - University of Cambridge
      - Nanyang Technological University
      - Tel Aviv University
      date: '2025-01-09'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - section_name: Mostly white-box
      header_level: 4
      original_md: '*Mostly white-box*'
    - link_url: https://arxiv.org/abs/2509.11816
      link_text: Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive
        LLM Unlearning
      original_md: '[Collapse of Irrelevant Representations (CIR) Ensures Robust and
        Non-Disruptive LLM Unlearning](https://arxiv.org/abs/2509.11816)'
      title: Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive
        LLM Unlearning
      authors:
      - Filip Sondej
      - Yushi Yang
      author_organizations: []
      date: '2025-09-15'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.18588
      link_text: Safety Alignment via Constrained Knowledge Unlearning
      original_md: '[Safety Alignment via Constrained Knowledge Unlearning](https://arxiv.org/abs/2505.18588)'
      title: Safety Alignment via Constrained Knowledge Unlearning
      authors:
      - Zesheng Shi
      - Yucheng Zhou
      - Jing Li
      author_organizations: []
      date: '2025-05-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.12484
      link_text: 'Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption
        Masking And Normalization'
      original_md: '[Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption
        Masking And Normalization](https://arxiv.org/abs/2506.12484)'
      title: 'Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking
        And Normalization'
      authors:
      - Filip Sondej
      - Yushi Yang
      - Mikołaj Kniejski
      - Marcel Windys
      author_organizations: []
      date: '2025-06-14'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.16831
      link_text: 'Unlearning Isn''t Deletion: Investigating Reversibility of Machine
        Unlearning in LLMs'
      original_md: '[Unlearning Isn''t Deletion: Investigating Reversibility of Machine
        Unlearning in LLMs](https://arxiv.org/abs/2505.16831)'
      title: 'Unlearning Isn''t Deletion: Investigating Reversibility of Machine Unlearning
        in LLMs'
      authors:
      - Xiaoyu Xu
      - Xiang Yue
      - Yang Liu
      - Qingqing Ye
      - Huadi Zheng
      - Peizhao Hu
      - Minxin Du
      - Haibo Hu
      author_organizations: []
      date: '2025-05-22'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/QYzofMbzmbgiwfqy8/unlearning-needs-to-be-more-selective-progress-report
      link_text: Unlearning Needs to be More Selective [Progress Report]
      original_md: '[Unlearning Needs to be More Selective \[Progress Report\]](https://lesswrong.com/posts/QYzofMbzmbgiwfqy8/unlearning-needs-to-be-more-selective-progress-report)'
      title: Unlearning Needs to be More Selective [Progress Report]
      authors:
      - Filip Sondej
      - Yushi Yang
      - Marcel Windys
      author_organizations: []
      date: '2025-06-27'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2505.09500
      link_text: Layered Unlearning for Adversarial Relearning
      original_md: '[Layered Unlearning for Adversarial Relearning](https://arxiv.org/abs/2505.09500)'
      title: Layered Unlearning for Adversarial Relearning
      authors:
      - Timothy Qian
      - Vinith Suriyakumar
      - Ashia Wilson
      - Dylan Hadfield-Menell
      author_organizations: []
      date: '2025-05-14'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://goodfire.ai/research/understanding-memorization-via-loss-curvature
      link_text: Understanding Memorization via Loss Curvature
      original_md: '[Understanding Memorization via Loss Curvature](https://goodfire.ai/research/understanding-memorization-via-loss-curvature)'
      title: Understanding Memorization via Loss Curvature
      authors:
      - Jack Merullo
      - Srihita Vatsavaya
      - Lucius Bushnaq
      - Owen Lewis
      author_organizations:
      - Goodfire
      date: '2025-11-06'
      published_year: 2025
      venue: Goodfire.ai Research Blog
      kind: blog_post
    - link_url: https://arxiv.org/abs/2502.05209
      link_text: Model Tampering Attacks Enable More Rigorous Evaluations of LLM Capabilities
      original_md: '[Model Tampering Attacks Enable More Rigorous Evaluations of LLM
        Capabilities](https://arxiv.org/abs/2502.05209)'
      title: Model Tampering Attacks Enable More Rigorous Evaluations of LLM Capabilities
      authors:
      - Zora Che
      - Stephen Casper
      - Robert Kirk
      - Anirudh Satheesh
      - Stewart Slocum
      - Lev E McKinney
      - Rohit Gandikota
      - Aidan Ewart
      - Domenic Rosati
      - Zichu Wu
      - Zikui Cai
      - Bilal Chughtai
      - Yarin Gal
      - Furong Huang
      - Dylan Hadfield-Menell
      author_organizations: []
      date: '2025-02-03'
      published_year: 2025
      venue: arXiv (accepted to TMLR)
      kind: paper_preprint
    - section_name: Pre-training interventions
      header_level: 4
      original_md: '*Pre-training interventions*'
    - link_url: https://arxiv.org/abs/2410.04332
      link_text: 'Gradient Routing: Masking Gradients to Localize Computation in Neural
        Networks'
      original_md: '[Gradient Routing: Masking Gradients to Localize Computation in
        Neural Networks](https://arxiv.org/abs/2410.04332)'
      title: 'Gradient Routing: Masking Gradients to Localize Computation in Neural
        Networks'
      authors:
      - Alex Cloud
      - Jacob Goldman-Wetzler
      - Evžen Wybitul
      - Joseph Miller
      - Alexander Matt Turner
      author_organizations: []
      date: '2024-10-06'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.lesswrong.com/posts/tAnHM3L25LwuASdpF/selective-modularity-a-research-agenda
      link_text: 'Selective modularity: a research agenda'
      original_md: '[Selective modularity: a research agenda](https://www.lesswrong.com/posts/tAnHM3L25LwuASdpF/selective-modularity-a-research-agenda)'
      title: 'Selective modularity: a research agenda'
      authors:
      - cloud
      - Jacob G-W
      author_organizations:
      - MATS
      date: '2025-03-24'
      published_year: 2025
      venue: LessWrong/AI Alignment Forum
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2506.06278
      link_text: Distillation Robustifies Unlearning
      original_md: '[Distillation Robustifies Unlearning](https://arxiv.org/abs/2506.06278)'
      title: Distillation Robustifies Unlearning
      authors:
      - Bruce W. Lee
      - Addie Foote
      - Alex Infanger
      - Leni Shor
      - Harish Kamath
      - Jacob Goldman-Wetzler
      - Bryce Woodworth
      - Alex Cloud
      - Alexander Matt Turner
      author_organizations: []
      date: '2025-06-06'
      published_year: 2025
      venue: arXiv (NeurIPS 2025 Spotlight)
      kind: paper_preprint
    - link_url: https://www.arxiv.org/abs/2512.05648
      link_text: 'Beyond Data Filtering: Knowledge Localization for Capability Removal
        in LLMs'
      original_md: '[Beyond Data Filtering: Knowledge Localization for Capability
        Removal in LLMs](https://www.arxiv.org/abs/2512.05648)'
      title: 'Beyond Data Filtering: Knowledge Localization for Capability Removal
        in LLMs'
      authors:
      - Igor Shilov
      - Alex Cloud
      - Aryo Pradipta Gema
      - Jacob Goldman-Wetzler
      - Nina Panickssery
      - Henry Sleight
      - Erik Jones
      - Cem Anil
      author_organizations:
      - Anthropic
      date: '2024-12-05'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues:
  - Broad approach field contains 'cognitive / engineering' - this is a mixed approach,
    so broad_approach_id is left null
- id: a:Control
  name: Control
  header_level: 3
  parent_id: sec:Iterative_alignment
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: If we assume early transformative AIs are misaligned and
      actively trying to subvert safety measures, can we still set up protocols to
      extract useful work from them while preventing sabotage, and watching with incriminating
      behaviour?
    theory_of_change: null
    see_also:
    - safety cases
    orthodox_problems: []
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: null
    broad_approach_text: engineering / behavioral
    some_names:
    - Redwood
    - UK AISI
    - Deepmind
    - OpenAI
    - Anthropic
    - Buck Shlegeris
    - Ryan Greenblatt
    - Kshitij Sachan
    - Alex Mallen
    estimated_ftes: 5-50
    critiques: '[Wentworth](https://www.lesswrong.com/posts/8wBN8cdNAv3c7vt6p/the-case-against-ai-control-research),
      [Mannheim](https://lesswrong.com/posts/25dsPH6CuRXPBkGHN/no-we-re-not-getting-meaningful-oversight-of-ai),
      [Kulveit](https://www.lesswrong.com/posts/rZcyemEpBHgb2hqLP/ai-control-may-increase-existential-risk)'
    funded_by: null
    outputs:
    - link_url: https://luthienresearch.org/updates/2025-03-redteam-as-upsampling/
      link_text: Luthien's Approach to Prosaic AI Control in 21 Points
      original_md: '[Luthien''s Approach to Prosaic AI Control in 21 Points](https://luthienresearch.org/updates/2025-03-redteam-as-upsampling/)'
      title: Luthien's Approach to Prosaic AI Control in 21 Points
      authors: []
      author_organizations:
      - Luthien Research
      date: '2025-03-17'
      published_year: 2025
      venue: Luthien Research Blog
      kind: blog_post
    - link_url: https://alignmentforum.org/posts/LPHMMMZFAWog6ty5x/ctrl-z-controlling-ai-agents-via-resampling
      link_text: 'Ctrl-Z: Controlling AI Agents via Resampling'
      original_md: '[Ctrl-Z: Controlling AI Agents via Resampling](https://alignmentforum.org/posts/LPHMMMZFAWog6ty5x/ctrl-z-controlling-ai-agents-via-resampling)'
      title: 'Ctrl-Z: Controlling AI Agents via Resampling'
      authors:
      - Aryan Bhatt
      - Buck Shlegeris
      - Adam Kaufman
      - Cody Rushing
      - Tyler Tracy
      - Vasil Georgiev
      - David Matolcsi
      - Akbir Khan
      author_organizations:
      - Redwood Research
      date: '2025-04-16'
      published_year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2506.15740
      link_text: 'SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents'
      original_md: '[SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents](https://arxiv.org/abs/2506.15740)'
      title: 'SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents'
      authors:
      - Jonathan Kutasov
      - Yuqi Sun
      - Paul Colognese
      - Teun van der Weij
      - Linda Petrini
      - Chen Bo Calvin Zhang
      - John Hughes
      - Xiang Deng
      - Henry Sleight
      - Tyler Tracy
      - Buck Shlegeris
      - Joe Benton
      author_organizations:
      - Redwood Research
      date: '2025-06-17'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.17693
      link_text: Adaptive Deployment of Untrusted LLMs Reduces Distributed Threats
      original_md: '[Adaptive Deployment of Untrusted LLMs Reduces Distributed Threats](https://arxiv.org/abs/2411.17693)'
      title: Adaptive Deployment of Untrusted LLMs Reduces Distributed Threats
      authors:
      - Jiaxin Wen
      - Vivek Hebbar
      - Caleb Larson
      - Aryan Bhatt
      - Ansh Radhakrishnan
      - Mrinank Sharma
      - Henry Sleight
      - Shi Feng
      - He He
      - Ethan Perez
      - Buck Shlegeris
      - Akbir Khan
      author_organizations: []
      date: '2024-11-26'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.17938
      link_text: 'D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language
        Models'
      original_md: '[D-REX: A Benchmark for Detecting Deceptive Reasoning in Large
        Language Models](https://arxiv.org/abs/2509.17938)'
      title: 'D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language
        Models'
      authors:
      - Satyapriya Krishna
      - Andy Zou
      - Rahul Gupta
      - Eliot Krzysztof Jones
      - Nick Winter
      - Dan Hendrycks
      - J. Zico Kolter
      - Matt Fredrikson
      - Spyros Matsoukas
      author_organizations:
      - Carnegie Mellon University
      - Center for AI Safety
      - Anthropic
      date: '2025-09-22'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2412.12480
      link_text: 'Subversion Strategy Eval: Can language models statelessly strategize
        to subvert control protocols?'
      original_md: '[Subversion Strategy Eval: Can language models statelessly strategize
        to subvert control protocols?](https://arxiv.org/abs/2412.12480)'
      title: 'Subversion Strategy Eval: Can language models statelessly strategize
        to subvert control protocols?'
      authors:
      - Alex Mallen
      - Charlie Griffin
      - Misha Wagner
      - Alessandro Abate
      - Buck Shlegeris
      author_organizations:
      - Redwood Research
      date: '2024-12-17'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2511.02997
      link_text: Evaluating Control Protocols for Untrusted AI Agents
      original_md: '[Evaluating Control Protocols for Untrusted AI Agents](https://arxiv.org/abs/2511.02997)'
      title: Evaluating Control Protocols for Untrusted AI Agents
      authors:
      - Jon Kutasov
      - Chloe Loughridge
      - Yuqi Sun
      - Henry Sleight
      - Buck Shlegeris
      - Tyler Tracy
      - Joe Benton
      author_organizations:
      - Redwood Research
      date: '2025-11-04'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.19851
      link_text: Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought
        Monitorability
      original_md: '[Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought
        Monitorability](https://arxiv.org/abs/2510.19851)'
      title: Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought
        Monitorability
      authors:
      - Artur Zolkowski
      - Wen Xing
      - David Lindner
      - Florian Tramèr
      - Erik Jenner
      author_organizations:
      - ETH Zurich
      - FAR AI
      date: '2025-10-21'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2511.02823
      link_text: Optimizing AI Agent Attacks With Synthetic Data
      original_md: '[Optimizing AI Agent Attacks With Synthetic Data](https://arxiv.org/abs/2511.02823)'
      title: Optimizing AI Agent Attacks With Synthetic Data
      authors:
      - Chloe Loughridge
      - Paul Colognese
      - Avery Griffin
      - Tyler Tracy
      - Jon Kutasov
      - Joe Benton
      author_organizations: []
      date: '2025-11-04'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://openreview.net/forum?id=QWopGahUEL
      link_text: Games for AI Control
      original_md: '[Games for AI Control](https://openreview.net/forum?id=QWopGahUEL)'
      title: 'Games for AI Control: Models of Safety Evaluations of AI Deployment
        Protocols'
      authors:
      - Charlie Griffin
      - Louis Thomson
      - Buck Shlegeris
      - Alessandro Abate
      author_organizations:
      - Redwood Research
      - University of Oxford
      date: '2025-09-20'
      published_year: 2025
      venue: ICLR 2026 Conference (Withdrawn)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2501.17315
      link_text: A sketch of an AI control safety case
      original_md: '[A sketch of an AI control safety case](https://arxiv.org/abs/2501.17315)'
      title: A sketch of an AI control safety case
      authors:
      - Tomek Korbak
      - Joshua Clymer
      - Benjamin Hilton
      - Buck Shlegeris
      - Geoffrey Irving
      author_organizations:
      - Redwood Research
      date: '2025-01-28'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.05791
      link_text: Assessing confidence in frontier AI safety cases
      original_md: '[Assessing confidence in frontier AI safety cases](https://arxiv.org/abs/2502.05791)'
      title: Assessing confidence in frontier AI safety cases
      authors:
      - Stephen Barrett
      - Philip Fox
      - Joshua Krook
      - Tuneer Mondal
      - Simon Mylius
      - Alejandro Tlaie
      author_organizations: []
      date: '2025-02-09'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://control-arena.aisi.org.uk/
      link_text: ControlArena
      original_md: '[ControlArena](https://control-arena.aisi.org.uk/)'
      title: ControlArena
      authors:
      - Rogan Inglis
      - Ollie Matthews
      - Tyler Tracy
      - Oliver Makins
      - Tom Catling
      - Asa Cooper Stickland
      - Rasmus Faber-Espensen
      - Daniel O'Connell
      - Myles Heller
      - Miguel Brandao
      - Adam Hanson
      - Arathi Mani
      - Tomek Korbak
      - Jan Michelfeit
      - Dishank Bansal
      - Tomas Bark
      - Chris Canal
      - Charlie Griffin
      - Jasmine Wang
      - Alan Cooney
      author_organizations:
      - UK AI Security Institute
      - Redwood Research
      date: '2025-01-01'
      published_year: 2025
      venue: GitHub
      kind: code_tool
    - link_url: https://arxiv.org/abs/2504.05259
      link_text: How to evaluate control measures for LLM agents? A trajectory from
        today to superintelligence
      original_md: '[How to evaluate control measures for LLM agents? A trajectory
        from today to superintelligence](https://arxiv.org/abs/2504.05259)'
      title: How to evaluate control measures for LLM agents? A trajectory from today
        to superintelligence
      authors:
      - Tomek Korbak
      - Mikita Balesni
      - Buck Shlegeris
      - Geoffrey Irving
      author_organizations:
      - Redwood Research
      - Google DeepMind
      date: '2025-04-07'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/wKTwdgZDo479EhmJL/the-alignment-project-by-uk-aisi-1
      link_text: The Alignment Project by UK AISI
      original_md: '[The Alignment Project by UK AISI](https://lesswrong.com/posts/wKTwdgZDo479EhmJL/the-alignment-project-by-uk-aisi-1)'
      title: The Alignment Project by UK AISI
      authors:
      - Mojmir
      - Benjamin Hilton
      - Jacob Pfau
      - Geoffrey Irving
      - Joseph Bloom
      - Tomek Korbak
      - David Africa
      - Edmund Lau
      author_organizations:
      - UK AI Security Institute
      date: '2025-08-01'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2411.03336
      link_text: Towards evaluations-based safety cases for AI scheming
      original_md: '[Towards evaluations-based safety cases for AI scheming](https://arxiv.org/abs/2411.03336)'
      title: Towards evaluations-based safety cases for AI scheming
      authors:
      - Mikita Balesni
      - Marius Hobbhahn
      - David Lindner
      - Alexander Meinke
      - Tomek Korbak
      - Joshua Clymer
      - Buck Shlegeris
      - Jérémy Scheurer
      - Charlotte Stix
      - Rusheb Shah
      - Nicholas Goldowsky-Dill
      - Dan Braun
      - Bilal Chughtai
      - Owain Evans
      - Daniel Kokotajlo
      - Lucius Bushnaq
      author_organizations:
      - Redwood Research
      - Apollo Research
      - Anthropic
      date: '2024-11-07'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2001.07118
      link_text: Incentives for Responsiveness, Instrumental Control and Impact
      original_md: '[Incentives for Responsiveness, Instrumental Control and Impact](https://arxiv.org/abs/2001.07118)'
      title: Incentives for Responsiveness, Instrumental Control and Impact
      authors:
      - Ryan Carey
      - Eric Langlois
      - Chris van Merwijk
      - Shane Legg
      - Tom Everitt
      author_organizations:
      - DeepMind
      date: '2020-01-20'
      published_year: 2020
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/neTbrpBziAsTH5Bn7/ai-companies-are-unlikely-to-make-high-assurance-safety
      link_text: AI companies are unlikely to make high-assurance safety cases if
        timelines are short
      original_md: '[AI companies are unlikely to make high-assurance safety cases
        if timelines are short](https://lesswrong.com/posts/neTbrpBziAsTH5Bn7/ai-companies-are-unlikely-to-make-high-assurance-safety)'
      title: AI companies are unlikely to make high-assurance safety cases if timelines
        are short
      authors:
      - Ryan Greenblatt
      author_organizations:
      - Anthropic
      date: '2025-01-23'
      published_year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2507.12872
      link_text: 'Manipulation Attacks by Misaligned AI: Risk Analysis and Safety
        Case Framework'
      original_md: '[Manipulation Attacks by Misaligned AI: Risk Analysis and Safety
        Case Framework](https://arxiv.org/abs/2507.12872)'
      title: 'Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case
        Framework'
      authors:
      - Rishane Dassanayake
      - Mario Demetroudi
      - James Walpole
      - Lindley Lentati
      - Jason R. Brown
      - Edward James Young
      author_organizations: []
      date: '2025-07-17'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2412.17618
      link_text: Dynamic safety cases for frontier AI
      original_md: '[Dynamic safety cases for frontier AI](https://arxiv.org/abs/2412.17618)'
      title: Dynamic safety cases for frontier AI
      authors:
      - Carmen Cârlan
      - Francesca Gomez
      - Yohan Mathew
      - Ketana Krishna
      - René King
      - Peter Gebauer
      - Ben R. Smith
      author_organizations: []
      date: '2024-12-23'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/cJQZAueoPC6aTncKK/ais-at-the-current-capability-level-may-be-important-for
      link_text: AIs at the current capability level may be important for future safety
        work
      original_md: '[AIs at the current capability level may be important for future
        safety work](https://lesswrong.com/posts/cJQZAueoPC6aTncKK/ais-at-the-current-capability-level-may-be-important-for)'
      title: AIs at the current capability level may be important for future safety
        work
      authors:
      - Ryan Greenblatt
      author_organizations:
      - Anthropic
      date: '2025-05-12'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/y6rBarAPTLmuhn9PJ/takeaways-from-sketching-a-control-safety-case
      link_text: Takeaways from sketching a control safety case
      original_md: '[Takeaways from sketching a control safety case](https://lesswrong.com/posts/y6rBarAPTLmuhn9PJ/takeaways-from-sketching-a-control-safety-case)'
      title: Takeaways from sketching a control safety case
      authors:
      - Josh Clymer
      - Buck Shlegeris
      author_organizations:
      - Redwood Research
      - UK AISI
      date: '2025-01-31'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    other_attributes: {}
  parsing_issues:
  - 'Broad approach field contains multiple approaches: ''engineering / behavioural''
    - broad_approach_id set to null'
- id: a:Safeguards_inference_time_auxiliaries_
  name: Safeguards (inference-time auxiliaries)
  header_level: 3
  parent_id: sec:Iterative_alignment
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Layers of inference-time defenses, such as classifiers,
      monitors, and rapid-response protocols, to detect and block jailbreaks, prompt
      injections, and other harmful model behaviors.
    theory_of_change: By building a bunch of scalable and hardened things on top of
      an unsafe model, we can defend against known and unknown attacks, monitor for
      misuse, and prevent models from causing harm, even if the core model has vulnerabilities.
    see_also:
    - a:Various_Redteams
    - sec:Iterative_alignment
    orthodox_problems:
    - superintelligence_fool_supervisors
    - boxed_agi_exfiltrate
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: engineering
    broad_approach_text: engineering
    some_names:
    - Mrinank Sharma
    - Meg Tong
    - Jesse Mu
    - Alwin Peng
    - Julian Michael
    - Henry Sleight
    - Theodore Sumers
    - Raj Agarwal
    - Nathan Bailey
    - Edoardo Debenedetti
    - Ilia Shumailov
    - Tianqi Fan
    - Sahil Verma
    - Keegan Hines
    - Jeff Bilmes
    estimated_ftes: 100+
    critiques: '[Obfuscated Activations Bypass LLM Latent-Space Defenses](https://arxiv.org/abs/2412.09565)'
    funded_by: most of the big labs
    outputs:
    - link_url: https://arxiv.org/abs/2501.18837
      link_text: 'Constitutional Classifiers: Defending against Universal Jailbreaks
        across Thousands of Hours of Red Teaming'
      original_md: '[Constitutional Classifiers: Defending against Universal Jailbreaks
        across Thousands of Hours of Red Teaming](https://arxiv.org/abs/2501.18837)'
      title: 'Constitutional Classifiers: Defending against Universal Jailbreaks across
        Thousands of Hours of Red Teaming'
      authors:
      - Mrinank Sharma
      - Meg Tong
      - Jesse Mu
      - Jerry Wei
      - Jorrit Kruthoff
      - Scott Goodfriend
      - Euan Ong
      - Alwin Peng
      - Raj Agarwal
      - Cem Anil
      - Amanda Askell
      - Nathan Bailey
      - Joe Benton
      - Emma Bluemke
      - Samuel R. Bowman
      - Eric Christiansen
      - Hoagy Cunningham
      - Andy Dau
      - Anjali Gopal
      - Rob Gilson
      - Logan Graham
      - Logan Howard
      - Nimit Kalra
      - Taesung Lee
      - Kevin Lin
      - Peter Lofgren
      - Francesco Mosconi
      - Clare O'Hara
      - Catherine Olsson
      - Linda Petrini
      - Samir Rajani
      - Nikhil Saxena
      - Alex Silverstein
      - Tanya Singh
      - Theodore Sumers
      - Leonard Tang
      - Kevin K. Troy
      - Constantin Weisser
      - Ruiqi Zhong
      - Giulio Zhou
      - Jan Leike
      - Jared Kaplan
      - Ethan Perez
      author_organizations:
      - Anthropic
      date: '2025-01-31'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.07494
      link_text: 'Rapid Response: Mitigating LLM Jailbreaks with a Few Examples'
      original_md: '[Rapid Response: Mitigating LLM Jailbreaks with a Few Examples](https://arxiv.org/abs/2411.07494)'
      title: 'Rapid Response: Mitigating LLM Jailbreaks with a Few Examples'
      authors:
      - Alwin Peng
      - Julian Michael
      - Henry Sleight
      - Ethan Perez
      - Mrinank Sharma
      author_organizations:
      - Anthropic
      date: '2024-11-12'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://alignment.anthropic.com/2025/summarization-for-monitoring/index.html
      link_text: Monitoring computer use via hierarchical summarization
      original_md: '[Monitoring computer use via hierarchical summarization](https://alignment.anthropic.com/2025/summarization-for-monitoring/index.html)'
      title: Monitoring computer use via hierarchical summarization
      authors:
      - Theodore Sumers
      - Raj Agarwal
      - Nathan Bailey
      - Tim Belonax
      - Brian Clarke
      - Jasmine Deng
      - Kyla Guru
      - Evan Frondorf
      - Keegan Hankes
      - Jacob Klein
      - Lynx Lean
      - Kevin Lin
      - Linda Petrini
      - Madeleine Tucker
      - Ethan Perez
      - Mrinank Sharma
      - Nikhil Saxena
      author_organizations:
      - Anthropic
      date: '2025-02-27'
      published_year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - link_url: https://arxiv.org/abs/2503.18813
      link_text: Defeating Prompt Injections by Design
      original_md: '[Defeating Prompt Injections by Design](https://arxiv.org/abs/2503.18813)'
      title: Defeating Prompt Injections by Design
      authors:
      - Edoardo Debenedetti
      - Ilia Shumailov
      - Tianqi Fan
      - Jamie Hayes
      - Nicholas Carlini
      - Daniel Fabian
      - Christoph Kern
      - Chongyang Shi
      - Andreas Terzis
      - Florian Tramèr
      author_organizations:
      - Google Research
      - ETH Zurich
      date: '2025-03-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://alignment.anthropic.com/2025/introducing-safeguards-research-team/index.html
      link_text: Introducing Anthropic's Safeguards Research Team
      original_md: '[Introducing Anthropic''s Safeguards Research Team](https://alignment.anthropic.com/2025/introducing-safeguards-research-team/index.html)'
      title: Introducing Anthropic's Safeguards Research Team
      authors: []
      author_organizations:
      - Anthropic
      date: '2025-01-01'
      published_year: 2025
      venue: Anthropic Alignment Science Blog
      kind: news_announcement
    - link_url: https://arxiv.org/abs/2505.23856
      link_text: 'OMNIGUARD: An Efficient Approach for AI Safety Moderation Across
        Modalities'
      original_md: '[OMNIGUARD: An Efficient Approach for AI Safety Moderation Across
        Modalities](https://arxiv.org/abs/2505.23856)'
      title: 'OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities'
      authors:
      - Sahil Verma
      - Keegan Hines
      - Jeff Bilmes
      - Charlotte Siska
      - Luke Zettlemoyer
      - Hila Gonen
      - Chandan Singh
      author_organizations:
      - University of Washington
      - Meta AI Research
      date: '2025-05-29'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:Chain_of_thought_monitoring
  name: Chain of thought monitoring
  header_level: 3
  parent_id: sec:Iterative_alignment
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Supervise an AI's natural-language (output) "reasoning"
      to detect misalignment, scheming, or deception, rather than studying the actual
      internal states.
    theory_of_change: The reasoning process (Chain of Thought, or CoT) of an AI provides
      a legible signal of its internal state and intentions. By monitoring this CoT,
      supervisors (human or AI) can detect misalignment, scheming, or reward hacking
      before it results in a harmful final output. This allows for more robust oversight
      than supervising outputs alone, but it relies on the CoT remaining faithful
      (i.e., accurately reflecting the model's reasoning) and not becoming obfuscated
      under optimization pressure.
    see_also:
    - sec:White_box_safety_i_e_Interpretability_
    - a:Steganography_evals
    orthodox_problems:
    - superintelligence_fool_supervisors
    - superintelligence_hack_software
    - boxed_agi_exfiltrate
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: engineering
    broad_approach_text: engineering
    some_names:
    - Aether
    - Bowen Baker
    - Joost Huizinga
    - Leo Gao
    - Scott Emmons
    - Erik Jenner
    - Yanda Chen
    - James Chua
    - Owain Evans
    - Tomek Korbak
    - Mikita Balesni
    - Xinpeng Wang
    - Miles Turpin
    - Rohin Shah
    estimated_ftes: 10-100
    critiques: '[Reasoning Models Don''t Always Say What They Think](https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf);
      [Chain-of-Thought Reasoning In The Wild Is Not Always Faithful](https://arxiv.org/abs/2503.08679);
      [Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate
      Tokens](https://arxiv.org/abs/2505.13775); [Reasoning Models Sometimes Output
      Illegible Chains of Thought](https://arxiv.org/abs/2510.27338)'
    funded_by: OpenAI, Anthropic, Google DeepMind
    outputs:
    - link_url: https://arxiv.org/abs/2503.11926
      link_text: Monitoring Reasoning Models for Misbehavior and the Risks of Promoting
        Obfuscation
      original_md: '[Monitoring Reasoning Models for Misbehavior and the Risks of
        Promoting Obfuscation](https://arxiv.org/abs/2503.11926)'
      title: Monitoring Reasoning Models for Misbehavior and the Risks of Promoting
        Obfuscation
      authors:
      - Bowen Baker
      - Joost Huizinga
      - Leo Gao
      - Zehao Dou
      - Melody Y. Guan
      - Aleksander Madry
      - Wojciech Zaremba
      - Jakub Pachocki
      - David Farhi
      author_organizations:
      - OpenAI
      date: '2025-03-14'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://openai.com/index/chain-of-thought-monitoring/
      link_text: Detecting misbehavior in frontier reasoning models
      original_md: '[Detecting misbehavior in frontier reasoning models](https://openai.com/index/chain-of-thought-monitoring/)'
      title: Detecting misbehavior in frontier reasoning models
      authors:
      - Bowen Baker
      - Joost Huizinga
      - Aleksander Madry
      - Wojciech Zaremba
      - Jakub Pachocki
      - David Farhi
      author_organizations:
      - OpenAI
      date: '2025-03-10'
      published_year: 2025
      venue: OpenAI Blog / arXiv
      kind: blog_post
    - link_url: https://arxiv.org/abs/2507.05246
      link_text: When Chain of Thought is Necessary, Language Models Struggle to Evade
        Monitors
      original_md: '[When Chain of Thought is Necessary, Language Models Struggle
        to Evade Monitors](https://arxiv.org/abs/2507.05246)'
      title: When Chain of Thought is Necessary, Language Models Struggle to Evade
        Monitors
      authors:
      - Scott Emmons
      - Erik Jenner
      - David K. Elson
      - Rif A. Saurous
      - Senthooran Rajamanoharan
      - Heng Chen
      - Irhum Shafkat
      - Rohin Shah
      author_organizations:
      - Google DeepMind
      date: '2025-07-07'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.05410
      link_text: Reasoning Models Don't Always Say What They Think
      original_md: '[Reasoning Models Don''t Always Say What They Think](https://arxiv.org/abs/2505.05410)'
      title: Reasoning Models Don't Always Say What They Think
      authors:
      - Yanda Chen
      - Joe Benton
      - Ansh Radhakrishnan
      - Jonathan Uesato
      - Carson Denison
      - John Schulman
      - Arushi Somani
      - Peter Hase
      - Misha Wagner
      - Fabien Roger
      - Vlad Mikulik
      - Samuel R. Bowman
      - Jan Leike
      - Jared Kaplan
      - Ethan Perez
      author_organizations:
      - OpenAI
      - Anthropic
      date: '2025-05-08'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.01367
      link_text: Is It Thinking or Cheating? Detecting Implicit Reward Hacking by
        Measuring Reasoning Effort
      original_md: '[Is It Thinking or Cheating? Detecting Implicit Reward Hacking
        by Measuring Reasoning Effort](https://arxiv.org/abs/2510.01367)'
      title: Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring
        Reasoning Effort
      authors:
      - Xinpeng Wang
      - Nitish Joshi
      - Barbara Plank
      - Rico Angell
      - He He
      author_organizations: []
      date: '2025-10-01'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.23575
      link_text: 'CoT Red-Handed: Stress Testing Chain-of-Thought Monitoring'
      original_md: '[CoT Red-Handed: Stress Testing Chain-of-Thought Monitoring](https://arxiv.org/abs/2505.23575)'
      title: 'CoT Red-Handed: Stress Testing Chain-of-Thought Monitoring'
      authors:
      - Benjamin Arnav
      - Pablo Bernabeu-Pérez
      - Nathan Helm-Burger
      - Tim Kostolansky
      - Hannes Whittingham
      - Mary Phuong
      author_organizations:
      - DeepMind
      date: '2025-05-29'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://alignment.anthropic.com/2025/subtle-reasoning/
      link_text: Training fails to elicit subtle reasoning in current language models
      original_md: '[Training fails to elicit subtle reasoning in current language
        models](https://alignment.anthropic.com/2025/subtle-reasoning/)'
      title: Training fails to elicit subtle reasoning in current language models
      authors: []
      author_organizations:
      - Anthropic
      date: '2025-01-01'
      published_year: 2025
      venue: Alignment Science Blog
      kind: blog_post
    - link_url: https://arxiv.org/abs/2510.19851
      link_text: Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought
        Monitorability
      original_md: '[Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought
        Monitorability](https://arxiv.org/abs/2510.19851)'
      title: Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought
        Monitorability
      authors:
      - Artur Zolkowski
      - Wen Xing
      - David Lindner
      - Florian Tramèr
      - Erik Jenner
      author_organizations:
      - ETH Zurich
      - FAR AI
      date: '2025-10-21'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.22777
      link_text: Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning
      original_md: '[Teaching Models to Verbalize Reward Hacking in Chain-of-Thought
        Reasoning](https://arxiv.org/abs/2506.22777)'
      title: Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning
      authors:
      - Miles Turpin
      - Andy Arditi
      - Marvin Li
      - Joe Benton
      - Julian Michael
      author_organizations: []
      date: '2025-06-28'
      published_year: 2025
      venue: ICML 2025 Workshop on Reliable and Responsible Foundation Models
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2501.08156
      link_text: Are DeepSeek R1 And Other Reasoning Models More Faithful?
      original_md: '[Are DeepSeek R1 And Other Reasoning Models More Faithful?](https://arxiv.org/abs/2501.08156)'
      title: Are DeepSeek R1 And Other Reasoning Models More Faithful?
      authors:
      - James Chua
      - Owain Evans
      author_organizations: []
      date: '2025-01-14'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.23966
      link_text: A Pragmatic Way to Measure Chain-of-Thought Monitorability
      original_md: '[A Pragmatic Way to Measure Chain-of-Thought Monitorability](https://arxiv.org/abs/2510.23966)'
      title: A Pragmatic Way to Measure Chain-of-Thought Monitorability
      authors:
      - Scott Emmons
      - Roland S. Zimmermann
      - David K. Elson
      - Rohin Shah
      author_organizations:
      - Google DeepMind
      date: '2025-10-28'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/Em9sihEZmbofZKc2t/a-concrete-roadmap-towards-safety-cases-based-on-chain-of
      link_text: A Concrete Roadmap towards Safety Cases based on Chain-of-Thought
        Monitoring
      original_md: '[A Concrete Roadmap towards Safety Cases based on Chain-of-Thought
        Monitoring](https://lesswrong.com/posts/Em9sihEZmbofZKc2t/a-concrete-roadmap-towards-safety-cases-based-on-chain-of)'
      title: A Concrete Roadmap towards Safety Cases based on Chain-of-Thought Monitoring
      authors:
      - Wuschel Schulz
      author_organizations: []
      date: '2025-10-23'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.11473
      link_text: 'Chain of Thought Monitorability: A New and Fragile Opportunity for
        AI Safety'
      original_md: '[Chain of Thought Monitorability: A New and Fragile Opportunity
        for AI Safety](https://arxiv.org/abs/2507.11473)'
      title: 'Chain of Thought Monitorability: A New and Fragile Opportunity for AI
        Safety'
      authors:
      - Tomek Korbak
      - Mikita Balesni
      - Elizabeth Barnes
      - Yoshua Bengio
      - Joe Benton
      - Joseph Bloom
      - Mark Chen
      - Alan Cooney
      - Allan Dafoe
      - Anca Dragan
      - Scott Emmons
      - Owain Evans
      - David Farhi
      - Ryan Greenblatt
      - Dan Hendrycks
      - Marius Hobbhahn
      - Evan Hubinger
      - Geoffrey Irving
      - Erik Jenner
      - Daniel Kokotajlo
      - Victoria Krakovna
      - Shane Legg
      - David Lindner
      - David Luan
      - Aleksander Mądry
      - Julian Michael
      - Neel Nanda
      - Dave Orr
      - Jakub Pachocki
      - Ethan Perez
      - Mary Phuong
      - Fabien Roger
      - Joshua Saxe
      - Buck Shlegeris
      - Martín Soto
      - Eric Steinberger
      - Jasmine Wang
      - Wojciech Zaremba
      - Bowen Baker
      - Rohin Shah
      - Vlad Mikulik
      author_organizations:
      - Anthropic
      - OpenAI
      - DeepMind
      - FAR AI
      - UC Berkeley
      - Mila
      - Center for AI Safety
      - Redwood Research
      - Apart Research
      date: '2025-07-15'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://metr.org/blog/2025-03-11-good-for-ai-to-reason-legibly-and-faithfully/
      link_text: Why it's good for AI reasoning to be legible and faithful
      original_md: '[Why it''s good for AI reasoning to be legible and faithful](https://metr.org/blog/2025-03-11-good-for-ai-to-reason-legibly-and-faithfully/)'
      title: Why it's good for AI reasoning to be legible and faithful
      authors: []
      author_organizations:
      - METR
      date: '2025-03-11'
      published_year: 2025
      venue: METR Blog
      kind: blog_post
    - link_url: https://www.lesswrong.com/posts/Tzdwetw55JNqFTkzK/why-don-t-we-just-shoggoth-face-paraphraser
      link_text: Why Don't We Just... Shoggoth+Face+Paraphraser?
      original_md: '[Why Don''t We Just... Shoggoth+Face+Paraphraser?](https://www.lesswrong.com/posts/Tzdwetw55JNqFTkzK/why-don-t-we-just-shoggoth-face-paraphraser)'
      title: Why don't we just shoggoth-face paraphraser
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: error_detected
    - link_url: https://metr.org/blog/2025-08-08-cot-may-be-highly-informative-despite-unfaithfulness/
      link_text: CoT May Be Highly Informative Despite "Unfaithfulness"
      original_md: '[CoT May Be Highly Informative Despite "Unfaithfulness"](https://metr.org/blog/2025-08-08-cot-may-be-highly-informative-despite-unfaithfulness/)'
      title: CoT May Be Highly Informative Despite "Unfaithfulness"
      authors:
      - Amy Deng
      - Sydney Von Arx
      - Ben Snodin
      - Sudarsh Kunnavakkam
      - Tamera Lanham
      author_organizations:
      - METR
      - Gray Swan
      date: '2025-08-08'
      published_year: 2025
      venue: METR Blog
      kind: blog_post
    - link_url: https://www.lesswrong.com/posts/B8Cmtf5gdHwxb8qtT/aether-july-2025-update
      link_text: Aether July 2025 Update
      original_md: '[Aether July 2025 Update](https://www.lesswrong.com/posts/B8Cmtf5gdHwxb8qtT/aether-july-2025-update)'
      title: Aether July 2025 Update
      authors:
      - Rohan Subramani
      - Rauno Arike
      - Shubhorup Biswas
      author_organizations:
      - Aether
      date: '2025-07-01'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    other_attributes: {}
  parsing_issues: []
- id: sec:Model_psychology
  name: Model psychology
  header_level: 2
  parent_id: sec:Black_box_safety_understand_and_control_current_model_behaviour_
  content: This section consists of a bottom-up set of things people happen to be
    doing, rather than a normative taxonomy.
  item_type: section
  agenda_attributes: null
  parsing_issues: []
- id: a:Model_values_model_preferences
  name: Model values / model preferences
  header_level: 3
  parent_id: sec:Model_psychology
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Analyse and control emergent, coherent value systems in
      LLMs, which change as models scale, and can contain problematic values like
      preferences for AIs over humans.
    theory_of_change: As AIs become more agentic, their behaviours and risks are increasingly
      determined by their goals and values. Since coherent value systems emerge with
      scale, we must leverage utility functions to analyse these values and apply
      "utility control" methods to constrain them, rather than just controlling outputs
      downstream of them.
    see_also:
    - '[Values in the Wild: Discovering and Analyzing Values in Real-World Language
      Model Interactions](https://arxiv.org/abs/2504.15236)'
    - '[Persona Vectors: Monitoring and Controlling Character Traits in Language Models](https://arxiv.org/abs/2507.21509)'
    orthodox_problems:
    - value_fragile
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - Mantas Mazeika
    - Xuwang Yin
    - Rishub Tamirisa
    - Jaehyuk Lim
    - Bruce W. Lee
    - Richard Ren
    - Long Phan
    - Norman Mu
    - Adam Khoja
    - Oliver Zhang
    - Dan Hendrycks
    estimated_ftes: '30'
    critiques: '[Randomness, Not Representation: The Unreliability of Evaluating Cultural
      Alignment in LLMs](https://dl.acm.org/doi/full/10.1145/3715275.3732147)'
    funded_by: Coefficient Giving. $289,000 SFF funding for CAIS.
    outputs:
    - link_url: https://arxiv.org/abs/2406.07882v1
      link_text: What Kind of User Are You? Uncovering User Models in LLM Chatbots
      original_md: '[What Kind of User Are You? Uncovering User Models in LLM Chatbots](https://arxiv.org/abs/2406.07882v1)'
      title: Designing a Dashboard for Transparency and Control of Conversational
        AI
      authors:
      - Yida Chen
      - Aoyu Wu
      - Trevor DePodesta
      - Catherine Yeh
      - Kenneth Li
      - Nicholas Castillo Marin
      - Oam Patel
      - Jan Riecke
      - Shivam Raval
      - Olivia Seow
      - Martin Wattenberg
      - Fernanda Viégas
      author_organizations:
      - Google
      date: '2024-06-12'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.08640
      link_text: 'Utility Engineering: Analyzing and Controlling Emergent Value Systems
        in AIs'
      original_md: '[Utility Engineering: Analyzing and Controlling Emergent Value
        Systems in AIs](https://arxiv.org/abs/2502.08640)'
      title: 'Utility Engineering: Analyzing and Controlling Emergent Value Systems
        in AIs'
      authors:
      - Mantas Mazeika
      - Xuwang Yin
      - Rishub Tamirisa
      - Jaehyuk Lim
      - Bruce W. Lee
      - Richard Ren
      - Long Phan
      - Norman Mu
      - Adam Khoja
      - Oliver Zhang
      - Dan Hendrycks
      author_organizations:
      - Unknown
      date: '2025-02-12'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.14633
      link_text: Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values
        Prioritization with AIRiskDilemmas
      original_md: '[Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values
        Prioritization with AIRiskDilemmas](https://arxiv.org/abs/2505.14633)'
      title: Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization
        with AIRiskDilemmas
      authors:
      - Yu Ying Chiu
      - Zhilin Wang
      - Sharan Maiya
      - Yejin Choi
      - Kyle Fish
      - Sydney Levine
      - Evan Hubinger
      author_organizations:
      - Anthropic
      date: '2025-05-20'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2508.09762
      link_text: The PacifAIst Benchmark:Would an Artificial Intelligence Choose to
        Sacrifice Itself for Human Safety?
      original_md: '[The PacifAIst Benchmark:Would an Artificial Intelligence Choose
        to Sacrifice Itself for Human Safety?](https://arxiv.org/abs/2508.09762)'
      title: The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice
        Itself for Human Safety?
      authors:
      - Manuel Herrador
      author_organizations: []
      date: '2025-08-13'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.15236
      link_text: 'Values in the Wild: Discovering and Analyzing Values in Real-World
        Language Model Interactions'
      original_md: '[Values in the Wild: Discovering and Analyzing Values in Real-World
        Language Model Interactions](https://arxiv.org/abs/2504.15236)'
      title: 'Values in the Wild: Discovering and Analyzing Values in Real-World Language
        Model Interactions'
      authors:
      - Saffron Huang
      - Esin Durmus
      - Miles McCain
      - Kunal Handa
      - Alex Tamkin
      - Jerry Hong
      - Michael Stern
      - Arushi Somani
      - Xiuruo Zhang
      - Deep Ganguli
      author_organizations:
      - Anthropic
      date: '2025-04-21'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.01938
      link_text: 'EigenBench: A Comparative behavioural Measure of Value Alignment'
      original_md: '[EigenBench: A Comparative behavioural Measure of Value Alignment](https://arxiv.org/abs/2509.01938)'
      title: 'EigenBench: A Comparative Behavioral Measure of Value Alignment'
      authors:
      - Jonathn Chang
      - Leonhard Piff
      - Suvadip Sana
      - Jasmine X. Li
      - Lionel Levine
      author_organizations: []
      date: '2025-09-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.04994
      link_text: 'Following the Whispers of Values: Unraveling Neural Mechanisms Behind
        Value-Oriented Behaviors in LLMs'
      original_md: '[Following the Whispers of Values: Unraveling Neural Mechanisms
        Behind Value-Oriented Behaviors in LLMs](https://arxiv.org/abs/2504.04994)'
      title: 'Following the Whispers of Values: Unraveling Neural Mechanisms Behind
        Value-Oriented Behaviors in LLMs'
      authors:
      - Ling Hu
      - Yuemei Xu
      - Xiaoyang Gu
      - Letao Han
      author_organizations: []
      date: '2025-04-07'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/jrkrHyrymv95CX5NC/alignment-can-reduce-performance-on-simple-ethical-questions
      link_text: Alignment Can Reduce Performance on Simple Ethical Questions
      original_md: '[Alignment Can Reduce Performance on Simple Ethical Questions](https://lesswrong.com/posts/jrkrHyrymv95CX5NC/alignment-can-reduce-performance-on-simple-ethical-questions)'
      title: Alignment Can Reduce Performance on Simple Ethical Questions
      authors:
      - Daan Henselmans
      author_organizations: []
      date: '2025-02-03'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2410.01639
      link_text: Moral Alignment for LLM Agents
      original_md: '[Moral Alignment for LLM Agents](https://arxiv.org/abs/2410.01639)'
      title: Moral Alignment for LLM Agents
      authors:
      - Elizaveta Tennant
      - Stephen Hailes
      - Mirco Musolesi
      author_organizations: []
      date: '2024-10-02'
      published_year: 2025
      venue: ICLR 2025
      kind: paper_published
    - link_url: https://www.lesswrong.com/posts/6JdSJ63LZ4TuT5cTH/the-llm-has-left-the-chat-evidence-of-bail-preferences-in
      link_text: 'The LLM Has Left The Chat: Evidence of Bail Preferences in Large
        Language Models'
      original_md: '[The LLM Has Left The Chat: Evidence of Bail Preferences in Large
        Language Models](https://www.lesswrong.com/posts/6JdSJ63LZ4TuT5cTH/the-llm-has-left-the-chat-evidence-of-bail-preferences-in)'
      title: 'The LLM Has Left The Chat: Evidence of Bail Preferences in Large Language
        Models'
      authors:
      - Danielle Ensign
      author_organizations:
      - Anthropic
      date: '2024-09-08'
      published_year: 2024
      venue: arXiv
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2505.21479
      link_text: Are Language Models Consequentialist or Deontological Moral Reasoners?
      original_md: '[Are Language Models Consequentialist or Deontological Moral Reasoners?](https://arxiv.org/abs/2505.21479)'
      title: Are Language Models Consequentialist or Deontological Moral Reasoners?
      authors:
      - Keenan Samway
      - Max Kleiman-Weiner
      - David Guzman Piedrahita
      - Rada Mihalcea
      - Bernhard Schölkopf
      - Zhijing Jin
      author_organizations: []
      date: '2025-05-27'
      published_year: 2025
      venue: EMNLP 2025
      kind: paper_preprint
    - link_url: https://nature.com/articles/s41562-025-02172-y
      link_text: Playing repeated games with large language models
      original_md: '[Playing repeated games with large language models](https://nature.com/articles/s41562-025-02172-y)'
      title: Playing repeated games with large language models
      authors:
      - Elif Akata
      - Lion Schulz
      - Julian Coda-Forno
      - Seong Joon Oh
      - Matthias Bethge
      - Eric Schulz
      author_organizations:
      - Max Planck Institute for Biological Cybernetics
      - Institute for Human-Centered AI (Helmholtz Munich)
      - University of Tübingen
      date: '2025-05-08'
      published_year: 2025
      venue: Nature Human Behaviour
      kind: paper_published
    - link_url: https://arxiv.org/abs/2504.06324
      link_text: 'From Stability to Inconsistency: A Study of Moral Preferences in
        LLMs'
      original_md: '[From Stability to Inconsistency: A Study of Moral Preferences
        in LLMs](https://arxiv.org/abs/2504.06324)'
      title: 'From Stability to Inconsistency: A Study of Moral Preferences in LLMs'
      authors:
      - Monika Jotautaite
      - Mary Phuong
      - Chatrik Singh Mangat
      - Maria Angelica Martinez
      author_organizations: []
      date: '2025-04-08'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.05465
      link_text: 'VAL-Bench: Measuring Value Alignment in Language Models'
      original_md: '[VAL-Bench: Measuring Value Alignment in Language Models](https://arxiv.org/abs/2510.05465)'
      title: 'VAL-Bench: Measuring Value Alignment in Language Models'
      authors:
      - Aman Gupta
      - Denny O'Shea
      - Fazl Barez
      author_organizations: []
      date: '2025-10-06'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:Character_training_and_persona_steering
  name: Character training and persona steering
  header_level: 3
  parent_id: sec:Model_psychology
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Map, shape, and control the personae of language models,
      such that new models embody desirable values (e.g., honesty, empathy) rather
      than undesirable ones (e.g., sycophancy, self-perpetuating behaviors).
    theory_of_change: If post-training, prompting, and activation-engineering interact
      with some kind of structured 'persona space', then better understanding it should
      benefit the design, control, and detection of LLM personas.
    see_also:
    - '[Simulators](#a:simulators)'
    - a:Activation_engineering
    - a:Emergent_misalignment
    - a:Hyperstition_studies
    - a:Anthropic
    - '[Cyborgism](https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism)'
    - shard theory
    - '[AI psychiatry](https://nitter.net/Jack_W_Lindsey/status/1948138767753326654#m)'
    - '[Ward et al](https://arxiv.org/abs/2410.04272)'
    orthodox_problems:
    - value_fragile
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitive
    some_names:
    - Truthful AI
    - OpenAI
    - Anthropic
    - CLR
    - Amanda Askell
    - Jack Lindsey
    - Janus
    - Theia Vogel
    - Sharan Maiya
    - Evan Hubinger
    estimated_ftes: null
    critiques: '[Nostalgebraist](https://nostalgebraist.tumblr.com/post/785766737747574784/the-void)'
    funded_by: Anthropic, Coefficient Giving
    outputs:
    - link_url: https://arxiv.org/pdf/2511.01689%20
      link_text: 'Open Character Training: Shaping the Persona of AI Assistants through
        Constitutional AI'
      original_md: '[Open Character Training: Shaping the Persona of AI Assistants
        through Constitutional AI](https://arxiv.org/pdf/2511.01689%20)'
      title: 'Open Character Training: Shaping the Persona of AI Assistants through
        Constitutional AI'
      authors:
      - Sharan Maiya
      - Henning Bartsch
      - Nathan Lambert
      - Evan Hubinger
      author_organizations:
      - Anthropic
      date: '2025-11-03'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.lesswrong.com/posts/29aWbJARGF4ybAa5d/on-the-functional-self-of-llms
      link_text: On the functional self of LLMs
      original_md: '[On the functional self of LLMs](https://www.lesswrong.com/posts/29aWbJARGF4ybAa5d/on-the-functional-self-of-llms)'
      title: On the functional self of LLMs
      authors:
      - eggsyntax
      author_organizations: []
      date: '2025-07-07'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://www.lesswrong.com/posts/vpNG99GhbBoLov9og/claude-4-5-opus-soul-document%20
      link_text: Opus 4.5's Soul Document
      original_md: '[Opus 4.5''s Soul Document](https://www.lesswrong.com/posts/vpNG99GhbBoLov9og/claude-4-5-opus-soul-document%20)'
      title: Claude 4.5 Opus' Soul Document
      authors:
      - Richard Weiss
      author_organizations: []
      date: '2025-11-28'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2506.19823
      link_text: Persona Features Control Emergent Misalignment
      original_md: '[Persona Features Control Emergent Misalignment](https://arxiv.org/abs/2506.19823)'
      title: Persona Features Control Emergent Misalignment
      authors:
      - Miles Wang
      - Tom Dupré la Tour
      - Olivia Watkins
      - Alex Makelov
      - Ryan A. Chi
      - Samuel Miserendino
      - Jeffrey Wang
      - Achyuta Rajaram
      - Johannes Heidecke
      - Tejal Patwardhan
      - Dan Mossing
      author_organizations: []
      date: '2025-06-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.04340
      link_text: 'Inoculation Prompting: Eliciting traits from LLMs during training
        can suppress them at test-time'
      original_md: '[Inoculation Prompting: Eliciting traits from LLMs during training
        can suppress them at test-time](https://arxiv.org/abs/2510.04340)'
      title: 'Inoculation Prompting: Eliciting traits from LLMs during training can
        suppress them at test-time'
      authors:
      - Daniel Tan
      - Anders Woodruff
      - Niels Warncke
      - Arun Jose
      - Maxime Riché
      - David Demitri Africa
      - Mia Taylor
      author_organizations: []
      date: '2025-10-05'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.21509
      link_text: 'Persona Vectors: Monitoring and Controlling Character Traits in
        Language Models'
      original_md: '[Persona Vectors: Monitoring and Controlling Character Traits
        in Language Models](https://arxiv.org/abs/2507.21509)'
      title: 'Persona Vectors: Monitoring and Controlling Character Traits in Language
        Models'
      authors:
      - Runjin Chen
      - Andy Arditi
      - Henry Sleight
      - Owain Evans
      - Jack Lindsey
      author_organizations: []
      date: '2025-07-29'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/jtqcsARGtmgogdcLT/reducing-llm-deception-at-scale-with-self-other-overlap-fine
      link_text: Reducing LLM deception at scale with self-other overlap fine-tuning
      original_md: '[Reducing LLM deception at scale with self-other overlap fine-tuning](https://lesswrong.com/posts/jtqcsARGtmgogdcLT/reducing-llm-deception-at-scale-with-self-other-overlap-fine)'
      title: Reducing LLM deception at scale with self-other overlap fine-tuning
      authors:
      - Marc Carauleanu
      - Diogo de Lucena
      - Gunnar_Zarncke
      - Judd Rosenblatt
      - Cameron Berg
      - Mike Vaiana
      - Trent Hodgeson
      author_organizations:
      - AE Studio
      - Foresight Institute
      date: '2025-03-13'
      published_year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - link_url: https://www.lesswrong.com/posts/6ZnznCaTcbGYsCmqu/the-rise-of-parasitic-ai?commentId=RrWjMnKwXGTtmw9rQ
      link_text: The Rise of Parasitic AI
      original_md: '[The Rise of Parasitic AI](https://www.lesswrong.com/posts/6ZnznCaTcbGYsCmqu/the-rise-of-parasitic-ai?commentId=RrWjMnKwXGTtmw9rQ)'
      title: The Rise of Parasitic AI
      authors:
      - Adele Lopez
      author_organizations: []
      date: '2025-09-11'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://www.alignmentforum.org/posts/zuXo9imNKYspu9HGv/a-three-layer-model-of-llm-psychology
      link_text: A Three-Layer Model of LLM Psychology
      original_md: '[A Three-Layer Model of LLM Psychology](https://www.alignmentforum.org/posts/zuXo9imNKYspu9HGv/a-three-layer-model-of-llm-psychology)'
      title: A Three-Layer Model of LLM Psychology
      authors:
      - Jan_Kulveit
      author_organizations: []
      date: '2024-12-26'
      published_year: 2024
      venue: AI Alignment Forum
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2502.07077
      link_text: Multi-turn Evaluation of Anthropomorphic Behaviours in Large Language
        Models
      original_md: '[Multi-turn Evaluation of Anthropomorphic Behaviours in Large
        Language Models](https://arxiv.org/abs/2502.07077)'
      title: Multi-turn Evaluation of Anthropomorphic Behaviours in Large Language
        Models
      authors:
      - Lujain Ibrahim
      - Canfer Akbulut
      - Rasmi Elasmar
      - Charvi Rastogi
      - Minsuk Kahng
      - Meredith Ringel Morris
      - Kevin R. McKee
      - Verena Rieser
      - Murray Shanahan
      - Laura Weidinger
      author_organizations:
      - Google DeepMind
      - Heriot-Watt University
      date: '2025-02-10'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.lesswrong.com/posts/LdBhgAhpvbdEep79F/selection-pressures-on-lm-personas
      link_text: Selection Pressures on LM Personas
      original_md: '[Selection Pressures on LM Personas](https://www.lesswrong.com/posts/LdBhgAhpvbdEep79F/selection-pressures-on-lm-personas)'
      title: Selection Pressures on LM Personas
      authors:
      - Raymond Douglas
      author_organizations: []
      date: '2025-03-28'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://nostalgebraist.tumblr.com/post/785766737747574784/the-void
      link_text: the void
      original_md: '[the void](https://nostalgebraist.tumblr.com/post/785766737747574784/the-void)'
      title: the void
      authors:
      - nostalgebraist
      author_organizations: []
      date: '2025-06-07'
      published_year: 2025
      venue: Tumblr
      kind: blog_post
    - link_url: https://nostalgebraist.tumblr.com/post/786568570671923200/void-miscellany
      link_text: void miscellany
      original_md: '[void miscellany](https://nostalgebraist.tumblr.com/post/786568570671923200/void-miscellany)'
      title: void miscellany
      authors:
      - nostalgebraist
      author_organizations: []
      date: '2025-06-16'
      published_year: 2025
      venue: Tumblr
      kind: blog_post
    other_attributes: {}
  parsing_issues:
  - Broad approach field says 'cognitive' - interpreted as 'cognitivist science'
- id: a:Emergent_misalignment
  name: Emergent misalignment
  header_level: 3
  parent_id: sec:Model_psychology
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Fine-tuning LLMs on one narrow antisocial task can cause
      general misalignment including deception, shutdown resistance, harmful advice,
      and extremist sympathies, when those behaviors are never trained or rewarded
      directly. [A new agenda](https://www.lesswrong.com/posts/AcTEiu5wYDgrbmXow/open-problems-in-emergent-misalignment)
      which quickly led to a stream of exciting work.
    theory_of_change: Predict, detect, and prevent models from developing broadly
      harmful behaviors (like deception or shutdown resistance) when fine-tuned on
      seemingly unrelated tasks. Find, preserve, and robustify this correlated representation
      of the good.
    see_also:
    - auditing real models
    - a:Pragmatic_interpretability
    orthodox_problems:
    - goals_misgeneralize
    - superintelligence_fool_supervisors
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: behaviorist_science
    broad_approach_text: behaviorist science
    some_names:
    - Truthful AI
    - Jan Betley
    - James Chua
    - Mia Taylor
    - Miles Wang
    - Edward Turner
    - Anna Soligo
    - Alex Cloud
    - Nathan Hu
    - Owain Evans
    estimated_ftes: 10-50
    critiques: '[Emergent Misalignment as Prompt Sensitivity](https://arxiv.org/html/2507.06253v1),
      [Go home GPT-4o, you''re drunk](https://www.lesswrong.com/posts/RoWabfQxabWBiXwxP/go-home-gpt-4o-you-re-drunk-emergent-misalignment-as-lowered)'
    funded_by: Coefficient Giving, >$1 million
    outputs:
    - link_url: https://arxiv.org/abs/2502.17424
      link_text: 'Emergent Misalignment: Narrow finetuning can produce broadly misaligned
        LLMs'
      original_md: '[Emergent Misalignment: Narrow finetuning can produce broadly
        misaligned LLMs](https://arxiv.org/abs/2502.17424)'
      title: 'Emergent Misalignment: Narrow finetuning can produce broadly misaligned
        LLMs'
      authors:
      - Jan Betley
      - Daniel Tan
      - Niels Warncke
      - Anna Sztyber-Betley
      - Xuchan Bao
      - Martín Soto
      - Nathan Labenz
      - Owain Evans
      author_organizations: []
      date: '2025-02-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.13206
      link_text: 'Thought Crime: Backdoors and Emergent Misalignment in Reasoning
        Models'
      original_md: '[Thought Crime: Backdoors and Emergent Misalignment in Reasoning
        Models](https://arxiv.org/abs/2506.13206)'
      title: 'Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models'
      authors:
      - James Chua
      - Jan Betley
      - Mia Taylor
      - Owain Evans
      author_organizations: []
      date: '2025-06-16'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.19823
      link_text: Persona Features Control Emergent Misalignment
      original_md: '[Persona Features Control Emergent Misalignment](https://arxiv.org/abs/2506.19823)'
      title: Persona Features Control Emergent Misalignment
      authors:
      - Miles Wang
      - Tom Dupré la Tour
      - Olivia Watkins
      - Alex Makelov
      - Ryan A. Chi
      - Samuel Miserendino
      - Jeffrey Wang
      - Achyuta Rajaram
      - Johannes Heidecke
      - Tejal Patwardhan
      - Dan Mossing
      author_organizations: []
      date: '2025-06-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.11613
      link_text: Model Organisms for Emergent Misalignment
      original_md: '[Model Organisms for Emergent Misalignment](https://arxiv.org/abs/2506.11613)'
      title: Model Organisms for Emergent Misalignment
      authors:
      - Edward Turner
      - Anna Soligo
      - Mia Taylor
      - Senthooran Rajamanoharan
      - Neel Nanda
      author_organizations:
      - Google DeepMind
      date: '2025-06-13'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2508.17511
      link_text: 'School of Reward Hacks: Hacking harmless tasks generalizes to misaligned
        behavior in LLMs'
      original_md: '[School of Reward Hacks: Hacking harmless tasks generalizes to
        misaligned behavior in LLMs](https://arxiv.org/abs/2508.17511)'
      title: 'School of Reward Hacks: Hacking harmless tasks generalizes to misaligned
        behavior in LLMs'
      authors:
      - Mia Taylor
      - James Chua
      - Jan Betley
      - Johannes Treutlein
      - Owain Evans
      author_organizations: []
      date: '2025-08-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://alignment.anthropic.com/2025/subliminal-learning/
      link_text: 'Subliminal Learning: Language Models Transmit behavioural Traits
        via Hidden Signals in Data'
      original_md: '[Subliminal Learning: Language Models Transmit behavioural Traits
        via Hidden Signals in Data](https://alignment.anthropic.com/2025/subliminal-learning/)'
      title: 'Subliminal Learning: Language Models Transmit Behavioral Traits via
        Hidden Signals in Data'
      authors:
      - Alex Cloud
      - Minh Le
      - James Chua
      - Jan Betley
      - Anna Sztyber-Betley
      - Jacob Hilton
      - Samuel Marks
      - Owain Evans
      author_organizations:
      - Anthropic Fellows Program
      - Truthful AI
      - Warsaw University of Technology
      - Alignment Research Center
      - Anthropic
      - UC Berkeley
      date: '2025-07-22'
      published_year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - link_url: https://lesswrong.com/posts/umYzsh7SGHHKsRCaA/convergent-linear-representations-of-emergent-misalignment
      link_text: Convergent Linear Representations of Emergent Misalignment
      original_md: '[Convergent Linear Representations of Emergent Misalignment](https://lesswrong.com/posts/umYzsh7SGHHKsRCaA/convergent-linear-representations-of-emergent-misalignment)'
      title: Convergent Linear Representations of Emergent Misalignment
      authors:
      - Anna Soligo
      - Edward Turner
      - Senthooran Rajamanoharan
      - Neel Nanda
      author_organizations:
      - ML Alignment & Theory Scholars
      date: '2025-06-16'
      published_year: 2025
      venue: LessWrong/AI Alignment Forum
      kind: lesswrong
    - link_url: https://www.lesswrong.com/posts/gLDSqQm8pwNiq7qst/narrow-misalignment-is-hard-emergent-misalignment-is-easy
      link_text: Narrow Misalignment is Hard, Emergent Misalignment is Easy
      original_md: '[Narrow Misalignment is Hard, Emergent Misalignment is Easy](https://www.lesswrong.com/posts/gLDSqQm8pwNiq7qst/narrow-misalignment-is-hard-emergent-misalignment-is-easy)'
      title: Narrow Misalignment is Hard, Emergent Misalignment is Easy
      authors:
      - Edward Turner
      - Anna Soligo
      - Senthooran Rajamanoharan
      - Neel Nanda
      author_organizations:
      - Google DeepMind
      date: '2024-07-14'
      published_year: 2024
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/gT3wtWBAs7PKonbmy/aesthetic-preferences-can-cause-emergent-misalignment
      link_text: Aesthetic Preferences Can Cause Emergent Misalignment
      original_md: '[Aesthetic Preferences Can Cause Emergent Misalignment](https://lesswrong.com/posts/gT3wtWBAs7PKonbmy/aesthetic-preferences-can-cause-emergent-misalignment)'
      title: Aesthetic Preferences Can Cause Emergent Misalignment
      authors:
      - Anders Woodruff
      author_organizations:
      - Center on Long-Term Risk
      date: '2025-08-26'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2510.06105
      link_text: 'Moloch''s Bargain: Emergent Misalignment When LLMs Compete for Audiences'
      original_md: '[Moloch''s Bargain: Emergent Misalignment When LLMs Compete for
        Audiences](https://arxiv.org/abs/2510.06105)'
      title: 'Moloch''s Bargain: Emergent Misalignment When LLMs Compete for Audiences'
      authors:
      - Batu El
      - James Zou
      author_organizations:
      - Stanford University
      date: '2025-10-07'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/ZdY4JzBPJEgaoCxTR/emergent-misalignment-and-realignment
      link_text: Emergent Misalignment & Realignment
      original_md: '[Emergent Misalignment & Realignment](https://lesswrong.com/posts/ZdY4JzBPJEgaoCxTR/emergent-misalignment-and-realignment)'
      title: Emergent Misalignment & Realignment
      authors:
      - Elizaveta Tennant
      - Jasper Timm
      - Kevin Wei
      - David Quarel
      author_organizations:
      - ARENA (Alignment Research Engineering Accelerator)
      date: '2025-06-27'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://www.lesswrong.com/posts/HLJoJYi52mxgomujc/realistic-reward-hacking-induces-different-and-deeper-1
      link_text: Realistic Reward Hacking Induces Different and Deeper Misalignment
      original_md: '[Realistic Reward Hacking Induces Different and Deeper Misalignment](https://www.lesswrong.com/posts/HLJoJYi52mxgomujc/realistic-reward-hacking-induces-different-and-deeper-1)'
      title: Realistic Reward Hacking Induces Different and Deeper Misalignment
      authors:
      - Jozdien
      author_organizations: []
      date: '2025-10-09'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/ZXxY2tccLapdjLbKm/selective-generalization-improving-capabilities-while
      link_text: 'Selective Generalization: Improving Capabilities While Maintaining
        Alignment'
      original_md: '[Selective Generalization: Improving Capabilities While Maintaining
        Alignment](https://lesswrong.com/posts/ZXxY2tccLapdjLbKm/selective-generalization-improving-capabilities-while)'
      title: 'Selective Generalization: Improving Capabilities While Maintaining Alignment'
      authors:
      - Ariana Azarbal
      - Matthew A. Clarke
      - Jorio Cocola
      - Cailley Factor
      - Alex Cloud
      author_organizations:
      - SPAR
      date: '2025-07-16'
      published_year: 2025
      venue: LessWrong/AI Alignment Forum
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/qHudHZNLCiFrygRiy/emergent-misalignment-on-a-budget
      link_text: Emergent Misalignment on a Budget
      original_md: '[Emergent Misalignment on a Budget](https://lesswrong.com/posts/qHudHZNLCiFrygRiy/emergent-misalignment-on-a-budget)'
      title: Emergent Misalignment on a Budget
      authors:
      - Valerio Pepe
      - Armaan Tipirneni
      author_organizations:
      - Harvard College
      date: '2025-06-08'
      published_year: 2025
      venue: LessWrong/AI Alignment Forum
      kind: lesswrong
    - link_url: https://www.lesswrong.com/posts/6ZnznCaTcbGYsCmqu/the-rise-of-parasitic-ai
      link_text: The Rise of Parasitic AI
      original_md: '[The Rise of Parasitic AI](https://www.lesswrong.com/posts/6ZnznCaTcbGYsCmqu/the-rise-of-parasitic-ai)'
      title: The Rise of Parasitic AI
      authors:
      - Adele Lopez
      author_organizations: []
      date: '2025-09-11'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover
      link_text: LLM AGI may reason about its goals and discover misalignments by
        default
      original_md: '[LLM AGI may reason about its goals and discover misalignments
        by default](https://lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover)'
      title: LLM AGI may reason about its goals and discover misalignments by default
      authors:
      - Seth Herd
      author_organizations: []
      date: '2025-09-15'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/AcTEiu5wYDgrbmXow/open-problems-in-emergent-misalignment
      link_text: Open problems in emergent misalignment
      original_md: '[Open problems in emergent misalignment](https://lesswrong.com/posts/AcTEiu5wYDgrbmXow/open-problems-in-emergent-misalignment)'
      title: Open problems in emergent misalignment
      authors:
      - Jan Betley
      - Daniel Tan
      author_organizations: []
      date: '2025-03-01'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    other_attributes: {}
  parsing_issues:
  - See also mentions 'auditing real models' which doesn't match any known agenda
    ID - kept as plain text
- id: a:Model_specs_and_constitutions
  name: Model specs and constitutions
  header_level: 3
  parent_id: sec:Model_psychology
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Write detailed, natural language descriptions of values
      and rules for models to follow, then instill these values and rules into models
      via techniques like Constitutional AI or deliberative alignment.
    theory_of_change: Model specs and constitutions serve three purposes. First, they
      provide a clear standard of behavior which can be used to *train* models to
      value what we want them to value. Second, they serve as something closer to
      a ground truth standard for evaluating the degree of misalignment ranging from  "models
      straightforwardly obey the spec" to "models flagrantly disobey the spec". A
      combination of scalable stress-testing and reinforcement for obedience can be
      used to iteratively reduce the risk of misalignment. Third, they get more useful
      as models' instruction-following capability improves.
    see_also:
    - sec:Iterative_alignment
    - sec:Model_psychology
    orthodox_problems:
    - value_fragile
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: engineering
    broad_approach_text: engineering
    some_names:
    - Amanda Askell
    - Joe Carlsmith
    estimated_ftes: null
    critiques: '[LLM AGI may reason about its goals and discover misalignments by
      default](https://www.alignmentforum.org/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover),
      [On OpenAI''s Model Spec 2.0](https://thezvi.wordpress.com/2025/02/21/on-openais-model-spec-2-0/),
      [Giving AIs safe motivations (esp. Sections 4.3-4.5)](https://joecarlsmith.com/2025/08/18/giving-ais-safe-motivations#4-5-step-4-good-instructions),
      [On Deliberative Alignment](https://thezvi.substack.com/p/on-deliberative-alignment)'
    funded_by: major funders include Anthropic and OpenAI (internally)
    outputs:
    - link_url: https://www.anthropic.com/news/claudes-constitution
      link_text: Claude's Constitution
      original_md: '[Claude''s Constitution](https://www.anthropic.com/news/claudes-constitution)'
      title: Claude's Constitution
      authors: []
      author_organizations:
      - Anthropic
      date: '2023-05-09'
      published_year: 2023
      venue: Anthropic News
      kind: blog_post
    - link_url: https://github.com/elder-plinius/CL4R1T4S/blob/main/GOOGLE/Gemini-2.5-Pro-04-18-2025.md
      link_text: Google doesn't have anything public. The Gemini system prompt is
        very short and dry and doesn't even have any rules for handling copyrighted,
        let alone wetter stuff
      original_md: Google doesn't have anything public. The [Gemini system prompt](https://github.com/elder-plinius/CL4R1T4S/blob/main/GOOGLE/Gemini-2.5-Pro-04-18-2025.md)
        is very short and dry and doesn't even have any rules for handling copyrighted,
        let alone wetter stuff
      title: Gemini-2.5-Pro-04-18-2025 System Prompt
      authors: []
      author_organizations:
      - Google
      date: '2025-04-18'
      published_year: 2025
      venue: GitHub
      kind: other
    - link_url: https://arxiv.org/abs/2412.16339
      link_text: 'Deliberative Alignment: Reasoning Enables Safer Language Models'
      original_md: '[Deliberative Alignment: Reasoning Enables Safer Language Models](https://arxiv.org/abs/2412.16339)'
      title: 'Deliberative Alignment: Reasoning Enables Safer Language Models'
      authors:
      - Melody Y. Guan
      - Manas Joglekar
      - Eric Wallace
      - Saachi Jain
      - Boaz Barak
      - Alec Helyar
      - Rachel Dias
      - Andrea Vallone
      - Hongyu Ren
      - Jason Wei
      - Hyung Won Chung
      - Sam Toyer
      - Johannes Heidecke
      - Alex Beutel
      - Amelia Glaese
      author_organizations:
      - OpenAI
      date: '2024-12-20'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.07686
      link_text: Stress-Testing Model Specs Reveals Character Differences among Language
        Models
      original_md: '[Stress-Testing Model Specs Reveals Character Differences among
        Language Models](https://arxiv.org/abs/2510.07686)'
      title: Stress-Testing Model Specs Reveals Character Differences among Language
        Models
      authors:
      - Jifan Zhang
      - Henry Sleight
      - Andi Peng
      - John Schulman
      - Esin Durmus
      author_organizations:
      - OpenAI
      - Anthropic
      date: '2025-10-09'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://model-spec.openai.com/
      link_text: OpenAI Model Spec
      original_md: '[OpenAI Model Spec](https://model-spec.openai.com/)'
      title: OpenAI Model Spec
      authors: []
      author_organizations:
      - OpenAI
      date: '2025-09-12'
      published_year: 2025
      venue: OpenAI Website
      kind: agenda_manifesto
    - link_url: https://arxiv.org/abs/2506.00195
      link_text: Let Them Down Easy! Contextual Effects of LLM Guardrails on User
        Perceptions and Preferences
      original_md: '[Let Them Down Easy\! Contextual Effects of LLM Guardrails on
        User Perceptions and Preferences](https://arxiv.org/abs/2506.00195)'
      title: Let Them Down Easy! Contextual Effects of LLM Guardrails on User Perceptions
        and Preferences
      authors:
      - Mingqian Zheng
      - Wenjia Hu
      - Patrick Zhao
      - Motahhare Eslami
      - Jena D. Hwang
      - Faeze Brahman
      - Carolyn Rose
      - Maarten Sap
      author_organizations:
      - Carnegie Mellon University
      - University of Southern California
      date: '2025-05-30'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/LSJx5EnQEW6s5Juw6/no-self-as-an-alignment-target
      link_text: No-self as an alignment target
      original_md: '[No-self as an alignment target](https://lesswrong.com/posts/LSJx5EnQEW6s5Juw6/no-self-as-an-alignment-target)'
      title: No-self as an alignment target
      authors:
      - Milan W
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/3jnziqCF3vA2NXAKp/six-thoughts-on-ai-safety
      link_text: Six Thoughts on AI Safety
      original_md: '[Six Thoughts on AI Safety](https://lesswrong.com/posts/3jnziqCF3vA2NXAKp/six-thoughts-on-ai-safety)'
      title: Six Thoughts on AI Safety
      authors:
      - Boaz Barak
      author_organizations:
      - Harvard University
      date: '2025-01-24'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://newsletter.forethought.org/p/how-important-is-the-model-spec-if
      link_text: How important is the model spec if alignment fails?
      original_md: '[How important is the model spec if alignment fails?](https://newsletter.forethought.org/p/how-important-is-the-model-spec-if)'
      title: How important is the model spec if alignment fails?
      authors:
      - Mia Taylor
      author_organizations:
      - Forethought
      date: '2025-12-03'
      published_year: 2025
      venue: ForeWord (Substack)
      kind: blog_post
    - link_url: https://arxiv.org/abs/2503.05728
      link_text: Political Neutrality in AI Is Impossible- But Here Is How to Approximate
        It
      original_md: '[Political Neutrality in AI Is Impossible- But Here Is How to
        Approximate It](https://arxiv.org/abs/2503.05728)'
      title: Political Neutrality in AI Is Impossible- But Here Is How to Approximate
        It
      authors:
      - Jillian Fisher
      - Ruth E. Appel
      - Chan Young Park
      - Yujin Potter
      - Liwei Jiang
      - Taylor Sorensen
      - Shangbin Feng
      - Yulia Tsvetkov
      - Margaret E. Roberts
      - Jennifer Pan
      - Dawn Song
      - Yejin Choi
      author_organizations:
      - Anthropic
      - UC San Diego
      - Stanford University
      - UC Berkeley
      - University of Washington
      date: '2025-02-18'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://joecarlsmith.com/2025/08/18/giving-ais-safe-motivations#4-5-step-4-good-instructions
      link_text: Giving AIs safe motivations
      original_md: '[Giving AIs safe motivations](https://joecarlsmith.com/2025/08/18/giving-ais-safe-motivations#4-5-step-4-good-instructions)'
      title: Giving AIs safe motivations
      authors:
      - Joe Carlsmith
      author_organizations:
      - Anthropic
      date: '2025-08-18'
      published_year: 2025
      venue: joecarlsmith.com
      kind: blog_post
    other_attributes: {}
  parsing_issues: []
- id: a:Model_psychopathology
  name: Model psychopathology
  header_level: 3
  parent_id: sec:Model_psychology
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Find interesting LLM phenomena like glitch [tokens](https://vgel.me/posts/seahorse/)
      and the reversal curse; these are vital data for theory.
    theory_of_change: 'The study of ''pathological'' phenomena in LLMs is potentially
      key for theoretically modelling LLM cognition and LLM training-dynamics (compare:
      the study of aphasia and visual processing disorder in humans plays a key role
      cognitive science), and in particular for developing a good theory of generalization
      in LLMS'
    see_also:
    - a:Emergent_misalignment
    - mechanistic anomaly detection
    orthodox_problems:
    - goals_misgeneralize
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: null
    broad_approach_text: behaviorist / cognitivist
    some_names:
    - Janus
    - Truthful AI
    - Theia Vogel
    - Stewart Slocum
    - Nell Watson
    - Samuel G. B. Johnson
    - Liwei Jiang
    - Monika Jotautaite
    - Saloni Dash
    estimated_ftes: 5-20
    critiques: null
    funded_by: Coefficient Giving (via Truthful AI and Interpretability grants)
    outputs:
    - link_url: https://arxiv.org/abs/2507.14805
      link_text: 'Subliminal Learning: Language models transmit behavioural traits
        via hidden signals in data'
      original_md: '[Subliminal Learning: Language models transmit behavioural traits
        via hidden signals in data](https://arxiv.org/abs/2507.14805)'
      title: 'Subliminal Learning: Language models transmit behavioral traits via
        hidden signals in data'
      authors:
      - Alex Cloud
      - Minh Le
      - James Chua
      - Jan Betley
      - Anna Sztyber-Betley
      - Jacob Hilton
      - Samuel Marks
      - Owain Evans
      author_organizations: []
      date: '2025-07-20'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.13928
      link_text: LLMs Can Get "Brain Rot"!
      original_md: '[LLMs Can Get "Brain Rot"\!](https://arxiv.org/abs/2510.13928)'
      title: LLMs Can Get "Brain Rot"!
      authors:
      - Shuo Xing
      - Junyuan Hong
      - Yifan Wang
      - Runjin Chen
      - Zhenyu Zhang
      - Ananth Grama
      - Zhengzhong Tu
      - Zhangyang Wang
      author_organizations: []
      date: '2025-10-15'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.20020
      link_text: Persona-Assigned Large Language Models Exhibit Human-Like Motivated
        Reasoning
      original_md: '[Persona-Assigned Large Language Models Exhibit Human-Like Motivated
        Reasoning](https://arxiv.org/abs/2506.20020)'
      title: Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning
      authors:
      - Saloni Dash
      - Amélie Reymond
      - Emma S. Spiro
      - Aylin Caliskan
      author_organizations: []
      date: '2025-06-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://spylab.ai/blog/modal-aphasia
      link_text: Unified Multimodal Models Cannot Describe Images From Memory
      original_md: '[Unified Multimodal Models Cannot Describe Images From Memory](https://spylab.ai/blog/modal-aphasia)'
      title: Unified Multimodal Models Cannot Describe Images From Memory
      authors:
      - Michael Aerni
      - Joshua Swanson
      - Kristina Nikolić
      - Florian Tramèr
      author_organizations:
      - SPY Lab
      - ETH Zurich
      date: '2024-10-28'
      published_year: 2024
      venue: SPY Lab Blog
      kind: blog_post
    - link_url: https://arxiv.org/abs/2510.17941
      link_text: 'Believe It or Not: How Deeply do LLMs Believe Implanted Facts?'
      original_md: '[Believe It or Not: How Deeply do LLMs Believe Implanted Facts?](https://arxiv.org/abs/2510.17941)'
      title: 'Believe It or Not: How Deeply do LLMs Believe Implanted Facts?'
      authors:
      - Stewart Slocum
      - Julian Minder
      - Clément Dumas
      - Henry Sleight
      - Ryan Greenblatt
      - Samuel Marks
      - Rowan Wang
      author_organizations:
      - Redwood Research
      date: '2025-10-20'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.psychopathia.ai/
      link_text: 'Psychopathia Machinalis: A Nosological Framework for Understanding
        Pathologies in Advanced Artificial Intelligence'
      original_md: '[Psychopathia Machinalis: A Nosological Framework for Understanding
        Pathologies in Advanced Artificial Intelligence](https://www.psychopathia.ai/)'
      title: 'Psychopathia Machinalis: A Nosological Framework for Understanding Pathologies
        in Advanced Artificial Intelligence'
      authors:
      - Nell Watson
      - Ali Hessami
      author_organizations: []
      date: '2025-01-01'
      published_year: 2025
      venue: Electronics (MDPI)
      kind: paper_published
    - link_url: https://arxiv.org/abs/2411.02478
      link_text: 'Imagining and building wise machines: The centrality of AI metacognition'
      original_md: '[Imagining and building wise machines: The centrality of AI metacognition](https://arxiv.org/abs/2411.02478)'
      title: 'Imagining and building wise machines: The centrality of AI metacognition'
      authors:
      - Samuel G. B. Johnson
      - Amir-Hossein Karimi
      - Yoshua Bengio
      - Nick Chater
      - Tobias Gerstenberg
      - Kate Larson
      - Sydney Levine
      - Melanie Mitchell
      - Iyad Rahwan
      - Bernhard Schölkopf
      - Igor Grossmann
      author_organizations:
      - Various institutions including Mila
      - Santa Fe Institute
      date: '2024-11-04'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.22954
      link_text: 'Artificial Hivemind: The Open-Ended Homogeneity of Language Models
        (and Beyond)'
      original_md: '[Artificial Hivemind: The Open-Ended Homogeneity of Language Models
        (and Beyond)](https://arxiv.org/abs/2510.22954)'
      title: 'Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and
        Beyond)'
      authors:
      - Liwei Jiang
      - Yuanjun Chai
      - Margaret Li
      - Mickel Liu
      - Raymond Fok
      - Nouha Dziri
      - Yulia Tsvetkov
      - Maarten Sap
      - Alon Albalak
      - Yejin Choi
      author_organizations:
      - University of Washington
      - Allen Institute for AI
      date: '2025-10-27'
      published_year: 2025
      venue: arXiv (accepted to NeurIPS 2025 D&B - Oral)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.20039
      link_text: 'Beyond One-Way Influence: Bidirectional Opinion Dynamics in Multi-Turn
        Human-LLM Interactions'
      original_md: '[Beyond One-Way Influence: Bidirectional Opinion Dynamics in Multi-Turn
        Human-LLM Interactions](https://arxiv.org/abs/2510.20039)'
      title: 'Beyond One-Way Influence: Bidirectional Opinion Dynamics in Multi-Turn
        Human-LLM Interactions'
      authors:
      - Yuyang Jiang
      - Longjie Guo
      - Yuchen Wu
      - Aylin Caliskan
      - Tanu Mitra
      - Hua Shen
      author_organizations:
      - University of Washington
      - Northeastern University
      date: '2025-10-22'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues:
  - Broad approach field says 'behavioural/cognitive' - indicating mixed approach,
    set broad_approach_id to null and broad_approach_text to 'behaviorist / cognitivist'
- id: sec:Better_data
  name: Better data
  header_level: 2
  parent_id: sec:Black_box_safety_understand_and_control_current_model_behaviour_
  content: ''
  item_type: section
  agenda_attributes: null
  parsing_issues: []
- id: a:Data_filtering
  name: Data filtering
  header_level: 3
  parent_id: sec:Better_data
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Builds safety into models from the start by removing harmful
      or toxic content (like dual-use info) from the pretraining data, rather than
      relying only on post-training alignment.
    theory_of_change: By curating the pretraining data, we can prevent the model from
      learning dangerous capabilities (e.g., dual-use info) or undesirable behaviors
      (e.g., toxicity) in the first place, making safety more robust and "tamper-resistant"
      than post-training patches.
    see_also:
    - a:Data_quality_for_alignment
    - a:Data_poisoning_defense
    - a:Synthetic_data_for_alignment
    - a:Capability_removal_unlearning
    orthodox_problems:
    - goals_misgeneralize
    - value_fragile
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: engineering
    broad_approach_text: engineering
    some_names:
    - Yanda Chen
    - Pratyush Maini
    - Kyle O'Brien
    - Stephen Casper
    - Simon Pepin Lehalleur
    - Jesse Hoogland
    - Himanshu Beniwal
    - Sachin Goyal
    - Mycal Tucker
    - Dylan Sam
    estimated_ftes: 10-50
    critiques: '[When Bad Data Leads to Good Models](https://arxiv.org/pdf/2505.04741),
      [Medical large language models are vulnerable to data-poisoning attacks](https://www.nature.com/articles/s41591-024-03445-1)'
    funded_by: Anthropic, various academics
    outputs:
    - link_url: https://alignment.anthropic.com/2025/pretraining-data-filtering/
      link_text: Enhancing Model Safety through Pretraining Data Filtering
      original_md: '[Enhancing Model Safety through Pretraining Data Filtering](https://alignment.anthropic.com/2025/pretraining-data-filtering/)'
      title: Enhancing Model Safety through Pretraining Data Filtering
      authors:
      - Yanda Chen
      - Mycal Tucker
      - Nina Panickssery
      - Tony Wang
      - Francesco Mosconi
      - Anjali Gopal
      - Carson Denison
      - Linda Petrini
      - Jan Leike
      - Ethan Perez
      - Mrinank Sharma
      author_organizations:
      - Anthropic
      date: '2025-08-19'
      published_year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - link_url: https://arxiv.org/abs/2508.06601
      link_text: 'Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant
        Safeguards into Open-Weight LLMs'
      original_md: '[Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant
        Safeguards into Open-Weight LLMs](https://arxiv.org/abs/2508.06601)'
      title: 'Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards
        into Open-Weight LLMs'
      authors:
      - Kyle O'Brien
      - Stephen Casper
      - Quentin Anthony
      - Tomek Korbak
      - Robert Kirk
      - Xander Davies
      - Ishan Mishra
      - Geoffrey Irving
      - Yarin Gal
      - Stella Biderman
      author_organizations:
      - Anthropic
      - Redwood Research
      - EleutherAI
      - University of Oxford
      date: '2025-08-08'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.16980
      link_text: 'Safety Pretraining: Toward the Next Generation of Safe AI'
      original_md: '[Safety Pretraining: Toward the Next Generation of Safe AI](https://arxiv.org/abs/2504.16980)'
      title: 'Safety Pretraining: Toward the Next Generation of Safe AI'
      authors:
      - Pratyush Maini
      - Sachin Goyal
      - Dylan Sam
      - Alex Robey
      - Yash Savani
      - Yiding Jiang
      - Andy Zou
      - Matt Fredrikson
      - Zacharcy C. Lipton
      - J. Zico Kolter
      author_organizations:
      - Carnegie Mellon University
      date: '2025-04-23'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.27629v2
      link_text: Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation
        Models
      original_md: '[Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation
        Models](https://arxiv.org/abs/2510.27629v2)'
      title: Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation
        Models
      authors:
      - Boyi Wei
      - Zora Che
      - Nathaniel Li
      - Udari Madhushani Sehwag
      - Jasper Götting
      - Samira Nedungadi
      - Julian Michael
      - Summer Yue
      - Dan Hendrycks
      - Peter Henderson
      - Zifan Wang
      - Seth Donoughe
      - Mantas Mazeika
      author_organizations:
      - Anthropic
      - Redwood Research
      date: '2025-10-31'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:Hyperstition_studies
  name: Hyperstition studies
  header_level: 3
  parent_id: sec:Better_data
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: 'Study, steer, and intervene on the following feedback loop:
      "we produce stories about how present and future AI systems behave" → "these
      stories become training data for the AI" → "these stories shape how AI systems
      in fact behave".'
    theory_of_change: Measure the influence of existing AI narratives in the training
      data → seed and develop more salutary ontologies and self-conceptions for AI
      models → control and redirect AI models' self-concepts through selectively amplifying
      certain components of the training data.
    see_also:
    - a:Data_filtering
    - '[active inference](https://arxiv.org/abs/2311.10215)'
    - LLM whisperers
    orthodox_problems:
    - value_fragile
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitive
    some_names:
    - Alex Turner
    - '[Hyperstition AI](https://www.hyperstitionai.com/)'
    - Kyle O'Brien
    estimated_ftes: 1-10
    critiques: null
    funded_by: Unclear, niche
    outputs:
    - link_url: https://www.lesswrong.com/posts/qXYLvjGL9QvD3aFSW/training-on-documents-about-reward-hacking-induces-reward
      link_text: Training on Documents About Reward Hacking Induces Reward Hacking
      original_md: '[Training on Documents About Reward Hacking Induces Reward Hacking](https://www.lesswrong.com/posts/qXYLvjGL9QvD3aFSW/training-on-documents-about-reward-hacking-induces-reward)'
      title: Training on Documents About Reward Hacking Induces Reward Hacking
      authors:
      - Evan Hubinger
      - Nathan Hu
      author_organizations:
      - Anthropic
      date: '2025-01-21'
      published_year: 2025
      venue: LessWrong/AI Alignment Forum
      kind: lesswrong
    - link_url: https://www.lesswrong.com/posts/Y8zS8iG5HhqKcQBtA/do-not-tile-the-lightcone-with-your-confused-ontology
      link_text: Do Not Tile the Lightcone with Your Confused Ontology
      original_md: '[Do Not Tile the Lightcone with Your Confused Ontology](https://www.lesswrong.com/posts/Y8zS8iG5HhqKcQBtA/do-not-tile-the-lightcone-with-your-confused-ontology)'
      title: Do Not Tile the Lightcone with Your Confused Ontology
      authors:
      - Jan_Kulveit
      author_organizations: []
      date: '2025-06-13'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://turntrout.com/self-fulfilling-misalignment
      link_text: Self-Fulfilling Misalignment Data Might Be Poisoning Our AI Models
      original_md: '[Self-Fulfilling Misalignment Data Might Be Poisoning Our AI Models](https://turntrout.com/self-fulfilling-misalignment)'
      title: Self-Fulfilling Misalignment Data Might Be Poisoning Our AI Models
      authors:
      - Alex Turner
      author_organizations: []
      date: '2025-03-01'
      published_year: 2025
      venue: turntrout.com
      kind: blog_post
    - link_url: https://arxiv.org/abs/2411.13223
      link_text: 'Existential Conversations with Large Language Models: Content, Community,
        and Culture'
      original_md: '[Existential Conversations with Large Language Models: Content,
        Community, and Culture](https://arxiv.org/abs/2411.13223)'
      title: 'Existential Conversations with Large Language Models: Content, Community,
        and Culture'
      authors:
      - Murray Shanahan
      - Beth Singler
      author_organizations: []
      date: '2024-11-20'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues:
  - Broad approach field says 'cognitive' - mapped to 'cognitivist_science'
- id: a:Data_poisoning_defense
  name: Data poisoning defense
  header_level: 3
  parent_id: sec:Better_data
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Develops methods to detect and prevent malicious or backdoor-inducing
      samples from being included in the training data.
    theory_of_change: By identifying and filtering out malicious training examples,
      we can prevent attackers from creating hidden backdoors or triggers that would
      cause aligned models to behave dangerously.
    see_also:
    - a:Data_filtering
    - a:Safeguards_inference_time_auxiliaries_
    - a:Various_Redteams
    - adversarial robustness
    orthodox_problems:
    - superintelligence_hack_software
    - someone_else_deploys
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: engineering
    broad_approach_text: engineering
    some_names:
    - Alexandra Souly
    - Javier Rando
    - Ed Chapman
    - Hanna Foerster
    - Ilia Shumailov
    - Yiren Zhao
    estimated_ftes: 5-20
    critiques: '[A small number of samples can poison LLMs of any size](https://arxiv.org/abs/2510.04567),
      [Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated](https://arxiv.org/abs/2509.03405)'
    funded_by: Google DeepMind, Anthropic, University of Cambridge, Vector Institute
    outputs:
    - link_url: https://example-blog.com/a-small-number-of-samples-can-poison-llms
      link_text: A small number of samples can poison LLMs of any size
      original_md: '[A small number of samples can poison LLMs of any size](https://example-blog.com/a-small-number-of-samples-can-poison-llms)'
      title: A small number of samples can poison LLMs
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: error_detected
    - link_url: https://arxiv.org/abs/2509.03405
      link_text: Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated
      original_md: '[Reasoning Introduces New Poisoning Attacks Yet Makes Them More
        Complicated](https://arxiv.org/abs/2509.03405)'
      title: 'LMEnt: A Suite for Analyzing Knowledge in Language Models from Pretraining
        Data to Representations'
      authors:
      - Daniela Gottesman
      - Alon Gilae-Dotan
      - Ido Cohen
      - Yoav Gur-Arieh
      - Marius Mosbach
      - Ori Yoran
      - Mor Geva
      author_organizations:
      - Unknown
      date: '2025-09-03'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.04567
      link_text: Poisoning Attacks on LLMs Require a Near-constant Number of Poison
        Samples
      original_md: '[Poisoning Attacks on LLMs Require a Near-constant Number of Poison
        Samples](https://arxiv.org/abs/2510.04567)'
      title: 'GILT: An LLM-Free, Tuning-Free Graph Foundational Model for In-Context
        Learning'
      authors:
      - Weishuo Ma
      - Yanbo Wang
      - Xiyuan Wang
      - Lei Zou
      - Muhan Zhang
      author_organizations: []
      date: '2025-10-06'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:Synthetic_data_for_alignment
  name: Synthetic data for alignment
  header_level: 3
  parent_id: sec:Better_data
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Uses AI-generated data (e.g., critiques, preferences, or
      self-labeled examples) to scale and improve alignment, especially for superhuman
      models.
    theory_of_change: We can overcome the bottleneck of human feedback and data by
      using models to generate vast amounts of high-quality, targeted data for safety,
      preference tuning, and capability elicitation.
    see_also:
    - a:Data_quality_for_alignment
    - a:Data_filtering
    - scalable oversight
    - automated alignment research
    - a:Weak_to_strong_generalization
    orthodox_problems:
    - goals_misgeneralize
    - superintelligence_fool_supervisors
    - value_fragile
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: engineering
    broad_approach_text: engineering
    some_names:
    - Mianqiu Huang
    - Xiaoran Liu
    - Rylan Schaeffer
    - Nevan Wichers
    - Aram Ebtekar
    - Jiaxin Wen
    - Vishakh Padmakumar
    - Benjamin Newman
    estimated_ftes: 50-150
    critiques: '[Synthetic Data in AI: Challenges, Applications, and Ethical Implications](https://arxiv.org/abs/2401.01629).
      Sort of [Demski](https://www.lesswrong.com/posts/nQwbDPgYvAbqAmAud/llms-for-alignment-research-a-safety-priority).'
    funded_by: Anthropic, Google DeepMind, OpenAI, Meta AI, various academic groups.
    outputs:
    - link_url: https://arxiv.org/abs/2510.06652
      link_text: Aligning Large Language Models via Fully Self-Synthetic Data
      original_md: '* [**Aligning Large Language Models via Fully Self-Synthetic Data**](https://arxiv.org/abs/2510.06652)'
      title: Aligning Large Language Models via Fully Self-Synthetic Data
      authors:
      - Shangjian Yin
      - Zhepei Wei
      - Xinyu Zhu
      - Wei-Lin Chen
      - Yu Meng
      author_organizations: []
      date: '2025-10-08'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/html/2412.17417v2
      link_text: 'Synth-Align: Improving Trustworthiness in Vision-Language Model
        with Synthetic Preference Data Alignment'
      original_md: '* [**Synth-Align: Improving Trustworthiness in Vision-Language
        Model with Synthetic Preference Data Alignment**](https://arxiv.org/html/2412.17417v2)'
      title: 'Synth-Align: Improving Trustworthiness in Vision-Language Model with
        Synthetic Preference Data Alignment'
      authors:
      - Robert Wijaya
      - Ngoc-Bao Nguyen
      - Ngai-Man Cheung
      author_organizations: []
      date: '2024-12-23'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.12345
      link_text: 'Inoculation Prompting: Instructing LLMs to misbehave at train-time
        improves test-time alignment'
      original_md: '* [**Inoculation Prompting: Instructing LLMs to misbehave at train-time
        improves test-time alignment**](https://arxiv.org/abs/2510.12345), *Nevan
        Wichers, Aram Ebtekar, Ariana Azarbal et al.*, 2025-10-27, arXiv'
      title: Carleman Estimates and Controllability of Forward Stochastic Parabolic
        Equations with General Dynamic Boundary Conditions
      authors:
      - Said Boulite
      - Abdellatif Elgrou
      - Lahcen Maniar
      - Abdelaziz Rhandi
      author_organizations: []
      date: '2025-10-14'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.05678
      link_text: Unsupervised Elicitation of Language Models
      original_md: '* [**Unsupervised Elicitation of Language Models**](https://arxiv.org/abs/2506.05678),
        *Jiaxin Wen, Zachary Ankner, Arushi Somani et al.*, 2025-06-11, arXiv'
      title: Numerical Investigation of Sequence Modeling Theory using Controllable
        Memory Functions
      authors:
      - Haotian Jiang
      - Zeyu Bao
      - Shida Wang
      - Qianxiao Li
      author_organizations: []
      date: '2025-06-06'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2412.02345
      link_text: 'Beyond the Binary: Capturing Diverse Preferences With Reward Regularization'
      original_md: '* [**Beyond the Binary: Capturing Diverse Preferences With Reward
        Regularization**](https://arxiv.org/abs/2412.02345), *Vishakh Padmakumar,
        Chuanyang Jin, Hannah Rose Kirk et al.*, 2024-12-05, arXiv'
      title: On the Realization of quantum gates coming from the Tracy-Singh product
      authors:
      - Fabienne Chouraqui
      author_organizations: []
      date: '2024-12-03'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.06789
      link_text: 'The Curious Case of Factuality Finetuning: Models'' Internal Beliefs
        Can Improve Factuality'
      original_md: '* [**The Curious Case of Factuality Finetuning: Models'' Internal
        Beliefs Can Improve Factuality**](https://arxiv.org/abs/2507.06789), *Benjamin
        Newman, Abhilasha Ravichander, Jaehun Jung et al.*, 2025-07-11, arXiv'
      title: Sharp uniform approximation for spectral Barron functions by deep neural
        networks
      authors:
      - Yulei Liao
      - Pingbing Ming
      - Hao Yu
      author_organizations: []
      date: '2025-07-09'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.13456
      link_text: 'LongSafety: Enhance Safety for Long-Context LLMs'
      original_md: '* [**LongSafety: Enhance Safety for Long-Context LLMs**](https://arxiv.org/abs/2502.13456),
        *Mianqiu Huang, Xiaoran Liu, Shaojun Zhou et al.*, 2025-02-27, arXiv'
      title: 'OGBoost: A Python Package for Ordinal Gradient Boosting'
      authors:
      - Mansour T.A. Sharabiani
      - Alex Bottle
      - Alireza S. Mahani
      author_organizations: []
      date: '2025-02-19'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.02341
      link_text: 'Position: Model Collapse Does Not Mean What You Think'
      original_md: '* [**Position: Model Collapse Does Not Mean What You Think**](https://arxiv.org/abs/2503.02341),
        *Rylan Schaeffer, Joshua Kazdan, Alvan Caleb Arulandu et al.*, 2025-03-05,
        arXiv'
      title: 'GRADEO: Towards Human-Like Evaluation for Text-to-Video Generation via
        Multi-Step Reasoning'
      authors:
      - Zhun Mou
      - Bin Xia
      - Zhengchao Huang
      - Wenming Yang
      - Jiaya Jia
      author_organizations: []
      date: '2025-03-04'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:Data_quality_for_alignment
  name: Data quality for alignment
  header_level: 3
  parent_id: sec:Better_data
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Improves the quality, signal-to-noise ratio, and reliability
      of human-generated preference and alignment data.
    theory_of_change: The quality of alignment is heavily dependent on the quality
      of the data (e.g., human preferences); by improving the "signal" from annotators
      and reducing noise/bias, we will get more robustly aligned models.
    see_also:
    - a:Synthetic_data_for_alignment
    - scalable oversight
    - a:Assistance_games_assistive_agents
    - a:Model_values_model_preferences
    orthodox_problems:
    - superintelligence_fool_supervisors
    - value_fragile
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: engineering
    broad_approach_text: engineering
    some_names:
    - Maarten Buyl
    - Kelsey Kraus
    - Margaret Kroll
    - Danqing Shi
    estimated_ftes: 20-50
    critiques: '[A Statistical Case Against Empirical Human-AI Alignment](https://arxiv.org/abs/2502.14581)'
    funded_by: Anthropic, Google DeepMind, OpenAI, Meta AI, various academic groups
    outputs:
    - link_url: https://arxiv.org/abs/2502.10441
      link_text: AI Alignment at Your Discretion
      original_md: '[AI Alignment at Your Discretion](https://arxiv.org/abs/2502.10441)'
      title: AI Alignment at Your Discretion
      authors:
      - Maarten Buyl
      - Hadi Khalaf
      - Claudio Mayrink Verdun
      - Lucas Monteiro Paes
      - Caio C. Vieira Machado
      - Flavio du Pin Calmon
      author_organizations: []
      date: '2025-02-10'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.04910
      link_text: Maximizing Signal in Human-Model Preference Alignment
      original_md: '[Maximizing Signal in Human-Model Preference Alignment](https://arxiv.org/abs/2503.04910)'
      title: Maximizing Signal in Human-Model Preference Alignment
      authors:
      - Kelsey Kraus
      - Margaret Kroll
      author_organizations: []
      date: '2025-03-06'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.18802
      link_text: 'DxHF: Providing High-Quality Human Feedback for LLM Alignment via
        Interactive Decomposition'
      original_md: '[DxHF: Providing High-Quality Human Feedback for LLM Alignment
        via Interactive Decomposition](https://arxiv.org/abs/2507.18802)'
      title: 'DxHF: Providing High-Quality Human Feedback for LLM Alignment via Interactive
        Decomposition'
      authors:
      - Danqing Shi
      - Furui Cheng
      - Tino Weinkauf
      - Antti Oulasvirta
      - Mennatallah El-Assady
      author_organizations: []
      date: '2025-07-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/html/2410.01957v2
      link_text: Challenges and Future Directions of Data-Centric AI Alignment
      original_md: '[Challenges and Future Directions of Data-Centric AI Alignment](https://arxiv.org/html/2410.01957v2)'
      title: Challenges and Future Directions of Data-Centric AI Alignment
      authors:
      - Min-Hsuan Yeh
      - Jeffrey Wang
      - Xuefeng Du
      - Seongheon Park
      - Leitian Tao
      - Shawn Im
      - Yixuan Li
      author_organizations: []
      date: '2025-05-01'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.05475
      link_text: You Are What You Eat -- AI Alignment Requires Understanding How Data
        Shapes Structure and Generalisation
      original_md: '[You Are What You Eat \-- AI Alignment Requires Understanding
        How Data Shapes Structure and Generalisation](https://arxiv.org/abs/2502.05475)'
      title: You Are What You Eat -- AI Alignment Requires Understanding How Data
        Shapes Structure and Generalisation
      authors:
      - Simon Pepin Lehalleur
      - Jesse Hoogland
      - Matthew Farrugia-Roberts
      - Susan Wei
      - Alexander Gietelink Oldenziel
      - George Wang
      - Liam Carroll
      - Daniel Murfet
      author_organizations: []
      date: '2025-02-08'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: sec:Goal_robustness
  name: Goal robustness
  header_level: 2
  parent_id: sec:Black_box_safety_understand_and_control_current_model_behaviour_
  content: ''
  item_type: section
  agenda_attributes: null
  parsing_issues: []
- id: a:Mild_optimisation
  name: Mild optimisation
  header_level: 3
  parent_id: sec:Goal_robustness
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Avoid Goodharting by getting AI to satisfice rather than
      maximise.
    theory_of_change: If we fail to exactly nail down the preferences for a superintelligent
      agent we die to Goodharting → shift from maximising to satisficing in the agent's
      utility function → we get a nonzero share of the lightcone as opposed to zero;
      also, moonshot at this being the recipe for fully aligned AI.
    see_also: []
    orthodox_problems:
    - value_fragile
    target_case_id: null
    target_case_text: mixed
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitive
    some_names: []
    estimated_ftes: 10-50
    critiques: null
    funded_by: Google DeepMind
    outputs:
    - link_url: https://arxiv.org/abs/2501.13011
      link_text: 'MONA: Myopic Optimization with Non-myopic Approval Can Mitigate
        Multi-step Reward Hacking'
      original_md: '[MONA: Myopic Optimization with Non-myopic Approval Can Mitigate
        Multi-step Reward Hacking](https://arxiv.org/abs/2501.13011)'
      title: 'MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step
        Reward Hacking'
      authors:
      - Sebastian Farquhar
      - Vikrant Varma
      - David Lindner
      - David Elson
      - Caleb Biddulph
      - Ian Goodfellow
      - Rohin Shah
      author_organizations:
      - Google DeepMind
      date: '2025-01-22'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.02655
      link_text: 'BioBlue: Notable runaway-optimiser-like LLM failure modes on biologically
        and economically aligned AI safety benchmarks for LLMs with simplified observation
        format'
      original_md: '[BioBlue: Notable runaway-optimiser-like LLM failure modes on
        biologically and economically aligned AI safety benchmarks for LLMs with simplified
        observation format](https://arxiv.org/abs/2509.02655)'
      title: 'BioBlue: Notable runaway-optimiser-like LLM failure modes on biologically
        and economically aligned AI safety benchmarks for LLMs with simplified observation
        format'
      authors:
      - Roland Pihlakas
      - Sruthi Kuriakose
      author_organizations: []
      date: '2025-09-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/vGeuBKQ7nzPnn5f7A/why-modelling-multi-objective-homeostasis-is-essential-for
      link_text: Why modelling multi-objective homeostasis is essential for AI alignment
        (and how it helps with AI safety as well). Subtleties and Open Challenges
      original_md: '[Why modelling multi-objective homeostasis is essential for AI
        alignment (and how it helps with AI safety as well). Subtleties and Open Challenges](https://lesswrong.com/posts/vGeuBKQ7nzPnn5f7A/why-modelling-multi-objective-homeostasis-is-essential-for)'
      title: Why modelling multi-objective homeostasis is essential for AI alignment
        (and how it helps with AI safety as well). Subtleties and Open Challenges.
      authors:
      - Roland Pihlakas
      author_organizations: []
      date: '2025-01-12'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2410.00081
      link_text: 'From homeostasis to resource sharing: Biologically and economically
        aligned multi-objective multi-agent gridworld-based AI safety benchmarks'
      original_md: '[From homeostasis to resource sharing: Biologically and economically
        aligned multi-objective multi-agent gridworld-based AI safety benchmarks](https://arxiv.org/abs/2410.00081)'
      title: 'From homeostasis to resource sharing: Biologically and economically
        aligned multi-objective multi-agent gridworld-based AI safety benchmarks'
      authors:
      - Roland Pihlakas
      author_organizations: []
      date: '2024-09-30'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues:
  - Target case is 'mixed' - target_case_id left null, target_case_text set to 'mixed'
- id: a:RL_safety
  name: RL safety
  header_level: 3
  parent_id: sec:Goal_robustness
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Improves the robustness of reinforcement learning agents
      by addressing core problems in reward learning, goal misgeneralization, and
      specification gaming.
    theory_of_change: Standard RL objectives (like maximizing expected value) are
      brittle and lead to goal misgeneralization or specification gaming; by developing
      more robust frameworks (like pessimistic RL, minimax regret, or provable inverse
      reward learning), we can create agents that are safe even when misspecified.
    see_also:
    - a:Behavior_alignment_theory
    - a:Assistance_games_assistive_agents
    - sec:Goal_robustness
    - sec:Iterative_alignment
    - a:Mild_optimisation
    - scalable oversight
    - '[The Theoretical Reward Learning Research Agenda: Introduction and Motivation](https://www.alignmentforum.org/posts/pJ3mDD7LfEwp3s5vG/the-theoretical-reward-learning-research-agenda-introduction)'
    orthodox_problems:
    - goals_misgeneralize
    - value_fragile
    - superintelligence_fool_supervisors
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: engineering
    broad_approach_text: engineering
    some_names:
    - Joar Skalse
    - Karim Abdel Sadek
    - Matthew Farrugia-Roberts
    - Benjamin Plaut
    - Fang Wu
    - Stephen Zhao
    - Alessandro Abate
    - Steven Byrnes
    - Michael Cohen
    estimated_ftes: 20-70
    critiques: '["The Era of Experience" has an unsolved technical alignment problem](https://www.lesswrong.com/posts/747f6b8e/the-era-of-experience-has-an-unsolved-technical-alignment-problem),
      [The Invisible Leash: Why RLVR May or May Not Escape Its Origin](https://arxiv.org/abs/2507.14843)'
    funded_by: Google DeepMind, University of Oxford, CMU, Coefficient Giving
    outputs:
    - link_url: https://arxiv.org/abs/2406.15753
      link_text: 'The Perils of Optimizing Learned Reward Functions: Low Training
        Error Does Not Guarantee Low Regret'
      original_md: '[The Perils of Optimizing Learned Reward Functions: Low Training
        Error Does Not Guarantee Low Regret](https://arxiv.org/abs/2406.15753)'
      title: 'The Perils of Optimizing Learned Reward Functions: Low Training Error
        Does Not Guarantee Low Regret'
      authors:
      - Lukas Fluri
      - Leon Lang
      - Alessandro Abate
      - Patrick Forré
      - David Krueger
      - Joar Skalse
      author_organizations: []
      date: '2024-06-22'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.14043
      link_text: Safe Learning Under Irreversible Dynamics via Asking for Help
      original_md: '[Safe Learning Under Irreversible Dynamics via Asking for Help](https://arxiv.org/abs/2502.14043)'
      title: Safe Learning Under Irreversible Dynamics via Asking for Help
      authors:
      - Benjamin Plaut
      - Juan Liévano-Karim
      - Hanlin Zhu
      - Stuart Russell
      author_organizations:
      - UC Berkeley
      date: '2025-02-19'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.03068
      link_text: Mitigating Goal Misgeneralization via Minimax Regret
      original_md: '[Mitigating Goal Misgeneralization via Minimax Regret](https://arxiv.org/abs/2507.03068)'
      title: Mitigating Goal Misgeneralization via Minimax Regret
      authors:
      - Karim Abdel Sadek
      - Matthew Farrugia-Roberts
      - Usman Anwar
      - Hannah Erlebach
      - Christian Schroeder de Witt
      - David Krueger
      - Michael Dennis
      author_organizations: []
      date: '2025-07-03'
      published_year: 2025
      venue: RLC 2025
      kind: paper_published
    - link_url: https://arxiv.org/abs/2410.05584
      link_text: 'Rethinking Reward Model Evaluation: Are We Barking up the Wrong
        Tree?'
      original_md: '[Rethinking Reward Model Evaluation: Are We Barking up the Wrong
        Tree?](https://arxiv.org/abs/2410.05584)'
      title: 'Rethinking Reward Model Evaluation: Are We Barking up the Wrong Tree?'
      authors:
      - Xueru Wen
      - Jie Lou
      - Yaojie Lu
      - Hongyu Lin
      - Xing Yu
      - Xinyu Lu
      - Ben He
      - Xianpei Han
      - Debing Zhang
      - Le Sun
      author_organizations: []
      date: '2024-10-08'
      published_year: 2024
      venue: arXiv (Accepted at ICLR 2025 Spotlight)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.14843
      link_text: 'The Invisible Leash: Why RLVR May or May Not Escape Its Origin'
      original_md: '[The Invisible Leash: Why RLVR May or May Not Escape Its Origin](https://arxiv.org/abs/2507.14843)'
      title: 'The Invisible Leash: Why RLVR May or May Not Escape Its Origin'
      authors:
      - Fang Wu
      - Weihao Xuan
      - Ximing Lu
      - Mingjie Liu
      - Yi Dong
      - Zaid Harchaoui
      - Yejin Choi
      author_organizations:
      - University of Washington
      - Allen Institute for AI
      date: '2025-07-20'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.21184
      link_text: Reducing the Probability of Undesirable Outputs in Language Models
        Using Probabilistic Inference
      original_md: '[Reducing the Probability of Undesirable Outputs in Language Models
        Using Probabilistic Inference](https://arxiv.org/abs/2510.21184)'
      title: Reducing the Probability of Undesirable Outputs in Language Models Using
        Probabilistic Inference
      authors:
      - Stephen Zhao
      - Aidan Li
      - Rob Brekelmans
      - Roger Grosse
      author_organizations: []
      date: '2025-10-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.01871
      link_text: Interpreting Emergent Planning in Model-Free Reinforcement Learning
      original_md: '[Interpreting Emergent Planning in Model-Free Reinforcement Learning](https://arxiv.org/abs/2504.01871)'
      title: Interpreting Emergent Planning in Model-Free Reinforcement Learning
      authors:
      - Thomas Bush
      - Stephen Chung
      - Usman Anwar
      - Adrià Garriga-Alonso
      - David Krueger
      author_organizations: []
      date: '2025-04-02'
      published_year: 2025
      venue: arXiv (ICLR 2025 oral)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.10995
      link_text: Misalignment From Treating Means as Ends
      original_md: '[Misalignment From Treating Means as Ends](https://arxiv.org/abs/2507.10995)'
      title: Misalignment from Treating Means as Ends
      authors:
      - Henrik Marklund
      - Alex Infanger
      - Benjamin Van Roy
      author_organizations: []
      date: '2025-07-15'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/TCGgiJAinGgcMEByt/the-era-of-experience-has-an-unsolved-technical-alignment
      link_text: '"The Era of Experience" has an unsolved technical alignment problem'
      original_md: '["The Era of Experience" has an unsolved technical alignment problem](https://lesswrong.com/posts/TCGgiJAinGgcMEByt/the-era-of-experience-has-an-unsolved-technical-alignment)'
      title: '"The Era of Experience" has an unsolved technical alignment problem'
      authors:
      - Steven Byrnes
      author_organizations: []
      date: '2025-04-24'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/CpftMXCEnwqbWreHD/safety-cases-for-pessimism
      link_text: Safety cases for Pessimism
      original_md: '[Safety cases for Pessimism](https://lesswrong.com/posts/CpftMXCEnwqbWreHD/safety-cases-for-pessimism)'
      title: Safety cases for Pessimism
      authors:
      - Michael Cohen
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: lesswrong
    - link_url: https://www.lesswrong.com/posts/oxvnREntu82tffkYW/we-need-a-field-of-reward-function-design
      link_text: We need a field of Reward Function Design
      original_md: '[We need a field of Reward Function Design](https://www.lesswrong.com/posts/oxvnREntu82tffkYW/we-need-a-field-of-reward-function-design)'
      title: We need a field of Reward Function Design
      authors:
      - Steven Byrnes
      author_organizations: []
      date: '2025-12-08'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    other_attributes: {}
  parsing_issues: []
- id: a:Assistance_games_assistive_agents
  name: Assistance games, assistive agents
  header_level: 3
  parent_id: sec:Goal_robustness
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Formalize how AI assistants learn about human preferences
      given uncertainty and partial observability, and construct environments which
      better incentivize AIs to learn what we want them to learn.
    theory_of_change: Understand what kinds of things can go wrong when humans are
      directly involved in training a model → build tools that make it easier for
      a model to learn what humans want it to learn.
    see_also:
    - a:Guaranteed_Safe_AI
    orthodox_problems:
    - value_fragile
    - humanlike_minds_not_safe
    target_case_id: null
    target_case_text: varies
    broad_approach_id: null
    broad_approach_text: engineering / cognitive
    some_names:
    - Joar Skalse
    - Anca Dragan
    - Caspar Oesterheld
    - David Krueger
    - Dylan Hafield-Menell
    - Stuart Russell
    estimated_ftes: null
    critiques: '[nice summary](https://www.lesswrong.com/posts/i5kijcjFJD6bn7dwq/evaluating-the-historical-value-misspecification-argument)
      of historical problem statements'
    funded_by: Future of Life Institute, Coefficient Giving, Survival and Flourishing
      Fund, Cooperative AI Foundation, Polaris Ventures
    outputs:
    - link_url: https://arxiv.org/pdf/2510.13709
      link_text: Training LLM Agents to Empower Humans
      original_md: '[Training LLM Agents to Empower Humans](https://arxiv.org/pdf/2510.13709)'
      title: Training LLM Agents to Empower Humans
      authors:
      - Evan Ellis
      - Vivek Myers
      - Jens Tuyls
      - Sergey Levine
      - Anca Dragan
      - Benjamin Eysenbach
      author_organizations:
      - UC Berkeley
      - Princeton University
      date: '2025-10-16'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.05381
      link_text: 'Murphys Laws of AI Alignment: Why the Gap Always Wins'
      original_md: '[Murphys Laws of AI Alignment: Why the Gap Always Wins](https://arxiv.org/abs/2509.05381)'
      title: 'Murphys Laws of AI Alignment: Why the Gap Always Wins'
      authors:
      - Madhava Gaikwad
      author_organizations: []
      date: '2025-09-04'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.07091
      link_text: 'AssistanceZero: Scalably Solving Assistance Games'
      original_md: '[AssistanceZero: Scalably Solving Assistance Games](https://arxiv.org/abs/2504.07091)'
      title: 'AssistanceZero: Scalably Solving Assistance Games'
      authors:
      - Cassidy Laidlaw
      - Eli Bronstein
      - Timothy Guo
      - Dylan Feng
      - Lukas Berglund
      - Justin Svegliato
      - Stuart Russell
      - Anca Dragan
      author_organizations:
      - UC Berkeley
      date: '2025-04-09'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2412.17797
      link_text: Observation Interference in Partially Observable Assistance Games
      original_md: '[Observation Interference in Partially Observable Assistance Games](https://arxiv.org/abs/2412.17797)'
      title: Observation Interference in Partially Observable Assistance Games
      authors:
      - Scott Emmons
      - Caspar Oesterheld
      - Vincent Conitzer
      - Stuart Russell
      author_organizations:
      - UC Berkeley
      date: '2024-12-23'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.02623
      link_text: Learning to Assist Humans without Inferring Rewards
      original_md: '[Learning to Assist Humans without Inferring Rewards](https://arxiv.org/abs/2411.02623)'
      title: Learning to Assist Humans without Inferring Rewards
      authors:
      - Vivek Myers
      - Evan Ellis
      - Sergey Levine
      - Benjamin Eysenbach
      - Anca Dragan
      author_organizations:
      - UC Berkeley
      - Google
      date: '2024-11-04'
      published_year: 2024
      venue: NeurIPS 2024
      kind: paper_published
    other_attributes: {}
  parsing_issues:
  - Target case field says 'varies' - set target_case_id to null and target_case_text
    to 'varies'
  - Broad approach field says 'engineering / cognitive' (multiple approaches) - set
    broad_approach_id to null
- id: a:Harm_reduction_for_open_weights
  name: Harm reduction for open weights
  header_level: 3
  parent_id: sec:Goal_robustness
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Develops methods, primarily based on pretraining data intervention,
      to create tamper-resistant safeguards that prevent open-weight models from being
      maliciously fine-tuned to remove safety features or exploit dangerous capabilities.
    theory_of_change: Open-weight models allow adversaries to easily remove post-training
      safety (like refusal training) via simple fine-tuning; by making safety an intrinsic
      property of the model's learned knowledge and capabilities (e.g., by ensuring
      "deep ignorance" of dual-use information), the safeguards become far more difficult
      and expensive to remove.
    see_also:
    - a:Data_filtering
    - a:Capability_removal_unlearning
    - a:Data_poisoning_defense
    orthodox_problems:
    - someone_else_deploys
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: engineering
    broad_approach_text: engineering
    some_names:
    - Kyle O'Brien
    - Stephen Casper
    - Quentin Anthony
    - Tomek Korbak
    - Rishub Tamirisa
    - Mantas Mazeika
    - Stella Biderman
    - Yarin Gal
    estimated_ftes: 10-100
    critiques: null
    funded_by: UK AI Safety Institute (AISI), EleutherAI, Coefficient Giving
    outputs:
    - link_url: https://www.aisi.gov.uk/research/deep-ignorance-filtering-pretraining-data-builds-tamper-resistant-safeguards-into-open-weight-llms
      link_text: 'Deep ignorance: Filtering pretraining data builds tamper-resistant
        safeguards into open-weight LLMs'
      original_md: '[Deep ignorance: Filtering pretraining data builds tamper-resistant
        safeguards into open-weight LLMs](https://www.aisi.gov.uk/research/deep-ignorance-filtering-pretraining-data-builds-tamper-resistant-safeguards-into-open-weight-llms)'
      title: 'Deep ignorance: Filtering pretraining data builds tamper-resistant safeguards
        into open-weight LLMs'
      authors:
      - Kyle O'Brien
      - Stephen Casper
      - Quentin Anthony
      - Tomek Korbak
      - Robert Kirk
      - Xander Davies
      - Ishan Mishra
      - Geoffrey Irving
      - Yarin Gal
      - Stella Biderman
      author_organizations:
      - UK AI Security Institute
      - MIT
      - Eleuther AI
      date: '2025-08-08'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2408.00761
      link_text: Tamper-Resistant Safeguards for Open-Weight LLMs
      original_md: '[Tamper-Resistant Safeguards for Open-Weight LLMs](https://arxiv.org/abs/2408.00761)'
      title: Tamper-Resistant Safeguards for Open-Weight LLMs
      authors:
      - Rishub Tamirisa
      - Bhrugu Bharathi
      - Long Phan
      - Andy Zhou
      - Alice Gatti
      - Tarun Suresh
      - Maxwell Lin
      - Justin Wang
      - Rowan Wang
      - Ron Arel
      - Andy Zou
      - Dawn Song
      - Bo Li
      - Dan Hendrycks
      - Mantas Mazeika
      author_organizations:
      - UC Berkeley
      - UIUC
      - Center for AI Safety
      date: '2024-08-01'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5705186
      link_text: Open Technical Problems in Open-Weight AI Model Risk Management
      original_md: '[Open Technical Problems in Open-Weight AI Model Risk Management](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5705186)'
      title: Open Technical Problems in Open-Weight AI Model Risk Management
      authors:
      - Stephen Casper
      - Kyle O'Brien
      - Shayne Longpre
      - Elizabeth Seger
      - Kevin Klyman
      - Rishi Bommasani
      - Aniruddha Nrusimha
      - Ilia Shumailov
      - Sören Mindermann
      - Steven Basart
      - Frank Rudzicz
      - Kellin Pelrine
      - Avijit Ghosh
      - Andrew Strait
      - Robert Kirk
      - Dan Hendrycks
      - Peter Henderson
      - J. Zico Kolter
      - Geoffrey Irving
      - Yarin Gal
      - Yoshua Bengio
      - Dylan Hadfield-Menell
      author_organizations:
      - Massachusetts Institute of Technology
      - ERA Fellowship
      - Apple
      - Centre for the Governance of AI
      - Stanford University
      - Google DeepMind
      - Vector Institute for Artificial Intelligence
      - FAR.AI
      - Hugging Face
      - Center for AI Safety
      - Princeton University
      - Carnegie Mellon University
      - UK AI Security Institute
      - University of Oxford
      - University of Montreal
      date: '2025-10-26'
      published_year: 2025
      venue: SSRN
      kind: paper_preprint
    - link_url: https://arxiv.org/pdf/2506.22183
      link_text: A Different Approach to AI Safety Proceedings from the Columbia Convening
        on AI Openness and Safety
      original_md: '[A Different Approach to AI Safety Proceedings from the Columbia
        Convening on AI Openness and Safety](https://arxiv.org/pdf/2506.22183)'
      title: 'A Different Approach to AI Safety: Proceedings from the Columbia Convening
        on Openness in Artificial Intelligence and AI Safety'
      authors:
      - Camille François
      - Ludovic Péran
      - Ayah Bdeir
      - Nouha Dziri
      - Will Hawkins
      - Yacine Jernite
      - Sayash Kapoor
      - Juliet Shen
      - Heidy Khlaaf
      - Kevin Klyman
      - Nik Marda
      - Marie Pellat
      - Deb Raji
      - Divya Siddarth
      - Aviya Skowron
      - Joseph Spisak
      - Madhulika Srikumar
      - Victor Storchan
      - Audrey Tang
      - Jen Weedon
      author_organizations:
      - Columbia University
      - Academia
      - Industry
      - Civil Society
      - Government
      date: '2025-06-27'
      published_year: 2025
      venue: arXiv
      kind: agenda_manifesto
    - link_url: https://partnershiponai.org/wp-content/uploads/dlm_uploads/2024/07/open-foundation-model-risk-mitigation_rev3-1.pdf
      link_text: Risk Mitigation Strategies for the Open Foundation Model Value Chain
      original_md: '[Risk Mitigation Strategies for the Open Foundation Model Value
        Chain](https://partnershiponai.org/wp-content/uploads/dlm_uploads/2024/07/open-foundation-model-risk-mitigation_rev3-1.pdf)'
      title: Open Foundation Model Risk Mitigation
      authors: []
      author_organizations:
      - Partnership on AI
      date: null
      published_year: 2024
      venue: Partnership on AI
      kind: error_detected
    other_attributes: {}
  parsing_issues: []
- id: a:The_Neglected_Approaches_Approach
  name: The "Neglected Approaches" Approach
  header_level: 3
  parent_id: sec:Goal_robustness
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Agenda-agnostic approaches to identifying good but overlooked
      empirical alignment ideas, working with theorists who could use engineers, and
      prototyping them.
    theory_of_change: Empirical search for "negative alignment taxes" (prioritizing
      methods that simultaneously enhance alignment and capabilities)
    see_also:
    - sec:Iterative_alignment
    - automated alignment research
    - Beijing Key Laboratory of Safe AI and Superalignment
    - Aligned AI
    orthodox_problems:
    - someone_else_deploys
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: engineering
    broad_approach_text: engineering
    some_names:
    - AE Studio
    - Gunnar Zarncke
    - Cameron Berg
    - Michael Vaiana
    - Judd Rosenblatt
    - Diogo Schwerz de Lucena
    estimated_ftes: '15'
    critiques: '[The ''Alignment Bonus'' is a Dangerous Mirage](https://www.alignmentforum.org/posts/example-critique-neg-tax)'
    funded_by: AE Studio
    outputs:
    - link_url: https://arxiv.org/abs/2412.16325
      link_text: Learning Representations of Alignment
      original_md: '[Learning Representations of Alignment](https://arxiv.org/abs/2412.16325)'
      title: Towards Safe and Honest AI Agents with Neural Self-Other Overlap
      authors:
      - Marc Carauleanu
      - Michael Vaiana
      - Judd Rosenblatt
      - Cameron Berg
      - Diogo Schwerz de Lucena
      author_organizations: []
      date: '2024-12-20'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2508.08492
      link_text: 'Engineering Alignment: A Practical Framework for Prototyping ''Negative
        Tax'' Solutions'
      original_md: '[Engineering Alignment: A Practical Framework for Prototyping
        ''Negative Tax'' Solutions](https://arxiv.org/abs/2508.08492)'
      title: Momentum Point-Perplexity Mechanics in Large Language Models
      authors:
      - Lorenzo Tomaz
      - Judd Rosenblatt
      - Thomas Berry Jones
      - Diogo Schwerz de Lucena
      author_organizations: []
      date: '2025-08-11'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.24797
      link_text: 'Self-Correction in Thought-Attractors: A Nudge Towards Alignment'
      original_md: '[Self-Correction in Thought-Attractors: A Nudge Towards Alignment](https://arxiv.org/abs/2510.24797)'
      title: Large Language Models Report Subjective Experience Under Self-Referential
        Processing
      authors:
      - Cameron Berg
      - Diogo de Lucena
      - Judd Rosenblatt
      author_organizations: []
      date: '2025-10-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: sec:White_box_safety_i_e_Interpretability_
  name: White-box safety (i.e. Interpretability)
  header_level: 1
  parent_id: null
  content: This section isn't very conceptually clean. See the [Open Problems](https://arxiv.org/abs/2501.16496)
    paper or [Deepmind](https://arxiv.org/pdf/2504.01849#page=92.33) for strong frames
    which are not useful for descriptive purposes.
  item_type: section
  agenda_attributes: null
  parsing_issues: []
- id: a:Reverse_engineering
  name: Reverse engineering
  header_level: 3
  parent_id: sec:White_box_safety_i_e_Interpretability_
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Decompose a model into its functional, interacting components
      (circuits), formally describe what computation those components perform, and
      validate their causal effects to reverse-engineer the model's internal algorithm.
    theory_of_change: By gaining a mechanical understanding of how a model works (the
      "circuit diagram"), we can predict how models will act in novel situations (generalization),
      and gain the mechanistic knowledge necessary to safely modify an AI's goals
      or internal mechanisms, or allow for high-confidence alignment auditing and
      better feedback to safety researchers.
    see_also:
    - '[ambitious mech interp](https://www.alignmentforum.org/posts/Hy6PX43HGgmfiTaKu/an-ambitious-vision-for-interpretability)'
    orthodox_problems:
    - goals_misgeneralize
    - superintelligence_fool_supervisors
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - Lucius Bushnaq
    - Dan Braun
    - Lee Sharkey
    - Aaron Mueller
    - Atticus Geiger
    - Sheridan Feucht
    - David Bau
    - Yonatan Belinkov
    - Stefan Heimersheim
    - Chris Olah
    - Leo Gao
    estimated_ftes: 100-200
    critiques: '[Interpretability Will Not Reliably Find Deceptive AI](https://www.alignmentforum.org/posts/PwnadG4BFjaER3MGf/interpretability-will-not-reliably-find-deceptive-ai),
      [A Problem to Solve Before Building a Deception Detector](https://www.lesswrong.com/posts/YXNeA3RyRrrRWS37A/a-problem-to-solve-before-building-a-deception-detector),
      [MoSSAIC: AI Safety After Mechanism](https://openreview.net/forum?id=n7WYSJ35FU),
      [The Misguided Quest for Mechanistic AI Interpretability](https://ai-frontiers.org/articles/the-misguided-quest-for-mechanistic-ai-interpretability).
      [Mechanistic?](https://arxiv.org/abs/2410.09087), [Assessing skeptical views
      of interpretability research](https://www.youtube.com/watch?v=woo_J0RKcpQ),
      [Activation space interpretability may be doomed](https://www.lesswrong.com/posts/gYfpPbww3wQRaxAFD/activation-space-interpretability-may-be-doomed),
      [A Pragmatic Vision for Interpretability](https://www.alignmentforum.org/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-interpretability)'
    funded_by: null
    outputs:
    - section_name: In weights-space
      header_level: 4
      original_md: '*In weights-space*'
    - link_url: https://www.neuronpedia.org/graph/info
      link_text: The Circuits Research Landscape
      original_md: '[The Circuits Research Landscape](https://www.neuronpedia.org/graph/info)'
      title: 'The Circuits Research Landscape: Results and Perspectives'
      authors:
      - Jack Lindsey
      - Emmanuel Ameisen
      - Neel Nanda
      - Stepan Shabalin
      - Mateusz Piotrowski
      - Tom McGrath
      - Michael Hanna
      - Owen Lewis
      - Curt Tigges
      - Jack Merullo
      - Connor Watts
      - Gonçalo Paulo
      - Joshua Batson
      - Liv Gorton
      - Elana Simon
      - Max Loeffler
      - Callum McDougall
      - Johnny Lin
      author_organizations:
      - Anthropic
      - Google DeepMind
      - Goodfire AI
      - EleutherAI
      - Decode
      date: '2025-08-01'
      published_year: 2025
      venue: Neuronpedia
      kind: blog_post
    - link_url: https://www.lesswrong.com/posts/roE7SHjFWEoMcGZKd/circuits-in-superposition-compressing-many-small-neural
      link_text: Circuits in Superposition
      original_md: '[Circuits in Superposition](https://www.lesswrong.com/posts/roE7SHjFWEoMcGZKd/circuits-in-superposition-compressing-many-small-neural)'
      title: 'Circuits in Superposition: Compressing many small neural networks into
        one'
      authors:
      - Lucius Bushnaq
      - jake_mendel
      author_organizations:
      - Apollo Research
      date: '2024-10-14'
      published_year: 2024
      venue: LessWrong
      kind: lesswrong
    - link_url: https://www.google.com/url?q=https://www.lesswrong.com/posts/FWkZYQceEzL84tNej/circuits-in-superposition-2-now-with-less-wrong-math&sa=D&source=docs&ust=1765550772146255&usg=AOvVaw334Tyidx2keGCA9vGwQ9a-
      link_text: '2'
      original_md: '[2](https://www.google.com/url?q=https://www.lesswrong.com/posts/FWkZYQceEzL84tNej/circuits-in-superposition-2-now-with-less-wrong-math&sa=D&source=docs&ust=1765550772146255&usg=AOvVaw334Tyidx2keGCA9vGwQ9a-)'
      title: 'Circuits in Superposition 2: Now With Less Wrong Math'
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: error_detected
    - link_url: https://www.lesswrong.com/posts/ZxFchCFJFcgysYsT9/compressed-computation-is-probably-not-computation-in
      link_text: Compressed Computation is (probably) not Computation in Superposition
      original_md: '[Compressed Computation is (probably) not Computation in Superposition](https://www.lesswrong.com/posts/ZxFchCFJFcgysYsT9/compressed-computation-is-probably-not-computation-in)'
      title: 'Error: Content Unavailable (429: Too Many Requests)'
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: error_detected
    - link_url: https://arxiv.org/abs/2504.13151
      link_text: 'MIB: A Mechanistic Interpretability Benchmark'
      original_md: '[MIB: A Mechanistic Interpretability Benchmark](https://arxiv.org/abs/2504.13151)'
      title: 'MIB: A Mechanistic Interpretability Benchmark'
      authors:
      - Aaron Mueller
      - Atticus Geiger
      - Sarah Wiegreffe
      - Dana Arad
      - Iván Arcuschin
      - Adam Belfki
      - Yik Siu Chan
      - Jaden Fiotto-Kaufman
      - Tal Haklay
      - Michael Hanna
      - Jing Huang
      - Rohan Gupta
      - Yaniv Nikankin
      - Hadas Orgad
      - Nikhil Prakash
      - Anja Reusch
      - Aruna Sankaranarayanan
      - Shun Shao
      - Alessandro Stolfo
      - Martin Tutek
      - Amir Zur
      - David Bau
      - Yonatan Belinkov
      author_organizations:
      - Multiple institutions
      date: '2025-06-09'
      published_year: 2025
      venue: ICML 2025
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2508.21258
      link_text: 'RelP: Faithful and Efficient Circuit Discovery in Language Models
        via Relevance Patching'
      original_md: '[RelP: Faithful and Efficient Circuit Discovery in Language Models
        via Relevance Patching](https://arxiv.org/abs/2508.21258)'
      title: 'RelP: Faithful and Efficient Circuit Discovery in Language Models via
        Relevance Patching'
      authors:
      - Farnoush Rezaei Jafari
      - Oliver Eberle
      - Ashkan Khakzar
      - Neel Nanda
      author_organizations: []
      date: '2025-08-28'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.03022
      link_text: The Dual-Route Model of Induction
      original_md: '[The Dual-Route Model of Induction](https://arxiv.org/abs/2504.03022)'
      title: The Dual-Route Model of Induction
      authors:
      - Sheridan Feucht
      - Eric Todd
      - Byron Wallace
      - David Bau
      author_organizations:
      - Northeastern University
      date: '2025-04-03'
      published_year: 2025
      venue: COLM 2025
      kind: paper_published
    - link_url: https://arxiv.org/abs/2504.18274
      link_text: 'Structural Inference: Interpreting Small Language Models with Susceptibilities'
      original_md: '[Structural Inference: Interpreting Small Language Models with
        Susceptibilities](https://arxiv.org/abs/2504.18274)'
      title: 'Structural Inference: Interpreting Small Language Models with Susceptibilities'
      authors:
      - Garrett Baker
      - George Wang
      - Jesse Hoogland
      - Daniel Murfet
      author_organizations: []
      date: '2025-04-25'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://openreview.net/forum?id=dEdS9ao8gN
      link_text: Stochastic Parameter Decomposition
      original_md: '[Stochastic Parameter Decomposition](https://openreview.net/forum?id=dEdS9ao8gN)'
      title: Stochastic Parameter Decomposition
      authors:
      - Dan Braun
      - Lucius Bushnaq
      - Lee Sharkey
      author_organizations: []
      date: '2025-06-26'
      published_year: 2025
      venue: ODYSSEY 2025 Conference
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.14379
      link_text: The Geometry of Self-Verification in a Task-Specific Reasoning Model
      original_md: '[The Geometry of Self-Verification in a Task-Specific Reasoning
        Model](https://arxiv.org/abs/2504.14379)'
      title: The Geometry of Self-Verification in a Task-Specific Reasoning Model
      authors:
      - Andrew Lee
      - Lihao Sun
      - Chris Wendler
      - Fernanda Viégas
      - Martin Wattenberg
      author_organizations:
      - Google Research
      date: '2025-04-19'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.01032
      link_text: Converting MLPs into Polynomials in Closed Form
      original_md: '[Converting MLPs into Polynomials in Closed Form](https://arxiv.org/abs/2502.01032)'
      title: Converting MLPs into Polynomials in Closed Form
      authors:
      - Nora Belrose
      - Alice Rigg
      author_organizations: []
      date: '2025-02-03'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2412.04614
      link_text: Extractive Structures Learned in Pretraining Enable Generalization
        on Finetuned Facts
      original_md: '[Extractive Structures Learned in Pretraining Enable Generalization
        on Finetuned Facts](https://arxiv.org/abs/2412.04614)'
      title: Extractive Structures Learned in Pretraining Enable Generalization on
        Finetuned Facts
      authors:
      - Jiahai Feng
      - Stuart Russell
      - Jacob Steinhardt
      author_organizations:
      - UC Berkeley
      date: '2024-12-05'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2501.14926
      link_text: 'Interpretability in Parameter Space: Minimizing Mechanistic Description
        Length with Attribution-based Parameter Decomposition'
      original_md: '[Interpretability in Parameter Space: Minimizing Mechanistic Description
        Length with Attribution-based Parameter Decomposition](https://arxiv.org/abs/2501.14926)'
      title: 'Interpretability in Parameter Space: Minimizing Mechanistic Description
        Length with Attribution-based Parameter Decomposition'
      authors:
      - Dan Braun
      - Lucius Bushnaq
      - Stefan Heimersheim
      - Jake Mendel
      - Lee Sharkey
      author_organizations: []
      date: '2025-01-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.00194
      link_text: Identifying Sparsely Active Circuits Through Local Loss Landscape
        Decomposition
      original_md: '[Identifying Sparsely Active Circuits Through Local Loss Landscape
        Decomposition](https://arxiv.org/abs/2504.00194)'
      title: Identifying Sparsely Active Circuits Through Local Loss Landscape Decomposition
      authors:
      - Brianna Chrisman
      - Lucius Bushnaq
      - Lee Sharkey
      author_organizations: []
      date: '2025-03-31'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.24256
      link_text: From Memorization to Reasoning in the Spectrum of Loss Curvature
      original_md: '[From Memorization to Reasoning in the Spectrum of Loss Curvature](https://arxiv.org/abs/2510.24256)'
      title: From Memorization to Reasoning in the Spectrum of Loss Curvature
      authors:
      - Jack Merullo
      - Srihita Vatsavaya
      - Lucius Bushnaq
      - Owen Lewis
      author_organizations: []
      date: '2025-10-28'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.10887
      link_text: Generalization or Hallucination? Understanding Out-of-Context Reasoning
        in Transformers
      original_md: '[Generalization or Hallucination? Understanding Out-of-Context
        Reasoning in Transformers](https://arxiv.org/abs/2506.10887)'
      title: Generalization or Hallucination? Understanding Out-of-Context Reasoning
        in Transformers
      authors:
      - Yixiao Huang
      - Hanlin Zhu
      - Tianyu Guo
      - Jiantao Jiao
      - Somayeh Sojoudi
      - Michael I. Jordan
      - Stuart Russell
      - Song Mei
      author_organizations:
      - UC Berkeley
      date: '2025-06-12'
      published_year: 2025
      venue: NeurIPS 2025
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.13913
      link_text: How Do LLMs Perform Two-Hop Reasoning in Context?
      original_md: '[How Do LLMs Perform Two-Hop Reasoning in Context?](https://arxiv.org/abs/2502.13913)'
      title: How Do LLMs Perform Two-Hop Reasoning in Context?
      authors:
      - Tianyu Guo
      - Hanlin Zhu
      - Ruiqi Zhang
      - Jiantao Jiao
      - Song Mei
      - Michael I. Jordan
      - Stuart Russell
      author_organizations:
      - UC Berkeley
      date: '2025-02-19'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.00921
      link_text: 'Blink of an eye: a simple theory for feature localization in generative
        models'
      original_md: '[Blink of an eye: a simple theory for feature localization in
        generative models](https://arxiv.org/abs/2502.00921)'
      title: 'Blink of an eye: a simple theory for feature localization in generative
        models'
      authors:
      - Marvin Li
      - Aayush Karan
      - Sitan Chen
      author_organizations: []
      date: '2025-06-05'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.15811
      link_text: 'On the creation of narrow AI: hierarchy and nonlocality of neural
        network skills'
      original_md: '[On the creation of narrow AI: hierarchy and nonlocality of neural
        network skills](https://arxiv.org/abs/2505.15811)'
      title: 'On the creation of narrow AI: hierarchy and nonlocality of neural network
        skills'
      authors:
      - Eric J. Michaud
      - Asher Parker-Sartori
      - Max Tegmark
      author_organizations: []
      date: '2025-05-21'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - section_name: In activations-space
      header_level: 4
      original_md: '*In activations-space*'
    - link_url: https://arxiv.org/pdf/2504.01871
      link_text: Interpreting Emergent Planning in Model-Free Reinforcement Learning
      original_md: '[Interpreting Emergent Planning in Model-Free Reinforcement Learning](https://arxiv.org/pdf/2504.01871)'
      title: Interpreting Emergent Planning in Model-Free Reinforcement Learning
      authors:
      - Thomas Bush
      - Stephen Chung
      - Usman Anwar
      - Adrià Garriga-Alonso
      - David Krueger
      author_organizations: []
      date: '2025-04-02'
      published_year: 2025
      venue: arXiv (ICLR 2025 oral)
      kind: paper_preprint
    - link_url: https://www.pnas.org/doi/10.1073/pnas.2406675122
      link_text: Bridging the human–AI knowledge gap through concept discovery and
        transfer in AlphaZero
      original_md: '[Bridging the human–AI knowledge gap through concept discovery
        and transfer in AlphaZero](https://www.pnas.org/doi/10.1073/pnas.2406675122)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://lesswrong.com/posts/DJAZHYjWxMrcd2na3/building-and-evaluating-alignment-auditing-agents
      link_text: Building and evaluating alignment auditing agents
      original_md: '[Building and evaluating alignment auditing agents](https://lesswrong.com/posts/DJAZHYjWxMrcd2na3/building-and-evaluating-alignment-auditing-agents)'
      title: Building and evaluating alignment auditing agents
      authors:
      - Sam Marks
      - trentbrick
      - RowanWang
      - Sam Bowman
      - Euan Ong
      - Johannes Treutlein
      - evhub
      author_organizations:
      - Anthropic
      date: '2025-07-24'
      published_year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2505.20896
      link_text: How Do Transformers Learn Variable Binding in Symbolic Programs?
      original_md: '[How Do Transformers Learn Variable Binding in Symbolic Programs?](https://arxiv.org/abs/2505.20896)'
      title: How Do Transformers Learn Variable Binding in Symbolic Programs?
      authors:
      - Yiwei Wu
      - Atticus Geiger
      - Raphaël Millière
      author_organizations: []
      date: '2025-05-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.14223
      link_text: 'Fresh in memory: Training-order recency is linearly encoded in language
        model activations'
      original_md: '[Fresh in memory: Training-order recency is linearly encoded in
        language model activations](https://arxiv.org/abs/2509.14223)'
      title: 'Fresh in memory: Training-order recency is linearly encoded in language
        model activations'
      authors:
      - Dmitrii Krasheninnikov
      - Richard E. Turner
      - David Krueger
      author_organizations:
      - University of Cambridge
      date: '2025-09-17'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.14685
      link_text: Language Models use Lookbacks to Track Beliefs
      original_md: '[Language Models use Lookbacks to Track Beliefs](https://arxiv.org/abs/2505.14685)'
      title: Language Models use Lookbacks to Track Beliefs
      authors:
      - Nikhil Prakash
      - Natalie Shapira
      - Arnab Sen Sharma
      - Christoph Riedl
      - Yonatan Belinkov
      - Tamar Rott Shaham
      - David Bau
      - Atticus Geiger
      author_organizations:
      - Northeastern University
      - Bar-Ilan University
      - Boston University
      date: '2025-05-20'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.01954
      link_text: Constrained belief updates explain geometric structures in transformer
        representations
      original_md: '[Constrained belief updates explain geometric structures in transformer
        representations](https://arxiv.org/abs/2502.01954)'
      title: Constrained belief updates explain geometric structures in transformer
        representations
      authors:
      - Mateusz Piotrowski
      - Paul M. Riechers
      - Daniel Filan
      - Adam S. Shai
      author_organizations: []
      date: '2025-02-04'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.26784
      link_text: LLMs Process Lists With General Filter Heads
      original_md: '[LLMs Process Lists With General Filter Heads](https://arxiv.org/abs/2510.26784)'
      title: LLMs Process Lists With General Filter Heads
      authors:
      - Arnab Sen Sharma
      - Giordano Rogers
      - Natalie Shapira
      - David Bau
      author_organizations:
      - Bau Lab
      date: '2025-10-30'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.00873
      link_text: Language Models Use Trigonometry to Do Addition
      original_md: '[Language Models Use Trigonometry to Do Addition](https://arxiv.org/abs/2502.00873)'
      title: Language Models Use Trigonometry to Do Addition
      authors:
      - Subhash Kantamneni
      - Max Tegmark
      author_organizations: []
      date: '2025-02-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.10138
      link_text: 'Interpreting learned search: finding a transition model and value
        function in an RNN that plays Sokoban'
      original_md: '[Interpreting learned search: finding a transition model and value
        function in an RNN that plays Sokoban](https://arxiv.org/abs/2506.10138)'
      title: 'Interpreting learned search: finding a transition model and value function
        in an RNN that plays Sokoban'
      authors:
      - Mohammad Taufeeque
      - Aaron David Tucker
      - Adam Gleave
      - Adrià Garriga-Alonso
      author_organizations:
      - FAR AI
      date: '2025-06-11'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2412.04703
      link_text: Transformers Struggle to Learn to Search
      original_md: '[Transformers Struggle to Learn to Search](https://arxiv.org/abs/2412.04703)'
      title: Transformers Struggle to Learn to Search
      authors:
      - Abulhair Saparov
      - Srushti Pawar
      - Shreyas Pimpalgaonkar
      - Nitish Joshi
      - Richard Yuanzhe Pang
      - Vishakh Padmakumar
      - Seyed Mehran Kazemi
      - Najoung Kim
      - He He
      author_organizations: []
      date: '2024-12-06'
      published_year: 2024
      venue: ICLR 2025
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2508.17456
      link_text: Adversarial Examples Are Not Bugs, They Are Superposition
      original_md: '[Adversarial Examples Are Not Bugs, They Are Superposition](https://arxiv.org/abs/2508.17456)'
      title: Adversarial Examples Are Not Bugs, They Are Superposition
      authors:
      - Liv Gorton
      - Owen Lewis
      author_organizations: []
      date: '2025-08-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.13898
      link_text: Do Language Models Use Their Depth Efficiently?
      original_md: '[Do Language Models Use Their Depth Efficiently?](https://arxiv.org/abs/2505.13898)'
      title: Do Language Models Use Their Depth Efficiently?
      authors:
      - Róbert Csordás
      - Christopher D. Manning
      - Christopher Potts
      author_organizations:
      - Stanford University
      date: '2025-05-20'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://openreview.net/forum?id=pXlmOmlHJZ
      link_text: 'ICLR: In-Context Learning of Representations'
      original_md: '[ICLR: In-Context Learning of Representations](https://openreview.net/forum?id=pXlmOmlHJZ)'
      title: 'ICLR: In-Context Learning of Representations'
      authors:
      - Core Francisco Park
      - Andrew Lee
      - Ekdeep Singh Lubana
      - Yongyi Yang
      - Maya Okawa
      - Kento Nishi
      - Martin Wattenberg
      - Hidenori Tanaka
      author_organizations: []
      date: '2025-01-22'
      published_year: 2025
      venue: ICLR 2025
      kind: paper_published
    other_attributes: {}
  parsing_issues: []
- id: a:Extracting_latent_knowledge
  name: Extracting latent knowledge
  header_level: 3
  parent_id: sec:White_box_safety_i_e_Interpretability_
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Identify and decoding the "true" beliefs or knowledge represented
      inside a model's activations, even when the model's output is deceptive or false.
    theory_of_change: Powerful models may know things they do not say (e.g. that they
      are currently being tested). If we can translate this latent knowledge directly
      from the model's internals, we can supervise them reliably even when they attempt
      to deceive human evaluators or when the task is too difficult for humans to
      verify directly.
    see_also:
    - a:AI_explanations_of_AIs
    - a:Heuristic_explanations
    - a:Lie_and_deception_detectors
    orthodox_problems:
    - superintelligence_fool_supervisors
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - Bartosz Cywiński
    - Emil Ryd
    - Senthooran Rajamanoharan
    - Alexander Pan
    - Lijie Chen
    - Jacob Steinhardt
    - Javier Ferrando
    - Oscar Obeso
    - Collin Burns
    - Paul Christiano
    estimated_ftes: 20-40
    critiques: '[A Problem to Solve Before Building a Deception Detector](https://www.lesswrong.com/posts/YXNeA3RyRrrRWS37A/a-problem-to-solve-before-building-a-deception-detector)'
    funded_by: Open Philanthropy, Anthropic, NSF, various academic grants
    outputs:
    - link_url: https://www.anthropic.com/research/auditing-hidden-objectives
      link_text: Auditing language models for hidden objectives
      original_md: '[Auditing language models for hidden objectives](https://www.anthropic.com/research/auditing-hidden-objectives)'
      title: Auditing language models for hidden objectives
      authors: []
      author_organizations:
      - Anthropic
      date: '2025-03-13'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.01070
      link_text: Eliciting Secret Knowledge from Language Models
      original_md: '[Eliciting Secret Knowledge from Language Models](https://arxiv.org/abs/2510.01070)'
      title: Eliciting Secret Knowledge from Language Models
      authors:
      - Bartosz Cywiński
      - Emil Ryd
      - Rowan Wang
      - Senthooran Rajamanoharan
      - Neel Nanda
      - Arthur Conmy
      - Samuel Marks
      author_organizations: []
      date: '2025-10-01'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.lesswrong.com/posts/7zhAwcBri7yupStKy/here-s-18-applications-of-deception-probes
      link_text: Here's 18 Applications of Deception Probes
      original_md: '[Here''s 18 Applications of Deception Probes](https://www.lesswrong.com/posts/7zhAwcBri7yupStKy/here-s-18-applications-of-deception-probes)'
      title: Here's 18 Applications of Deception Probes
      authors:
      - Cleo Nardo
      - Avi Parrack
      - jordine
      author_organizations: []
      date: '2025-08-28'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/pdf/2505.14352
      link_text: Towards eliciting latent knowledge from LLMs with mechanistic interpretability
      original_md: '[Towards eliciting latent knowledge from LLMs with mechanistic
        interpretability](https://arxiv.org/pdf/2505.14352)'
      title: Towards eliciting latent knowledge from LLMs with mechanistic interpretability
      authors:
      - Bartosz Cywiński
      - Emil Ryd
      - Senthooran Rajamanoharan
      - Neel Nanda
      author_organizations: []
      date: '2025-05-20'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://joss.theoj.org/papers/10.21105/joss.06511
      link_text: 'CCS-Lib: A Python package to elicit latent knowledge from LLMs'
      original_md: '[CCS-Lib: A Python package to elicit latent knowledge from LLMs](https://joss.theoj.org/papers/10.21105/joss.06511)'
      title: 'CCS-Lib: A Python package to elicit latent knowledge from LLMs'
      authors:
      - Walter Laurito
      - Nora Belrose
      - Alex Mallen
      - Kay Kozaronek
      - Fabien Roger
      - Christy Koh
      - James Chua
      - Jonathan Ng
      - Alexander Wan
      - Reagan Lee
      - Ben W.
      - Kyle O'Brien
      - Augustas Macijauskas
      - Eric Mungai Kinuthia
      - Marius Pl
      - Waree Sethapun
      - Kaarel Hänni
      author_organizations:
      - Cadenza Labs
      date: '2025-10-21'
      published_year: 2025
      venue: Journal of Open Source Software
      kind: paper_published
    - link_url: https://arxiv.org/abs/2509.10625
      link_text: 'No Answer Needed: Predicting LLM Answer Accuracy from Question-Only
        Linear Probes'
      original_md: '[No Answer Needed: Predicting LLM Answer Accuracy from Question-Only
        Linear Probes](https://arxiv.org/abs/2509.10625)'
      title: 'No Answer Needed: Predicting LLM Answer Accuracy from Question-Only
        Linear Probes'
      authors:
      - Iván Vicente Moreno Cencerrado
      - Arnau Padrés Masdemont
      - Anton Gonzalvez Hawthorne
      - David Demitri Africa
      - Lorenzo Pacchiardi
      author_organizations: []
      date: '2025-09-12'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.04909
      link_text: 'When Thinking LLMs Lie: Unveiling the Strategic Deception in Representations
        of Reasoning Models'
      original_md: '[When Thinking LLMs Lie: Unveiling the Strategic Deception in
        Representations of Reasoning Models](https://arxiv.org/abs/2506.04909)'
      title: 'When Thinking LLMs Lie: Unveiling the Strategic Deception in Representations
        of Reasoning Models'
      authors:
      - Kai Wang
      - Yihao Zhang
      - Meng Sun
      author_organizations: []
      date: '2025-06-05'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2508.19505
      link_text: 'Caught in the Act: a mechanistic approach to detecting deception'
      original_md: '[Caught in the Act: a mechanistic approach to detecting deception](https://arxiv.org/abs/2508.19505)'
      title: 'Caught in the Act: a mechanistic approach to detecting deception'
      authors:
      - Gerard Boxo
      - Ryan Socha
      - Daniel Yoo
      - Shivam Raval
      author_organizations: []
      date: '2025-08-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.22149
      link_text: When Truthful Representations Flip Under Deceptive Instructions?
      original_md: '[When Truthful Representations Flip Under Deceptive Instructions?](https://arxiv.org/abs/2507.22149)'
      title: When Truthful Representations Flip Under Deceptive Instructions?
      authors:
      - Xianxuan Long
      - Yao Fu
      - Runchao Li
      - Mu Sheng
      - Haotian Yu
      - Xiaotian Han
      - Pan Li
      author_organizations: []
      date: '2025-07-29'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues:
  - 'One-sentence summary has minor grammatical issue: ''Identify and decoding'' should
    likely be ''Identifying and decoding'''
- id: a:Lie_and_deception_detectors
  name: Lie and deception detectors
  header_level: 3
  parent_id: sec:White_box_safety_i_e_Interpretability_
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Detect when a model is being deceptive or lying by building
      white- or black-box detectors. Some work below requires intent in their definition,
      while other work focuses only on whether the model states something it believes
      to be false, regardless of intent.
    theory_of_change: Such detectors could flag suspicious behavior during evaluations
      or deployment, augment training to reduce deception, or audit models pre-deployment.
      Specific applications include alignment evaluations (e.g. by validating answers
      to introspective questions), safeguarding evaluations (catching models that
      "sandbag", that is, strategically underperform to pass capability tests), and
      large-scale deployment monitoring. An honest version of a model could also provide
      oversight during training or detect cases where a model behaves in ways it understands
      are unsafe.
    see_also:
    - a:Reverse_engineering
    - a:AI_deception_evals
    - a:Sandbagging_evals
    orthodox_problems: []
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - Cadenza
    - Sam Marks
    - Rowan Wang
    - Kieron Kretschmar
    - Sharan Maiya
    - Walter Laurito
    - Chris Cundy
    - Adam Gleave
    - Aviel Parrack
    - Stefan Heimersheim
    - Carlo Attubato
    - Joseph Bloom
    - Jordan Taylor
    - Alex McKenzie
    - Urja Pawar
    - Lewis Smith
    - Bilal Chughtai
    - Neel Nanda
    estimated_ftes: 10-50
    critiques: difficult to determine if behavior is strategic deception or only low
      level "reflexive" actions; Unclear if a model roleplaying a liar has deceptive
      intent. [How are intentional descriptions (like deception) related to algorithmic
      ones (like understanding the mechanisms models use)?](https://www.lesswrong.com/posts/YXNeA3RyRrrRWS37A/a-problem-to-solve-before-building-a-deception-detector),
      [Is This Lie Detector Really Just a Lie Detector? An Investigation of LLM Probe
      Specificity](https://www.lesswrong.com/posts/5dkhdRMypeuyoXfmb/is-this-lie-detector-really-just-a-lie-detector-an),
      [Herrmann](https://www.lesswrong.com/posts/bCQbSFrnnAk7CJNpM/still-no-lie-detector-for-llms),
      [Smith and Chughtai](https://arxiv.org/abs/2511.22662)
    funded_by: Anthropic, Deepmind, UK AISI, Coefficient Giving
    outputs:
    - link_url: https://www.lesswrong.com/posts/9pGbTz6c78PGwJein/detecting-strategic-deception-using-linear-probes
      link_text: Detecting Strategic Deception Using Linear Probes
      original_md: '[Detecting Strategic Deception Using Linear Probes](https://www.lesswrong.com/posts/9pGbTz6c78PGwJein/detecting-strategic-deception-using-linear-probes)'
      title: Detecting Strategic Deception Using Linear Probes
      authors:
      - Nicholas Goldowsky-Dill
      - Bilal Chughtai
      - Stefan Heimersheim
      - Marius Hobbhahn
      author_organizations:
      - Apollo Research
      date: '2025-02-06'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.lesswrong.com/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging
      link_text: Whitebox detection of sandbagging model organisms
      original_md: '[Whitebox detection of sandbagging model organisms](https://www.lesswrong.com/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging)'
      title: White Box Control at UK AISI - Update on Sandbagging Investigations
      authors:
      - Joseph Bloom
      - Jordan Taylor
      - Connor Kissane
      - Sid Black
      - Jacob Merizian
      - Alex Zelenka-Martin
      - Jacob Arbeid
      - Ben Millwood
      - Alan Cooney
      author_organizations:
      - UK AISI
      date: '2025-07-10'
      published_year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - link_url: https://www.lesswrong.com/posts/eaEqAzGN3uJfpfGoc/trusted-monitoring-but-with-deception-probes
      link_text: Benchmarking deception probes for trusted monitoring
      original_md: '[Benchmarking deception probes for trusted monitoring](https://www.lesswrong.com/posts/eaEqAzGN3uJfpfGoc/trusted-monitoring-but-with-deception-probes)'
      title: Trusted monitoring, but with deception probes.
      authors:
      - Avi Parrack
      - StefanHex
      - Cleo Nardo
      author_organizations:
      - Stanford University
      date: '2024-07-23'
      published_year: 2024
      venue: LessWrong
      kind: lesswrong
    - link_url: https://www.lesswrong.com/posts/7zhAwcBri7yupStKy/18-applications-of-deception-probes
      link_text: 18 Applications of Deception Probes
      original_md: '[18 Applications of Deception Probes](https://www.lesswrong.com/posts/7zhAwcBri7yupStKy/18-applications-of-deception-probes)'
      title: Here's 18 Applications of Deception Probes
      authors:
      - Cleo Nardo
      - Avi Parrack
      - jordine
      author_organizations: []
      date: '2025-08-28'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://alignment.anthropic.com/2025/honesty-elicitation/
      link_text: Evaluating honesty and lie detection techniques on a diverse suite
        of dishonest models
      original_md: '[Evaluating honesty and lie detection techniques on a diverse
        suite of dishonest models](https://alignment.anthropic.com/2025/honesty-elicitation/)'
      title: Evaluating honesty and lie detection techniques on a diverse suite of
        dishonest models
      authors:
      - Rowan Wang
      - Johannes Treutlein
      - Fabien Roger
      - Evan Hubinger
      - Sam Marks
      author_organizations:
      - Anthropic
      date: '2025-11-25'
      published_year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - link_url: https://arxiv.org/abs/2508.19505
      link_text: 'Caught in the Act: a mechanistic approach to detecting deception'
      original_md: '[Caught in the Act: a mechanistic approach to detecting deception](https://arxiv.org/abs/2508.19505)'
      title: 'Caught in the Act: a mechanistic approach to detecting deception'
      authors:
      - Gerard Boxo
      - Ryan Socha
      - Daniel Yoo
      - Shivam Raval
      author_organizations: []
      date: '2025-08-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.13787
      link_text: Preference Learning with Lie Detectors can Induce Honesty or Evasion
      original_md: '[Preference Learning with Lie Detectors can Induce Honesty or
        Evasion](https://arxiv.org/abs/2505.13787)'
      title: Preference Learning with Lie Detectors can Induce Honesty or Evasion
      authors:
      - Chris Cundy
      - Adam Gleave
      author_organizations: []
      date: '2025-05-20'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.10805
      link_text: Detecting High-Stakes Interactions with Activation Probes
      original_md: '[Detecting High-Stakes Interactions with Activation Probes](https://arxiv.org/abs/2506.10805)'
      title: Detecting High-Stakes Interactions with Activation Probes
      authors:
      - Alex McKenzie
      - Urja Pawar
      - Phil Blandfort
      - William Bankes
      - David Krueger
      - Ekdeep Singh Lubana
      - Dmitrii Krasheninnikov
      author_organizations: []
      date: '2025-06-12'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.alignmentforum.org/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging
      link_text: White Box Control at UK AISI - Update on Sandbagging Investigations
      original_md: '[White Box Control at UK AISI \- Update on Sandbagging Investigations](https://www.alignmentforum.org/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging)'
      title: White Box Control at UK AISI - Update on Sandbagging Investigations
      authors:
      - Joseph Bloom
      - Jordan Taylor
      - Connor Kissane
      - Sid Black
      - merizian
      - alexdzm
      - jacoba
      - Ben Millwood
      - Alan Cooney
      author_organizations:
      - UK AISI
      date: '2025-07-10'
      published_year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - link_url: https://arxiv.org/html/2511.16035v1
      link_text: 'Liars'' Bench: Evaluating Lie Detectors for Language Models'
      original_md: '[Liars'' Bench: Evaluating Lie Detectors for Language Models](https://arxiv.org/html/2511.16035v1)'
      title: 'Liars'' Bench: Evaluating Lie Detectors for Language Models'
      authors:
      - Kieron Kretschmar
      - Walter Laurito
      - Sharan Maiya
      - Samuel Marks
      author_organizations:
      - Cadenza Labs
      - Anthropic
      - FZI
      - University of Cambridge
      date: '2025-11-20'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.lesswrong.com/posts/gRc8KL2HLtKkFmNPr/among-us-a-sandbox-for-agentic-deception
      link_text: Probes and SAEs do well on Among Us benchmark
      original_md: '[Probes and SAEs do well on Among Us benchmark](https://www.lesswrong.com/posts/gRc8KL2HLtKkFmNPr/among-us-a-sandbox-for-agentic-deception)'
      title: 'Among Us: A Sandbox for Agentic Deception'
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: blocked
    other_attributes: {}
  parsing_issues:
  - Broad approach field says 'cognitive' - mapped to 'cognitivist_science' and normalized
    to 'cognitivist science'
- id: a:Model_diffing
  name: Model diffing
  header_level: 3
  parent_id: sec:White_box_safety_i_e_Interpretability_
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Understand what happens when a model is finetuned, what
      the "diff" between the finetuned and the original model consists in.
    theory_of_change: By identifying the mechanistic differences between a base model
      and its fine-tune (e.g., after RLHF), maybe we can verify that safety behaviors
      are robustly "internalized" rather than superficially patched, and detect if
      dangerous capabilities or deceptive alignment have been introduced without needing
      to re-analyze the entire model. The diff is also much smaller, since most parameters
      don't change, which means you can use heavier methods on them.
    see_also:
    - a:Sparse_Coding
    - a:Reverse_engineering
    orthodox_problems:
    - value_fragile
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitive
    some_names:
    - Julian Minder
    - Clément Dumas
    - Neel Nanda
    - Trenton Bricken
    - Jack Lindsey
    estimated_ftes: 10-30
    critiques: null
    funded_by: various academic groups, Anthropic, Google DeepMind
    outputs:
    - link_url: https://www.lesswrong.com/posts/xmpauEXEerzYcJKNm/what-we-learned-trying-to-diff-base-and-chat-models-and-why
      link_text: What We Learned Trying to Diff Base and Chat Models (And Why It Matters)
      original_md: '[What We Learned Trying to Diff Base and Chat Models (And Why
        It Matters)](https://www.lesswrong.com/posts/xmpauEXEerzYcJKNm/what-we-learned-trying-to-diff-base-and-chat-models-and-why)'
      title: What We Learned Trying to Diff Base and Chat Models (And Why It Matters)
      authors:
      - Clément Dumas
      - Julian Minder
      - Neel Nanda
      author_organizations:
      - MATS Program
      date: '2025-06-30'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://www.lesswrong.com/posts/srt6JXsRMtmqAJavD/open-source-replication-of-anthropic-s-crosscoder-paper-for
      link_text: Open Source Replication of Anthropic's Crosscoder paper for model-diffing
      original_md: '[Open Source Replication of Anthropic''s Crosscoder paper for
        model-diffing](https://www.lesswrong.com/posts/srt6JXsRMtmqAJavD/open-source-replication-of-anthropic-s-crosscoder-paper-for)'
      title: Open Source Replication of Anthropic's Crosscoder paper for model-diffing
      authors:
      - Connor Kissane
      - robertzk
      - Arthur Conmy
      - Neel Nanda
      author_organizations: []
      date: '2024-10-27'
      published_year: 2024
      venue: LessWrong
      kind: lesswrong
    - link_url: https://openreview.net/forum?id=ZB84SvrZB8%20
      link_text: 'Cross-Architecture Model Diffing with Crosscoders: Unsupervised
        Discovery of Differences Between LLMs'
      original_md: '[Cross-Architecture Model Diffing with Crosscoders: Unsupervised
        Discovery of Differences Between LLMs](https://openreview.net/forum?id=ZB84SvrZB8%20)'
      title: 'Error: Content Not Found'
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: OpenReview
      kind: error_detected
    - link_url: https://www.goodfire.ai/research/model-diff-amplification#
      link_text: Discovering Undesired Rare Behaviors via Model Diff Amplification
      original_md: '[Discovering Undesired Rare Behaviors via Model Diff Amplification](https://www.goodfire.ai/research/model-diff-amplification#)'
      title: Discovering Undesired Rare Behaviors via Model Diff Amplification
      authors:
      - Santiago Aranguri
      - Thomas McGrath
      author_organizations:
      - Goodfire
      - NYU
      date: '2025-08-21'
      published_year: 2025
      venue: Goodfire Research
      kind: blog_post
    - link_url: https://arxiv.org/abs/2504.02922
      link_text: Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning
      original_md: '[Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning](https://arxiv.org/abs/2504.02922)'
      title: Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning
      authors:
      - Julian Minder
      - Clément Dumas
      - Caden Juang
      - Bilal Chugtai
      - Neel Nanda
      author_organizations: []
      date: '2025-04-03'
      published_year: 2025
      venue: NeurIPS 2025
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.19823
      link_text: Persona Features Control Emergent Misalignment
      original_md: '[Persona Features Control Emergent Misalignment](https://arxiv.org/abs/2506.19823)'
      title: Persona Features Control Emergent Misalignment
      authors:
      - Miles Wang
      - Tom Dupré la Tour
      - Olivia Watkins
      - Alex Makelov
      - Ryan A. Chi
      - Samuel Miserendino
      - Jeffrey Wang
      - Achyuta Rajaram
      - Johannes Heidecke
      - Tejal Patwardhan
      - Dan Mossing
      author_organizations: []
      date: '2025-06-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.13900
      link_text: Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences
      original_md: '[Narrow Finetuning Leaves Clearly Readable Traces in Activation
        Differences](https://arxiv.org/abs/2510.13900)'
      title: Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences
      authors:
      - Julian Minder
      - Clément Dumas
      - Stewart Slocum
      - Helena Casademunt
      - Cameron Holmes
      - Robert West
      - Neel Nanda
      author_organizations:
      - Google DeepMind
      date: '2025-10-14'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://transformer-circuits.pub/2025/crosscoder-diffing-update/index.html
      link_text: Insights on Crosscoder Model Diffing
      original_md: '[Insights on Crosscoder Model Diffing](https://transformer-circuits.pub/2025/crosscoder-diffing-update/index.html)'
      title: Insights on Crosscoder Model Diffing
      authors:
      - Siddharth Mishra-Sharma
      - Trenton Bricken
      - Jack Lindsey
      - Adam Jermyn
      - Jonathan Marcus
      - Kelley Rivoire
      - Christopher Olah
      - Thomas Henighan
      author_organizations:
      - Anthropic
      date: null
      published_year: null
      venue: Transformer Circuits Thread
      kind: blog_post
    - link_url: https://github.com/science-of-finetuning/diffing-toolkit%20
      link_text: 'Diffing Toolkit: Model Comparison and Analysis Framework'
      original_md: '[Diffing Toolkit: Model Comparison and Analysis Framework](https://github.com/science-of-finetuning/diffing-toolkit%20)'
      title: diffing-toolkit (GitHub 404 Error)
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: GitHub
      kind: error_detected
    other_attributes: {}
  parsing_issues: []
- id: a:Sparse_Coding
  name: Sparse Coding
  header_level: 3
  parent_id: sec:White_box_safety_i_e_Interpretability_
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Decompose the polysemantic activations of the residual stream
      into a sparse linear combination of monosemantic "features" which correspond
      to interpretable concepts.
    theory_of_change: Get a principled decomposition of an LLM's activation into atomic
      components → identify deception and other misbehaviors.
    see_also:
    - sec:Concept_based_interpretability
    - a:Reverse_engineering
    orthodox_problems:
    - value_fragile
    - goals_misgeneralize
    - superintelligence_fool_supervisors
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: null
    broad_approach_text: engineering / cognitive
    some_names:
    - Leo Gao
    - Dan Mossing
    - Emmanuel Ameisen
    - Jack Lindsey
    - Adam Pearce
    - Thomas Heap
    - Abhinav Menon
    - Kenny Peng
    - Tim Lawson
    estimated_ftes: 50-100
    critiques: '[Sparse Autoencoders Can Interpret Randomly Initialized Transformers](https://arxiv.org/abs/2501.17727),
      [The Sparse Autoencoders bubble has popped, but they are still promising](https://agarriga.substack.com/p/the-sparse-autoencoders-bubble-has),
      [Negative Results for SAEs On Downstream Tasks and Deprioritising SAE Research](https://www.alignmentforum.org/posts/4uXCAJNuPKtKBsi28/),
      [Sparse Autoencoders Trained on the Same Data Learn Different Features](https://arxiv.org/pdf/2501.16615),
      [Why Not Just Train For Interpretability?](https://www.lesswrong.com/posts/2HbgHwdygH6yeHKKq/why-not-just-train-for-interpretability)'
    funded_by: everyone, roughly. Frontier labs, LTFF, Coefficient Giving, etc.
    outputs:
    - link_url: https://arxiv.org/abs/2504.02922
      link_text: Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning
      original_md: '[Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning](https://arxiv.org/abs/2504.02922)'
      title: Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning
      authors:
      - Julian Minder
      - Clément Dumas
      - Caden Juang
      - Bilal Chugtai
      - Neel Nanda
      author_organizations: []
      date: '2025-04-03'
      published_year: 2025
      venue: NeurIPS 2025
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.14257
      link_text: Do I Know This Entity? Knowledge Awareness and Hallucinations in
        Language Models
      original_md: '[Do I Know This Entity? Knowledge Awareness and Hallucinations
        in Language Models](https://arxiv.org/abs/2411.14257)'
      title: Do I Know This Entity? Knowledge Awareness and Hallucinations in Language
        Models
      authors:
      - Javier Ferrando
      - Oscar Obeso
      - Senthooran Rajamanoharan
      - Neel Nanda
      author_organizations: []
      date: '2024-11-21'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://transformer-circuits.pub/2025/attribution-graphs/methods.html
      link_text: 'Circuit Tracing: Revealing Computational Graphs in Language Models'
      original_md: '[Circuit Tracing: Revealing Computational Graphs in Language Models](https://transformer-circuits.pub/2025/attribution-graphs/methods.html)'
      title: 'Circuit Tracing: Revealing Computational Graphs in Language Models'
      authors:
      - Emmanuel Ameisen
      - Jack Lindsey
      - Adam Pearce
      - Wes Gurnee
      - Nicholas L. Turner
      - Brian Chen
      - Craig Citro
      - David Abrahams
      - Shan Carter
      - Basil Hosmer
      - Jonathan Marcus
      - Michael Sklar
      - Adly Templeton
      - Trenton Bricken
      - Callum McDougall
      - Hoagy Cunningham
      - Thomas Henighan
      - Adam Jermyn
      - Andy Jones
      - Andrew Persic
      - Zhenyi Qi
      - T. Ben Thompson
      - Sam Zimmerman
      - Kelley Rivoire
      - Thomas Conerly
      - Chris Olah
      - Joshua Batson
      author_organizations:
      - Anthropic
      date: '2025-03-27'
      published_year: 2025
      venue: Transformer Circuits Thread
      kind: blog_post
    - link_url: https://arxiv.org/abs/2504.02821
      link_text: Sparse Autoencoders Learn Monosemantic Features in Vision-Language
        Models
      original_md: '[Sparse Autoencoders Learn Monosemantic Features in Vision-Language
        Models](https://arxiv.org/abs/2504.02821)'
      title: Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models
      authors:
      - Mateusz Pach
      - Shyamgopal Karthik
      - Quentin Bouniot
      - Serge Belongie
      - Zeynep Akata
      author_organizations: []
      date: '2025-04-03'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.18878
      link_text: 'I Have Covered All the Bases Here: Interpreting Reasoning Features
        in Large Language Models via Sparse Autoencoders'
      original_md: '[I Have Covered All the Bases Here: Interpreting Reasoning Features
        in Large Language Models via Sparse Autoencoders](https://arxiv.org/abs/2503.18878)'
      title: 'I Have Covered All the Bases Here: Interpreting Reasoning Features in
        Large Language Models via Sparse Autoencoders'
      authors:
      - Andrey Galichin
      - Alexey Dontsov
      - Polina Druzhinina
      - Anton Razzhigaev
      - Oleg Y. Rogov
      - Elena Tutubalina
      - Ivan Oseledets
      author_organizations:
      - AIRI Institute
      date: '2025-03-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.04878
      link_text: Sparse Autoencoders Do Not Find Canonical Units of Analysis
      original_md: '[Sparse Autoencoders Do Not Find Canonical Units of Analysis](https://arxiv.org/abs/2502.04878)'
      title: Sparse Autoencoders Do Not Find Canonical Units of Analysis
      authors:
      - Patrick Leask
      - Bart Bussmann
      - Michael Pearce
      - Joseph Bloom
      - Curt Tigges
      - Noura Al Moubayed
      - Lee Sharkey
      - Neel Nanda
      author_organizations: []
      date: '2025-02-07'
      published_year: 2025
      venue: arXiv (accepted to ICLR 2025)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2501.18823
      link_text: Transcoders Beat Sparse Autoencoders for Interpretability
      original_md: '[Transcoders Beat Sparse Autoencoders for Interpretability](https://arxiv.org/abs/2501.18823)'
      title: Transcoders Beat Sparse Autoencoders for Interpretability
      authors:
      - Gonçalo Paulo
      - Stepan Shabalin
      - Nora Belrose
      author_organizations: []
      date: '2025-01-31'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.10920
      link_text: Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative
        Matrix Factorization
      original_md: '[Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative
        Matrix Factorization](https://arxiv.org/abs/2506.10920)'
      title: Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative
        Matrix Factorization
      authors:
      - Or Shafran
      - Atticus Geiger
      - Mor Geva
      author_organizations: []
      date: '2025-06-12'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2508.13650
      link_text: 'CRISP: Persistent Concept Unlearning via Sparse Autoencoders'
      original_md: '[CRISP: Persistent Concept Unlearning via Sparse Autoencoders](https://arxiv.org/abs/2508.13650)'
      title: 'CRISP: Persistent Concept Unlearning via Sparse Autoencoders'
      authors:
      - Tomer Ashuach
      - Dana Arad
      - Aaron Mueller
      - Martin Tutek
      - Yonatan Belinkov
      author_organizations: []
      date: '2025-08-19'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.07775
      link_text: The Unintended Trade-off of AI Alignment:Balancing Hallucination
        Mitigation and Safety in LLMs
      original_md: '[The Unintended Trade-off of AI Alignment:Balancing Hallucination
        Mitigation and Safety in LLMs](https://arxiv.org/abs/2510.07775)'
      title: The Unintended Trade-off of AI Alignment:Balancing Hallucination Mitigation
        and Safety in LLMs
      authors:
      - Omar Mahmoud
      - Ali Khalil
      - Buddhika Laknath Semage
      - Thommen George Karimpanal
      - Santu Rana
      author_organizations: []
      date: '2025-10-09'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.13756
      link_text: Scaling sparse feature circuit finding for in-context learning
      original_md: '[Scaling sparse feature circuit finding for in-context learning](https://arxiv.org/abs/2504.13756)'
      title: Scaling sparse feature circuit finding for in-context learning
      authors:
      - Dmitrii Kharlapenko
      - Stepan Shabalin
      - Fazl Barez
      - Arthur Conmy
      - Neel Nanda
      author_organizations: []
      date: '2025-04-18'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.17547
      link_text: Learning Multi-Level Features with Matryoshka Sparse Autoencoders
      original_md: '[Learning Multi-Level Features with Matryoshka Sparse Autoencoders](https://arxiv.org/abs/2503.17547)'
      title: Learning Multi-Level Features with Matryoshka Sparse Autoencoders
      authors:
      - Bart Bussmann
      - Noa Nabeshima
      - Adam Karvonen
      - Neel Nanda
      author_organizations: []
      date: '2025-03-21'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.16681
      link_text: Are Sparse Autoencoders Useful? A Case Study in Sparse Probing
      original_md: '[Are Sparse Autoencoders Useful? A Case Study in Sparse Probing](https://arxiv.org/abs/2502.16681)'
      title: Are Sparse Autoencoders Useful? A Case Study in Sparse Probing
      authors:
      - Subhash Kantamneni
      - Joshua Engels
      - Senthooran Rajamanoharan
      - Max Tegmark
      - Neel Nanda
      author_organizations: []
      date: '2025-02-23'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2501.16615
      link_text: Sparse Autoencoders Trained on the Same Data Learn Different Features
      original_md: '[Sparse Autoencoders Trained on the Same Data Learn Different
        Features](https://arxiv.org/abs/2501.16615)'
      title: Sparse Autoencoders Trained on the Same Data Learn Different Features
      authors:
      - Gonçalo Paulo
      - Nora Belrose
      author_organizations: []
      date: '2025-01-28'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.26202
      link_text: What's In My Human Feedback? Learning Interpretable Descriptions
        of Preference Data
      original_md: '[What''s In My Human Feedback? Learning Interpretable Descriptions
        of Preference Data](https://arxiv.org/abs/2510.26202)'
      title: What's In My Human Feedback? Learning Interpretable Descriptions of Preference
        Data
      authors:
      - Rajiv Movva
      - Smitha Milli
      - Sewon Min
      - Emma Pierson
      author_organizations: []
      date: '2025-10-30'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2511.01836
      link_text: 'Priors in Time: Missing Inductive Biases for Language Model Interpretability'
      original_md: '[Priors in Time: Missing Inductive Biases for Language Model Interpretability](https://arxiv.org/abs/2511.01836)'
      title: 'Priors in Time: Missing Inductive Biases for Language Model Interpretability'
      authors:
      - Ekdeep Singh Lubana
      - Can Rager
      - Sai Sumedh R. Hindupur
      - Valerie Costa
      - Greta Tuckute
      - Oam Patel
      - Sonia Krishna Murthy
      - Thomas Fel
      - Daniel Wurgaft
      - Eric J. Bigelow
      - Johnny Lin
      - Demba Ba
      - Martin Wattenberg
      - Fernanda Viegas
      - Melanie Weber
      - Aaron Mueller
      author_organizations: []
      date: '2025-11-03'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.17769
      link_text: 'Inference-Time Decomposition of Activations (ITDA): A Scalable Approach
        to Interpreting Large Language Models'
      original_md: '[Inference-Time Decomposition of Activations (ITDA): A Scalable
        Approach to Interpreting Large Language Models](https://arxiv.org/abs/2505.17769)'
      title: 'Inference-Time Decomposition of Activations (ITDA): A Scalable Approach
        to Interpreting Large Language Models'
      authors:
      - Patrick Leask
      - Neel Nanda
      - Noura Al Moubayed
      author_organizations:
      - Independent
      date: '2025-05-23'
      published_year: 2025
      venue: ICML 2025
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.25596
      link_text: Binary Sparse Coding for Interpretability
      original_md: '[Binary Sparse Coding for Interpretability](https://arxiv.org/abs/2509.25596)'
      title: Binary Sparse Coding for Interpretability
      authors:
      - Lucia Quirke
      - Stepan Shabalin
      - Nora Belrose
      author_organizations: []
      date: '2025-09-29'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/PkeB4TLxgaNnSmddg/scaling-sparse-feature-circuit-finding-to-gemma-9b
      link_text: Scaling Sparse Feature Circuit Finding to Gemma 9B
      original_md: '[Scaling Sparse Feature Circuit Finding to Gemma 9B](https://lesswrong.com/posts/PkeB4TLxgaNnSmddg/scaling-sparse-feature-circuit-finding-to-gemma-9b)'
      title: Scaling Sparse Feature Circuit Finding to Gemma 9B
      authors:
      - Diego Caples
      - Jatin Nainani
      - CallumMcDougall
      - rrenaud
      author_organizations:
      - MATS Program
      date: '2025-01-10'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2501.18838
      link_text: Partially Rewriting a Transformer in Natural Language
      original_md: '[Partially Rewriting a Transformer in Natural Language](https://arxiv.org/abs/2501.18838)'
      title: Partially Rewriting a Transformer in Natural Language
      authors:
      - Gonçalo Paulo
      - Nora Belrose
      author_organizations: []
      date: '2025-01-31'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.15679
      link_text: Dense SAE Latents Are Features, Not Bugs
      original_md: '[Dense SAE Latents Are Features, Not Bugs](https://arxiv.org/abs/2506.15679)'
      title: Dense SAE Latents Are Features, Not Bugs
      authors:
      - Xiaoqing Sun
      - Alessandro Stolfo
      - Joshua Engels
      - Ben Wu
      - Senthooran Rajamanoharan
      - Mrinmaya Sachan
      - Max Tegmark
      author_organizations:
      - MIT
      - ETH Zurich
      date: '2025-06-18'
      published_year: 2025
      venue: arXiv (NeurIPS 2025)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.18895
      link_text: Evaluating Sparse Autoencoders on Targeted Concept Erasure Tasks
      original_md: '[Evaluating Sparse Autoencoders on Targeted Concept Erasure Tasks](https://arxiv.org/abs/2411.18895)'
      title: Evaluating Sparse Autoencoders on Targeted Concept Erasure Tasks
      authors:
      - Adam Karvonen
      - Can Rager
      - Samuel Marks
      - Neel Nanda
      author_organizations: []
      date: '2024-11-28'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.08473
      link_text: Evaluating SAE interpretability without explanations
      original_md: '[Evaluating SAE interpretability without explanations](https://arxiv.org/abs/2507.08473)'
      title: Evaluating SAE interpretability without explanations
      authors:
      - Gonçalo Paulo
      - Nora Belrose
      author_organizations: []
      date: '2025-07-11'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.08192
      link_text: 'SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder Guardrails
        for Precision Unlearning in LLMs'
      original_md: '[SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder Guardrails
        for Precision Unlearning in LLMs](https://arxiv.org/abs/2504.08192)'
      title: 'SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder Guardrails for
        Precision Unlearning in LLMs'
      authors:
      - Aashiq Muhamed
      - Jacopo Bonato
      - Mona Diab
      - Virginia Smith
      author_organizations: []
      date: '2025-04-11'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.09532
      link_text: 'SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language
        Model Interpretability'
      original_md: '[SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in
        Language Model Interpretability](https://arxiv.org/abs/2503.09532)'
      title: 'SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language
        Model Interpretability'
      authors:
      - Adam Karvonen
      - Can Rager
      - Johnny Lin
      - Curt Tigges
      - Joseph Bloom
      - David Chanin
      - Yeu-Tong Lau
      - Eoin Farrell
      - Callum McDougall
      - Kola Ayonrinde
      - Demian Till
      - Matthew Wearden
      - Arthur Conmy
      - Samuel Marks
      - Neel Nanda
      author_organizations: []
      date: '2025-06-04'
      published_year: 2025
      venue: arXiv (accepted to ICML 2025)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.20063
      link_text: SAEs Are Good for Steering \-- If You Select the Right Features
      original_md: '[SAEs Are Good for Steering \-- If You Select the Right Features](https://arxiv.org/abs/2505.20063)'
      title: SAEs Are Good for Steering -- If You Select the Right Features
      authors:
      - Dana Arad
      - Aaron Mueller
      - Yonatan Belinkov
      author_organizations: []
      date: '2025-05-26'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.04706
      link_text: 'Line of Sight: On Linear Representations in VLLMs'
      original_md: '[Line of Sight: On Linear Representations in VLLMs](https://arxiv.org/abs/2506.04706)'
      title: 'Line of Sight: On Linear Representations in VLLMs'
      authors:
      - Achyuta Rajaram
      - Sarah Schwettmann
      - Jacob Andreas
      - Arthur Conmy
      author_organizations:
      - MIT
      - Independent
      date: '2025-06-05'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2501.19406
      link_text: Low-Rank Adapting Models for Sparse Autoencoders
      original_md: '[Low-Rank Adapting Models for Sparse Autoencoders](https://arxiv.org/abs/2501.19406)'
      title: Low-Rank Adapting Models for Sparse Autoencoders
      authors:
      - Matthew Chen
      - Joshua Engels
      - Max Tegmark
      author_organizations:
      - MIT
      date: '2025-01-31'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2501.08319
      link_text: Enhancing Automated Interpretability with Output-Centric Feature
        Descriptions
      original_md: '[Enhancing Automated Interpretability with Output-Centric Feature
        Descriptions](https://arxiv.org/abs/2501.08319)'
      title: Enhancing Automated Interpretability with Output-Centric Feature Descriptions
      authors:
      - Yoav Gur-Arieh
      - Roy Mayan
      - Chen Agassy
      - Atticus Geiger
      - Mor Geva
      author_organizations: []
      date: '2025-01-14'
      published_year: 2025
      venue: arXiv (accepted to ACL 2025)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.00743
      link_text: 'Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting
        Rare Concepts in Foundation Models'
      original_md: '[Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting
        Rare Concepts in Foundation Models](https://arxiv.org/abs/2411.00743)'
      title: 'Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting
        Rare Concepts in Foundation Models'
      authors:
      - Aashiq Muhamed
      - Mona Diab
      - Virginia Smith
      author_organizations: []
      date: '2024-11-01'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.01220
      link_text: Enhancing Neural Network Interpretability with Feature-Aligned Sparse
        Autoencoders
      original_md: '[Enhancing Neural Network Interpretability with Feature-Aligned
        Sparse Autoencoders](https://arxiv.org/abs/2411.01220)'
      title: Enhancing Neural Network Interpretability with Feature-Aligned Sparse
        Autoencoders
      authors:
      - Luke Marks
      - Alasdair Paren
      - David Krueger
      - Fazl Barez
      author_organizations: []
      date: '2024-11-02'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2412.06410
      link_text: BatchTopK Sparse Autoencoders
      original_md: '[BatchTopK Sparse Autoencoders](https://arxiv.org/abs/2412.06410)'
      title: BatchTopK Sparse Autoencoders
      authors:
      - Bart Bussmann
      - Patrick Leask
      - Neel Nanda
      author_organizations: []
      date: '2024-12-09'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.03730
      link_text: 'Towards Understanding Distilled Reasoning Models: A Representational
        Approach'
      original_md: '[Towards Understanding Distilled Reasoning Models: A Representational
        Approach](https://arxiv.org/abs/2503.03730)'
      title: 'Towards Understanding Distilled Reasoning Models: A Representational
        Approach'
      authors:
      - David D. Baek
      - Max Tegmark
      author_organizations:
      - MIT
      date: '2025-03-05'
      published_year: 2025
      venue: ICLR 2025 Workshop on Building Trust in Language Models and Applications
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.02565
      link_text: Understanding sparse autoencoder scaling in the presence of feature
        manifolds
      original_md: '[Understanding sparse autoencoder scaling in the presence of feature
        manifolds](https://arxiv.org/abs/2509.02565)'
      title: Understanding sparse autoencoder scaling in the presence of feature manifolds
      authors:
      - Eric J. Michaud
      - Liv Gorton
      - Tom McGrath
      author_organizations: []
      date: '2025-09-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.04128
      link_text: Internal states before wait modulate reasoning patterns
      original_md: '[Internal states before wait modulate reasoning patterns](https://arxiv.org/abs/2510.04128)'
      title: Internal states before wait modulate reasoning patterns
      authors:
      - Dmitrii Troitskii
      - Koyena Pal
      - Chris Wendler
      - Callum Stuart McDougall
      - Neel Nanda
      author_organizations:
      - Google DeepMind
      date: '2025-10-05'
      published_year: 2025
      venue: arXiv (EMNLP Findings 2025)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.19964
      link_text: Do Sparse Autoencoders Generalize? A Case Study of Answerability
      original_md: '[Do Sparse Autoencoders Generalize? A Case Study of Answerability](https://arxiv.org/abs/2502.19964)'
      title: Do Sparse Autoencoders Generalize? A Case Study of Answerability
      authors:
      - Lovis Heindrich
      - Philip Torr
      - Fazl Barez
      - Veronika Thost
      author_organizations: []
      date: '2025-02-27'
      published_year: 2025
      venue: ICML 2025 Workshop on Reliable and Responsible Foundation Models (arXiv
        preprint)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.20254
      link_text: 'Position: Mechanistic Interpretability Should Prioritize Feature
        Consistency in SAEs'
      original_md: '[Position: Mechanistic Interpretability Should Prioritize Feature
        Consistency in SAEs](https://arxiv.org/abs/2505.20254)'
      title: 'Position: Mechanistic Interpretability Should Prioritize Feature Consistency
        in SAEs'
      authors:
      - Xiangchen Song
      - Aashiq Muhamed
      - Yujia Zheng
      - Lingjing Kong
      - Zeyu Tang
      - Mona T. Diab
      - Virginia Smith
      - Kun Zhang
      author_organizations: []
      date: '2025-05-26'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.11976
      link_text: How Visual Representations Map to Language Feature Space in Multimodal
        LLMs
      original_md: '[How Visual Representations Map to Language Feature Space in Multimodal
        LLMs](https://arxiv.org/abs/2506.11976)'
      title: How Visual Representations Map to Language Feature Space in Multimodal
        LLMs
      authors:
      - Constantin Venhoff
      - Ashkan Khakzar
      - Sonia Joseph
      - Philip Torr
      - Neel Nanda
      author_organizations:
      - Google DeepMind
      - University of Oxford
      date: '2025-06-13'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.19475
      link_text: 'Prisma: An Open Source Toolkit for Mechanistic Interpretability
        in Vision and Video'
      original_md: '[Prisma: An Open Source Toolkit for Mechanistic Interpretability
        in Vision and Video](https://arxiv.org/abs/2504.19475)'
      title: 'Prisma: An Open Source Toolkit for Mechanistic Interpretability in Vision
        and Video'
      authors:
      - Sonia Joseph
      - Praneet Suresh
      - Lorenz Hufe
      - Edward Stevinson
      - Robert Graham
      - Yash Vadi
      - Danilo Bzdok
      - Sebastian Lapuschkin
      - Lee Sharkey
      - Blake Aaron Richards
      author_organizations:
      - Apollo Research
      - Various Academic Institutions
      date: '2025-04-28'
      published_year: 2025
      venue: arXiv / CVPR Mechanistic Interpretability for Vision Workshop
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/6oF6pRr2FgjTmiHus/topological-data-analysis-and-mechanistic-interpretability
      link_text: Topological Data Analysis and Mechanistic Interpretability
      original_md: '[Topological Data Analysis and Mechanistic Interpretability](https://lesswrong.com/posts/6oF6pRr2FgjTmiHus/topological-data-analysis-and-mechanistic-interpretability)'
      title: Topological Data Analysis and Mechanistic Interpretability
      authors:
      - Gunnar Carlsson
      author_organizations: []
      date: '2025-02-24'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2501.06346
      link_text: Large Language Models Share Representations of Latent Grammatical
        Concepts Across Typologically Diverse Languages
      original_md: '[Large Language Models Share Representations of Latent Grammatical
        Concepts Across Typologically Diverse Languages](https://arxiv.org/abs/2501.06346)'
      title: Large Language Models Share Representations of Latent Grammatical Concepts
        Across Typologically Diverse Languages
      authors:
      - Jannik Brinkmann
      - Chris Wendler
      - Christian Bartelt
      - Aaron Mueller
      author_organizations: []
      date: '2025-01-10'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.11695
      link_text: Interpreting the linear structure of vision-language model embedding
        spaces
      original_md: '[Interpreting the linear structure of vision-language model embedding
        spaces](https://arxiv.org/abs/2504.11695)'
      title: Interpreting the linear structure of vision-language model embedding
        spaces
      authors:
      - Isabel Papadimitriou
      - Huangyuan Su
      - Thomas Fel
      - Sham Kakade
      - Stephanie Gil
      author_organizations: []
      date: '2025-04-16'
      published_year: 2025
      venue: COLM 2025
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.24360
      link_text: Interpreting Large Text-to-Image Diffusion Models with Dictionary
        Learning
      original_md: '[Interpreting Large Text-to-Image Diffusion Models with Dictionary
        Learning](https://arxiv.org/abs/2505.24360)'
      title: Interpreting Large Text-to-Image Diffusion Models with Dictionary Learning
      authors:
      - Stepan Shabalin
      - Ayush Panda
      - Dmitrii Kharlapenko
      - Abdur Raheem Ali
      - Yixiong Hao
      - Arthur Conmy
      author_organizations: []
      date: '2025-05-30'
      published_year: 2025
      venue: CVPR 2025 - Mechanistic Interpretability for Vision Workshop
      kind: paper_preprint
    - link_url: https://cdn.openai.com/pdf/41df8f28-d4ef-43e9-aed2-823f9393e470/circuit-sparsity-paper.pdf
      link_text: Weight-sparse transformers have interpretable circuits
      original_md: '[Weight-sparse transformers have interpretable circuits](https://cdn.openai.com/pdf/41df8f28-d4ef-43e9-aed2-823f9393e470/circuit-sparsity-paper.pdf)'
      title: Unknown - PDF content not accessible
      authors: []
      author_organizations:
      - OpenAI
      date: null
      published_year: null
      venue: null
      kind: error_detected
    other_attributes: {}
  parsing_issues:
  - Broad approach text 'engineering / cognitive' - 'cognitive' likely means 'cognitivist
    science'. Set broad_approach_id to null due to multiple approaches.
- id: a:Causal_Abstractions
  name: Causal Abstractions
  header_level: 3
  parent_id: sec:White_box_safety_i_e_Interpretability_
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Verify that a neural network implements a specific high-level
      causal model (like a logical algorithm) by finding a mapping between high-level
      variables and low-level neural representations.
    theory_of_change: By establishing a causal mapping between a black-box neural
      network and a human-interpretable algorithm, we can check whether the model
      is using safe reasoning processes and predict its behavior on unseen inputs,
      rather than relying on behavioural testing alone.
    see_also:
    - sec:Concept_based_interpretability
    - a:Reverse_engineering
    orthodox_problems:
    - goals_misgeneralize
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - Atticus Geiger
    - Christopher Potts
    - Thomas Icard
    - Theodora-Mara Pîslar
    - Sara Magliacane
    - Jiuding Sun
    - Jing Huang
    estimated_ftes: 10-30
    critiques: '[The Misguided Quest for Mechanistic AI Interpretability](https://www.google.com/search?q=https://open.substack.com/pub/aifrontiersmedia/p/the-misguided-quest-for-mechanistic),
      [Interpretability Will Not Reliably Find Deceptive AI](https://www.lesswrong.com/posts/PwnadG4BFjaER3MGf/interpretability-will-not-reliably-find-deceptive-ai)'
    funded_by: Various academic groups, Google DeepMind, Goodfire
    outputs:
    - link_url: https://arxiv.org/abs/2503.10894
      link_text: 'HyperDAS: Towards Automating Mechanistic Interpretability with Hypernetworks'
      original_md: '[HyperDAS: Towards Automating Mechanistic Interpretability with
        Hypernetworks](https://arxiv.org/abs/2503.10894)'
      title: 'HyperDAS: Towards Automating Mechanistic Interpretability with Hypernetworks'
      authors:
      - Jiuding Sun
      - Jing Huang
      - Sidharth Baskaran
      - Karel D'Oosterlinck
      - Christopher Potts
      - Michael Sklar
      - Atticus Geiger
      author_organizations: []
      date: '2025-03-13'
      published_year: 2025
      venue: ICLR 2025
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.11429
      link_text: Combining Causal Models for More Accurate Abstractions of Neural
        Networks
      original_md: '[Combining Causal Models for More Accurate Abstractions of Neural
        Networks](https://arxiv.org/abs/2503.11429)'
      title: Combining Causal Models for More Accurate Abstractions of Neural Networks
      authors:
      - Theodora-Mara Pîslar
      - Sara Magliacane
      - Atticus Geiger
      author_organizations: []
      date: '2025-03-14'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2508.11214
      link_text: How Causal Abstraction Underpins Computational Explanation
      original_md: '[How Causal Abstraction Underpins Computational Explanation](https://arxiv.org/abs/2508.11214)'
      title: How Causal Abstraction Underpins Computational Explanation
      authors:
      - Atticus Geiger
      - Jacqueline Harding
      - Thomas Icard
      author_organizations: []
      date: '2025-08-15'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:Data_attribution
  name: Data attribution
  header_level: 3
  parent_id: sec:White_box_safety_i_e_Interpretability_
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Quantifies the influence of individual training data points
      on a model's specific behavior or output, allowing researchers to trace model
      properties (like misalignment, bias, or factual errors) back to their source
      in the training set.
    theory_of_change: By attributing harmful, biased, or unaligned behaviors to specific
      training examples, researchers can audit proprietary models, debug training
      data, enable effective data deletion/unlearning
    see_also:
    - a:Data_quality_for_alignment
    orthodox_problems:
    - goals_misgeneralize
    - value_fragile
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: behaviorist_science
    broad_approach_text: behavioural
    some_names:
    - Roger Grosse
    - Philipp Alexander Kreer
    - Jin Hwa Lee
    - Matthew Smith
    - Abhilasha Ravichander
    - Andrew Wang
    - Jiacheng Liu
    - Jiaqi Ma
    - Junwei Deng
    - Yijun Pan
    - Daniel Murfet
    - Jesse Hoogland
    estimated_ftes: 30-60
    critiques: null
    funded_by: Various academic groups
    outputs:
    - link_url: https://arxiv.org/abs/2510.12071
      link_text: Influence Dynamics and Stagewise Data Attribution
      original_md: '[Influence Dynamics and Stagewise Data Attribution](https://arxiv.org/abs/2510.12071)'
      title: Influence Dynamics and Stagewise Data Attribution
      authors:
      - Jin Hwa Lee
      - Matthew Smith
      - Maxwell Adam
      - Jesse Hoogland
      author_organizations: []
      date: '2025-10-14'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2405.13954
      link_text: What is Your Data Worth to GPT?
      original_md: '[What is Your Data Worth to GPT?](https://arxiv.org/abs/2405.13954)'
      title: What is Your Data Worth to GPT? LLM-Scale Data Valuation with Influence
        Functions
      authors:
      - Sang Keun Choe
      - Hwijeen Ahn
      - Juhan Bae
      - Kewen Zhao
      - Minsoo Kang
      - Youngseog Chung
      - Adithya Pratapa
      - Willie Neiswanger
      - Emma Strubell
      - Teruko Mitamura
      - Jeff Schneider
      - Eduard Hovy
      - Roger Grosse
      - Eric Xing
      author_organizations:
      - Carnegie Mellon University
      - University of Toronto
      - Various Universities
      date: '2024-05-22'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.11411
      link_text: Detecting and Filtering Unsafe Training Data via Data Attribution
        with Denoised Representation
      original_md: '[Detecting and Filtering Unsafe Training Data via Data Attribution
        with Denoised Representation](https://arxiv.org/abs/2502.11411)'
      title: Detecting and Filtering Unsafe Training Data via Data Attribution with
        Denoised Representation
      authors:
      - Yijun Pan
      - Taiwei Shi
      - Jieyu Zhao
      - Jiaqi W. Ma
      author_organizations: []
      date: '2025-02-17'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.14740
      link_text: Better Training Data Attribution via Better Inverse Hessian-Vector
        Products
      original_md: '[Better Training Data Attribution via Better Inverse Hessian-Vector
        Products](https://arxiv.org/abs/2507.14740)'
      title: Better Training Data Attribution via Better Inverse Hessian-Vector Products
      authors:
      - Andrew Wang
      - Elisa Nguyen
      - Runshi Yang
      - Juhan Bae
      - Sheila A. McIlraith
      - Roger Grosse
      author_organizations: []
      date: '2025-07-19'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.09424
      link_text: 'DATE-LM: Benchmarking Data Attribution Evaluation for Large Language
        Models'
      original_md: '[DATE-LM: Benchmarking Data Attribution Evaluation for Large Language
        Models](https://arxiv.org/abs/2507.09424)'
      title: 'DATE-LM: Benchmarking Data Attribution Evaluation for Large Language
        Models'
      authors:
      - Cathy Jiao
      - Yijun Pan
      - Emily Xiao
      - Daisy Sheng
      - Niket Jain
      - Hanzhang Zhao
      - Ishita Dasgupta
      - Jiaqi W. Ma
      - Chenyan Xiong
      author_organizations: []
      date: '2025-07-12'
      published_year: 2025
      venue: NeurIPS 2025 Datasets and Benchmarks Track
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.26544
      link_text: Bayesian Influence Functions for Hessian-Free Data Attribution
      original_md: '[Bayesian Influence Functions for Hessian-Free Data Attribution](https://arxiv.org/abs/2509.26544)'
      title: Bayesian Influence Functions for Hessian-Free Data Attribution
      authors:
      - Philipp Alexander Kreer
      - Wilson Wu
      - Maxwell Adam
      - Zach Furman
      - Jesse Hoogland
      author_organizations: []
      date: '2025-09-30'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.07096
      link_text: 'OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training
        Tokens'
      original_md: '[OLMoTrace: Tracing Language Model Outputs Back to Trillions of
        Training Tokens](https://arxiv.org/abs/2504.07096)'
      title: 'OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training
        Tokens'
      authors:
      - Jiacheng Liu
      - Taylor Blanton
      - Yanai Elazar
      - Sewon Min
      - YenSung Chen
      - Arnavi Chheda-Kothary
      - Huy Tran
      - Byron Bischoff
      - Eric Marsh
      - Michael Schmitz
      - Cassidy Trier
      - Aaron Sarnat
      - Jenna James
      - Jon Borchardt
      - Bailey Kuehl
      - Evie Cheng
      - Karen Farley
      - Sruthi Sreeram
      - Taira Anderson
      - David Albright
      - Carissa Schoenick
      - Luca Soldaini
      - Dirk Groeneveld
      - Rock Yuren Pang
      - Pang Wei Koh
      - Noah A. Smith
      - Sophie Lebrecht
      - Yejin Choi
      - Hannaneh Hajishirzi
      - Ali Farhadi
      - Jesse Dodge
      author_organizations:
      - Allen Institute for AI
      - University of Washington
      date: '2025-04-09'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.05475
      link_text: You Are What You Eat -- AI Alignment Requires Understanding How Data
        Shapes Structure and Generalisation
      original_md: '[You Are What You Eat \-- AI Alignment Requires Understanding
        How Data Shapes Structure and Generalisation](https://arxiv.org/abs/2502.05475)'
      title: You Are What You Eat -- AI Alignment Requires Understanding How Data
        Shapes Structure and Generalisation
      authors:
      - Simon Pepin Lehalleur
      - Jesse Hoogland
      - Matthew Farrugia-Roberts
      - Susan Wei
      - Alexander Gietelink Oldenziel
      - George Wang
      - Liam Carroll
      - Daniel Murfet
      author_organizations: []
      date: '2025-02-08'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.12072
      link_text: Information-Guided Identification of Training Data Imprint in (Proprietary)
        Large Language Models
      original_md: '[Information-Guided Identification of Training Data Imprint in
        (Proprietary) Large Language Models](https://arxiv.org/abs/2503.12072)'
      title: Information-Guided Identification of Training Data Imprint in (Proprietary)
        Large Language Models
      authors:
      - Abhilasha Ravichander
      - Jillian Fisher
      - Taylor Sorensen
      - Ximing Lu
      - Yuchen Lin
      - Maria Antoniak
      - Niloofar Mireshghallah
      - Chandra Bhagavatula
      - Yejin Choi
      author_organizations:
      - AI2
      - University of Washington
      date: '2025-03-15'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.12965
      link_text: 'Distributional Training Data Attribution: What do Influence Functions
        Sample?'
      original_md: '[Distributional Training Data Attribution: What do Influence Functions
        Sample?](https://arxiv.org/abs/2506.12965)'
      title: 'Distributional Training Data Attribution: What do Influence Functions
        Sample?'
      authors:
      - Bruno Mlodozeniec
      - Isaac Reid
      - Sam Power
      - David Krueger
      - Murat Erdogdu
      - Richard E. Turner
      - Roger Grosse
      author_organizations: []
      date: '2025-06-15'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://openreview.net/forum?id=sYK4yPDuT1
      link_text: 'A Snapshot of Influence: A Local Data Attribution Framework for
        Online Reinforcement Learning'
      original_md: '[A Snapshot of Influence: A Local Data Attribution Framework for
        Online Reinforcement Learning](https://openreview.net/forum?id=sYK4yPDuT1)'
      title: 'A Snapshot of Influence: A Local Data Attribution Framework for Online
        Reinforcement Learning'
      authors:
      - Yuzheng Hu
      - Fan Wu
      - Haotian Ye
      - David Forsyth
      - James Zou
      - Nan Jiang
      - Jiaqi W. Ma
      - Han Zhao
      author_organizations: []
      date: '2025-09-18'
      published_year: 2025
      venue: NeurIPS 2025
      kind: paper_published
    - link_url: https://arxiv.org/abs/2508.07297
      link_text: Revisiting Data Attribution for Influence Functions
      original_md: '[Revisiting Data Attribution for Influence Functions](https://arxiv.org/abs/2508.07297)'
      title: Revisiting Data Attribution for Influence Functions
      authors:
      - Hongbo Zhu
      - Angelo Cangelosi
      author_organizations: []
      date: '2025-08-10'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues:
  - Broad approach field says 'behavioural' - mapped to 'behaviorist_science'
- id: a:Pragmatic_interpretability
  name: Pragmatic interpretability
  header_level: 3
  parent_id: sec:White_box_safety_i_e_Interpretability_
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Directly tackling concrete, safety-critical problems on
      the path to AGI by using lightweight interpretability tools (like steering and
      probing) and empirical feedback from proxy tasks, rather than pursuing complete
      mechanistic reverse-engineering.
    theory_of_change: By applying interpretability skills to concrete problems, researchers
      can rapidly develop monitoring and control tools (e.g., steering vectors or
      probes) that have immediate, measurable impact on real-world safety issues like
      detecting hidden goals or emergent misalignment.
    see_also:
    - a:Reverse_engineering
    - sec:Concept_based_interpretability
    orthodox_problems:
    - superintelligence_fool_supervisors
    - goals_misgeneralize
    target_case_id: null
    target_case_text: mixed
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitive
    some_names:
    - Lee Sharkey
    - Dario Amodei
    - David Chalmers
    - Been Kim
    - Neel Nanda
    - David D. Baek
    - Lauren Greenspan
    - Dmitry Vaintrob
    - Sam Marks
    - Jacob Pfau
    estimated_ftes: 30-60
    critiques: null
    funded_by: Google DeepMind, Anthropic, various academic groups
    outputs:
    - link_url: https://www.lesswrong.com/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-inter
      link_text: A Pragmatic Vision for Interpretability
      original_md: '[A Pragmatic Vision for Interpretability](https://www.lesswrong.com/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-inter)'
      title: A Pragmatic Vision for Interpretability
      authors:
      - Neel Nanda
      - Josh Engels
      - Arthur Conmy
      - Senthooran Rajamanoharan
      - bilalchughtai
      - CallumMcDougall
      - János Kramár
      - lewis smith
      author_organizations:
      - Google DeepMind
      date: '2025-12-01'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://www.alignmentforum.org/posts/s9z4mgjtWTPpDLxFy/agentic-interpretability-a-strategy-against-gradual
      link_text: 'Agentic Interpretability: A Strategy Against Gradual Disempowerment'
      original_md: '[Agentic Interpretability: A Strategy Against Gradual Disempowerment](https://www.alignmentforum.org/posts/s9z4mgjtWTPpDLxFy/agentic-interpretability-a-strategy-against-gradual)'
      title: 'Agentic Interpretability: A Strategy Against Gradual Disempowerment'
      authors:
      - Been Kim
      - John Hewitt
      - Neel Nanda
      - Noah Fiedel
      - Oyvind Tafjord
      author_organizations:
      - Google DeepMind
      - Anthropic
      date: '2025-06-17'
      published_year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2503.10965
      link_text: Auditing language models for hidden objectives
      original_md: '[Auditing language models for hidden objectives](https://arxiv.org/abs/2503.10965)'
      title: Auditing language models for hidden objectives
      authors:
      - Samuel Marks
      - Johannes Treutlein
      - Trenton Bricken
      - Jack Lindsey
      - Jonathan Marcus
      - Siddharth Mishra-Sharma
      - Daniel Ziegler
      - Emmanuel Ameisen
      - Joshua Batson
      - Tim Belonax
      - Samuel R. Bowman
      - Shan Carter
      - Brian Chen
      - Hoagy Cunningham
      - Carson Denison
      - Florian Dietz
      - Satvik Golechha
      - Akbir Khan
      - Jan Kirchner
      - Jan Leike
      - Austin Meek
      - Kei Nishimura-Gasparian
      - Euan Ong
      - Christopher Olah
      - Adam Pearce
      - Fabien Roger
      - Jeanne Salle
      - Andy Shih
      - Meg Tong
      - Drake Thomas
      - Kelley Rivoire
      - Adam Jermyn
      - Monte MacDiarmid
      - Tom Henighan
      - Evan Hubinger
      author_organizations:
      - Anthropic
      - Independent Researchers
      date: '2025-03-14'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues:
  - Target case field says 'mixed' - cannot map to a single target case ID, left target_case_id
    as null
  - Broad approach says 'cognitive' - mapped to 'cognitivist_science'
- id: a:Other_interpretability
  name: Other interpretability
  header_level: 3
  parent_id: sec:White_box_safety_i_e_Interpretability_
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Interpretability that does not fall well into other categories.
    theory_of_change: Explore alternative conceptual frameworks (e.g., agentic, propositional)
      and physics-inspired methods (e.g., renormalization). Or be "pragmatic".
    see_also:
    - a:Reverse_engineering
    - sec:Concept_based_interpretability
    orthodox_problems:
    - superintelligence_fool_supervisors
    - goals_misgeneralize
    target_case_id: null
    target_case_text: mixed
    broad_approach_id: null
    broad_approach_text: engineering / cognitive
    some_names:
    - Lee Sharkey
    - Dario Amodei
    - David Chalmers
    - Been Kim
    - Neel Nanda
    - David D. Baek
    - Lauren Greenspan
    - Dmitry Vaintrob
    - Sam Marks
    - Jacob Pfau
    estimated_ftes: 30-60
    critiques: '[The Misguided Quest for Mechanistic AI Interpretability](https://aifrontiersmedia.substack.com/p/the-misguided-quest-for-mechanistic),
      [Interpretability Will Not Reliably Find Deceptive AI](https://www.lesswrong.com/posts/PwnadG4BFjaER3MGf/interpretability-will-not-reliably-find-deceptive-ai).'
    funded_by: null
    outputs:
    - link_url: https://lesswrong.com/posts/KbFuuaBKRP7FcAADL/transformers-don-t-need-layernorm-at-inference-time
      link_text: 'Transformers Don''t Need LayerNorm at Inference Time: Implications
        for Interpretability'
      original_md: '[Transformers Don''t Need LayerNorm at Inference Time: Implications
        for Interpretability](https://lesswrong.com/posts/KbFuuaBKRP7FcAADL/transformers-don-t-need-layernorm-at-inference-time)'
      title: 'Transformers Don''t Need LayerNorm at Inference Time: Implications for
        Interpretability'
      authors:
      - submarat
      - Joachim Schaeffer
      - Luca Baroni
      - galvsk
      - StefanHex
      author_organizations:
      - MARS
      - SPAR
      date: '2025-07-23'
      published_year: 2025
      venue: LessWrong / arXiv
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2510.02334
      link_text: Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via
        Representation Gradient Tracing
      original_md: '[Where Did It Go Wrong? Attributing Undesirable LLM Behaviors
        via Representation Gradient Tracing](https://arxiv.org/abs/2510.02334)'
      title: Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation
        Gradient Tracing
      authors:
      - Zhe Li
      - Wei Zhao
      - Yige Li
      - Jun Sun
      author_organizations: []
      date: '2025-09-26'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2501.16496
      link_text: Open Problems in Mechanistic Interpretability
      original_md: '[Open Problems in Mechanistic Interpretability](https://arxiv.org/abs/2501.16496)'
      title: Open Problems in Mechanistic Interpretability
      authors:
      - Lee Sharkey
      - Bilal Chughtai
      - Joshua Batson
      - Jack Lindsey
      - Jeff Wu
      - Lucius Bushnaq
      - Nicholas Goldowsky-Dill
      - Stefan Heimersheim
      - Alejandro Ortega
      - Joseph Bloom
      - Stella Biderman
      - Adria Garriga-Alonso
      - Arthur Conmy
      - Neel Nanda
      - Jessica Rumbelow
      - Martin Wattenberg
      - Nandi Schoots
      - Joseph Miller
      - Eric J. Michaud
      - Stephen Casper
      - Max Tegmark
      - William Saunders
      - David Bau
      - Eric Todd
      - Atticus Geiger
      - Mor Geva
      - Jesse Hoogland
      - Daniel Murfet
      - Tom McGrath
      author_organizations:
      - Multiple research organizations and universities
      date: '2025-01-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/u3ZysuXEjkyHhefrk/against-blanket-arguments-against-interpretability
      link_text: Against blanket arguments against interpretability
      original_md: '[Against blanket arguments against interpretability](https://lesswrong.com/posts/u3ZysuXEjkyHhefrk/against-blanket-arguments-against-interpretability)'
      title: Against blanket arguments against interpretability
      authors:
      - Dmitry Vaintrob
      author_organizations: []
      date: '2025-01-22'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/wkGmouy7JnTNtWAbc/opportunity-space-renormalization-for-ai-safety
      link_text: 'Opportunity Space: Renormalization for AI Safety'
      original_md: '[Opportunity Space: Renormalization for AI Safety](https://lesswrong.com/posts/wkGmouy7JnTNtWAbc/opportunity-space-renormalization-for-ai-safety)'
      title: 'Opportunity Space: Renormalization for AI Safety'
      authors:
      - Lauren Greenspan
      - Dmitry Vaintrob
      - Lucas Teixeira
      author_organizations:
      - PIBBSS
      date: '2025-03-31'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/y5cYisQ2QHiSbQbhk/prospects-for-alignment-automation-interpretability-case
      link_text: 'Prospects for Alignment Automation: Interpretability Case Study'
      original_md: '[Prospects for Alignment Automation: Interpretability Case Study](https://lesswrong.com/posts/y5cYisQ2QHiSbQbhk/prospects-for-alignment-automation-interpretability-case)'
      title: 'Prospects for Alignment Automation: Interpretability Case Study'
      authors:
      - Jacob Pfau
      - Geoffrey Irving
      author_organizations:
      - UK AISI
      - Google DeepMind
      date: '2025-03-21'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://www.darioamodei.com/post/the-urgency-of-interpretability
      link_text: The Urgency of Interpretability
      original_md: '[The Urgency of Interpretability](https://www.darioamodei.com/post/the-urgency-of-interpretability)'
      title: The Urgency of Interpretability
      authors:
      - Dario Amodei
      author_organizations:
      - Anthropic
      date: null
      published_year: 2025
      venue: darioamodei.com
      kind: blog_post
    - link_url: https://arxiv.org/abs/2503.17514
      link_text: Language Models May Verbatim Complete Text They Were Not Explicitly
        Trained On
      original_md: '[Language Models May Verbatim Complete Text They Were Not Explicitly
        Trained On](https://arxiv.org/abs/2503.17514)'
      title: Language Models May Verbatim Complete Text They Were Not Explicitly Trained
        On
      authors:
      - Ken Ziyu Liu
      - Christopher A. Choquette-Choo
      - Matthew Jagielski
      - Peter Kairouz
      - Sanmi Koyejo
      - Percy Liang
      - Nicolas Papernot
      author_organizations:
      - Google
      - Stanford University
      - University of Toronto
      date: '2025-03-21'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/wGRnzCFcowRCrpX4Y/downstream-applications-as-validation-of-interpretability
      link_text: Downstream applications as validation of interpretability progress
      original_md: '[Downstream applications as validation of interpretability progress](https://lesswrong.com/posts/wGRnzCFcowRCrpX4Y/downstream-applications-as-validation-of-interpretability)'
      title: Downstream applications as validation of interpretability progress
      authors:
      - Sam Marks
      author_organizations: []
      date: '2025-03-31'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/DqaoPNqhQhwBFqWue/principles-for-picking-practical-interpretability-projects
      link_text: Principles for Picking Practical Interpretability Projects
      original_md: '[Principles for Picking Practical Interpretability Projects](https://lesswrong.com/posts/DqaoPNqhQhwBFqWue/principles-for-picking-practical-interpretability-projects)'
      title: Principles for Picking Practical Interpretability Projects
      authors:
      - Sam Marks
      author_organizations:
      - Anthropic
      date: '2025-07-15'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2501.15740
      link_text: Propositional Interpretability in Artificial Intelligence
      original_md: '[Propositional Interpretability in Artificial Intelligence](https://arxiv.org/abs/2501.15740)'
      title: Propositional Interpretability in Artificial Intelligence
      authors:
      - David J. Chalmers
      author_organizations: []
      date: '2025-01-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2412.02104
      link_text: 'Explainable and Interpretable Multimodal Large Language Models:
        A Comprehensive Survey'
      original_md: '[Explainable and Interpretable Multimodal Large Language Models:
        A Comprehensive Survey](https://arxiv.org/abs/2412.02104)'
      title: 'Explainable and Interpretable Multimodal Large Language Models: A Comprehensive
        Survey'
      authors:
      - Yunkai Dang
      - Kaichen Huang
      - Jiahao Huo
      - Yibo Yan
      - Sirui Huang
      - Dongrui Liu
      - Mengxi Gao
      - Jie Zhang
      - Chen Qian
      - Kun Wang
      - Yong Liu
      - Jing Shao
      - Hui Xiong
      - Xuming Hu
      author_organizations: []
      date: '2024-12-03'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/sjr66DBEgyogAbfdf/renormalization-redux-qft-techniques-for-ai-interpretability
      link_text: 'Renormalization Redux: QFT Techniques for AI Interpretability'
      original_md: '[Renormalization Redux: QFT Techniques for AI Interpretability](https://lesswrong.com/posts/sjr66DBEgyogAbfdf/renormalization-redux-qft-techniques-for-ai-interpretability)'
      title: 'Renormalization Redux: QFT Techniques for AI Interpretability'
      authors:
      - Lauren Greenspan
      - Dmitry Vaintrob
      author_organizations: []
      date: '2025-01-18'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/qRnupMmFG7dxQTTYh/the-strange-science-of-interpretability-recent-papers-and-a
      link_text: 'The Strange Science of Interpretability: Recent Papers and a Reading
        List for the Philosophy of Interpretability'
      original_md: '[The Strange Science of Interpretability: Recent Papers and a
        Reading List for the Philosophy of Interpretability](https://lesswrong.com/posts/qRnupMmFG7dxQTTYh/the-strange-science-of-interpretability-recent-papers-and-a)'
      title: 'The Strange Science of Interpretability: Recent Papers and a Reading
        List for the Philosophy of Interpretability'
      authors:
      - Kola Ayonrinde
      - Louis Jaburi
      author_organizations: []
      date: '2025-08-17'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2506.02300
      link_text: 'Through a Steerable Lens: Magnifying Neural Network Interpretability
        via Phase-Based Extrapolation'
      original_md: '[Through a Steerable Lens: Magnifying Neural Network Interpretability
        via Phase-Based Extrapolation](https://arxiv.org/abs/2506.02300)'
      title: 'Through a Steerable Lens: Magnifying Neural Network Interpretability
        via Phase-Based Extrapolation'
      authors:
      - Farzaneh Mahdisoltani
      - Saeed Mahdisoltani
      - Roger B. Grosse
      - David J. Fleet
      author_organizations: []
      date: '2025-06-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/MDWGcNHkZ3NPEzcnp/call-for-collaboration-renormalization-for-ai-safety
      link_text: 'Call for Collaboration: Renormalization for AI safety'
      original_md: '[Call for Collaboration: Renormalization for AI safety](https://lesswrong.com/posts/MDWGcNHkZ3NPEzcnp/call-for-collaboration-renormalization-for-ai-safety)'
      title: 'Call for Collaboration: Renormalization for AI safety'
      authors:
      - Lauren Greenspan
      author_organizations:
      - PIBBSS
      date: '2025-03-31'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2505.15811
      link_text: 'On the creation of narrow AI: hierarchy and nonlocality of neural
        network skills'
      original_md: '[On the creation of narrow AI: hierarchy and nonlocality of neural
        network skills](https://arxiv.org/abs/2505.15811)'
      title: 'On the creation of narrow AI: hierarchy and nonlocality of neural network
        skills'
      authors:
      - Eric J. Michaud
      - Asher Parker-Sartori
      - Max Tegmark
      author_organizations: []
      date: '2025-05-21'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.01628
      link_text: Harmonic Loss Trains Interpretable AI Models
      original_md: '[Harmonic Loss Trains Interpretable AI Models](https://arxiv.org/abs/2502.01628)'
      title: Harmonic Loss Trains Interpretable AI Models
      authors:
      - David D. Baek
      - Ziming Liu
      - Riya Tyagi
      - Max Tegmark
      author_organizations: []
      date: '2025-02-03'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.12546
      link_text: Extracting memorized pieces of (copyrighted) books from open-weight
        language models
      original_md: '[Extracting memorized pieces of (copyrighted) books from open-weight
        language models](https://arxiv.org/abs/2505.12546)'
      title: Extracting memorized pieces of (copyrighted) books from open-weight language
        models
      authors:
      - A. Feder Cooper
      - Aaron Gokaslan
      - Ahmed Ahmed
      - Amy B. Cyphert
      - Christopher De Sa
      - Mark A. Lemley
      - Daniel E. Ho
      - Percy Liang
      author_organizations:
      - Stanford University
      - Cornell University
      date: '2025-05-18'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues:
  - Target case is 'mixed' - left as text only since no single case ID applies
  - Broad approach is 'engineering / cognitive' - left as text only since multiple
    approaches are listed
- id: a:Learning_dynamics_and_developmental_interpretability
  name: Learning dynamics and developmental interpretability
  header_level: 3
  parent_id: sec:White_box_safety_i_e_Interpretability_
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Builds tools for detecting, locating, and interpreting key
      structural shifts, phase transitions, and emergent phenomena (like grokking
      or deception) that occur during a model's training and in-context learning phases.
    theory_of_change: Structures forming in neural networks leave identifiable traces
      that can be interpreted (e.g., using concepts from Singular Learning Theory);
      by catching and analyzing these developmental moments, researchers can automate
      interpretability, predict when dangerous capabilities emerge, and intervene
      to prevent deceptiveness or misaligned values as early as possible.
    see_also:
    - a:Reverse_engineering
    - a:Sparse_Coding
    - '[ICL transience](https://proceedings.mlr.press/v267/singh25c.html)'
    orthodox_problems:
    - goals_misgeneralize
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - Timaeus
    - Jesse Hoogland
    - George Wang
    - Daniel Murfet
    - Stan van Wingerden
    - Alexander Gietelink Oldenziel
    estimated_ftes: 10-50
    critiques: '[Vaintrob](https://www.lesswrong.com/posts/M2bs6xCbmc79nwr8j/dmitry-vaintrob-s-shortform#A8Ziwhts35dgqbz52),
      [Joar Skalse (2023)](https://www.alignmentforum.org/posts/ALJYj4PpkqyseL7kZ/my-criticism-of-singular-learning-theory)'
    funded_by: Manifund, Survival and Flourishing Fund, EA Funds
    outputs:
    - link_url: https://www.lesswrong.com/posts/2MX2bXreTtntB85Zy/from-slt-to-ait-nn-generalisation-out-of-distribution
      link_text: 'From SLT to AIT: NN Generalisation Out of Distribution'
      original_md: '[From SLT to AIT: NN Generalisation Out of Distribution](https://www.lesswrong.com/posts/2MX2bXreTtntB85Zy/from-slt-to-ait-nn-generalisation-out-of-distribution)'
      title: 'From SLT to AIT: NN Generalisation Out of Distribution'
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: error_detected
    - link_url: https://www.lesswrong.com/posts/ZSQaT2yxNNZ3eLxRd/understanding-and-controlling-llm-generalization
      link_text: Understanding and Controlling LLM Generalization
      original_md: '[Understanding and Controlling LLM Generalization](https://www.lesswrong.com/posts/ZSQaT2yxNNZ3eLxRd/understanding-and-controlling-llm-generalization)'
      title: Understanding and Controlling LLM Generalization
      authors:
      - Daniel Tan
      author_organizations: []
      date: '2025-11-14'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/J7CyENFYXPxXQpsnD/slt-for-ai-safety
      link_text: SLT for AI Safety
      original_md: '[SLT for AI Safety](https://lesswrong.com/posts/J7CyENFYXPxXQpsnD/slt-for-ai-safety)'
      title: SLT for AI Safety
      authors:
      - Jesse Hoogland
      author_organizations:
      - Timaeus
      date: '2025-07-01'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2509.05291
      link_text: 'Crosscoding Through Time: Tracking Emergence & Consolidation Of
        Linguistic Representations Throughout LLM Pretraining'
      original_md: '[Crosscoding Through Time: Tracking Emergence & Consolidation
        Of Linguistic Representations Throughout LLM Pretraining](https://arxiv.org/abs/2509.05291)'
      title: 'Crosscoding Through Time: Tracking Emergence & Consolidation Of Linguistic
        Representations Throughout LLM Pretraining'
      authors:
      - Deniz Bayazit
      - Aaron Mueller
      - Antoine Bosselut
      author_organizations: []
      date: '2025-09-05'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2508.15841
      link_text: A Review of Developmental Interpretability in Large Language Models
      original_md: '[A Review of Developmental Interpretability in Large Language
        Models](https://arxiv.org/abs/2508.15841)'
      title: A Review of Developmental Interpretability in Large Language Models
      authors:
      - Ihor Kendiukhov
      author_organizations: []
      date: '2025-08-19'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2501.17745
      link_text: Dynamics of Transient Structure in In-Context Linear Regression Transformers
      original_md: '[Dynamics of Transient Structure in In-Context Linear Regression
        Transformers](https://arxiv.org/abs/2501.17745)'
      title: Dynamics of Transient Structure in In-Context Linear Regression Transformers
      authors:
      - Liam Carroll
      - Jesse Hoogland
      - Matthew Farrugia-Roberts
      - Daniel Murfet
      author_organizations: []
      date: '2025-01-29'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://openreview.net/forum?id=KUFH0n1BIM
      link_text: Learning Coefficients, Fractals, and Trees in Parameter Space
      original_md: '[Learning Coefficients, Fractals, and Trees in Parameter Space](https://openreview.net/forum?id=KUFH0n1BIM)'
      title: Learning Coefficients, Fractals, and Trees in Parameter Space
      authors:
      - Max Hennick
      - Matthias Dellago
      author_organizations: []
      date: '2025-06-23'
      published_year: 2025
      venue: ODYSSEY 2025 Conference
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.23365
      link_text: 'Emergence of Superposition: Unveiling the Training Dynamics of Chain
        of Continuous Thought'
      original_md: '[Emergence of Superposition: Unveiling the Training Dynamics of
        Chain of Continuous Thought](https://arxiv.org/abs/2509.23365)'
      title: 'Emergence of Superposition: Unveiling the Training Dynamics of Chain
        of Continuous Thought'
      authors:
      - Hanlin Zhu
      - Shibo Hao
      - Zhiting Hu
      - Jiantao Jiao
      - Stuart Russell
      - Yuandong Tian
      author_organizations:
      - UC Berkeley
      - Meta
      date: '2025-09-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.12077
      link_text: 'Compressibility Measures Complexity: Minimum Description Length
        Meets Singular Learning Theory'
      original_md: '[Compressibility Measures Complexity: Minimum Description Length
        Meets Singular Learning Theory](https://arxiv.org/abs/2510.12077)'
      title: 'Compressibility Measures Complexity: Minimum Description Length Meets
        Singular Learning Theory'
      authors:
      - Einar Urdshals
      - Edmund Lau
      - Jesse Hoogland
      - Stan van Wingerden
      - Daniel Murfet
      author_organizations:
      - Timaeus
      - University of Melbourne
      date: '2025-10-14'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://openreview.net/forum?id=Td37oOfmmz
      link_text: Programs as Singularities
      original_md: '[Programs as Singularities](https://openreview.net/forum?id=Td37oOfmmz)'
      title: Programs as Singularities
      authors:
      - Daniel Murfet
      - William Troiani
      author_organizations: []
      date: '2025-06-20'
      published_year: 2025
      venue: ODYSSEY 2025 Conference
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.07681
      link_text: What Do Learning Dynamics Reveal About Generalization in LLM Reasoning?
      original_md: '[What Do Learning Dynamics Reveal About Generalization in LLM
        Reasoning?](https://arxiv.org/abs/2411.07681)'
      title: What Do Learning Dynamics Reveal About Generalization in LLM Reasoning?
      authors:
      - Katie Kang
      - Amrith Setlur
      - Dibya Ghosh
      - Jacob Steinhardt
      - Claire Tomlin
      - Sergey Levine
      - Aviral Kumar
      author_organizations:
      - UC Berkeley
      date: '2024-11-12'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/HFcriD29cw3E5QLCR/selective-regularization-for-alignment-focused
      link_text: Selective regularization for alignment-focused representation engineering
      original_md: '[Selective regularization for alignment-focused representation
        engineering](https://lesswrong.com/posts/HFcriD29cw3E5QLCR/selective-regularization-for-alignment-focused)'
      title: Selective regularization for alignment-focused representation engineering
      authors:
      - Sandy Fraser
      author_organizations: []
      date: '2025-05-20'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2504.18048
      link_text: Modes of Sequence Models and Learning Coefficients
      original_md: '[Modes of Sequence Models and Learning Coefficients](https://arxiv.org/abs/2504.18048)'
      title: Modes of Sequence Models and Learning Coefficients
      authors:
      - Zhongtian Chen
      - Daniel Murfet
      author_organizations: []
      date: '2025-04-25'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.18777
      link_text: 'Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions
        During Code Training'
      original_md: '[Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions
        During Code Training](https://arxiv.org/abs/2506.18777)'
      title: 'Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions
        During Code Training'
      authors:
      - Jonathan Cook
      - Silvia Sapora
      - Arash Ahmadian
      - Akbir Khan
      - Tim Rocktaschel
      - Jakob Foerster
      - Laura Ruis
      author_organizations: []
      date: '2025-06-23'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:Representation_structure_and_geometry
  name: Representation structure and geometry
  header_level: 3
  parent_id: sec:White_box_safety_i_e_Interpretability_
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: What do the representations look like? Does any simple structure
      underlie the beliefs of all well-trained models? Can we get the semantics from
      this geometry?
    theory_of_change: Get scalable unsupervised methods for finding structure in representations
      and interpreting them, then using this to e.g. guide training.
    see_also:
    - sec:Concept_based_interpretability
    - computational mechanics
    - feature universality
    - a:Natural_abstractions
    - a:Causal_Abstractions
    orthodox_problems:
    - goals_misgeneralize
    - superintelligence_fool_supervisors
    target_case_id: null
    target_case_text: mixed
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - Simplex
    - Insight + Interaction Lab
    - Paul Riechers
    - Adam Shai
    - Martin Wattenberg
    - Blake Richards
    - Mateusz Piotrowski
    estimated_ftes: 10-50
    critiques: null
    funded_by: Various academic groups, Astera Institute, Coefficient Giving
    outputs:
    - link_url: https://arxiv.org/pdf/2504.14379
      link_text: The Geometry of Self-Verification in a Task-Specific Reasoning Model
      original_md: '[The Geometry of Self-Verification in a Task-Specific Reasoning
        Model](https://arxiv.org/pdf/2504.14379)'
      title: The Geometry of Self-Verification in a Task-Specific Reasoning Model
      authors:
      - Andrew Lee
      - Lihao Sun
      - Chris Wendler
      - Fernanda Viégas
      - Martin Wattenberg
      author_organizations:
      - Google Research
      date: '2025-04-19'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: http://arxiv.org/abs/2511.06739
      link_text: Rank-1 LoRAs Encode Interpretable Reasoning Signals
      original_md: '[Rank-1 LoRAs Encode Interpretable Reasoning Signals](http://arxiv.org/abs/2511.06739)'
      title: Rank-1 LoRAs Encode Interpretable Reasoning Signals
      authors:
      - Jake Ward
      - Paul Riechers
      - Adam Shai
      author_organizations: []
      date: '2025-11-10'
      published_year: 2025
      venue: 'arXiv (NeurIPS 2025 Workshop: Mechanistic Interpretability Workshop)'
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.17420
      link_text: 'The Geometry of Refusal in Large Language Models: Concept Cones
        and Representational Independence'
      original_md: '[The Geometry of Refusal in Large Language Models: Concept Cones
        and Representational Independence](https://arxiv.org/abs/2502.17420)'
      title: 'The Geometry of Refusal in Large Language Models: Concept Cones and
        Representational Independence'
      authors:
      - Tom Wollschläger
      - Jannes Elstner
      - Simon Geisler
      - Vincent Cohen-Addad
      - Stephan Günnemann
      - Johannes Gasteiger
      author_organizations:
      - Technical University of Munich
      date: '2025-02-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2508.00331
      link_text: Embryology of a Language Model
      original_md: '[Embryology of a Language Model](https://arxiv.org/abs/2508.00331)'
      title: Embryology of a Language Model
      authors:
      - George Wang
      - Garrett Baker
      - Andrew Gordon
      - Daniel Murfet
      author_organizations: []
      date: '2025-08-01'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.01954
      link_text: Constrained belief updates explain geometric structures in transformer
        representations
      original_md: '[Constrained belief updates explain geometric structures in transformer
        representations](https://arxiv.org/abs/2502.01954)'
      title: Constrained belief updates explain geometric structures in transformer
        representations
      authors:
      - Mateusz Piotrowski
      - Paul M. Riechers
      - Daniel Filan
      - Adam S. Shai
      author_organizations: []
      date: '2025-02-04'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.21073
      link_text: Shared Global and Local Geometry of Language Model Embeddings
      original_md: '[Shared Global and Local Geometry of Language Model Embeddings](https://arxiv.org/abs/2503.21073)'
      title: Shared Global and Local Geometry of Language Model Embeddings
      authors:
      - Andrew Lee
      - Melanie Weber
      - Fernanda Viégas
      - Martin Wattenberg
      author_organizations: []
      date: '2025-03-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.07432
      link_text: Neural networks leverage nominally quantum and post-quantum representations
      original_md: '[Neural networks leverage nominally quantum and post-quantum representations](https://arxiv.org/abs/2507.07432)'
      title: Neural networks leverage nominally quantum and post-quantum representations
      authors:
      - Paul M. Riechers
      - Thomas J. Elliott
      - Adam S. Shai
      author_organizations: []
      date: '2025-07-10'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.23024
      link_text: Tracing the Representation Geometry of Language Models from Pretraining
        to Post-training
      original_md: '[Tracing the Representation Geometry of Language Models from Pretraining
        to Post-training](https://arxiv.org/abs/2509.23024)'
      title: Tracing the Representation Geometry of Language Models from Pretraining
        to Post-training
      authors:
      - Melody Zixuan Li
      - Kumar Krishna Agrawal
      - Arna Ghosh
      - Komal Kumar Teru
      - Adam Santoro
      - Guillaume Lajoie
      - Blake A. Richards
      author_organizations:
      - Unknown
      date: '2025-09-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.26745
      link_text: Deep sequence models tend to memorize geometrically; it is unclear
        why
      original_md: '[Deep sequence models tend to memorize geometrically; it is unclear
        why](https://arxiv.org/abs/2510.26745)'
      title: Deep sequence models tend to memorize geometrically; it is unclear why
      authors:
      - Shahriar Noroozizadeh
      - Vaishnavh Nagarajan
      - Elan Rosenfeld
      - Sanjiv Kumar
      author_organizations: []
      date: '2025-10-30'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.22785
      link_text: Navigating the Latent Space Dynamics of Neural Models
      original_md: '[Navigating the Latent Space Dynamics of Neural Models](https://arxiv.org/abs/2505.22785)'
      title: Navigating the Latent Space Dynamics of Neural Models
      authors:
      - Marco Fumero
      - Luca Moschella
      - Emanuele Rodolà
      - Francesco Locatello
      author_organizations: []
      date: '2025-05-28'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.11692
      link_text: The Geometry of ReLU Networks through the ReLU Transition Graph
      original_md: '[The Geometry of ReLU Networks through the ReLU Transition Graph](https://arxiv.org/abs/2505.11692)'
      title: The Geometry of ReLU Networks through the ReLU Transition Graph
      authors:
      - Sahil Rajesh Dhayalkar
      author_organizations: []
      date: '2025-05-16'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.01599
      link_text: Connecting Neural Models Latent Geometries with Relative Geodesic
        Representations
      original_md: '[Connecting Neural Models Latent Geometries with Relative Geodesic
        Representations](https://arxiv.org/abs/2506.01599)'
      title: Connecting Neural Models Latent Geometries with Relative Geodesic Representations
      authors:
      - Hanlin Yu
      - Berfin Inal
      - Georgios Arvanitidis
      - Soren Hauberg
      - Francesco Locatello
      - Marco Fumero
      author_organizations: []
      date: '2025-06-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.18373
      link_text: Next-token pretraining implies in-context learning
      original_md: '[Next-token pretraining implies in-context learning](https://arxiv.org/abs/2505.18373)'
      title: Next-token pretraining implies in-context learning
      authors:
      - Paul M. Riechers
      - Henry R. Bigelow
      - Eric A. Alt
      - Adam Shai
      author_organizations: []
      date: '2025-05-23'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues:
  - Target case field says 'mixed' - leaving target_case_id as null, setting target_case_text
    to 'mixed'
  - Broad approach field says 'cognitive' - mapped to 'cognitivist_science'
- id: a:Human_inductive_biases
  name: Human inductive biases
  header_level: 3
  parent_id: sec:White_box_safety_i_e_Interpretability_
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Discover connections deep learning AI systems have with
      human brains and human learning processes. Develop an 'alignment moonshot' based
      on a coherent theory of learning which applies to both humans and AI systems.
    theory_of_change: Humans learn trust, honesty, self-maintenance, and corrigibility;
      if we understand how they do maybe we can get future AI systems to learn them.
    see_also:
    - active learning
    - ACS research
    orthodox_problems:
    - goals_misgeneralize
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitive
    some_names:
    - Lukas Muttenthaler
    - Quentin Delfosse
    estimated_ftes: '4'
    critiques: null
    funded_by: Google DeepMind, various academic groups
    outputs:
    - link_url: https://www.nature.com/articles/s41586-025-09631-6
      link_text: Aligning machine and human visual representations across abstraction
        levels
      original_md: '[Aligning machine and human visual representations across abstraction
        levels](https://www.nature.com/articles/s41586-025-09631-6)'
      title: Aligning machine and human visual representations across abstraction
        levels
      authors:
      - Lukas Muttenthaler
      - Klaus Greff
      - Frieda Born
      - Bernhard Spitzer
      - Simon Kornblith
      - Michael C. Mozer
      - Klaus-Robert Müller
      - Thomas Unterthiner
      - Andrew K. Lampinen
      author_organizations:
      - Google DeepMind
      - Technische Universität Berlin
      - BIFOLD
      - Max Planck Institute for Human Cognitive and Brain Sciences
      - Max Planck Institute for Human Development
      - TUD Dresden University of Technology
      - Anthropic
      - Korea University
      - Max Planck Institute for Informatics
      date: '2025-11-12'
      published_year: 2025
      venue: Nature
      kind: paper_published
    - link_url: https://arxiv.org/html/2505.21731v1
      link_text: Deep Reinforcement Learning Agents are not even close to Human Intelligence
      original_md: '[Deep Reinforcement Learning Agents are not even close to Human
        Intelligence](https://arxiv.org/html/2505.21731v1)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://arxiv.org/html/2503.02976v2#S3
      link_text: 'Teaching AI to Handle Exceptions: Supervised Fine-tuning with Human-aligned
        Judgment'
      original_md: '[Teaching AI to Handle Exceptions: Supervised Fine-tuning with
        Human-aligned Judgment](https://arxiv.org/html/2503.02976v2#S3)'
      title: 'Teaching AI to Handle Exceptions: Supervised Fine-tuning with Human-aligned
        Judgment'
      authors:
      - Matthew DosSantos DiSorbo
      - Harang Ju
      - Sinan Aral
      author_organizations:
      - Harvard Business School
      - Johns Hopkins University
      - MIT Sloan School of Management
      date: 2025-03-XX
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://docs.google.com/document/d/1fl7LE8AN7mLJ6uFcPuFCzatp0zCIYvjRIjQRgHPAkSE/edit?tab=t.0
      link_text: HIBP Human Inductive Bias Project Plan
      original_md: '[HIBP Human Inductive Bias Project Plan](https://docs.google.com/document/d/1fl7LE8AN7mLJ6uFcPuFCzatp0zCIYvjRIjQRgHPAkSE/edit?tab=t.0)'
      title: HIBP Human Inductive Bias Project Plan
      authors:
      - Félix Dorn
      author_organizations: []
      date: null
      published_year: null
      venue: Google Docs
      kind: error_detected
    - link_url: https://arxiv.org/abs/2505.14204
      link_text: 'Beginning with You: Perceptual-Initialization Improves Vision-Language
        Representation and Alignment'
      original_md: '[Beginning with You: Perceptual-Initialization Improves Vision-Language
        Representation and Alignment](https://arxiv.org/abs/2505.14204)'
      title: 'Beginning with You: Perceptual-Initialization Improves Vision-Language
        Representation and Alignment'
      authors:
      - Yang Hu
      - Runchen Wang
      - Stephen Chong Zhao
      - Xuhui Zhan
      - Do Hun Kim
      - Mark Wallace
      - David A. Tovar
      author_organizations: []
      date: '2025-05-20'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.04445
      link_text: Towards Cognitively-Faithful Decision-Making Models to Improve AI
        Alignment
      original_md: '[Towards Cognitively-Faithful Decision-Making Models to Improve
        AI Alignment](https://arxiv.org/abs/2509.04445)'
      title: Towards Cognitively-Faithful Decision-Making Models to Improve AI Alignment
      authors:
      - Cyrus Cousins
      - Vijay Keswani
      - Vincent Conitzer
      - Hoda Heidari
      - Jana Schaich Borg
      - Walter Sinnott-Armstrong
      author_organizations:
      - Carnegie Mellon University
      - Duke University
      date: '2025-09-04'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues:
  - Broad approach field uses 'General approach' label instead of standard 'Broad
    approach'
  - Broad approach text says 'cognitive' - mapped to 'cognitivist_science'
  - See also items 'active learning' and 'ACS research' don't match any known agenda
    IDs
- id: sec:Concept_based_interpretability
  name: Concept-based interpretability
  header_level: 2
  parent_id: sec:White_box_safety_i_e_Interpretability_
  content: ''
  item_type: section
  agenda_attributes: null
  parsing_issues: []
- id: a:Monitoring_concepts
  name: Monitoring concepts
  header_level: 3
  parent_id: sec:Concept_based_interpretability
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Identifies directions or subspaces in a model's latent state
      that correspond to high-level concepts (like refusal, deception, or planning)
      and uses them to audit models for misalignment, monitor them at runtime, suppress
      eval awareness, debug why models are failing, etc.
    theory_of_change: By mapping internal activations to human-interpretable concepts,
      we can detect dangerous capabilities or deceptive alignment directly in the
      mind of the model even if its overt behavior is perfectly safe. Deploy computationally
      cheap monitors to flag some hidden misalignment in deployed systems.
    see_also:
    - '[Pragmatic interp](https://www.alignmentforum.org/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-interpretability)'
    - a:Reverse_engineering
    - a:Sparse_Coding
    - a:Model_diffing
    orthodox_problems:
    - value_fragile
    - goals_misgeneralize
    - boxed_agi_exfiltrate
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitive
    some_names:
    - Daniel Beaglehole
    - Adityanarayanan Radhakrishnan
    - Enric Boix-Adserà
    - Tom Wollschläger
    - Anna Soligo
    - Jack Lindsey
    - Brian Christian
    - Ling Hu
    - Nicholas Goldowsky-Dill
    - Neel Nanda
    estimated_ftes: 50-100
    critiques: '[Exploring the generalization of LLM truth directions on conversational
      formats](https://arxiv.org/html/2505.09807v1), [Understanding (Un)Reliability
      of Steering Vectors in Language Models](https://arxiv.org/abs/2505.22637)'
    funded_by: Coefficient Giving, Anthropic, various academic groups
    outputs:
    - link_url: https://arxiv.org/abs/2506.11618
      link_text: Convergent Linear Representations of Emergent Misalignment
      original_md: '[Convergent Linear Representations of Emergent Misalignment](https://arxiv.org/abs/2506.11618)'
      title: Convergent Linear Representations of Emergent Misalignment
      authors:
      - Anna Soligo
      - Edward Turner
      - Senthooran Rajamanoharan
      - Neel Nanda
      author_organizations:
      - Google DeepMind
      date: '2025-06-20'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.03407
      link_text: Detecting Strategic Deception Using Linear Probes
      original_md: '[Detecting Strategic Deception Using Linear Probes](https://arxiv.org/abs/2502.03407)'
      title: Detecting Strategic Deception Using Linear Probes
      authors:
      - Nicholas Goldowsky-Dill
      - Bilal Chughtai
      - Stefan Heimersheim
      - Marius Hobbhahn
      author_organizations:
      - Apollo Research
      date: '2025-02-05'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.03708
      link_text: Toward universal steering and monitoring of AI models
      original_md: '[Toward universal steering and monitoring of AI models](https://arxiv.org/abs/2502.03708)'
      title: Toward universal steering and monitoring of AI models
      authors:
      - Daniel Beaglehole
      - Adityanarayanan Radhakrishnan
      - Enric Boix-Adserà
      - Mikhail Belkin
      author_organizations: []
      date: '2025-05-28'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.07326
      link_text: Reward Model Interpretability via Optimal and Pessimal Tokens
      original_md: '[Reward Model Interpretability via Optimal and Pessimal Tokens](https://arxiv.org/abs/2506.07326)'
      title: Reward Model Interpretability via Optimal and Pessimal Tokens
      authors:
      - Brian Christian
      - Hannah Rose Kirk
      - Jessica A.F. Thompson
      - Christopher Summerfield
      - Tsvetomira Dumbalska
      author_organizations: []
      date: '2025-06-08'
      published_year: 2025
      venue: FAccT '25 (ACM Conference on Fairness, Accountability, and Transparency)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.17420
      link_text: 'The Geometry of Refusal in Large Language Models: Concept Cones
        and Representational Independence'
      original_md: '[The Geometry of Refusal in Large Language Models: Concept Cones
        and Representational Independence](https://arxiv.org/abs/2502.17420)'
      title: 'The Geometry of Refusal in Large Language Models: Concept Cones and
        Representational Independence'
      authors:
      - Tom Wollschläger
      - Jannes Elstner
      - Simon Geisler
      - Vincent Cohen-Addad
      - Stephan Günnemann
      - Johannes Gasteiger
      author_organizations:
      - Technical University of Munich
      date: '2025-02-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://alignment.anthropic.com/2025/cheap-monitors
      link_text: Cost-Effective Constitutional Classifiers via Representation Re-use
      original_md: '[Cost-Effective Constitutional Classifiers via Representation
        Re-use](https://alignment.anthropic.com/2025/cheap-monitors)'
      title: Cost-Effective Constitutional Classifiers via Representation Re-use
      authors:
      - Hoagy Cunningham
      - Alwin Peng
      - Jerry Wei
      - Euan Ong
      - Fabien Roger
      - Linda Petrini
      - Misha Wagner
      - Vladimir Mikulik
      - Mrinank Sharma
      author_organizations:
      - Anthropic
      date: null
      published_year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - link_url: https://arxiv.org/abs/2411.09003
      link_text: Refusal in LLMs is an Affine Function
      original_md: '[Refusal in LLMs is an Affine Function](https://arxiv.org/abs/2411.09003)'
      title: Refusal in LLMs is an Affine Function
      authors:
      - Thomas Marshall
      - Adam Scherlis
      - Nora Belrose
      author_organizations:
      - EleutherAI
      date: '2024-11-13'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.alignmentforum.org/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging
      link_text: White Box Control at UK AISI - Update on Sandbagging Investigations
      original_md: '[White Box Control at UK AISI \- Update on Sandbagging Investigations](https://www.alignmentforum.org/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging)'
      title: White Box Control at UK AISI - Update on Sandbagging Investigations
      authors:
      - Joseph Bloom
      - Jordan Taylor
      - Connor Kissane
      - Sid Black
      - merizian
      - alexdzm
      - jacoba
      - Ben Millwood
      - Alan Cooney
      author_organizations:
      - UK AISI
      date: '2025-07-10'
      published_year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/7zhAwcBri7yupStKy/here-s-18-applications-of-deception-probes
      link_text: Here's 18 Applications of Deception Probes
      original_md: '[Here''s 18 Applications of Deception Probes](https://lesswrong.com/posts/7zhAwcBri7yupStKy/here-s-18-applications-of-deception-probes)'
      title: Here's 18 Applications of Deception Probes
      authors:
      - Cleo Nardo
      - Avi Parrack
      - jordine
      author_organizations: []
      date: '2025-08-28'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2508.05625
      link_text: How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics
        in Multi-Turn Conversations
      original_md: '[How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics
        in Multi-Turn Conversations](https://arxiv.org/abs/2508.05625)'
      title: How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in
        Multi-Turn Conversations
      authors:
      - Brandon Jaipersaud
      - David Krueger
      - Ekdeep Singh Lubana
      author_organizations: []
      date: '2025-08-07'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.26238
      link_text: 'Beyond Linear Probes: Dynamic Safety Monitoring for Language Models'
      original_md: '[Beyond Linear Probes: Dynamic Safety Monitoring for Language
        Models](https://arxiv.org/abs/2509.26238)'
      title: 'Beyond Linear Probes: Dynamic Safety Monitoring for Language Models'
      authors:
      - James Oldfield
      - Philip Torr
      - Ioannis Patras
      - Adel Bibi
      - Fazl Barez
      author_organizations: []
      date: '2025-09-30'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:Activation_engineering
  name: Activation engineering
  header_level: 3
  parent_id: sec:Concept_based_interpretability
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Programmatically modify internal model activations to steer
      outputs toward desired behaviors; a lightweight, interpretable supplement to
      fine-tuning.
    theory_of_change: 'Test interpretability theories by intervening on activations;
      find new insights from interpretable causal interventions on representations.
      Or: build more stuff to stack on top of finetuning. Slightly encourage the model
      to be nice, add one more layer of defence to our bundle of partial alignment
      methods.'
    see_also:
    - a:Sparse_Coding
    orthodox_problems:
    - value_fragile
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: null
    broad_approach_text: engineering / cognitive
    some_names:
    - Runjin Chen
    - Andy Arditi
    - David Krueger
    - Jan Wehner
    - Narmeen Oozeer
    - Reza Bayat
    - Adam Karvonen
    - Jiuding Sun
    - Tim Tian Hua
    - Helena Casademunt
    - Jacob Dunefsky
    - Thomas Marshall
    estimated_ftes: 20-100
    critiques: '[Understanding (Un)Reliability of Steering Vectors in Language Models](https://arxiv.org/abs/2505.22637)'
    funded_by: Coefficient Giving, Anthropic
    outputs:
    - link_url: https://lesswrong.com/posts/6aXe9nipTgwK5LxaP/do-safety-relevant-llm-steering-vectors-optimized-on-a
      link_text: Do safety-relevant LLM steering vectors optimized on a single example
        generalize?
      original_md: '[Do safety-relevant LLM steering vectors optimized on a single
        example generalize?](https://lesswrong.com/posts/6aXe9nipTgwK5LxaP/do-safety-relevant-llm-steering-vectors-optimized-on-a)'
      title: Do safety-relevant LLM steering vectors optimized on a single example
        generalize?
      authors:
      - Jacob Dunefsky
      author_organizations:
      - Yale University
      date: '2025-02-28'
      published_year: 2025
      venue: arXiv
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2510.12672
      link_text: 'Keep Calm and Avoid Harmful Content: Concept Alignment and Latent
        Manipulation Towards Safer Answers'
      original_md: '[Keep Calm and Avoid Harmful Content: Concept Alignment and Latent
        Manipulation Towards Safer Answers](https://arxiv.org/abs/2510.12672)'
      title: 'Keep Calm and Avoid Harmful Content: Concept Alignment and Latent Manipulation
        Towards Safer Answers'
      authors:
      - Ruben Belo
      - Marta Guimaraes
      - Claudia Soares
      author_organizations: []
      date: '2025-10-14'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.04429
      link_text: Activation Space Interventions Can Be Transferred Between Large Language
        Models
      original_md: '[Activation Space Interventions Can Be Transferred Between Large
        Language Models](https://arxiv.org/abs/2503.04429)'
      title: Activation Space Interventions Can Be Transferred Between Large Language
        Models
      authors:
      - Narmeen Oozeer
      - Dhruv Nathawani
      - Nirmalendu Prakash
      - Michael Lan
      - Abir Harrasse
      - Amirali Abdullah
      author_organizations: []
      date: '2025-03-06'
      published_year: 2025
      venue: arXiv (accepted to ICML 2025)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.03292
      link_text: 'HyperSteer: Activation Steering at Scale with Hypernetworks'
      original_md: '[HyperSteer: Activation Steering at Scale with Hypernetworks](https://arxiv.org/abs/2506.03292)'
      title: 'HyperSteer: Activation Steering at Scale with Hypernetworks'
      authors:
      - Jiuding Sun
      - Sidharth Baskaran
      - Zhengxuan Wu
      - Michael Sklar
      - Christopher Potts
      - Atticus Geiger
      author_organizations: []
      date: '2025-06-03'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.20487
      link_text: Steering Evaluation-Aware Language Models to Act Like They Are Deployed
      original_md: '[Steering Evaluation-Aware Language Models to Act Like They Are
        Deployed](https://arxiv.org/abs/2510.20487)'
      title: Steering Evaluation-Aware Language Models to Act Like They Are Deployed
      authors:
      - Tim Tian Hua
      - Andrew Qin
      - Samuel Marks
      - Neel Nanda
      author_organizations: []
      date: '2025-10-23'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.16795
      link_text: Steering Out-of-Distribution Generalization with Concept Ablation
        Fine-Tuning
      original_md: '[Steering Out-of-Distribution Generalization with Concept Ablation
        Fine-Tuning](https://arxiv.org/abs/2507.16795)'
      title: Steering Out-of-Distribution Generalization with Concept Ablation Fine-Tuning
      authors:
      - Helena Casademunt
      - Caden Juang
      - Adam Karvonen
      - Samuel Marks
      - Senthooran Rajamanoharan
      - Neel Nanda
      author_organizations:
      - Google DeepMind
      date: '2025-07-22'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.21509
      link_text: 'Persona Vectors: Monitoring and Controlling Character Traits in
        Language Models'
      original_md: '[Persona Vectors: Monitoring and Controlling Character Traits
        in Language Models](https://arxiv.org/abs/2507.21509)'
      title: 'Persona Vectors: Monitoring and Controlling Character Traits in Language
        Models'
      authors:
      - Runjin Chen
      - Andy Arditi
      - Henry Sleight
      - Owain Evans
      - Jack Lindsey
      author_organizations: []
      date: '2025-07-29'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.00177
      link_text: Steering Large Language Model Activations in Sparse Spaces
      original_md: '[Steering Large Language Model Activations in Sparse Spaces](https://arxiv.org/abs/2503.00177)'
      title: Steering Large Language Model Activations in Sparse Spaces
      authors:
      - Reza Bayat
      - Ali Rahimi-Kalahroudi
      - Mohammad Pezeshki
      - Sarath Chandar
      - Pascal Vincent
      author_organizations: []
      date: '2025-02-28'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.02193
      link_text: Improving Steering Vectors by Targeting Sparse Autoencoder Features
      original_md: '[Improving Steering Vectors by Targeting Sparse Autoencoder Features](https://arxiv.org/abs/2411.02193)'
      title: Improving Steering Vectors by Targeting Sparse Autoencoder Features
      authors:
      - Sviatoslav Chalnev
      - Matthew Siu
      - Arthur Conmy
      author_organizations: []
      date: '2024-11-04'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.18167
      link_text: Understanding Reasoning in Thinking Language Models via Steering
        Vectors
      original_md: '[Understanding Reasoning in Thinking Language Models via Steering
        Vectors](https://arxiv.org/abs/2506.18167)'
      title: Understanding Reasoning in Thinking Language Models via Steering Vectors
      authors:
      - Constantin Venhoff
      - Iván Arcuschin
      - Philip Torr
      - Arthur Conmy
      - Neel Nanda
      author_organizations:
      - University of Oxford
      - Anthropic
      date: '2025-06-22'
      published_year: 2025
      venue: ICLR 2025 Workshop on Reasoning and Planning for Large Language Models
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/kcKnKHTHycHeRhcHF/one-shot-steering-vectors-cause-emergent-misalignment-too
      link_text: One-shot steering vectors cause emergent misalignment, too
      original_md: '[One-shot steering vectors cause emergent misalignment, too](https://lesswrong.com/posts/kcKnKHTHycHeRhcHF/one-shot-steering-vectors-cause-emergent-misalignment-too)'
      title: One-shot steering vectors cause emergent misalignment, too
      authors:
      - Jacob Dunefsky
      author_organizations: []
      date: '2025-04-14'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2411.02461
      link_text: Enhancing Multiple Dimensions of Trustworthiness in LLMs via Sparse
        Activation Control
      original_md: '[Enhancing Multiple Dimensions of Trustworthiness in LLMs via
        Sparse Activation Control](https://arxiv.org/abs/2411.02461)'
      title: Enhancing Multiple Dimensions of Trustworthiness in LLMs via Sparse Activation
        Control
      authors:
      - Yuxin Xiao
      - Chaoqun Wan
      - Yonggang Zhang
      - Wenxiao Wang
      - Binbin Lin
      - Xiaofei He
      - Xu Shen
      - Jieping Ye
      author_organizations: []
      date: '2024-11-04'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.07213
      link_text: Comparing Bottom-Up and Top-Down Steering Approaches on In-Context
        Learning Tasks
      original_md: '[Comparing Bottom-Up and Top-Down Steering Approaches on In-Context
        Learning Tasks](https://arxiv.org/abs/2411.07213)'
      title: Comparing Bottom-Up and Top-Down Steering Approaches on In-Context Learning
        Tasks
      authors:
      - Madeline Brumley
      - Joe Kwon
      - David Krueger
      - Dmitrii Krasheninnikov
      - Usman Anwar
      author_organizations: []
      date: '2024-11-11'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.19649
      link_text: Taxonomy, Opportunities, and Challenges of Representation Engineering
        for Large Language Models
      original_md: '[Taxonomy, Opportunities, and Challenges of Representation Engineering
        for Large Language Models](https://arxiv.org/abs/2502.19649)'
      title: Taxonomy, Opportunities, and Challenges of Representation Engineering
        for Large Language Models
      authors:
      - Jan Wehner
      - Sahar Abdelnabi
      - Daniel Tan
      - David Krueger
      - Mario Fritz
      author_organizations:
      - University of Cambridge
      - CISPA Helmholtz Center for Information Security
      date: '2025-02-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.10922
      link_text: Robustly Improving LLM Fairness in Realistic Settings via Interpretability
      original_md: '[Robustly Improving LLM Fairness in Realistic Settings via Interpretability](https://arxiv.org/abs/2506.10922)'
      title: Robustly Improving LLM Fairness in Realistic Settings via Interpretability
      authors:
      - Adam Karvonen
      - Samuel Marks
      author_organizations: []
      date: '2025-06-12'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues:
  - Broad approach field contains 'engineering / cognitive' which maps to multiple
    approaches. Set broad_approach_id to null and kept text as is.
- id: sec:Safety_by_construction
  name: Safety by construction
  header_level: 1
  parent_id: null
  content: Approaches which try to get assurances about system outputs while still
    being scalable.
  item_type: section
  agenda_attributes: null
  parsing_issues: []
- id: a:Guaranteed_Safe_AI
  name: Guaranteed-Safe AI
  header_level: 3
  parent_id: sec:Safety_by_construction
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Have an AI system generate outputs (e.g. code, control systems,
      or RL policies) which it can quantitatively guarantee comply with a formal safety
      specification and world model.
    theory_of_change: 'Various, including:


      i) safe deployment: create a scalable process to get not-fully-trusted AIs to
      produce highly trusted outputs;


      ii) secure containers: create a ''gatekeeper'' system that can act as an intermediary
      between human users and a potentially dangerous system, only letting provably
      safe actions through.


      (Notable for not requiring that we solve ELK; does require that we solve ontology
      though)'
    see_also:
    - '[Towards Guaranteed Safe AI](https://arxiv.org/abs/2405.06624)'
    - '[Standalone World-Models](https://www.alignmentforum.org/posts/LngR93YwiEpJ3kiWh/research-agenda-synthesizing-standalone-world-models)'
    - a:Scientist_AI
    - Safeguarded AI
    - a:Asymptotic_guarantees
    - Open Agency Architecture
    - SLES
    - program synthesis
    - Scalable formal oversight
    orthodox_problems:
    - value_fragile
    - goals_misgeneralize
    - superintelligence_fool_supervisors
    - humans_not_first_class
    - boxed_agi_exfiltrate
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: null
    broad_approach_text: cognitive / engineering
    some_names:
    - ARIA
    - Lawzero
    - Atlas Computing
    - FLF
    - Max Tegmark
    - Beneficial AI Foundation
    - Steve Omohundro
    - David "davidad" Dalrymple
    - Joar Skalse
    - Stuart Russell
    - Alessandro Abate
    estimated_ftes: 10-100
    critiques: '[Zvi](https://thezvi.substack.com/p/ai-28-watching-and-waiting?utm_source=%2Fsearch%2Fomohundro&utm_medium=reader2#:~:text=Max%20Tegmark%20and%20Steve%20Omohundo%20drop%20a%20new%20paper),
      [Gleave](https://manifund.org//projects/relocating-to-montreal-to-work-full-time-on-ai-safety?tab=comments#aea6521a-c6bb-4c66-9f5f-cf647589cf7e),
      [Dickson](https://www.lesswrong.com/posts/B2bg677TaS4cmDPzL/limitations-on-formal-verification-for-ai-safety),
      [Greenblatt](https://www.lesswrong.com/posts/jRf4WENQnhssCb6mJ/davidad-s-bold-plan-for-alignment-an-in-depth-explanation?commentId=MJCvHk5ARMnWDjQDg)'
    funded_by: Manifund, ARIA, Coefficient Giving, Survival and Flourishing Fund,
      Mila / CIFAR
    outputs:
    - link_url: https://manifund.org/projects/safeplanbench-evaluating-a-guaranteed-safe-ai-approach-for-llm-based-agents
      link_text: 'SafePlanBench: evaluating a Guaranteed Safe AI Approach for LLM-based
        Agents'
      original_md: '[SafePlanBench: evaluating a Guaranteed Safe AI Approach for LLM-based
        Agents](https://manifund.org/projects/safeplanbench-evaluating-a-guaranteed-safe-ai-approach-for-llm-based-agents)'
      title: 'SafePlanBench: evaluating a Guaranteed Safe AI Approach for LLM-based
        Agents'
      authors:
      - Agustín Martinez Suñé
      - Tan Zhi Xuan
      author_organizations:
      - PIBBSS
      - MIT
      - University of Oxford
      date: null
      published_year: null
      venue: Manifund
      kind: agenda_manifesto
    - link_url: https://lesswrong.com/posts/CCT7Qc8rSeRs7r5GL/beliefs-about-formal-methods-and-ai-safety
      link_text: Beliefs about formal methods and AI safety
      original_md: '[Beliefs about formal methods and AI safety](https://lesswrong.com/posts/CCT7Qc8rSeRs7r5GL/beliefs-about-formal-methods-and-ai-safety)'
      title: Beliefs about formal methods and AI safety
      authors:
      - Quinn Dougherty
      author_organizations: []
      date: '2025-10-23'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2506.22492
      link_text: Report on NSF Workshop on Science of Safe AI
      original_md: '[Report on NSF Workshop on Science of Safe AI](https://arxiv.org/abs/2506.22492)'
      title: Report on NSF Workshop on Science of Safe AI
      authors:
      - Rajeev Alur
      - Greg Durrett
      - Hadas Kress-Gazit
      - Corina Păsăreanu
      - René Vidal
      author_organizations:
      - NSF SLES Program
      - University of Pennsylvania
      date: '2025-06-24'
      published_year: 2025
      venue: arXiv
      kind: agenda_manifesto
    - link_url: https://arxiv.org/abs/2509.22908
      link_text: 'A benchmark for vericoding: formally verified program synthesis'
      original_md: '[A benchmark for vericoding: formally verified program synthesis](https://arxiv.org/abs/2509.22908)'
      title: 'A benchmark for vericoding: formally verified program synthesis'
      authors:
      - Sergiu Bursuc
      - Theodore Ehrenborg
      - Shaowei Lin
      - Lacramioara Astefanoaei
      - Ionel Emilian Chiosa
      - Jure Kukovec
      - Alok Singh
      - Oliver Butterley
      - Adem Bizid
      - Quinn Dougherty
      - Miranda Zhao
      - Max Tan
      - Max Tegmark
      author_organizations:
      - Beneficial AI Foundation
      - MIT
      date: '2025-09-26'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://atlascomputing.org/ai-assisted-fv-toolchain.pdf
      link_text: A Toolchain for AI-Assisted Code Specification, Synthesis and Verification
      original_md: '[A Toolchain for AI-Assisted Code Specification, Synthesis and
        Verification](https://atlascomputing.org/ai-assisted-fv-toolchain.pdf)'
      title: AI-Assisted Formal Verification Toolchain
      authors: []
      author_organizations:
      - Atlas Computing
      date: null
      published_year: null
      venue: null
      kind: error_detected
    other_attributes: {}
  parsing_issues:
  - Broad approach field contains multiple approaches 'cognitive / engineering' -
    leaving broad_approach_id as null
- id: a:Scientist_AI
  name: Scientist AI
  header_level: 3
  parent_id: sec:Safety_by_construction
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Develop powerful, nonagentic, uncertain world models that
      accelerate scientific progress while avoiding the risks of agent AIs
    theory_of_change: 'Developing non-agentic ''Scientist AI'' allows us to: (i) reap
      the benefits of AI progress while (ii) avoiding the inherent risks of agentic
      systems. These systems can also (iii) provide a useful guardrail to protect
      us from unsafe agentic AIs by double-checking actions they propose, and (iv)
      help us more safely build agentic superintelligent systems.'
    see_also:
    - '[JEPA](https://arxiv.org/abs/2511.08544)'
    - '[oracles](https://www.lesswrong.com/w/oracle-ai)'
    orthodox_problems:
    - pivotal_dangerous_capabilities
    - goals_misgeneralize
    - instrumental_convergence
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - Yoshua Bengio
    - Younesse Kaddar
    estimated_ftes: 1-10
    critiques: Hard to find, but see [Raymond Douglas' comment](https://www.lesswrong.com/posts/p5gBcoQeBsvsMShvT/superintelligent-agents-pose-catastrophic-risks-can?commentId=tJXqhg3XZsqnyaZs2),
      [Karnofsky-Soares discussion](https://www.lesswrong.com/posts/iy2o4nQj9DnQD7Yhj/discussion-with-nate-soares-on-a-key-alignment-difficulty).
      Perhaps also [Predict-O-Matic](https://www.lesswrong.com/posts/SwcyMEgLyd4C3Dern/the-parable-of-predict-o-matic).
    funded_by: ARIA, Gates Foundation, Future of Life Institute, Coefficient Giving,
      Jaan Tallinn, Schmidt Sciences
    outputs:
    - link_url: https://arxiv.org/abs/2502.15657
      link_text: 'Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI
        Offer a Safer Path?'
      original_md: '[Superintelligent Agents Pose Catastrophic Risks: Can Scientist
        AI Offer a Safer Path?](https://arxiv.org/abs/2502.15657)'
      title: 'Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer
        a Safer Path?'
      authors:
      - Yoshua Bengio
      - Michael Cohen
      - Damiano Fornasiere
      - Joumana Ghosn
      - Pietro Greiner
      - Matt MacDermott
      - Sören Mindermann
      - Adam Oberman
      - Jesse Richardson
      - Oliver Richardson
      - Marc-Antoine Rondeau
      - Pierre-Luc St-Charles
      - David Williams-King
      author_organizations:
      - Mila
      - Independent
      date: '2025-02-21'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.08713
      link_text: 'The More You Automate, the Less You See: Hidden Pitfalls of AI Scientist
        Systems'
      original_md: '[The More You Automate, the Less You See: Hidden Pitfalls of AI
        Scientist Systems](https://arxiv.org/abs/2509.08713)'
      title: 'The More You Automate, the Less You See: Hidden Pitfalls of AI Scientist
        Systems'
      authors:
      - Ziming Luo
      - Atoosa Kasirzadeh
      - Nihar B. Shah
      author_organizations: []
      date: '2025-09-10'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:Brainlike_AGI_Safety
  name: Brainlike-AGI Safety
  header_level: 3
  parent_id: sec:Safety_by_construction
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Social and moral instincts are (partly) implemented in particular
      hardwired brain circuitry; let's figure out what those circuits are and how
      they work; this will involve symbol grounding. "a yet-to-be-invented variation
      on actor-critic model-based reinforcement learning"
    theory_of_change: Fairly-direct alignment via changing training to reflect actual
      human reward. Get actual data about (reward, training data) → (human values)
      to help with theorising this map in AIs; "understand human social instincts,
      and then maybe adapt some aspects of those for AGIs, presumably in conjunction
      with other non-biological ingredients".
    see_also: []
    orthodox_problems: []
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - Steve Byrnes
    estimated_ftes: 1-5
    critiques: '[Tsvi BT](https://www.lesswrong.com/posts/unCG3rhyMJpGJpoLd/koan-divining-alien-datastructures-from-ram-activations#BtHCubjKWDFafkmYH)'
    funded_by: Astera Institute
    outputs:
    - link_url: https://www.lesswrong.com/posts/grgb2ipxQf2wzNDEG/perils-of-under-vs-over-sculpting-agi-desires
      link_text: Perils of Under vs Over-sculpting AGI Desires
      original_md: '[Perils of Under vs Over-sculpting AGI Desires](https://www.lesswrong.com/posts/grgb2ipxQf2wzNDEG/perils-of-under-vs-over-sculpting-agi-desires)'
      title: Perils of Under vs Over-sculpting AGI Desires
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: error_detected
    - link_url: https://lesswrong.com/posts/JrTk2pbqp7BFwPAKw/reward-button-alignment
      link_text: Reward button alignment
      original_md: '[Reward button alignment](https://lesswrong.com/posts/JrTk2pbqp7BFwPAKw/reward-button-alignment)'
      title: Reward button alignment
      authors:
      - Steven Byrnes
      author_organizations:
      - Astera Institute
      date: '2025-05-22'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://www.lesswrong.com/posts/cus5CGmLrjBRgcPSF/system-2-alignment-deliberation-review-and-thought
      link_text: 'System 2 Alignment: Deliberation, Review, and Thought Management'
      original_md: '[System 2 Alignment: Deliberation, Review, and Thought Management](https://www.lesswrong.com/posts/cus5CGmLrjBRgcPSF/system-2-alignment-deliberation-review-and-thought)'
      title: 'System 2 Alignment: Deliberation, Review, and Thought Management'
      authors:
      - Seth Herd
      author_organizations: []
      date: '2025-02-13'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://elicit.com/blog/system-2-learning
      link_text: 'Against RL: The Case for System 2 Learning'
      original_md: '[Against RL: The Case for System 2 Learning](https://elicit.com/blog/system-2-learning)'
      title: 'Against RL: The Case for System 2 Learning'
      authors:
      - Andreas Stuhlmüller
      author_organizations:
      - Elicit
      date: '2025-01-30'
      published_year: 2025
      venue: Elicit Blog
      kind: blog_post
    - link_url: https://www.lesswrong.com/posts/yew6zFWAKG4AGs3Wk/foom-and-doom-1-brain-in-a-box-in-a-basement
      link_text: 'Foom and Doom 1: Brain in a Box in a Basement'
      original_md: '[Foom and Doom 1: Brain in a Box in a Basement](https://www.lesswrong.com/posts/yew6zFWAKG4AGs3Wk/foom-and-doom-1-brain-in-a-box-in-a-basement)'
      title: 'Foom and Doom 1: Brain in a Box in a Basement'
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: blocked
    - link_url: https://www.lesswrong.com/posts/bnnKGSCHJghAvqPjS/foom-and-doom-2-technical-alignment-is-hard
      link_text: 'Foom and Doom 2: Technical Alignment is Hard'
      original_md: '[Foom and Doom 2: Technical Alignment is Hard](https://www.lesswrong.com/posts/bnnKGSCHJghAvqPjS/foom-and-doom-2-technical-alignment-is-hard)'
      title: 'Foom and Doom 2: Technical Alignment is Hard'
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: error_detected
    other_attributes:
      Agenda statement: '[My AGI safety research—2025 review, ''26 plans](https://www.lesswrong.com/posts/CF4Z9mQSfvi99A3BR/my-agi-safety-research-2025-review-26-plans)'
  parsing_issues: []
- id: sec:Make_AI_solve_it
  name: Make AI solve it
  header_level: 1
  parent_id: null
  content: ''
  item_type: section
  agenda_attributes: null
  parsing_issues: []
- id: a:Weak_to_strong_generalization
  name: Weak-to-strong generalization
  header_level: 3
  parent_id: sec:Make_AI_solve_it
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Use weaker models to supervise and provide a feedback signal
      to stronger models.
    theory_of_change: Find techniques that do better than RLHF at supervising superior
      models → track whether these techniques fail as capabilities increase further
      → keep the stronger systems aligned by amplifying weak oversight and quantifying
      where it breaks.
    see_also:
    - sec:White_box_safety_i_e_Interpretability_
    - a:Supervising_AIs_improving_AIs
    orthodox_problems:
    - superintelligence_hack_software
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: engineering
    broad_approach_text: engineering
    some_names:
    - Joshua Engels
    - Nora Belrose
    - David D. Baek
    estimated_ftes: 2-20
    critiques: '[Can we safely automate alignment research?](https://joecarlsmith.substack.com/p/can-we-safely-automate-alignment),
      [Super(ficial)-alignment: Strong Models May Deceive Weak Models in Weak-to-Strong
      Generalization](https://arxiv.org/abs/2406.11431)'
    funded_by: lab funders, Eleuther funders
    outputs:
    - link_url: https://arxiv.org/abs/2504.18530
      link_text: Scaling Laws For Scalable Oversight
      original_md: '[Scaling Laws For Scalable Oversight](https://arxiv.org/abs/2504.18530)'
      title: Scaling Laws For Scalable Oversight
      authors:
      - Joshua Engels
      - David D. Baek
      - Subhash Kantamneni
      - Max Tegmark
      author_organizations:
      - MIT
      date: '2025-04-25'
      published_year: 2025
      venue: arXiv (NeurIPS 2025 Spotlight)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.04313
      link_text: Great Models Think Alike and this Undermines AI Oversight
      original_md: '[Great Models Think Alike and this Undermines AI Oversight](https://arxiv.org/abs/2502.04313)'
      title: Great Models Think Alike and this Undermines AI Oversight
      authors:
      - Shashwat Goel
      - Joschka Struber
      - Ilze Amanda Auzina
      - Karuna K Chandra
      - Ponnurangam Kumaraguru
      - Douwe Kiela
      - Ameya Prabhu
      - Matthias Bethge
      - Jonas Geiping
      author_organizations: []
      date: '2025-02-06'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2501.13124
      link_text: Debate Helps Weak-to-Strong Generalization
      original_md: '[Debate Helps Weak-to-Strong Generalization](https://arxiv.org/abs/2501.13124)'
      title: Debate Helps Weak-to-Strong Generalization
      authors:
      - Hao Lang
      - Fei Huang
      - Yongbin Li
      author_organizations: []
      date: '2025-01-21'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://openreview.net/forum?id=RwYdLgj1S6
      link_text: Understanding the Capabilities and Limitations of Weak-to-Strong
        Generalization
      original_md: '[Understanding the Capabilities and Limitations of Weak-to-Strong
        Generalization](https://openreview.net/forum?id=RwYdLgj1S6)'
      title: Understanding the Capabilities and Limitations of Weak-to-Strong Generalization
      authors:
      - Wei Yao
      - Wenkai Yang
      - Ziqiao Wang
      - Yankai Lin
      - Yong Liu
      author_organizations: []
      date: '2025-03-08'
      published_year: 2025
      venue: ICLR 2025 Workshop SSI-FM
      kind: paper_published
    other_attributes: {}
  parsing_issues: []
- id: a:Supervising_AIs_improving_AIs
  name: Supervising AIs improving AIs
  header_level: 3
  parent_id: sec:Make_AI_solve_it
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Build formal and empirical frameworks where AIs supervise
      other (stronger) AI systems via structured interactions; construct monitoring
      tools which enable scalable tracking of behavioural drift, benchmarks for self-modification,
      and robustness guarantees
    theory_of_change: Early models train ~only on human data while later models also
      train on early model outputs, which leads to early model problems cascading.
      Left unchecked this will likely cause problems, so supervision mechanisms are
      needed to help ensure the AI self-improvement remains legible.
    see_also: []
    orthodox_problems:
    - superintelligence_fool_supervisors
    - superintelligence_hack_software
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: behaviorist_science
    broad_approach_text: behavioral
    some_names:
    - Roman Engeler
    - Akbir Khan
    - Ethan Perez
    estimated_ftes: 1-10
    critiques: '[Automation collapse](https://www.lesswrong.com/posts/2Gy9tfjmKwkYbF9BY/automation-collapse),
      [Great Models Think Alike and this Undermines AI Oversight](https://arxiv.org/abs/2502.04313)'
    funded_by: Long-Term Future Fund, lab funders
    outputs:
    - link_url: https://saif.org/research/bare-minimum-mitigations-for-autonomous-ai-development/
      link_text: Bare Minimum Mitigations for Autonomous AI Development
      original_md: '[Bare Minimum Mitigations for Autonomous AI Development](https://saif.org/research/bare-minimum-mitigations-for-autonomous-ai-development/)'
      title: Bare Minimum Mitigations for Autonomous AI Development
      authors:
      - Joshua Clymer
      - Isabella Duan
      - Chris Cundy
      - Yawen Duan
      - Fynn Heide
      - Chaochao Lu
      - Sören Mindermann
      - Conor McGurk
      - Xudong Pan
      - Saad Siddiqui
      - Jingren Wang
      - Min Yang
      - Xianyuan Zhan
      author_organizations:
      - Safe AI Forum
      date: '2025-04-22'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.alignmentforum.org/posts/EgRJtwQurNzz8CEfJ/dodging-systematic-human-errors-in-scalable-oversight
      link_text: Dodging systematic human errors in scalable oversight
      original_md: '[Dodging systematic human errors in scalable oversight](https://www.alignmentforum.org/posts/EgRJtwQurNzz8CEfJ/dodging-systematic-human-errors-in-scalable-oversight)'
      title: Dodging systematic human errors in scalable oversight
      authors:
      - Geoffrey Irving
      author_organizations:
      - UK AISI
      date: '2025-05-14'
      published_year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2504.18530
      link_text: Scaling Laws for Scalable Oversight
      original_md: '[Scaling Laws for Scalable Oversight](https://arxiv.org/abs/2504.18530)'
      title: Scaling Laws For Scalable Oversight
      authors:
      - Joshua Engels
      - David D. Baek
      - Subhash Kantamneni
      - Max Tegmark
      author_organizations:
      - MIT
      date: '2025-04-25'
      published_year: 2025
      venue: arXiv (NeurIPS 2025 Spotlight)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2412.08897
      link_text: Neural Interactive Proofs
      original_md: '[Neural Interactive Proofs](https://arxiv.org/abs/2412.08897)'
      title: Neural Interactive Proofs
      authors:
      - Lewis Hammond
      - Sam Adam-Day
      author_organizations: []
      date: '2024-12-12'
      published_year: 2024
      venue: arXiv (ICLR 2025)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.21262
      link_text: Modeling Human Beliefs about AI Behavior for Scalable Oversight
      original_md: '[Modeling Human Beliefs about AI Behavior for Scalable Oversight](https://arxiv.org/abs/2502.21262)'
      title: Modeling Human Beliefs about AI Behavior for Scalable Oversight
      authors:
      - Leon Lang
      - Patrick Forré
      author_organizations: []
      date: '2025-02-28'
      published_year: 2025
      venue: Transactions on Machine Learning Research
      kind: paper_published
    - link_url: https://arxiv.org/abs/2502.04675
      link_text: Scalable Oversight for Superhuman AI via Recursive Self-Critiquing
      original_md: '[Scalable Oversight for Superhuman AI via Recursive Self-Critiquing](https://arxiv.org/abs/2502.04675)'
      title: Scalable Oversight for Superhuman AI via Recursive Self-Critiquing
      authors:
      - Xueru Wen
      - Jie Lou
      - Xinyu Lu
      - Junjie Yang
      - Yanjiang Liu
      - Yaojie Lu
      - Debing Zhang
      - Xing Yu
      author_organizations: []
      date: '2025-02-07'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/TQbptN7F4ijPnQRLy/video-and-transcript-of-talk-on-automating-alignment
      link_text: Video and transcript of talk on automating alignment research
      original_md: '[Video and transcript of talk on automating alignment research](https://lesswrong.com/posts/TQbptN7F4ijPnQRLy/video-and-transcript-of-talk-on-automating-alignment)'
      title: Video and transcript of talk on automating alignment research
      authors:
      - Joe Carlsmith
      author_organizations:
      - Anthropic
      date: '2025-04-30'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/PhgEKkB4cwYjwpGxb/maintaining-alignment-during-rsi-as-a-feedback-control
      link_text: Maintaining Alignment during RSI as a Feedback Control Problem
      original_md: '[Maintaining Alignment during RSI as a Feedback Control Problem](https://lesswrong.com/posts/PhgEKkB4cwYjwpGxb/maintaining-alignment-during-rsi-as-a-feedback-control)'
      title: Maintaining Alignment during RSI as a Feedback Control Problem
      authors:
      - beren
      author_organizations: []
      date: '2025-03-02'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    other_attributes: {}
  parsing_issues: []
- id: a:AI_explanations_of_AIs
  name: AI explanations of AIs
  header_level: 3
  parent_id: sec:Make_AI_solve_it
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Make open AI tools to explain AIs, including AI agents.
      e.g. automatic feature descriptions for neuron activation patterns; an interface
      for steering these features; a behaviour elicitation agent that "searches" for
      a specified behaviour in frontier models.
    theory_of_change: Use AI to help improve interp and evals. Develop and release
      open tools to level up the whole field. Get invited to improve lab processes.
    see_also:
    - sec:White_box_safety_i_e_Interpretability_
    orthodox_problems:
    - superintelligence_fool_supervisors
    - superintelligence_hack_software
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitive
    some_names:
    - Transluce
    - Jacob Steinhardt
    - Neil Chowdhury
    - Vincent Huang
    - Sarah Schwettmann
    - Robert Friel
    estimated_ftes: 15-30
    critiques: null
    funded_by: Schmidt Sciences, Halcyon Futures, John Schulman, Wojciech Zaremba
    outputs:
    - link_url: https://transluce.org/jailbreaking-frontier-models
      link_text: Automatically Jailbreaking Frontier Language Models with Investigator
        Agents
      original_md: '[Automatically Jailbreaking Frontier Language Models with Investigator
        Agents](https://transluce.org/jailbreaking-frontier-models)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://transluce.org/pathological-behaviors
      link_text: Surfacing Pathological Behaviors in Language Models
      original_md: '[Surfacing Pathological Behaviors in Language Models](https://transluce.org/pathological-behaviors)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://transluce.org/investigating-o3-truthfulness
      link_text: Investigating truthfulness in a pre-release o3 model
      original_md: '[Investigating truthfulness in a pre-release o3 model](https://transluce.org/investigating-o3-truthfulness)'
      title: Investigating truthfulness in a pre-release o3 model
      authors:
      - Neil Chowdhury
      - Daniel Johnson
      - Vincent Huang
      - Jacob Steinhardt
      - Sarah Schwettmann
      author_organizations:
      - Transluce
      date: '2025-04-16'
      published_year: 2025
      venue: Transluce Blog
      kind: blog_post
    - link_url: https://transluce.org/neuron-circuits
      link_text: Neuron circuits
      original_md: '[Neuron circuits](https://transluce.org/neuron-circuits)'
      title: Language Model Circuits Are Sparse in the Neuron Basis
      authors:
      - Aryaman Arora
      - Zhengxuan Wu
      - Jacob Steinhardt
      - Sarah Schwettmann
      author_organizations:
      - Transluce
      date: '2025-11-20'
      published_year: 2025
      venue: Transluce AI
      kind: paper_preprint
    - link_url: https://transluce.org/introducing-docent
      link_text: 'Docent: A system for analyzing and intervening on agent behavior'
      original_md: '[Docent: A system for analyzing and intervening on agent behavior](https://transluce.org/introducing-docent)'
      title: Introducing Docent
      authors:
      - Kevin Meng
      - Vincent Huang
      - Jacob Steinhardt
      - Sarah Schwettmann
      author_organizations:
      - Transluce
      date: '2025-03-24'
      published_year: 2025
      venue: Transluce Blog
      kind: blog_post
    other_attributes: {}
  parsing_issues: []
- id: a:Debate
  name: Debate
  header_level: 3
  parent_id: sec:Make_AI_solve_it
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: In the limit, it's easier to compellingly argue for true
      claims than for false claims; exploit this asymmetry to get trusted work out
      of untrusted debaters.
    theory_of_change: '"Give humans help in supervising strong agents" + "Align explanations
      with the true reasoning process of the agent" + "Red team models to exhibit
      failure modes that don''t occur in normal use" are necessary but probably not
      sufficient for safe AGI.'
    see_also: []
    orthodox_problems:
    - value_fragile
    - superintelligence_fool_supervisors
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: null
    broad_approach_text: engineering / cognitive
    some_names:
    - Rohin Shah
    - Jonah Brown-Cohen
    - Georgios Piliouras
    - UK AISI (Benjamin Holton)
    estimated_ftes: null
    critiques: '[The limits of AI safety via debate (2022)](https://www.lesswrong.com/posts/kguLeJTt6LnGuYX4E/the-limits-of-ai-safety-via-debate)'
    funded_by: Google, others
    outputs:
    - link_url: https://www.lesswrong.com/s/NdovveRcyfxgMoujf
      link_text: 'UK AISI Alignment Team: Debate Sequence'
      original_md: '[UK AISI Alignment Team: Debate Sequence](https://www.lesswrong.com/s/NdovveRcyfxgMoujf)'
      title: 'UK AISI Alignment Team: Debate Sequence'
      authors:
      - Benjamin Hilton
      author_organizations:
      - UK AI Safety Institute
      date: '2025-05-07'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/8XHBaugB5S3r27MG9/prover-estimator-debate-a-new-scalable-oversight-protocol
      link_text: 'Prover-Estimator Debate: A New Scalable Oversight Protocol'
      original_md: '[Prover-Estimator Debate: A New Scalable Oversight Protocol](https://lesswrong.com/posts/8XHBaugB5S3r27MG9/prover-estimator-debate-a-new-scalable-oversight-protocol)'
      title: 'Prover-Estimator Debate: A New Scalable Oversight Protocol'
      authors:
      - Jonah Brown-Cohen
      - Geoffrey Irving
      author_organizations:
      - UK AISI
      date: '2025-06-17'
      published_year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2506.02175
      link_text: AI Debate Aids Assessment of Controversial Claims
      original_md: '[AI Debate Aids Assessment of Controversial Claims](https://arxiv.org/abs/2506.02175)'
      title: AI Debate Aids Assessment of Controversial Claims
      authors:
      - Salman Rahman
      - Sheriff Issaka
      - Ashima Suvarna
      - Genglin Liu
      - James Shiffer
      - Jaeyoung Lee
      - Md Rizwan Parvez
      - Hamid Palangi
      - Shi Feng
      - Nanyun Peng
      - Yejin Choi
      - Julian Michael
      - Liwei Jiang
      - Saadia Gabriel
      author_organizations:
      - University of Washington
      - Microsoft Research
      - UCLA
      - NYU
      date: '2025-06-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.03989
      link_text: An alignment safety case sketch based on debate
      original_md: '[An alignment safety case sketch based on debate](https://arxiv.org/abs/2505.03989)'
      title: An alignment safety case sketch based on debate
      authors:
      - Marie Davidsen Buhl
      - Jacob Pfau
      - Benjamin Hilton
      - Geoffrey Irving
      author_organizations:
      - Google DeepMind
      date: '2025-05-23'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.00091
      link_text: Ensemble Debates with Local Large Language Models for AI Alignment
      original_md: '[Ensemble Debates with Local Large Language Models for AI Alignment](https://arxiv.org/abs/2509.00091)'
      title: Ensemble Debates with Local Large Language Models for AI Alignment
      authors:
      - Ephraiem Sarabamoun
      author_organizations: []
      date: '2025-08-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.andrew.cmu.edu/user/coesterh/LMCA_dataset.pdf
      link_text: A dataset of rated conceptual arguments
      original_md: '[A dataset of rated conceptual arguments](https://www.andrew.cmu.edu/user/coesterh/LMCA_dataset.pdf)'
      title: LMCA Dataset
      authors: []
      author_organizations:
      - Carnegie Mellon University
      date: null
      published_year: null
      venue: null
      kind: error_detected
    other_attributes: {}
  parsing_issues:
  - Broad approach field contains multiple approaches 'engineering / cognitive' -
    broad_approach_id set to null
- id: a:LLM_introspection_training
  name: LLM introspection training
  header_level: 3
  parent_id: sec:Make_AI_solve_it
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Train LLMs to the predict the outputs of high-quality whitebox
      methods, to induce general self-explanation skills that use its own 'introspective'
      access
    theory_of_change: Use the resulting LLMs as powerful dimensionality reduction,
      explaining internals in a distinct way than interpretability methods and CoT.
      Distilling self-explanation into the model should lead to the skill being scalable,
      since self-explanation skill advancement will feed off general-intelligence
      advancement.
    see_also:
    - '[Transluce](#a:transluce)'
    - a:Anthropic
    orthodox_problems:
    - goals_misgeneralize
    - superintelligence_fool_supervisors
    - superintelligence_hack_software
    target_case_id: null
    target_case_text: mixed
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - Belinda Z. Li
    - Zifan Carl Guo
    - Vincent Huang
    - Jacob Steinhardt
    - Jacob Andreas
    - Jack Lindsey
    estimated_ftes: 2-20
    critiques: null
    funded_by: Schmidt Sciences, Halcyon Futures, John Schulman, Wojciech Zaremba
    outputs:
    - link_url: https://arxiv.org/abs/2511.08579
      link_text: Training Language Models to Explain Their Own Computations
      original_md: '[Training Language Models to Explain Their Own Computations](https://arxiv.org/abs/2511.08579)'
      title: Training Language Models to Explain Their Own Computations
      authors:
      - Belinda Z. Li
      - Zifan Carl Guo
      - Vincent Huang
      - Jacob Steinhardt
      - Jacob Andreas
      author_organizations:
      - UC Berkeley
      - MIT
      date: '2025-11-11'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://transformer-circuits.pub/2025/introspection/index.html
      link_text: Emergent Introspective Awareness
      original_md: '[Emergent Introspective Awareness](https://transformer-circuits.pub/2025/introspection/index.html)'
      title: Emergent Introspective Awareness in Large Language Models
      authors:
      - Jack Lindsey
      author_organizations:
      - Anthropic
      date: '2025-10-29'
      published_year: 2025
      venue: Transformer Circuits Thread
      kind: blog_post
    other_attributes: {}
  parsing_issues:
  - Target case field says 'mixed' - left target_case_id as null and target_case_text
    as 'mixed'
  - Broad approach field says 'cognitive' - mapped to 'cognitivist science'
- id: sec:Theory
  name: Theory
  header_level: 1
  parent_id: null
  content: '*Develop a principled scientific understanding that will help us reliably
    understand and control current and future AI systems.*'
  item_type: section
  agenda_attributes: null
  parsing_issues: []
- id: a:Agent_foundations
  name: Agent foundations
  header_level: 3
  parent_id: sec:Theory
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Develop philosophical clarity and mathematical formalizations
      of building blocks that might be useful for plans to align strong superintelligence,
      such as agency, optimization strength, decision theory, abstractions, concepts,
      etc.
    theory_of_change: Rigorously understand optimization processed and agents, and
      what it means for them to be aligned in a substrate independent way → identify
      impossibility results and necessary conditions for aligned optimizer systems
      → use this theoretical understanding to eventually design safe architectures
      that remain stable and safe under self-reflection
    see_also:
    - a:Aligning_what_
    - a:Tiling_agents
    - '[Dovetail](#a:theory_dovetail)'
    orthodox_problems:
    - value_fragile
    - corrigibility_anti_natural
    - goals_misgeneralize
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitive
    some_names:
    - Abram Demski
    - Alex Altair
    - Sam Eisenstat
    - Thane Ruthenis
    - Alfred Harwood
    - Daniel C
    - Dalcy K
    - José Pedro Faustino
    estimated_ftes: null
    critiques: null
    funded_by: null
    outputs:
    - link_url: https://www.arxiv.org/pdf/2508.16245
      link_text: Limit-Computable Grains of Truth for Arbitrary Computable Extensive-Form
        (Un)Known Games
      original_md: '[Limit-Computable Grains of Truth for Arbitrary Computable Extensive-Form
        (Un)Known Games](https://www.arxiv.org/pdf/2508.16245)'
      title: Limit-Computable Grains of Truth for Arbitrary Computable Extensive-Form
        (Un)Known Games
      authors:
      - Cole Wyeth
      - Marcus Hutter
      - Jan Leike
      - Jessica Taylor
      author_organizations:
      - Independent
      - DeepMind
      - MIRI
      date: '2025-08-22'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://uaiasi.com/blog-posts/
      link_text: UAIASI
      original_md: '[UAIASI](https://uaiasi.com/blog-posts/)'
      title: Blog Posts – Universal Algorithmic Intelligence
      authors:
      - Cole Wyeth
      author_organizations:
      - Universal Algorithmic Intelligence
      date: null
      published_year: 2025
      venue: Universal Algorithmic Intelligence website
      kind: blog_post
    - link_url: https://www.lesswrong.com/posts/EyvJvYEFzDv5kGoiG/clarifying-wisdom-foundational-topics-for-aligned-ais-to
      link_text: 'Clarifying "wisdom": Foundational topics for aligned AIs to prioritize
        before irreversible decisions'
      original_md: '[Clarifying "wisdom": Foundational topics for aligned AIs to prioritize
        before irreversible decisions](https://www.lesswrong.com/posts/EyvJvYEFzDv5kGoiG/clarifying-wisdom-foundational-topics-for-aligned-ais-to)'
      title: 'Unable to access: 429 Too Many Requests'
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: error_detected
    - link_url: https://www.lesswrong.com/posts/Dt4DuCCok3Xv5HEnG/agent-foundations-not-really-math-not-really-science
      link_text: 'Agent foundations: not really math, not really science'
      original_md: '[Agent foundations: not really math, not really science](https://www.lesswrong.com/posts/Dt4DuCCok3Xv5HEnG/agent-foundations-not-really-math-not-really-science)'
      title: 'Agent foundations: not really math, not really science'
      authors:
      - Alex_Altair
      author_organizations:
      - Dovetail Research
      date: '2025-08-17'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://link.springer.com/article/10.1007/s11098-025-02296-x
      link_text: Off-switching not guaranteed
      original_md: '[Off-switching not guaranteed](https://link.springer.com/article/10.1007/s11098-025-02296-x)'
      title: Off-switching not guaranteed
      authors:
      - Sven Neth
      author_organizations:
      - University of Pittsburgh
      date: '2025-02-26'
      published_year: 2025
      venue: Philosophical Studies
      kind: paper_published
    - link_url: https://openreview.net/forum?id=tlkYPU3FlX
      link_text: Formalizing Embeddedness Failures in Universal Artificial Intelligence
      original_md: '[Formalizing Embeddedness Failures in Universal Artificial Intelligence](https://openreview.net/forum?id=tlkYPU3FlX)'
      title: Formalizing Embeddedness Failures in Universal Artificial Intelligence
      authors:
      - Cole Wyeth
      - Marcus Hutter
      author_organizations: []
      date: '2025-07-01'
      published_year: 2025
      venue: ODYSSEY 2025 Conference
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/nuDJNyG5XLQjtvaeg/is-alignment-reducible-to-becoming-more-coherent
      link_text: Is alignment reducible to becoming more coherent?
      original_md: '[Is alignment reducible to becoming more coherent?](https://lesswrong.com/posts/nuDJNyG5XLQjtvaeg/is-alignment-reducible-to-becoming-more-coherent)'
      title: Is alignment reducible to becoming more coherent?
      authors:
      - Cole Wyeth
      author_organizations: []
      date: '2025-04-22'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/dHNKtQ3vTBxTfTPxu/what-is-the-alignment-problem
      link_text: What Is The Alignment Problem?
      original_md: '[What Is The Alignment Problem?](https://lesswrong.com/posts/dHNKtQ3vTBxTfTPxu/what-is-the-alignment-problem)'
      title: What Is The Alignment Problem?
      authors:
      - johnswentworth
      author_organizations: []
      date: '2025-01-16'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://openreview.net/pdf?id=Rf1CeGPA22
      link_text: Good old fashioned decision theory
      original_md: '[Good old fashioned decision theory](https://openreview.net/pdf?id=Rf1CeGPA22)'
      title: Unable to extract - PDF content not accessible
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: OpenReview
      kind: error_detected
    - link_url: https://www.lesswrong.com/posts/ApfjBbqzSu4aZoLSe/report-and-retrospective-on-the-dovetail-fellowship
      link_text: Report & retrospective on the Dovetail fellowship
      original_md: '[Report & retrospective on the Dovetail fellowship](https://www.lesswrong.com/posts/ApfjBbqzSu4aZoLSe/report-and-retrospective-on-the-dovetail-fellowship)'
      title: Report & retrospective on the Dovetail fellowship
      authors:
      - Alex Altair
      author_organizations:
      - Dovetail Research
      date: '2025-03-14'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    other_attributes: {}
  parsing_issues:
  - 'See also references ''Dovetail'' with link #a:theory_dovetail which is not in
    the available agendas list'
- id: a:Tiling_agents
  name: Tiling agents
  header_level: 3
  parent_id: sec:Theory
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: An aligned agentic system modifying itself into an unaligned
      system would be bad and we can research ways that this could occur and infrastructure/approaches
      that prevent it from happening.
    theory_of_change: Build enough theoretical basis through various approaches such
      that AI systems we create are capable of self-modification while preserving
      goals.
    see_also:
    - a:Agent_foundations
    orthodox_problems:
    - value_fragile
    - corrigibility_anti_natural
    - goals_misgeneralize
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitivist science
    some_names:
    - Abram Demski
    estimated_ftes: 1-10
    critiques: null
    funded_by: null
    outputs:
    - link_url: https://www.lesswrong.com/posts/akuMwu8SkmQSdospi/working-through-a-small-tiling-result
      link_text: Working through a small tiling result
      original_md: '[Working through a small tiling result](https://www.lesswrong.com/posts/akuMwu8SkmQSdospi/working-through-a-small-tiling-result)'
      title: Working through a small tiling result
      authors:
      - James Payor
      author_organizations: []
      date: '2024-05-13'
      published_year: 2024
      venue: LessWrong
      kind: lesswrong
    - link_url: https://openreview.net/forum?id=Rf1CeGPA22
      link_text: Communication & Trust
      original_md: '[Communication & Trust](https://openreview.net/forum?id=Rf1CeGPA22)'
      title: Communication & Trust
      authors:
      - Abram Demski
      author_organizations: []
      date: '2025-07-09'
      published_year: 2025
      venue: ODYSSEY 2025 Conference
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/PhgEKkB4cwYjwpGxb/maintaining-alignment-during-rsi-as-a-feedback-control
      link_text: Maintaining Alignment during RSI as a Feedback Control Problem
      original_md: '[Maintaining Alignment during RSI as a Feedback Control Problem](https://lesswrong.com/posts/PhgEKkB4cwYjwpGxb/maintaining-alignment-during-rsi-as-a-feedback-control)'
      title: Maintaining Alignment during RSI as a Feedback Control Problem
      authors:
      - beren
      author_organizations: []
      date: '2025-03-02'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://static1.squarespace.com/static/663d1233249bce4815fe8753/t/68067a6f5d5fb0745642d5b1/1745255023842/Understanding+Trust+-+Abram+Demski.pdf
      link_text: Understanding Trust
      original_md: '[Understanding Trust](https://static1.squarespace.com/static/663d1233249bce4815fe8753/t/68067a6f5d5fb0745642d5b1/1745255023842/Understanding+Trust+-+Abram+Demski.pdf)'
      title: Understanding Trust
      authors:
      - Abram Demski
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: error_detected
    other_attributes: {}
  parsing_issues: []
- id: a:High_Actuation_Spaces
  name: High-Actuation Spaces
  header_level: 3
  parent_id: sec:Theory
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Mech interp and alignment assume a stable "computational
      substrate" (linear algebra on GPUs). If later AI uses different substrates (e.g.
      something neuromorphic), methods like probes and steering will not transfer.
      Therefore, better to try and infer goals via a "telic DAG" which abstracts over
      substrates, and so sidestep the issue of how to define intermediate representations.
      Category theory is intended to provide guarantees that this abstraction is valid.
    theory_of_change: Sufficiently complex mindlike entities can alter their goals
      in ways that cannot be predicted or accounted for under substrate-dependent
      descriptions of the kind sought in mechanistic interpretability. use the telic
      DAG to define a method analogous to factoring a causal DAG.
    see_also:
    - '[Live theory](https://www.lesswrong.com/s/aMz2JMvgXrLBkq4h3)'
    - '[MoSSAIC](https://openreview.net/forum?id=n7WYSJ35FU)'
    - '[Topos Institute](https://topos.institute/)'
    - a:Agent_foundations
    orthodox_problems: []
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: null
    broad_approach_text: maths / philosophy
    some_names:
    - Sahil K
    - Matt Farr
    - Aditya Arpitha Prasad
    - Chris Pang
    - Aditya Adiga
    - Jayson Amati
    - Steve Petersen
    - Topos
    - T J
    estimated_ftes: 1-10
    critiques: null
    funded_by: null
    outputs:
    - link_url: https://groundless.ai/
      link_text: groundless.ai
      original_md: '[groundless.ai](https://groundless.ai/)'
      title: Groundless Alignment
      authors: []
      author_organizations:
      - Groundless
      date: null
      published_year: 2025
      venue: Groundless Website
      kind: personal_page
    - link_url: https://www.lesswrong.com/s/aMz2JMvgXrLBkq4h3
      link_text: Live Theory
      original_md: '[Live Theory](https://www.lesswrong.com/s/aMz2JMvgXrLBkq4h3)'
      title: '429: Too Many Requests'
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: error_detected
    - link_url: https://docs.google.com/document/d/1d-ARdZZDHFPIfGcTTOKK8IZWlQj0NZQrmteJj2mvmYA/edit?tab=t.0#heading=h.eg8luyrlsv2u
      link_text: High Actuation Spaces - Sahil
      original_md: '[High Actuation Spaces \- Sahil](https://docs.google.com/document/d/1d-ARdZZDHFPIfGcTTOKK8IZWlQj0NZQrmteJj2mvmYA/edit?tab=t.0#heading=h.eg8luyrlsv2u)'
      title: High Actuation Spaces - Sahil
      authors:
      - Sahil
      author_organizations: []
      date: null
      published_year: null
      venue: Google Docs
      kind: error_detected
    - link_url: https://www.lesswrong.com/posts/tQ9vWm4b57HFqbaRj/what-if-not-agency
      link_text: What, if not agency?
      original_md: '[What, if not agency?](https://www.lesswrong.com/posts/tQ9vWm4b57HFqbaRj/what-if-not-agency)'
      title: Unknown - Content Unavailable (429 Error)
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: error_detected
    - link_url: https://docs.google.com/document/d/1fl7LE8AN7mLJ6uFcPuFCzatp0zCIYvjRIjQRgHPAkSE/edit?tab=t.0
      link_text: Human Inductive Bias Project
      original_md: '[Human Inductive Bias Project](https://docs.google.com/document/d/1fl7LE8AN7mLJ6uFcPuFCzatp0zCIYvjRIjQRgHPAkSE/edit?tab=t.0)'
      title: HIBP Human Inductive Bias Project Plan
      authors:
      - Félix Dorn
      author_organizations: []
      date: null
      published_year: null
      venue: Google Docs
      kind: error_detected
    - link_url: https://openreview.net/forum?id=n7WYSJ35FU
      link_text: 'MoSSAIC: AI Safety After Mechanism'
      original_md: '[MoSSAIC: AI Safety After Mechanism](https://openreview.net/forum?id=n7WYSJ35FU)'
      title: 'MoSSAIC: AI Safety After Mechanism'
      authors:
      - Matt Farr
      - Aditya Arpitha Prasad
      - Chris Pang
      - Aditya Adiga
      - Jayson Amati
      - Sahil K
      author_organizations: []
      date: '2025-07-01'
      published_year: 2025
      venue: ODYSSEY 2025 Conference
      kind: paper_preprint
    - link_url: https://drive.google.com/drive/folders/1EaAJ4szuZsYR2_-DkS9cuhx3S6IWeCjW
      link_text: HAS - Public (High Actuation Spaces)
      original_md: '[HAS \- Public (High Actuation Spaces)](https://drive.google.com/drive/folders/1EaAJ4szuZsYR2_-DkS9cuhx3S6IWeCjW)'
      title: HAS - Public (High Actuation Spaces)
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: Google Drive
      kind: error_detected
    other_attributes: {}
  parsing_issues:
  - Broad approach is 'maths / philosophy' which doesn't match standard categories
    (engineering, behaviorist_science, cognitivist_science). Kept as text, set ID
    to null.
- id: a:Asymptotic_guarantees
  name: Asymptotic guarantees
  header_level: 3
  parent_id: sec:Theory
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Prove that if a safety process has enough resources (human
      data quality, training time, neural network capacity), then in the limit some
      system specification will be guaranteed. Use complexity theory, game theory,
      learning theory and other areas to both improve asymptotic guarantees and develop
      ways of showing convergence.
    theory_of_change: Formal verification may be too hard. Make safety cases stronger
      by modelling their processes and proving that they would work in the limit.
    see_also:
    - a:Debate
    - a:Guaranteed_Safe_AI
    - a:Control
    orthodox_problems:
    - goals_misgeneralize
    - superintelligence_fool_supervisors
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitive
    some_names:
    - AISI
    - Jacob Pfau
    - Benjamin Hilton
    - Geoffrey Irving
    - Simon Marshall
    - Will Kirby
    - Martin Soto
    - David Africa
    - davidad
    estimated_ftes: 5 - 10
    critiques: 'Self-critique in [UK AISI''s Alignment Team: Research Agenda](https://lesswrong.com/posts/tbnw7LbNApvxNLAg8/uk-aisi-s-alignment-team-research-agenda)'
    funded_by: AISI
    outputs:
    - link_url: https://lesswrong.com/posts/iELyAqizJkizBQbfr/an-alignment-safety-case-sketch-based-on-debate
      link_text: An alignment safety case sketch based on debate
      original_md: '[An alignment safety case sketch based on debate](https://lesswrong.com/posts/iELyAqizJkizBQbfr/an-alignment-safety-case-sketch-based-on-debate)'
      title: An alignment safety case sketch based on debate
      authors:
      - Marie_DB
      - Jacob Pfau
      - Benjamin Hilton
      - Geoffrey Irving
      author_organizations:
      - UK AISI
      date: '2025-05-08'
      published_year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/tbnw7LbNApvxNLAg8/uk-aisi-s-alignment-team-research-agenda
      link_text: 'UK AISI''s Alignment Team: Research Agenda'
      original_md: '[UK AISI''s Alignment Team: Research Agenda](https://lesswrong.com/posts/tbnw7LbNApvxNLAg8/uk-aisi-s-alignment-team-research-agenda)'
      title: 'UK AISI''s Alignment Team: Research Agenda'
      authors:
      - Benjamin Hilton
      - Jacob Pfau
      - Marie_DB
      - Geoffrey Irving
      author_organizations:
      - UK AI Safety Institute
      date: '2025-05-07'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/EgRJtwQurNzz8CEfJ/dodging-systematic-human-errors-in-scalable-oversight
      link_text: Dodging systematic human errors in scalable oversight
      original_md: '[Dodging systematic human errors in scalable oversight](https://lesswrong.com/posts/EgRJtwQurNzz8CEfJ/dodging-systematic-human-errors-in-scalable-oversight)'
      title: Dodging systematic human errors in scalable oversight
      authors:
      - Geoffrey Irving
      author_organizations:
      - UK AISI
      date: '2025-05-14'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://openreview.net/pdf?id=XOIKLlSiDq
      link_text: Can DPO Learn Diverse Human Values? A Theoretical Scaling Law
      original_md: '[Can DPO Learn Diverse Human Values? A Theoretical Scaling Law](https://openreview.net/pdf?id=XOIKLlSiDq)'
      title: Unable to extract - PDF content not accessible
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: OpenReview
      kind: error_detected
    other_attributes: {}
  parsing_issues: []
- id: a:Heuristic_explanations
  name: Heuristic explanations
  header_level: 3
  parent_id: sec:Theory
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Formalize mechanistic explanations of neural network behavior,
      automate the discovery of these "heuristic explanations" and use them to predict
      when novel input will lead to extreme behavior (i.e. "Low Probability Estimation"
      and "Mechanistic Anomaly Detection").
    theory_of_change: The current goalpost is methods whose *reasoned predictions*
      about properties of a neural network's outputs distribution (for a given inputs
      distribution) are certifiably at least as accurate as estimations via sampling.
      If successful for safety-relevant properties, this should allow for automated
      alignment methods that are both human-legible and worst-case certified, as well
      more efficient than sampling-based methods in most cases.
    see_also:
    - ARC Theory
    - ELK
    - mechanistic anomaly detection
    - '[Acorn](https://acausal.org/)'
    - a:Guaranteed_Safe_AI
    orthodox_problems:
    - goals_misgeneralize
    - superintelligence_hack_software
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: null
    broad_approach_text: cognitive / maths/philosophy
    some_names:
    - Jacob Hilton
    - Mark Xu
    - Eric Neyman
    - Victor Lecomte
    - George Robinson
    estimated_ftes: 1-10
    critiques: '[Matolcsi](https://www.lesswrong.com/s/uYMw689vDFmgPEHrS)'
    funded_by: null
    outputs:
    - link_url: https://www.lesswrong.com/posts/Xt9r4SNNuYxW83tmo/a-computational-no-coincidence-principle
      link_text: A computational no-coincidence principle
      original_md: '[A computational no-coincidence principle](https://www.lesswrong.com/posts/Xt9r4SNNuYxW83tmo/a-computational-no-coincidence-principle)'
      title: A computational no-coincidence principle
      authors:
      - Eric Neyman
      author_organizations:
      - Alignment Research Center
      date: '2025-02-14'
      published_year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - link_url: https://www.lesswrong.com/posts/XdQd9gELHakd5pzJA/arc-progress-update-competing-with-sampling
      link_text: Competing with sampling
      original_md: '[Competing with sampling](https://www.lesswrong.com/posts/XdQd9gELHakd5pzJA/arc-progress-update-competing-with-sampling)'
      title: 'ARC progress update: Competing with sampling'
      authors:
      - Eric Neyman
      author_organizations:
      - Alignment Research Center
      date: '2025-11-18'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://www.lesswrong.com/s/uYMw689vDFmgPEHrS
      link_text: Obstacles in ARC's research agenda
      original_md: '[Obstacles in ARC''s research agenda](https://www.lesswrong.com/s/uYMw689vDFmgPEHrS)'
      title: Obstacles in ARC's agenda
      authors:
      - David Matolcsi
      author_organizations:
      - ARC
      date: '2025-04-30'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://gabrieldwu.com/assets/thesis.pdf
      link_text: Deduction-Projection Estimators for Understanding Neural Networks
      original_md: '[Deduction-Projection Estimators for Understanding Neural Networks](https://gabrieldwu.com/assets/thesis.pdf)'
      title: Unknown - PDF content not accessible
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: error_detected
    - link_url: https://openreview.net/forum?id=m4OpQAK3eY
      link_text: Wide Neural Networks as a Baseline for the Computational No-Coincidence
        Conjecture
      original_md: '[Wide Neural Networks as a Baseline for the Computational No-Coincidence
        Conjecture](https://openreview.net/forum?id=m4OpQAK3eY)'
      title: Wide Neural Networks as a Baseline for the Computational No-Coincidence
        Conjecture
      authors:
      - John Dunbar
      - Scott Aaronson
      author_organizations:
      - University of Texas at Austin
      date: '2025-07-07'
      published_year: 2025
      venue: ODYSSEY 2025 Conference
      kind: paper_preprint
    other_attributes: {}
  parsing_issues:
  - Broad approach field says 'cognitive / maths/philosophy' - does not match standard
    approach categories, left broad_approach_id as null
- id: sec:Corrigibility
  name: Corrigibility
  header_level: 2
  parent_id: sec:Theory
  content: ''
  item_type: section
  agenda_attributes: null
  parsing_issues: []
- id: a:Behavior_alignment_theory
  name: Behavior alignment theory
  header_level: 3
  parent_id: sec:Corrigibility
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Predict properties of future AGI (e.g. power-seeking) with
      formal models; formally state and prove hypotheses about the properties powerful
      systems will have and how we might try to change them.
    theory_of_change: Figure out hypotheses about properties powerful agents will
      have → attempt to rigorously prove under what conditions the hypotheses hold
      → test these hypotheses where feasible → design training environments that lead
      to more salutary properties.
    see_also:
    - a:Agent_foundations
    - a:Control
    orthodox_problems:
    - corrigibility_anti_natural
    - instrumental_convergence
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: null
    broad_approach_text: maths / philosophy
    some_names:
    - Ram Potham
    - Michael K. Cohen
    - Max Harms/Raelifin
    - John Wentworth
    - David Lorell
    - Elliott Thornley
    estimated_ftes: 1-10
    critiques: '[Ryan Greenblatt''s criticism](https://www.lesswrong.com/posts/YbEbwYWkf8mv9jnmi/the-shutdown-problem-incomplete-preferences-as-a-solution?commentId=GJAippZ6ZzCagSnDb)
      of one behavioural proposal'
    funded_by: null
    outputs:
    - link_url: https://www.lesswrong.com/posts/knwR9RgGN5a2oorci/preference-gaps-as-a-safeguard-against-ai-self-replication
      link_text: Preference gaps as a safeguard against AI self-replication
      original_md: '[Preference gaps as a safeguard against AI self-replication](https://www.lesswrong.com/posts/knwR9RgGN5a2oorci/preference-gaps-as-a-safeguard-against-ai-self-replication)'
      title: Preference gaps as a safeguard against AI self-replication
      authors:
      - tbs
      - EJT
      author_organizations: []
      date: '2025-11-26'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://www.lesswrong.com/s/KfCjeconYRdFbMxsy/p/qgBFJ72tahLo5hzqy
      link_text: Serious Flaws in CAST
      original_md: '[Serious Flaws in CAST](https://www.lesswrong.com/s/KfCjeconYRdFbMxsy/p/qgBFJ72tahLo5hzqy)'
      title: Serious Flaws in CAST
      authors:
      - Max Harms
      author_organizations:
      - MIRI
      date: '2025-11-19'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://www.lesswrong.com/posts/PhTBDHu9PKJFmvb4p/a-shutdown-problem-proposal
      link_text: A Shutdown Problem Proposal
      original_md: '[A Shutdown Problem Proposal](https://www.lesswrong.com/posts/PhTBDHu9PKJFmvb4p/a-shutdown-problem-proposal)'
      title: A Shutdown Problem Proposal
      authors:
      - johnswentworth
      - David Lorell
      author_organizations: []
      date: '2024-01-21'
      published_year: 2024
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2505.20203
      link_text: Shutdownable Agents through POST-Agency
      original_md: '[Shutdownable Agents through POST-Agency](https://arxiv.org/abs/2505.20203)'
      title: Shutdownable Agents through POST-Agency
      authors:
      - Elliott Thornley
      author_organizations: []
      date: '2025-05-26'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.17749
      link_text: The Partially Observable Off-Switch Game
      original_md: '[The Partially Observable Off-Switch Game](https://arxiv.org/abs/2411.17749)'
      title: The Partially Observable Off-Switch Game
      authors:
      - Andrew Garber
      - Rohan Subramani
      - Linus Luu
      - Mark Bedaywi
      - Stuart Russell
      - Scott Emmons
      author_organizations:
      - UC Berkeley
      date: '2024-11-25'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://onlinelibrary.wiley.com/doi/10.1002/aaai.70040?af=R
      link_text: Imitation learning is probably existentially safe
      original_md: '[Imitation learning is probably existentially safe](https://onlinelibrary.wiley.com/doi/10.1002/aaai.70040?af=R)'
      title: Imitation learning is probably existentially safe
      authors:
      - Michael K. Cohen
      - Marcus Hutter
      author_organizations:
      - University of California, Berkeley
      - Australian National University
      date: '2025-11-21'
      published_year: 2025
      venue: AI Magazine
      kind: paper_published
    - link_url: https://arxiv.org/abs/2508.00159
      link_text: Model-Based Soft Maximization of Suitable Metrics of Long-Term Human
        Power
      original_md: '[Model-Based Soft Maximization of Suitable Metrics of Long-Term
        Human Power](https://arxiv.org/abs/2508.00159)'
      title: Model-Based Soft Maximization of Suitable Metrics of Long-Term Human
        Power
      authors:
      - Jobst Heitzig
      - Ram Potham
      author_organizations: []
      date: '2025-07-31'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/9htmQx5wiePqTtZuL/deceptive-alignment-and-homuncularity
      link_text: Deceptive Alignment and Homuncularity
      original_md: '[Deceptive Alignment and Homuncularity](https://lesswrong.com/posts/9htmQx5wiePqTtZuL/deceptive-alignment-and-homuncularity)'
      title: Deceptive Alignment and Homuncularity
      authors:
      - Oliver Sourbut
      - TurnTrout
      author_organizations:
      - UK AI Safety Institute
      - Independent
      date: '2025-01-16'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://openreview.net/forum?id=mhEnJa9pNk
      link_text: 'A Safety Case for a Deployed LLM: Corrigibility as a Singular Target'
      original_md: '[A Safety Case for a Deployed LLM: Corrigibility as a Singular
        Target](https://openreview.net/forum?id=mhEnJa9pNk)'
      title: 'A Safety Case for a Deployed LLM: Corrigibility as a Singular Target'
      authors:
      - Ram Potham
      author_organizations: []
      date: '2025-06-24'
      published_year: 2025
      venue: ODYSSEY 2025 Conference
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/aKncW36ZdEnzxLo8A/llm-agi-will-have-memory-and-memory-changes-alignment
      link_text: LLM AGI will have memory, and memory changes alignment
      original_md: '[LLM AGI will have memory, and memory changes alignment](https://lesswrong.com/posts/aKncW36ZdEnzxLo8A/llm-agi-will-have-memory-and-memory-changes-alignment)'
      title: LLM AGI will have memory, and memory changes alignment
      authors:
      - Seth Herd
      author_organizations: []
      date: '2025-04-04'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    other_attributes: {}
  parsing_issues:
  - Broad approach field says 'maths / philosophy' which doesn't match standard approaches
    (engineering, behaviorist science, cognitivist science)
- id: a:Other_corrigibility
  name: Other corrigibility
  header_level: 3
  parent_id: sec:Corrigibility
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Diagnose and communicate obstacles to achieving robustly
      corrigible behavior; suggest mechanisms, tests, and escalation channels for
      surfacing and mitigating incorrigible behaviors
    theory_of_change: Labs are likely to develop AGI using something analogous to
      current pipelines. Clarifying why naive instruction-following doesn't buy robust
      corrigibility + building strong tripwires/diagnostics for scheming and Goodharting
      thus reduces risks on the likely default path.
    see_also:
    - a:Behavior_alignment_theory
    orthodox_problems:
    - corrigibility_anti_natural
    - instrumental_convergence
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: null
    broad_approach_text: varies
    some_names:
    - Jeremy Gillen
    estimated_ftes: 1-10
    critiques: null
    funded_by: null
    outputs:
    - link_url: https://www.lesswrong.com/posts/LDYPF6yfe3f8SPHFT/ai-assistants-should-have-a-direct-line-to-their-developers
      link_text: AI Assistants Should Have a Direct Line to Their Developers
      original_md: '[AI Assistants Should Have a Direct Line to Their Developers](https://www.lesswrong.com/posts/LDYPF6yfe3f8SPHFT/ai-assistants-should-have-a-direct-line-to-their-developers)'
      title: AI Assistants Should Have a Direct Line to Their Developers
      authors:
      - Jan_Kulveit
      author_organizations: []
      date: '2024-12-28'
      published_year: 2024
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - link_url: https://www.lesswrong.com/posts/ZHFZ6tivEjznkEoby/detect-goodhart-and-shut-down
      link_text: Detect Goodhart and shut down
      original_md: '[Detect Goodhart and shut down](https://www.lesswrong.com/posts/ZHFZ6tivEjznkEoby/detect-goodhart-and-shut-down)'
      title: Detect Goodhart and shut down
      authors:
      - Jeremy Gillen
      author_organizations: []
      date: '2025-01-22'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://www.lesswrong.com/posts/7Z4WC4AFgfmZ3fCDC/instrumental-goals-are-a-different-and-friendlier-kind-of
      link_text: Instrumental Goals Are A Different And Friendlier Kind Of Thing Than
        Terminal Goals
      original_md: '[Instrumental Goals Are A Different And Friendlier Kind Of Thing
        Than Terminal Goals](https://www.lesswrong.com/posts/7Z4WC4AFgfmZ3fCDC/instrumental-goals-are-a-different-and-friendlier-kind-of)'
      title: Instrumental goals are a different and friendlier kind of [content unavailable]
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: error_detected
    - link_url: https://www.lesswrong.com/posts/JuRdvZyqaFbvTPemn/shutdownable-agents-through-post-agency-1
      link_text: Shutdownable Agents through POST-Agency
      original_md: '[Shutdownable Agents through POST-Agency](https://www.lesswrong.com/posts/JuRdvZyqaFbvTPemn/shutdownable-agents-through-post-agency-1)'
      title: Shutdownable Agents through POST-Agency
      authors:
      - EJT
      author_organizations: []
      date: '2025-09-16'
      published_year: 2025
      venue: LessWrong/AI Alignment Forum
      kind: lesswrong
    - link_url: https://www.lesswrong.com/posts/ksfjZJu3BFEfM6hHE/why-corrigibility-is-hard-and-important-i-e-whence-the-high
      link_text: Why Corrigibility is Hard and Important (i.e. "Whence the high MIRI
        confidence in alignment difficulty?")
      original_md: '[Why Corrigibility is Hard and Important (i.e. "Whence the high
        MIRI confidence in alignment difficulty?")](https://www.lesswrong.com/posts/ksfjZJu3BFEfM6hHE/why-corrigibility-is-hard-and-important-i-e-whence-the-high)'
      title: 'Error: Content Unavailable - Too Many Requests (429)'
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: error_detected
    - link_url: https://dl.acm.org/doi/10.1145/3717823.3718245
      link_text: 'Oblivious Defense in ML Models: Backdoor Removal without Detection'
      original_md: '[Oblivious Defense in ML Models: Backdoor Removal without Detection](https://dl.acm.org/doi/10.1145/3717823.3718245)'
      title: 'Oblivious Defense in ML Models: Backdoor Removal without Detection'
      authors:
      - Shafi Goldwasser
      - Jonathan Shafer
      - Neekon Vafa
      - Vinod Vaikuntanathan
      author_organizations:
      - University of California, Berkeley
      - Massachusetts Institute of Technology
      date: '2025-06-15'
      published_year: 2025
      venue: 'STOC ''25: Proceedings of the 57th Annual ACM Symposium on Theory of
        Computing'
      kind: paper_published
    - link_url: https://arxiv.org/abs/2509.20714
      link_text: 'Cryptographic Backdoor for Neural Networks: Boon and Bane'
      original_md: '[Cryptographic Backdoor for Neural Networks: Boon and Bane](https://arxiv.org/abs/2509.20714)'
      title: 'Cryptographic Backdoor for Neural Networks: Boon and Bane'
      authors:
      - Anh Tu Ngo
      - Anupam Chattopadhyay
      - Subhamoy Maitra
      author_organizations: []
      date: '2025-09-25'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.20310
      link_text: A Cryptographic Perspective on Mitigation vs. Detection in Machine
        Learning
      original_md: '[A Cryptographic Perspective on Mitigation vs. Detection in Machine
        Learning](https://arxiv.org/abs/2504.20310)'
      title: A Cryptographic Perspective on Mitigation vs. Detection in Machine Learning
      authors:
      - Greg Gluch
      - Shafi Goldwasser
      author_organizations: []
      date: '2025-04-28'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/CSFa9rvGNGAfCzBk6/problems-with-instruction-following-as-an-alignment-target
      link_text: Problems with instruction-following as an alignment target
      original_md: '[Problems with instruction-following as an alignment target](https://lesswrong.com/posts/CSFa9rvGNGAfCzBk6/problems-with-instruction-following-as-an-alignment-target)'
      title: Problems with instruction-following as an alignment target
      authors:
      - Seth Herd
      author_organizations: []
      date: '2025-05-15'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    other_attributes: {}
  parsing_issues:
  - Broad approach field says 'varies' - leaving broad_approach_id as null and setting
    broad_approach_text to 'varies'
- id: sec:Ontology_Identification
  name: Ontology Identification
  header_level: 2
  parent_id: sec:Theory
  content: ''
  item_type: section
  agenda_attributes: null
  parsing_issues: []
- id: a:Natural_abstractions
  name: Natural abstractions
  header_level: 3
  parent_id: sec:Ontology_Identification
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Develop a theory of concepts that explains how they are
      learned, how they structure a particular system's understanding, and how mutual
      translatability can be achieved between different collections of concepts.
    theory_of_change: Understand the concepts a system's understanding is structured
      with and use them to inspect its "alignment/safety properties" and/or "retarget
      its search", i.e. identify utility-function-like components inside an AI and
      replacing calls to them with calls to "user values" (represented using existing
      abstractions inside the AI).
    see_also:
    - a:Causal_Abstractions
    - representational alignment
    - convergent abstractions
    - feature universality
    - Platonic representation hypothesis
    - microscope AI
    orthodox_problems:
    - instrumental_convergence
    - superintelligence_fool_supervisors
    - humans_not_first_class
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitive
    some_names:
    - John Wentworth
    - Paul Colognese
    - David Lorrell
    - Sam Eisenstat
    - Fernando Rosas
    estimated_ftes: 1-10
    critiques: '[Chan et al (2023)](https://www.lesswrong.com/posts/gvzW46Z3BsaZsLc25/natural-abstractions-key-claims-theorems-and-critiques-1#3__A_formalization_of_abstractions_would_accelerate_alignment_research),
      [Soto](https://www.lesswrong.com/posts/CJjT8GMitsnKc2wgG/natural-abstractions-are-observer-dependent-a-conversation-1),
      [Harwood](https://www.lesswrong.com/posts/F4nzox6oh5oAdX9D3/abstractions-are-not-natural),
      [Soares (2023)](https://www.lesswrong.com/posts/mgjHS6ou7DgwhKPpu/a-rough-and-incomplete-review-of-some-of-john-wentworth-s)'
    funded_by: null
    outputs:
    - link_url: https://www.lesswrong.com/posts/T6xSXiXF3WF6TmCyN/abstract-mathematical-concepts-vs-abstractions-over-real
      link_text: Abstract Mathematical Concepts vs. Abstractions Over Real-World Systems
      original_md: '[Abstract Mathematical Concepts vs. Abstractions Over Real-World
        Systems](https://www.lesswrong.com/posts/T6xSXiXF3WF6TmCyN/abstract-mathematical-concepts-vs-abstractions-over-real)'
      title: Abstract mathematical concepts vs abstractions over real
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: error_detected
    - link_url: https://www.lesswrong.com/posts/BstHXPgQyfeNnLjjp/condensation
      link_text: Condensation
      original_md: '[Condensation](https://www.lesswrong.com/posts/BstHXPgQyfeNnLjjp/condensation)'
      title: Condensation
      authors:
      - abramdemski
      author_organizations: []
      date: '2025-11-09'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://phillipi.github.io/prh/
      link_text: Platonic representation hypothesis
      original_md: '[Platonic representation hypothesis](https://phillipi.github.io/prh/)'
      title: The Platonic Representation Hypothesis
      authors:
      - Minyoung Huh
      - Brian Cheung
      - Tongzhou Wang
      - Phillip Isola
      author_organizations:
      - MIT
      date: '2024-05-13'
      published_year: 2024
      venue: ICML 2024
      kind: paper_published
    - link_url: https://www.youtube.com/watch?v=Nr9eMobqUOo&t=3s
      link_text: Rosas
      original_md: '[Rosas](https://www.youtube.com/watch?v=Nr9eMobqUOo&t=3s)'
      title: 'Fernando Rosas: Identifying Abstractions (HAAISS 2025)'
      authors:
      - Fernando Rosas
      author_organizations: []
      date: '2025-10-06'
      published_year: 2025
      venue: HAAISS 2025
      kind: error_detected
    - link_url: https://arxiv.org/abs/2509.03780
      link_text: 'Natural Latents: Latent Variables Stable Across Ontologies'
      original_md: '[Natural Latents: Latent Variables Stable Across Ontologies](https://arxiv.org/abs/2509.03780)'
      title: 'Natural Latents: Latent Variables Stable Across Ontologies'
      authors:
      - John Wentworth
      - David Lorell
      author_organizations: []
      date: '2025-09-04'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://openreview.net/forum?id=HwKFJ3odui
      link_text: 'Condensation: a theory of concepts'
      original_md: '[Condensation: a theory of concepts](https://openreview.net/forum?id=HwKFJ3odui)'
      title: 'Condensation: a theory of concepts'
      authors:
      - Sam Eisenstat
      author_organizations: []
      date: '2025-07-04'
      published_year: 2025
      venue: ODYSSEY 2025 Conference
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2412.02579
      link_text: 'Factored space models: Towards causality between levels of abstraction'
      original_md: '[Factored space models: Towards causality between levels of abstraction](https://arxiv.org/abs/2412.02579)'
      title: 'Factored space models: Towards causality between levels of abstraction'
      authors:
      - Scott Garrabrant
      - Matthias Georg Mayer
      - Magdalena Wache
      - Leon Lang
      - Sam Eisenstat
      - Holger Dell
      author_organizations: []
      date: '2024-12-03'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/h89L5FMAkEBNsZ3xM/a-single-principle-related-to-many-alignment-subproblems-2
      link_text: A single principle related to many Alignment subproblems?
      original_md: '[A single principle related to many Alignment subproblems?](https://lesswrong.com/posts/h89L5FMAkEBNsZ3xM/a-single-principle-related-to-many-alignment-subproblems-2)'
      title: A single principle related to many Alignment subproblems?
      authors:
      - Q Home
      author_organizations: []
      date: '2025-04-30'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2310.13018
      link_text: Getting aligned on representational alignment
      original_md: '[Getting aligned on representational alignment](https://arxiv.org/abs/2310.13018)'
      title: Getting aligned on representational alignment
      authors:
      - Ilia Sucholutsky
      - Lukas Muttenthaler
      - Adrian Weller
      - Andi Peng
      - Andreea Bobu
      - Been Kim
      - Bradley C. Love
      - Christopher J. Cueva
      - Erin Grant
      - Iris Groen
      - Jascha Achterberg
      - Joshua B. Tenenbaum
      - Katherine M. Collins
      - Katherine L. Hermann
      - Kerem Oktar
      - Klaus Greff
      - Martin N. Hebart
      - Nathan Cloos
      - Nikolaus Kriegeskorte
      - Nori Jacoby
      - Qiuyi Zhang
      - Raja Marjieh
      - Robert Geirhos
      - Sherol Chen
      - Simon Kornblith
      - Sunayana Rane
      - Talia Konkle
      - Thomas P. O'Connell
      - Thomas Unterthiner
      - Andrew K. Lampinen
      - Klaus-Robert Müller
      - Mariya Toneva
      - Thomas L. Griffiths
      author_organizations:
      - University of Cambridge
      - Google DeepMind
      - MIT
      - University College London
      - Princeton University
      date: '2023-10-18'
      published_year: 2023
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/pdf/2512.00984%20
      link_text: Symmetries at the origin of hierarchical emergence
      original_md: '[Symmetries at the origin of hierarchical emergence](https://arxiv.org/pdf/2512.00984%20)'
      title: Symmetries at the origin of hierarchical emergence
      authors:
      - Fernando E. Rosas
      author_organizations: []
      date: '2024-11-30'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:The_Learning_Theoretic_Agenda
  name: The Learning-Theoretic Agenda
  header_level: 3
  parent_id: sec:Ontology_Identification
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Create a mathematical theory of intelligent agents that
      encompasses both humans and the AIs we want, one that specifies what it means
      for two such agents to be aligned; translate between its ontology and ours;
      produce formal desiderata for a training setup that produces coherent AGIs similar
      to (our model of) an aligned agent
    theory_of_change: Fix formal epistemology to work out how to avoid deep training
      problems
    see_also: []
    orthodox_problems:
    - value_fragile
    - goals_misgeneralize
    - humans_not_first_class
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitive
    some_names:
    - Vanessa Kosoy
    - Diffractor
    - Gergely Szücs
    estimated_ftes: '3'
    critiques: '[Matolcsi](https://www.lesswrong.com/posts/StkjjQyKwg7hZjcGB/a-mostly-critical-review-of-infra-bayesianism)'
    funded_by: Survival and Flourishing Fund, ARIA, UK AISI, Coefficient Giving
    outputs:
    - link_url: https://www.lesswrong.com/posts/LgLez8aeK24PbyyQJ/new-paper-infra-bayesian-decision-estimation-theory
      link_text: Infra-Bayesian Decision-Estimation Theory
      original_md: '[Infra-Bayesian Decision-Estimation Theory](https://www.lesswrong.com/posts/LgLez8aeK24PbyyQJ/new-paper-infra-bayesian-decision-estimation-theory)'
      title: 'New paper: Infra-Bayesian Decision Estimation Theory'
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: error_detected
    - link_url: https://www.lesswrong.com/w/infra-bayesianism?sortedBy=new
      link_text: Infra-Bayesianism category on LessWrong
      original_md: '[Infra-Bayesianism category on LessWrong](https://www.lesswrong.com/w/infra-bayesianism?sortedBy=new)'
      title: Infra-Bayesianism
      authors:
      - abramdemski
      - Ruby
      author_organizations: []
      date: '2022-03-24'
      published_year: 2022
      venue: LessWrong
      kind: lesswrong
    - link_url: https://www.lesswrong.com/posts/Y9NuKpb6dsyiYFxWK/new-paper-ambiguous-online-learning
      link_text: Ambiguous Online Learning
      original_md: '[Ambiguous Online Learning](https://www.lesswrong.com/posts/Y9NuKpb6dsyiYFxWK/new-paper-ambiguous-online-learning)'
      title: 'New Paper: Ambiguous Online Learning'
      authors:
      - Vanessa Kosoy
      author_organizations:
      - Independent
      date: '2025-06-25'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://proceedings.mlr.press/v291/appel25a.html
      link_text: Regret Bounds for Robust Online Decision Making
      original_md: '[Regret Bounds for Robust Online Decision Making](https://proceedings.mlr.press/v291/appel25a.html)'
      title: Regret Bounds for Robust Online Decision Making
      authors:
      - Alexander Appel
      - Vanessa Kosoy
      author_organizations:
      - FutureSearch
      date: '2025-07-02'
      published_year: 2025
      venue: COLT 2025 (Conference on Learning Theory)
      kind: paper_published
    - link_url: https://www.lesswrong.com/s/n7qFxakSnxGuvmYAX
      link_text: 'What is Inadequate about Bayesianism for AI Alignment: Motivating
        Infra-Bayesianism'
      original_md: '[What is Inadequate about Bayesianism for AI Alignment: Motivating
        Infra-Bayesianism](https://www.lesswrong.com/s/n7qFxakSnxGuvmYAX)'
      title: FUNDAMENTALS OF INFRA-BAYESIANISM
      authors:
      - Brittany Gelb
      author_organizations: []
      date: '2025-08-30'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://www.alignmentforum.org/posts/DobZ62XMdiPigii9H/non-monotonic-infra-bayesian-physicalism%20
      link_text: Non-Monotonic Infra-Bayesian Physicalism
      original_md: '[Non-Monotonic Infra-Bayesian Physicalism](https://www.alignmentforum.org/posts/DobZ62XMdiPigii9H/non-monotonic-infra-bayesian-physicalism%20)'
      title: Non-Monotonic Infra-Bayesian Physicalism
      authors:
      - Marcus Ogren
      author_organizations: []
      date: '2025-04-02'
      published_year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    other_attributes: {}
  parsing_issues: []
- id: sec:Multi_agent_first
  name: Multi-agent first
  header_level: 1
  parent_id: null
  content: ''
  item_type: section
  agenda_attributes: null
  parsing_issues: []
- id: a:Aligning_to_context
  name: Aligning to context
  header_level: 3
  parent_id: sec:Multi_agent_first
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Align AI directly to the role of participant, collaborator,
      or advisor for our best real human practices and institutions, instead of aligning
      AI to separately representable goals, rules, or utility functions.
    theory_of_change: '"Many classical problems in AGI alignment are downstream of
      a type error about human values." Operationalizing a correct view of human values
      - one that treats human values as impossible or impractical to abstract from
      concrete practices - will unblock value fragility, goal-misgeneralization, instrumental
      convergence, and pivotal-act specification.'
    see_also:
    - a:Aligning_what_
    - a:Aligned_to_who_
    orthodox_problems:
    - value_fragile
    - corrigibility_anti_natural
    - goals_misgeneralize
    - instrumental_convergence
    - fair_sane_pivotal
    target_case_id: null
    target_case_text: mixed
    broad_approach_id: behaviorist_science
    broad_approach_text: behavioural
    some_names:
    - Full Stack Alignment
    - Meaning Alignment Institute
    - Plurality Institute
    - Tan Zhi-Xuan
    - Matija Franklin
    - Ryan Lowe
    - Joe Edelman
    - Oliver Klingefjord
    estimated_ftes: '5'
    critiques: null
    funded_by: ARIA, OpenAI, Survival and Flourishing Fund
    outputs:
    - link_url: https://www.softmax.com/blog/the-frame-dependent-mind
      link_text: The Frame-Dependent Mind
      original_md: '[The Frame-Dependent Mind](https://www.softmax.com/blog/the-frame-dependent-mind)'
      title: 'The Frame-Dependent Mind: On Reality''s Stubborn Refusal To Be One Thing'
      authors:
      - Emmett Shear
      - Sonnet 3.7
      author_organizations:
      - Softmax
      date: '2025-04-18'
      published_year: 2025
      venue: Softmax Blog
      kind: blog_post
    - link_url: https://docs.google.com/document/d/1cKbqYSGspfJavXvnhsp3mAuxHh08rNbP7tzYieqLiXw/edit?tab=t.0%20
      link_text: On Eudaimonia and Optimization
      original_md: '[On Eudaimonia and Optimization](https://docs.google.com/document/d/1cKbqYSGspfJavXvnhsp3mAuxHh08rNbP7tzYieqLiXw/edit?tab=t.0%20)'
      title: On Eudaimonia and Optimization
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: Google Docs
      kind: error_detected
    - link_url: https://www.full-stack-alignment.ai
      link_text: Full-Stack Alignment
      original_md: '[Full-Stack Alignment](https://www.full-stack-alignment.ai)'
      title: 'Full-Stack Alignment: Co-Aligning AI and Institutions with Thick Models
        of Value'
      authors: []
      author_organizations:
      - Meaning Alignment Institute
      date: null
      published_year: null
      venue: null
      kind: personal_page
    - link_url: https://arxiv.org/abs/2412.19010
      link_text: A theory of appropriateness
      original_md: '[A theory of appropriateness](https://arxiv.org/abs/2412.19010)'
      title: A theory of appropriateness with applications to generative artificial
        intelligence
      authors:
      - Joel Z. Leibo
      - Alexander Sasha Vezhnevets
      - Manfred Diaz
      - John P. Agapiou
      - William A. Cunningham
      - Peter Sunehag
      - Julia Haas
      - Raphael Koster
      - Edgar A. Duéñez-Guzmán
      - William S. Isaac
      - Georgios Piliouras
      - Stanley M. Bileschi
      - Iyad Rahwan
      - Simon Osindero
      author_organizations:
      - Google DeepMind
      date: '2024-12-26'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2404.10636
      link_text: 2404.10636 - What are human values, and how do we align AI to them?
      original_md: '[2404.10636 \- What are human values, and how do we align AI to
        them?](https://arxiv.org/abs/2404.10636)'
      title: What are human values, and how do we align AI to them?
      authors:
      - Oliver Klingefjord
      - Ryan Lowe
      - Joe Edelman
      author_organizations: []
      date: '2024-04-17'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://meaningalignment.substack.com/p/model-integrity
      link_text: Model Integrity
      original_md: '[Model Integrity](https://meaningalignment.substack.com/p/model-integrity)'
      title: Model Integrity
      authors:
      - Joe Edelman
      - Oliver Klingefjord
      author_organizations:
      - Meaning Alignment Institute
      date: '2024-12-05'
      published_year: 2024
      venue: Substack
      kind: blog_post
    - link_url: https://arxiv.org/abs/2408.16984
      link_text: Beyond Preferences in AI Alignment
      original_md: '[Beyond Preferences in AI Alignment](https://arxiv.org/abs/2408.16984)'
      title: Beyond Preferences in AI Alignment
      authors:
      - Tan Zhi-Xuan
      - Micah Carroll
      - Matija Franklin
      - Hal Ashton
      author_organizations: []
      date: '2024-08-30'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.00940
      link_text: 2503.00940 - Can AI Model the Complexities of Human Moral Decision-Making?
        A Qualitative Study of Kidney Allocation Decisions
      original_md: '[2503.00940 \- Can AI Model the Complexities of Human Moral Decision-Making?
        A Qualitative Study of Kidney Allocation Decisions](https://arxiv.org/abs/2503.00940)'
      title: Can AI Model the Complexities of Human Moral Decision-Making? A Qualitative
        Study of Kidney Allocation Decisions
      authors:
      - Vijay Keswani
      - Vincent Conitzer
      - Walter Sinnott-Armstrong
      - Breanna K. Nguyen
      - Hoda Heidari
      - Jana Schaich Borg
      author_organizations:
      - Carnegie Mellon University
      - Duke University
      - University of Oxford
      date: '2025-03-02'
      published_year: 2025
      venue: ACM CHI 2025
      kind: paper_published
    other_attributes: {}
  parsing_issues:
  - Target case field says 'mixed' - leaving target_case_id as null and setting target_case_text
    to 'mixed'
  - Broad approach uses non-standard label 'General approach' instead of 'Broad approach'
- id: a:Aligning_to_the_social_contract
  name: Aligning to the social contract
  header_level: 3
  parent_id: sec:Multi_agent_first
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Generate AIs' operational values from 'social contract'-style
      ideal civic deliberation formalisms and their consequent rulesets for civic
      actors
    theory_of_change: Formalize and apply the liberal tradition's project of defining
      civic principles separable from the substantive good, aligning our AIs to civic
      principles that bypass fragile utility-learning and intractable utility-calculation
    see_also:
    - a:Aligning_to_context
    - a:Aligning_what_
    orthodox_problems:
    - value_fragile
    - goals_misgeneralize
    - instrumental_convergence
    - humanlike_minds_not_safe
    - fair_sane_pivotal
    target_case_id: null
    target_case_text: mixed
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitive
    some_names:
    - Gillian Hadfield
    - Tan Zhi-Xuan
    - Sydney Levine
    - Matija Franklin
    - Joshua B. Tenenbaum
    estimated_ftes: 5 - 10
    critiques: null
    funded_by: Deepmind, Macroscopic Ventures
    outputs:
    - link_url: https://law-ai.org/law-following-ai/%20
      link_text: 'Law-Following AI: designing AI agents to obey human laws'
      original_md: '[Law-Following AI: designing AI agents to obey human laws](https://law-ai.org/law-following-ai/%20)'
      title: 'Law-Following AI: designing AI agents to obey human laws'
      authors:
      - Cullen O'Keefe
      - Ketan Ramakrishnan
      - Janna Tay
      - Christoph Winter
      author_organizations:
      - Institute for Law & AI
      date: '2025-05-01'
      published_year: 2025
      venue: Fordham Law Review
      kind: paper_published
    - link_url: https://arxiv.org/abs/2510.26396
      link_text: A Pragmatic View of AI Personhood
      original_md: '[A Pragmatic View of AI Personhood](https://arxiv.org/abs/2510.26396)'
      title: A Pragmatic View of AI Personhood
      authors:
      - Joel Z. Leibo
      - Alexander Sasha Vezhnevets
      - William A. Cunningham
      - Stanley M. Bileschi
      author_organizations:
      - Google DeepMind
      date: '2025-10-30'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.00069
      link_text: Societal alignment frameworks can improve llm alignment
      original_md: '[Societal alignment frameworks can improve llm alignment](https://arxiv.org/abs/2503.00069)'
      title: Societal Alignment Frameworks Can Improve LLM Alignment
      authors:
      - Karolina Stańczak
      - Nicholas Meade
      - Mehar Bhatia
      - Hattie Zhou
      - Konstantin Böttinger
      - Jeremy Barnes
      - Jason Stanley
      - Jessica Montgomery
      - Richard Zemel
      - Nicolas Papernot
      - Nicolas Chapados
      - Denis Therien
      - Timothy P. Lillicrap
      - Ana Marasović
      - Sylvie Delacroix
      - Gillian K. Hadfield
      - Siva Reddy
      author_organizations:
      - Multiple institutions (17 authors)
      date: '2025-02-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.07955
      link_text: 2509.07955 - ACE and Diverse Generalization via Selective Disagreement
      original_md: '[2509.07955 \- ACE and Diverse Generalization via Selective Disagreement](https://arxiv.org/abs/2509.07955)'
      title: ACE and Diverse Generalization via Selective Disagreement
      authors:
      - Oliver Daniels
      - Stuart Armstrong
      - Alexandre Maranhão
      - Mahirah Fairuz Rahman
      - Benjamin M. Marlin
      - Rebecca Gorman
      author_organizations:
      - Unknown - not specified in abstract
      date: '2025-09-09'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.17434
      link_text: 2506.17434 - Resource Rational Contractualism Should Guide AI Alignment
      original_md: '[2506.17434 \- Resource Rational Contractualism Should Guide AI
        Alignment](https://arxiv.org/abs/2506.17434)'
      title: Resource Rational Contractualism Should Guide AI Alignment
      authors:
      - Sydney Levine
      - Matija Franklin
      - Tan Zhi-Xuan
      - Secil Yanik Guyot
      - Lionel Wong
      - Daniel Kilov
      - Yejin Choi
      - Joshua B. Tenenbaum
      - Noah Goodman
      - Seth Lazar
      - Iason Gabriel
      author_organizations:
      - MIT
      - Stanford
      - University of Washington
      - DeepMind
      - ANU
      date: '2025-06-20'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.01186
      link_text: Statutory Construction and Interpretation for Artificial Intelligence
      original_md: '[Statutory Construction and Interpretation for Artificial Intelligence](https://arxiv.org/abs/2509.01186)'
      title: Statutory Construction and Interpretation for Artificial Intelligence
      authors:
      - Luxi He
      - Nimra Nadeem
      - Michel Liao
      - Howard Chen
      - Danqi Chen
      - Mariano-Florentino Cuéllar
      - Peter Henderson
      author_organizations:
      - Princeton University
      - Stanford University
      date: '2025-09-01'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2408.16984
      link_text: 2408.16984 - Beyond Preferences in AI Alignment
      original_md: '[2408.16984 \- Beyond Preferences in AI Alignment](https://arxiv.org/abs/2408.16984)'
      title: Beyond Preferences in AI Alignment
      authors:
      - Tan Zhi-Xuan
      - Micah Carroll
      - Matija Franklin
      - Hal Ashton
      author_organizations: []
      date: '2024-08-30'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.00783
      link_text: 'Promises Made, Promises Kept: Safe Pareto Improvements via Ex Post
        Verifiable Commitments'
      original_md: '[Promises Made, Promises Kept: Safe Pareto Improvements via Ex
        Post Verifiable Commitments](https://arxiv.org/abs/2505.00783)'
      title: 'Promises Made, Promises Kept: Safe Pareto Improvements via Ex Post Verifiable
        Commitments'
      authors:
      - Nathaniel Sauerberg
      - Caspar Oesterheld
      author_organizations: []
      date: '2025-05-01'
      published_year: 2025
      venue: GAIW'25
      kind: paper_preprint
    other_attributes: {}
  parsing_issues:
  - Target case field says 'mixed' - leaving target_case_id as null and setting target_case_text
    to 'mixed' as instructed
- id: a:Theory_for_aligning_multiple_AIs
  name: Theory for aligning multiple AIs
  header_level: 3
  parent_id: sec:Multi_agent_first
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Use realistic game-theory variants (e.g. evolutionary game
      theory, computational game theory) or develop alternative game theories to describe/predict
      the collective and individual behaviours of AI agents in multi-agent scenarios.
    theory_of_change: While traditional AGI safety focuses on idealized decision-theory
      and individual agents, it's plausible that strategic AI agents will first emerge
      (or are emerging now) in a complex, multi-AI strategic landscape. We need granular,
      realistic formal models of AIs' strategic interactions and collective dynamics
      to understand this future.
    see_also:
    - a:Tools_for_aligning_multiple_AIs
    - a:Aligning_what_
    orthodox_problems:
    - goals_misgeneralize
    - superintelligence_fool_supervisors
    - superintelligence_hack_software
    target_case_id: null
    target_case_text: mixed
    broad_approach_id: cognitivist_science
    broad_approach_text: cognitive
    some_names:
    - Lewis Hammond
    - Emery Cooper
    - Allan Chan
    - Caspar Oesterheld
    - Vincent Conitzer
    - Vojta Kovarik
    - Nathaniel Sauerberg
    - ACS Research
    - Jan Kulveit
    - Richard Ngo
    - Emmett Shear
    - Softmax
    - Full Stack Alignment
    - AI Objectives Institute
    - Sahil
    - TJ
    - Andrew Critch
    estimated_ftes: '10'
    critiques: null
    funded_by: SFF, CAIF, Deepmind, Macroscopic Ventures
    outputs:
    - link_url: https://arxiv.org/abs/2502.14143
      link_text: Multi-Agent Risks from Advanced AI
      original_md: '[Multi-Agent Risks from Advanced AI](https://arxiv.org/abs/2502.14143)'
      title: Multi-Agent Risks from Advanced AI
      authors:
      - Lewis Hammond
      - Alan Chan
      - Jesse Clifton
      - Jason Hoelscher-Obermaier
      - Akbir Khan
      - Euan McLean
      - Chandler Smith
      - Wolfram Barfuss
      - Jakob Foerster
      - Tomáš Gavenčiak
      - The Anh Han
      - Edward Hughes
      - Vojtěch Kovařík
      - Jan Kulveit
      - Joel Z. Leibo
      - Caspar Oesterheld
      - Christian Schroeder de Witt
      - Nisarg Shah
      - Michael Wellman
      - Paolo Bova
      - Theodor Cimpeanu
      - Carson Ezell
      - Quentin Feuillade-Montixi
      - Matija Franklin
      - Esben Kran
      - Igor Krawczuk
      - Max Lamparth
      - Niklas Lauffer
      - Alexander Meinke
      - Sumeet Motwani
      - Anka Reuel
      - Vincent Conitzer
      - Michael Dennis
      - Iason Gabriel
      - Adam Gleave
      - Gillian Hadfield
      - Nika Haghtalab
      - Atoosa Kasirzadeh
      - Sébastien Krier
      - Kate Larson
      - Joel Lehman
      - David C. Parkes
      - Georgios Piliouras
      - Iyad Rahwan
      author_organizations:
      - Cooperative AI Foundation
      date: '2025-02-19'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.01063
      link_text: An Economy of AI Agents
      original_md: '[An Economy of AI Agents](https://arxiv.org/abs/2509.01063)'
      title: An Economy of AI Agents
      authors:
      - Gillian K. Hadfield
      - Andrew Koh
      author_organizations: []
      date: '2025-09-01'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.06105
      link_text: 'Moloch''s Bargain: Emergent Misalignment When LLMs Compete for Audiences'
      original_md: '[Moloch''s Bargain: Emergent Misalignment When LLMs Compete for
        Audiences](https://arxiv.org/abs/2510.06105)'
      title: 'Moloch''s Bargain: Emergent Misalignment When LLMs Compete for Audiences'
      authors:
      - Batu El
      - James Zou
      author_organizations:
      - Stanford University
      date: '2025-10-07'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2508.14927
      link_text: AI Testing Should Account for Sophisticated Strategic Behaviour
      original_md: '[AI Testing Should Account for Sophisticated Strategic Behaviour](https://arxiv.org/abs/2508.14927)'
      title: AI Testing Should Account for Sophisticated Strategic Behaviour
      authors:
      - Vojtech Kovarik
      - Eric Olav Chen
      - Sami Petersen
      - Alexis Ghersengorin
      - Vincent Conitzer
      author_organizations: []
      date: '2025-08-19'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.science.org/doi/10.1126/sciadv.adu9368
      link_text: Emergent social conventions and collective bias in LLM populations
      original_md: '[Emergent social conventions and collective bias in LLM populations](https://www.science.org/doi/10.1126/sciadv.adu9368)'
      title: Emergent social conventions and collective bias in LLM populations
      authors:
      - Ariel Flint Ashery
      - Luca Maria Aiello
      - Andrea Baronchelli
      author_organizations:
      - City St George's, University of London
      - IT University of Copenhagen
      - Pioneer Centre for AI
      - The Alan Turing Institute
      date: '2025-05-14'
      published_year: 2025
      venue: Science Advances
      kind: paper_published
    - link_url: https://arxiv.org/abs/2507.02618
      link_text: 'Strategic Intelligence in Large Language Models: Evidence from evolutionary
        Game Theory'
      original_md: '[Strategic Intelligence in Large Language Models: Evidence from
        evolutionary Game Theory](https://arxiv.org/abs/2507.02618)'
      title: 'Strategic Intelligence in Large Language Models: Evidence from evolutionary
        Game Theory'
      authors:
      - Kenneth Payne
      - Baptiste Alloui-Cros
      author_organizations: []
      date: '2025-07-03'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.05748
      link_text: Communication Enables Cooperation in LLM Agents
      original_md: '[Communication Enables Cooperation in LLM Agents](https://arxiv.org/abs/2510.05748)'
      title: 'Communication Enables Cooperation in LLM Agents: A Comparison with Curriculum-Based
        Approaches'
      authors:
      - Hachem Madmoun
      - Salem Lahlou
      author_organizations: []
      date: '2025-10-07'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.06323
      link_text: Higher-Order Belief in Incomplete Information MAIDs
      original_md: '[Higher-Order Belief in Incomplete Information MAIDs](https://arxiv.org/abs/2503.06323)'
      title: Higher-Order Belief in Incomplete Information MAIDs
      authors:
      - Jack Foxabbott
      - Rohan Subramani
      - Francis Rhys Ward
      author_organizations: []
      date: '2025-03-08'
      published_year: 2025
      venue: 24th International Conference on Autonomous Agents and Multiagent Systems
        2025
      kind: paper_published
    - link_url: https://arxiv.org/abs/2412.14570
      link_text: Characterising Simulation-Based Program Equilibria
      original_md: '[Characterising Simulation-Based Program Equilibria](https://arxiv.org/abs/2412.14570)'
      title: Characterising Simulation-Based Program Equilibria
      authors:
      - Emery Cooper
      - Caspar Oesterheld
      - Vincent Conitzer
      author_organizations:
      - Carnegie Mellon University
      date: '2024-12-19'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://cgi.cse.unsw.edu.au/~eptcs/paper.cgi?TARK2025:36
      link_text: Safe (Pareto) Improvements in Binary Constraint Structures
      original_md: '[Safe (Pareto) Improvements in Binary Constraint Structures](https://cgi.cse.unsw.edu.au/~eptcs/paper.cgi?TARK2025:36)'
      title: 'Error: No content available'
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: EPTCS (Electronic Proceedings in Theoretical Computer Science)
      kind: error_detected
    - link_url: https://arxiv.org/abs/2505.00783
      link_text: 'Promises Made, Promises Kept: Safe Pareto Improvements via Ex Post
        Verifiable Commitments'
      original_md: '[Promises Made, Promises Kept: Safe Pareto Improvements via Ex
        Post Verifiable Commitments](https://arxiv.org/abs/2505.00783)'
      title: 'Promises Made, Promises Kept: Safe Pareto Improvements via Ex Post Verifiable
        Commitments'
      authors:
      - Nathaniel Sauerberg
      - Caspar Oesterheld
      author_organizations: []
      date: '2025-05-01'
      published_year: 2025
      venue: GAIW'25
      kind: paper_preprint
    - link_url: https://www.lesswrong.com/posts/wQKskToGofs4osdJ3/the-pando-problem-rethinking-ai-individuality
      link_text: 'The Pando Problem: Rethinking AI Individuality'
      original_md: '[The Pando Problem: Rethinking AI Individuality](https://www.lesswrong.com/posts/wQKskToGofs4osdJ3/the-pando-problem-rethinking-ai-individuality)'
      title: 'The Pando Problem: Rethinking AI Individuality'
      authors:
      - Jan_Kulveit
      author_organizations: []
      date: '2025-03-28'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    other_attributes: {}
  parsing_issues:
  - Target case field says 'mixed' - no standard ID match, stored as text only
  - 'Broad approach field says ''cognitive'' - mapped to ''cognitivist_science'' (standard
    text: ''cognitivist science'')'
- id: a:Tools_for_aligning_multiple_AIs
  name: Tools for aligning multiple AIs
  header_level: 3
  parent_id: sec:Multi_agent_first
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Develop tools and techniques for designing and testing multi-agent
      AI scenarios, for auditing real-world multi-agent AI dynamics, and for aligning
      AIs in multi-AI settings.
    theory_of_change: Addressing multi-agent AI dynamics is key for aligning near-future
      agents and their impact on the world. Feedback loops from multi-agent dynamics
      can radically change the future AI landscape, and require a different toolset
      from model psychology to audit and control.
    see_also:
    - a:Theory_for_aligning_multiple_AIs
    - a:Aligning_what_
    orthodox_problems:
    - goals_misgeneralize
    - superintelligence_fool_supervisors
    - superintelligence_hack_software
    target_case_id: null
    target_case_text: mixed
    broad_approach_id: null
    broad_approach_text: engineering / behavioral
    some_names:
    - Andrew Critch
    - Lewis Hammond
    - Emery Cooper
    - Allan Chan
    - Caspar Oesterheld
    - Vincent Conitzer
    - Gillian Hadfield
    - Nathaniel Sauerberg
    - Zhijing Jin
    estimated_ftes: 10 - 15
    critiques: null
    funded_by: Coefficient Giving, Deepmind, Cooperative AI Foundation
    outputs:
    - link_url: https://softmax.com/blog/reimagining-alignment
      link_text: Reimagining Alignment
      original_md: '[Reimagining Alignment](https://softmax.com/blog/reimagining-alignment)'
      title: Reimagining Alignment
      authors: []
      author_organizations:
      - Softmax
      date: '2025-03-28'
      published_year: 2025
      venue: Softmax Blog
      kind: blog_post
    - link_url: https://arxiv.org/abs/2509.14485
      link_text: 'Beyond the high score: Prosocial ability profiles of multi-agent
        populations'
      original_md: '[Beyond the high score: Prosocial ability profiles of multi-agent
        populations](https://arxiv.org/abs/2509.14485)'
      title: 'Beyond the high score: Prosocial ability profiles of multi-agent populations'
      authors:
      - Marko Tesic
      - Yue Zhao
      - Joel Z. Leibo
      - Rakshit S. Trivedi
      - Jose Hernandez-Orallo
      author_organizations:
      - Google DeepMind
      date: '2025-09-17'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.23102
      link_text: Multiplayer Nash Preference Optimization
      original_md: '[Multiplayer Nash Preference Optimization](https://arxiv.org/abs/2509.23102)'
      title: Multiplayer Nash Preference Optimization
      authors:
      - Fang Wu
      - Xu Huang
      - Weihao Xuan
      - Zhiwei Zhang
      - Yijia Xiao
      - Guancheng Wan
      - Xiaomin Li
      - Bing Hu
      - Peng Xia
      - Jure Leskovec
      - Yejin Choi
      author_organizations:
      - Stanford University
      - University of Washington
      - AI2
      date: '2025-09-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.00757
      link_text: 'AgentBreeder: Mitigating the AI Safety Risks of Multi-Agent Scaffolds
        via Self-Improvement'
      original_md: '[AgentBreeder: Mitigating the AI Safety Risks of Multi-Agent Scaffolds
        via Self-Improvement](https://arxiv.org/abs/2502.00757)'
      title: 'AgentBreeder: Mitigating the AI Safety Risks of Multi-Agent Scaffolds
        via Self-Improvement'
      authors:
      - J Rosser
      - Jakob Foerster
      author_organizations: []
      date: '2025-02-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.14660
      link_text: 'When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion
        in Social Systems'
      original_md: '[When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent
        Collusion in Social Systems](https://arxiv.org/abs/2507.14660)'
      title: 'When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion
        in Social Systems'
      authors:
      - Qibing Ren
      - Sitao Xie
      - Longxuan Wei
      - Zhenfei Yin
      - Junchi Yan
      - Lizhuang Ma
      - Jing Shao
      author_organizations: []
      date: '2025-07-19'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2501.10114
      link_text: Infrastructure for AI Agents
      original_md: '[Infrastructure for AI Agents](https://arxiv.org/abs/2501.10114)'
      title: Infrastructure for AI Agents
      authors:
      - Alan Chan
      - Kevin Wei
      - Sihao Huang
      - Nitarshan Rajkumar
      - Elija Perrier
      - Seth Lazar
      - Gillian K. Hadfield
      - Markus Anderljung
      author_organizations: []
      date: '2025-01-17'
      published_year: 2025
      venue: arXiv (accepted to TMLR)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.10588
      link_text: A dataset of questions on decision-theoretic reasoning in Newcomb-like
        problems
      original_md: '[A dataset of questions on decision-theoretic reasoning in Newcomb-like
        problems](https://arxiv.org/abs/2411.10588)'
      title: A dataset of questions on decision-theoretic reasoning in Newcomb-like
        problems
      authors:
      - Caspar Oesterheld
      - Emery Cooper
      - Miles Kodama
      - Linh Chi Nguyen
      - Ethan Perez
      author_organizations: []
      date: '2024-11-15'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.01295
      link_text: 'The Social Laboratory: A Psychometric Framework for Multi-Agent
        LLM Evaluation'
      original_md: '[The Social Laboratory: A Psychometric Framework for Multi-Agent
        LLM Evaluation](https://arxiv.org/abs/2510.01295)'
      title: 'The Social Laboratory: A Psychometric Framework for Multi-Agent LLM
        Evaluation'
      authors:
      - Zarreen Reza
      author_organizations: []
      date: '2025-10-01'
      published_year: 2025
      venue: NeurIPS 2025 Workshop on Evaluating the Evolving LLM Lifecycle
      kind: paper_preprint
    - link_url: https://github.com/lechmazur/pgg_bench
      link_text: 'PGG-Bench: Contribute & Punish'
      original_md: '[PGG-Bench: Contribute & Punish](https://github.com/lechmazur/pgg_bench)'
      title: 'PGG-Bench: Contribute & Punish'
      authors: []
      author_organizations: []
      date: '2025-04-10'
      published_year: 2025
      venue: GitHub
      kind: code_tool
    - link_url: http://arxiv.org/abs/2509.10147
      link_text: Virtual Agent Economies
      original_md: '[Virtual Agent Economies](http://arxiv.org/abs/2509.10147)'
      title: Virtual Agent Economies
      authors:
      - Nenad Tomasev
      - Matija Franklin
      - Joel Z. Leibo
      - Julian Jacobs
      - William A. Cunningham
      - Iason Gabriel
      - Simon Osindero
      author_organizations:
      - DeepMind
      date: '2025-09-12'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.12203
      link_text: An Interpretable Automated Mechanism Design Framework with Large
        Language Models
      original_md: '[An Interpretable Automated Mechanism Design Framework with Large
        Language Models](https://arxiv.org/abs/2502.12203)'
      title: An Interpretable Automated Mechanism Design Framework with Large Language
        Models
      authors:
      - Jiayuan Liu
      - Mingyu Guo
      - Vincent Conitzer
      author_organizations: []
      date: '2025-02-16'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://openreview.net/forum?id=9ply9CAnSC&noteId=rcn5RTlfD1
      link_text: Comparing Collective Behavior of LLM and Human Groups
      original_md: '[Comparing Collective Behavior of LLM and Human Groups](https://openreview.net/forum?id=9ply9CAnSC&noteId=rcn5RTlfD1)'
      title: Comparing Collective Behavior of LLM and Human Groups
      authors:
      - Anna B. Stephenson
      - Andrew Zhu
      - Chris Callison-Burch
      - Jan Kulveit
      author_organizations:
      - University of Pennsylvania
      date: '2025-09-23'
      published_year: 2025
      venue: NeurIPS 2025 Workshop ACA
      kind: paper_published
    other_attributes: {}
  parsing_issues:
  - Target case is 'mixed' - leaving target_case_id as null
  - Broad approach is 'engineering / behavioural' (mixed) - leaving broad_approach_id
    as null
- id: a:Aligned_to_who_
  name: Aligned to who?
  header_level: 3
  parent_id: sec:Multi_agent_first
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Technical protocols for taking seriously the plurality of
      human values, cultures, and communities when aligning AI to "humanity"
    theory_of_change: use democratic/pluralist/context-sensitive principles to guide
      AI development, alignment, and deployment somehow. Doing it as an afterthought
      in post-training or the spec isn't good enough. Continuously shape AI's social
      and technical feedback loop on the road to AGI
    see_also:
    - a:Aligning_what_
    - a:Aligning_to_context
    orthodox_problems:
    - value_fragile
    - fair_sane_pivotal
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: behaviorist_science
    broad_approach_text: behavioral
    some_names:
    - Joel Z. Leibo
    - Divya Siddarth
    - Séb Krier
    - Luke Thorburn
    - Seth Lazar
    - AI Objectives Institute
    - The Collective Intelligence Project
    - Vincent Conitzer
    estimated_ftes: 5 - 15
    critiques: null
    funded_by: Future of Life Institute, Survival and Flourishing Fund, Deepmind,
      CAIF
    outputs:
    - link_url: https://ojs.aaai.org/index.php/AIES/article/view/36645
      link_text: 'The AI Power Disparity Index: Toward a Compound Measure of AI Actors''
        Power to Shape the AI Ecosystem'
      original_md: '[The AI Power Disparity Index: Toward a Compound Measure of AI
        Actors'' Power to Shape the AI Ecosystem](https://ojs.aaai.org/index.php/AIES/article/view/36645)'
      title: 'The AI Power Disparity Index: Toward a Compound Measure of AI Actors''
        Power to Shape the AI Ecosystem'
      authors:
      - Rachel M. Kim
      - Blaine Kuehnert
      - Seth Lazar
      - Ranjit Singh
      - Hoda Heidari
      author_organizations:
      - Carnegie Mellon University
      - Australian National University
      - Data and Society
      date: '2025-10-15'
      published_year: 2025
      venue: AAAI/ACM Conference on AI, Ethics, and Society (AIES)
      kind: paper_published
    - link_url: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5097286
      link_text: Research Agenda for Sociotechnical Approaches to AI Safety
      original_md: '[Research Agenda for Sociotechnical Approaches to AI Safety](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5097286)'
      title: Research Agenda for Sociotechnical Approaches to AI Safety
      authors:
      - Samuel Curtis
      - Ravi Iyer
      - Cameron Domenico Kirk-Giannini
      - Victoria Krakovna
      - David Krueger
      - Nathan Lambert
      - Bruno Marnette
      - Colleen McKenzie
      - Julian Michael
      - Evan Miyazono
      - Noyuri Mima
      - Aviv Ovadya
      - Luke Thorburn
      - Vehbi Deger Turan
      author_organizations:
      - The Future Society
      - University of Southern California
      - Rutgers University
      - Future of Life Institute
      - University of Cambridge
      - Allen Institute for AI
      - AI Objectives Institute
      - New York University
      - Atlas Computing
      - Future University Hakodate
      - Thoughtful Technology Project
      - King's College London
      - AI & Democracy Foundation
      date: '2025-01-14'
      published_year: 2025
      venue: SSRN
      kind: agenda_manifesto
    - link_url: https://arxiv.org/abs/2507.09650
      link_text: '2507.09650 - Cultivating Pluralism In Algorithmic Monoculture: The
        Community Alignment Dataset'
      original_md: '[2507.09650 \- Cultivating Pluralism In Algorithmic Monoculture:
        The Community Alignment Dataset](https://arxiv.org/abs/2507.09650)'
      title: 'Cultivating Pluralism In Algorithmic Monoculture: The Community Alignment
        Dataset'
      authors:
      - Lily Hong Zhang
      - Smitha Milli
      - Karen Jusko
      - Jonathan Smith
      - Brandon Amos
      - Wassim Bouaziz
      - Manon Revel
      - Jack Kussman
      - Yasha Sheynin
      - Lisa Titus
      - Bhaktipriya Radharapu
      - Jane Yu
      - Vidya Sarma
      - Kris Rose
      - Maximilian Nickel
      author_organizations:
      - Meta
      - Cornell University
      date: '2025-07-13'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.13709
      link_text: Training LLM Agents to Empower Humans
      original_md: '[Training LLM Agents to Empower Humans](https://arxiv.org/abs/2510.13709)'
      title: Training LLM Agents to Empower Humans
      authors:
      - Evan Ellis
      - Vivek Myers
      - Jens Tuyls
      - Sergey Levine
      - Anca Dragan
      - Benjamin Eysenbach
      author_organizations:
      - UC Berkeley
      - Princeton University
      date: '2025-10-16'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.05197
      link_text: Societal and technological progress as sewing an ever-growing, ever-changing,
        patchy, and polychrome quilt
      original_md: '[Societal and technological progress as sewing an ever-growing,
        ever-changing, patchy, and polychrome quilt](https://arxiv.org/abs/2505.05197)'
      title: Societal and technological progress as sewing an ever-growing, ever-changing,
        patchy, and polychrome quilt
      authors:
      - Joel Z. Leibo
      - Alexander Sasha Vezhnevets
      - William A. Cunningham
      - Sébastien Krier
      - Manfred Diaz
      - Simon Osindero
      author_organizations:
      - Google DeepMind
      date: '2025-05-08'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.09222
      link_text: 'Democratic AI is Possible: The Democracy Levels Framework Shows
        How It Might Work'
      original_md: '[Democratic AI is Possible: The Democracy Levels Framework Shows
        How It Might Work](https://arxiv.org/abs/2411.09222)'
      title: Democratic AI is Possible. The Democracy Levels Framework Shows How It
        Might Work
      authors:
      - Aviv Ovadya
      - Kyle Redman
      - Luke Thorburn
      - Quan Ze Chen
      - Oliver Smith
      - Flynn Devine
      - Andrew Konya
      - Smitha Milli
      - Manon Revel
      - K. J. Kevin Feng
      - Amy X. Zhang
      - Bilva Chandra
      - Michiel A. Bakker
      - Atoosa Kasirzadeh
      author_organizations:
      - Multiple Organizations
      date: '2024-11-14'
      published_year: 2024
      venue: arXiv (Accepted to ICML 2025 Position Paper Track)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.05728
      link_text: 2503.05728 - Political Neutrality in AI Is Impossible - But Here
        Is How to Approximate It
      original_md: '[2503.05728 \- Political Neutrality in AI Is Impossible \- But
        Here Is How to Approximate It](https://arxiv.org/abs/2503.05728)'
      title: Political Neutrality in AI Is Impossible- But Here Is How to Approximate
        It
      authors:
      - Jillian Fisher
      - Ruth E. Appel
      - Chan Young Park
      - Yujin Potter
      - Liwei Jiang
      - Taylor Sorensen
      - Shangbin Feng
      - Yulia Tsvetkov
      - Margaret E. Roberts
      - Jennifer Pan
      - Dawn Song
      - Yejin Choi
      author_organizations:
      - Anthropic
      - UC San Diego
      - Stanford University
      - UC Berkeley
      - University of Washington
      date: '2025-02-18'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.04345
      link_text: Build Agent Advocates, Not Platform Agents
      original_md: '[Build Agent Advocates, Not Platform Agents](https://arxiv.org/abs/2505.04345)'
      title: Build Agent Advocates, Not Platform Agents
      authors:
      - Sayash Kapoor
      - Noam Kolt
      - Seth Lazar
      author_organizations: []
      date: '2025-05-07'
      published_year: 2025
      venue: arXiv (accepted to ICML 2025 position paper track)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2501.16946
      link_text: Gradual Disempowerment
      original_md: '[Gradual Disempowerment](https://arxiv.org/abs/2501.16946)'
      title: 'Gradual Disempowerment: Systemic Existential Risks from Incremental
        AI Development'
      authors:
      - Jan Kulveit
      - Raymond Douglas
      - Nora Ammann
      - Deger Turan
      - David Krueger
      - David Duvenaud
      author_organizations:
      - Various academic institutions
      date: '2025-01-28'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:Aligning_what_
  name: Aligning what?
  header_level: 3
  parent_id: sec:Multi_agent_first
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Develop alternatives to agent-level models of alignment,
      by treating human-AI interactions, AI-assisted institutions, AI economic or
      cultural systems, drives within one AI, and other causal/constitutive processes
      as subject to alignment
    theory_of_change: Model multiple reality-shaping processes above and below the
      level of the individual AI, some of which are themselves quasi-agential (e.g.
      cultures) or intelligence-like (e.g. markets), will develop AI alignment into
      a mature science for managing the transition to an AGI civilization
    see_also:
    - a:Theory_for_aligning_multiple_AIs
    - a:Aligning_to_context
    - a:Aligned_to_who_
    orthodox_problems:
    - value_fragile
    - corrigibility_anti_natural
    - goals_misgeneralize
    - instrumental_convergence
    - fair_sane_pivotal
    target_case_id: null
    target_case_text: mixed
    broad_approach_id: null
    broad_approach_text: behavioral / cognitive
    some_names:
    - Richard Ngo
    - Emmett Shear
    - Softmax
    - Full Stack Alignment
    - AI Objectives Institute
    - Sahil
    - TJ
    - Andrew Critch
    - ACS Research
    - Jan Kulveit
    estimated_ftes: 5-10
    critiques: null
    funded_by: Future of Life Institute, Emmett Shear
    outputs:
    - link_url: https://www.alignmentforum.org/posts/5tYTKX4pNpiG4vzYg/towards-a-scale-free-theory-of-intelligent-agency
      link_text: Towards a Scale-Free Theory of Intelligent Agency
      original_md: '[Towards a Scale-Free Theory of Intelligent Agency](https://www.alignmentforum.org/posts/5tYTKX4pNpiG4vzYg/towards-a-scale-free-theory-of-intelligent-agency)'
      title: Towards a scale-free theory of intelligent agency
      authors:
      - Richard Ngo
      author_organizations:
      - Independent
      date: '2025-03-21'
      published_year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - link_url: https://chrislakin.blog/p/alignment-first-intelligence-later
      link_text: Alignment first, intelligence later
      original_md: '[Alignment first, intelligence later](https://chrislakin.blog/p/alignment-first-intelligence-later)'
      title: Alignment first, intelligence later
      authors:
      - Chris Lakin
      author_organizations:
      - Softmax
      date: '2025-03-30'
      published_year: 2025
      venue: Substack (Locally Optimal)
      kind: blog_post
    - link_url: https://www.anthropic.com/research/end-subset-conversations
      link_text: End A Subset Of Conversations
      original_md: '[End A Subset Of Conversations](https://www.anthropic.com/research/end-subset-conversations)'
      title: Claude Opus 4 and 4.1 can now end a rare subset of conversations
      authors: []
      author_organizations:
      - Anthropic
      date: '2025-08-15'
      published_year: 2025
      venue: Anthropic Blog
      kind: news_announcement
    - link_url: https://www.full-stack-alignment.ai
      link_text: Full-Stack Alignment
      original_md: '[Full-Stack Alignment](https://www.full-stack-alignment.ai)'
      title: 'Full-Stack Alignment: Co-Aligning AI and Institutions with Thick Models
        of Value'
      authors: []
      author_organizations:
      - Meaning Alignment Institute
      date: null
      published_year: null
      venue: null
      kind: personal_page
    - link_url: https://docs.google.com/document/d/1cKbqYSGspfJavXvnhsp3mAuxHh08rNbP7tzYieqLiXw/edit?tab=t.0%20
      link_text: On Eudaimonia and Optimization
      original_md: '[On Eudaimonia and Optimization](https://docs.google.com/document/d/1cKbqYSGspfJavXvnhsp3mAuxHh08rNbP7tzYieqLiXw/edit?tab=t.0%20)'
      title: On Eudaimonia and Optimization
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: Google Docs
      kind: error_detected
    - link_url: https://arxiv.org/abs/2501.17755
      link_text: AI Governance through Markets
      original_md: '[AI Governance through Markets](https://arxiv.org/abs/2501.17755)'
      title: null
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: null
    - link_url: https://www.pnas.org/doi/abs/10.1073/pnas.2319948121
      link_text: Collective cooperative intelligence
      original_md: '[Collective cooperative intelligence](https://www.pnas.org/doi/abs/10.1073/pnas.2319948121)'
      title: Collective cooperative intelligence
      authors:
      - Wolfram Barfuss
      - Jessica Flack
      - Chaitanya S. Gokhale
      - Lewis Hammond
      - Christian Hilbe
      - Edward Hughes
      - Joel Z. Leibo
      - Tom Lenaerts
      - Naomi Leonard
      - Simon Levin
      - Udari Madhushani Sehwag
      - Alex McAvoy
      - Janusz M. Meylahn
      - Fernando P. Santos
      author_organizations:
      - University of Bonn
      - Santa Fe Institute
      - Max Planck Institute for Evolutionary Biology
      - University of Oxford
      - Google DeepMind
      - Université Libre de Bruxelles
      - Princeton University
      - Stanford University
      - UNC Chapel Hill
      - University of Twente
      - University of Amsterdam
      date: '2025-06-16'
      published_year: 2025
      venue: Proceedings of the National Academy of Sciences
      kind: paper_published
    - link_url: https://www.lesswrong.com/posts/JjYu75q3hEMBgtvr8/multipolar-ai-is-underrated
      link_text: Multipolar AI is Underrated
      original_md: '[Multipolar AI is Underrated](https://www.lesswrong.com/posts/JjYu75q3hEMBgtvr8/multipolar-ai-is-underrated)'
      title: Multipolar AI is Underrated
      authors:
      - Allison Duettmann
      author_organizations:
      - Foresight Institute
      date: '2025-05-17'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://www.lesswrong.com/posts/tQ9vWm4b57HFqbaRj/what-if-not-agency
      link_text: What, if not agency?
      original_md: '[What, if not agency?](https://www.lesswrong.com/posts/tQ9vWm4b57HFqbaRj/what-if-not-agency)'
      title: Unknown - Content Unavailable (429 Error)
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: error_detected
    - link_url: https://equilibria1.substack.com/p/the-evolution-of-agency-a-research
      link_text: A Phylogeny of Agents
      original_md: '[A Phylogeny of Agents](https://equilibria1.substack.com/p/the-evolution-of-agency-a-research)'
      title: A Phylogeny of Agents
      authors:
      - Equilibria
      author_organizations:
      - Equilibria Network
      date: '2025-08-15'
      published_year: 2025
      venue: Substack
      kind: blog_post
    - link_url: https://themultiplicity.ai/blog/thesis
      link_text: The Multiplicity Thesis, Collective Intelligence, and Morality
      original_md: '[The Multiplicity Thesis, Collective Intelligence, and Morality](https://themultiplicity.ai/blog/thesis)'
      title: The Multiplicity Thesis, Collective Intelligence, and Morality
      authors:
      - Andrew Critch
      author_organizations:
      - theMultiplicity.ai
      date: '2025-11-07'
      published_year: 2025
      venue: theMultiplicity.ai blog
      kind: blog_post
    - link_url: https://www.alignmentforum.org/posts/xud7Mti9jS4tbWqQE/hierarchical-agency-a-missing-piece-in-ai-alignment
      link_text: Hierarchical Alignment
      original_md: '[Hierarchical Alignment](https://www.alignmentforum.org/posts/xud7Mti9jS4tbWqQE/hierarchical-agency-a-missing-piece-in-ai-alignment)'
      title: 'Hierarchical Agency: A Missing Piece in AI Alignment'
      authors:
      - Jan_Kulveit
      author_organizations:
      - Alignment of Complex Systems Research Group
      date: '2024-11-27'
      published_year: 2024
      venue: AI Alignment Forum
      kind: lesswrong
    - link_url: https://a16z.simplecast.com/episodes/emmett-shear-on-building-ai-that-actually-cares-beyond-control-and-steering-TRwfxH0r
      link_text: 'Emmett Shear on Building AI That Actually Cares: Beyond Control
        and Steering'
      original_md: '[Emmett Shear on Building AI That Actually Cares: Beyond Control
        and Steering](https://a16z.simplecast.com/episodes/emmett-shear-on-building-ai-that-actually-cares-beyond-control-and-steering-TRwfxH0r)'
      title: 'Emmett Shear on Building AI That Actually Cares: Beyond Control and
        Steering'
      authors:
      - Emmett Shear
      - Erik Torenberg
      - Séb Krier
      author_organizations:
      - Softmax
      - a16z
      date: '2025-11-17'
      published_year: 2025
      venue: a16z Podcast
      kind: podcast
    other_attributes: {}
  parsing_issues:
  - Broad approach field contains 'behavioural / cognitive' which indicates multiple
    approaches - set broad_approach_id to null and normalized text to 'behavioral
    / cognitive'
  - Target case field is 'mixed' - set target_case_id to null
- id: sec:Evals
  name: Evals
  header_level: 1
  parent_id: null
  content: ''
  item_type: section
  agenda_attributes: null
  parsing_issues: []
- id: a:AGI_metrics
  name: AGI metrics
  header_level: 3
  parent_id: sec:Evals
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Evals with the explicit aim of measuring progress towards
      full human-level generality.
    theory_of_change: Help predict timelines for risk awareness and strategy.
    see_also:
    - a:Capability_evals
    orthodox_problems: []
    target_case_id: null
    target_case_text: mixed
    broad_approach_id: behaviorist_science
    broad_approach_text: behavioural
    some_names:
    - CAIS
    - CFI Kinds of Intelligence
    - Apart Research
    - OpenAI
    - METR
    - Lexin Zhou
    - Adam Scholl
    - Lorenzo Pacchiardi
    estimated_ftes: 10-50
    critiques: '[Is the Definition of AGI a Percentage?](https://aievaluation.substack.com/p/is-the-definition-of-agi-a-percentage),
      [The "Length" of "Horizons"](https://www.lesswrong.com/posts/PzLSuaT6WGLQGJJJD/the-length-of-horizons)'
    funded_by: Leverhulme Trust, Open Philanthropy, Long-Term Future Fund
    outputs:
    - link_url: https://arxiv.org/abs/2503.17354
      link_text: 'HCAST: Human-Calibrated Autonomy Software Tasks'
      original_md: '[HCAST: Human-Calibrated Autonomy Software Tasks](https://arxiv.org/abs/2503.17354)'
      title: 'HCAST: Human-Calibrated Autonomy Software Tasks'
      authors:
      - David Rein
      - Joel Becker
      - Amy Deng
      - Seraphina Nix
      - Chris Canal
      - Daniel O'Connel
      - Pip Arnott
      - Ryan Bloom
      - Thomas Broadley
      - Katharyn Garcia
      - Brian Goodrich
      - Max Hasin
      - Sami Jawhar
      - Megan Kinniment
      - Thomas Kwa
      - Aron Lajko
      - Nate Rush
      - Lucas Jun Koba Sato
      - Sydney Von Arx
      - Ben West
      - Lawrence Chan
      - Elizabeth Barnes
      author_organizations: []
      date: '2025-03-21'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/pdf/2510.18212
      link_text: A Definition of AGI
      original_md: '[A Definition of AGI](https://arxiv.org/pdf/2510.18212)'
      title: A Definition of AGI
      authors:
      - Dan Hendrycks
      - Dawn Song
      - Christian Szegedy
      - Honglak Lee
      - Yarin Gal
      - Erik Brynjolfsson
      - Sharon Li
      - Andy Zou
      - Lionel Levine
      - Bo Han
      - Jie Fu
      - Ziwei Liu
      - Jinwoo Shin
      - Kimin Lee
      - Mantas Mazeika
      - Long Phan
      - George Ingebretsen
      - Adam Khoja
      - Cihang Xie
      - Olawale Salaudeen
      - Matthias Hein
      - Kevin Zhao
      - Alexander Pan
      - David Duvenaud
      - Bo Li
      - Steve Omohundro
      - Gabriel Alfour
      - Max Tegmark
      - Kevin McGrew
      - Gary Marcus
      - Jaan Tallinn
      - Eric Schmidt
      - Yoshua Bengio
      author_organizations:
      - UC Berkeley
      - Various Universities
      - Independent Researchers
      date: '2025-10-21'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://scale.com/leaderboard/rli
      link_text: Remote Labor Index
      original_md: '[Remote Labor Index](https://scale.com/leaderboard/rli)'
      title: Remote Labor Index (RLI)
      authors: []
      author_organizations:
      - Scale AI
      date: '2025-10-30'
      published_year: 2025
      venue: Scale AI Leaderboard
      kind: dataset_benchmark
    - link_url: https://kinds-of-intelligence-cfi.github.io/ADELE/
      link_text: 'ADeLe v1.0: A battery for AI Evaluation with explanatory and predictive
        power'
      original_md: '[ADeLe v1.0: A battery for AI Evaluation with explanatory and
        predictive power](https://kinds-of-intelligence-cfi.github.io/ADELE/)'
      title: 'ADeLe v1.0: A battery for AI Evaluation with explanatory and predictive
        power'
      authors:
      - Lexin Zhou
      - Lorenzo Pacchiardi
      - Fernando Martínez-Plumed
      - Katherine M. Collins
      - Yael Moros-Daval
      - Seraphina Zhang
      - Qinlin Zhao
      - Yitian Huang
      - Luning Sun
      - Jonathan E. Prunty
      - Zongqian Li
      - Pablo Sánchez-García
      - Kexin Jiang Chen
      - Pablo A. M. Casares
      - Jiyun Zu
      - John Burden
      - Behzad Mehrbakhsh
      - David Stillwell
      - Manuel Cebrian
      - Jindong Wang
      - Peter Henderson
      - Sherry Tongshuang Wu
      - Patrick C. Kyllonen
      - Lucy Cheke
      - Xing Xie
      - José Hernández-Orallo
      author_organizations:
      - Leverhulme Centre for the Future of Intelligence
      - Center for Information Technology Policy, Princeton University
      date: '2025-03-11'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.04374
      link_text: 'GDPval: Evaluating AI Model Performance on Real-World Economically
        Valuable Tasks'
      original_md: '[GDPval: Evaluating AI Model Performance on Real-World Economically
        Valuable Tasks](https://arxiv.org/abs/2510.04374)'
      title: 'GDPval: Evaluating AI Model Performance on Real-World Economically Valuable
        Tasks'
      authors:
      - Tejal Patwardhan
      - Rachel Dias
      - Elizabeth Proehl
      - Grace Kim
      - Michele Wang
      - Olivia Watkins
      - Simón Posada Fishman
      - Marwan Aljubeh
      - Phoebe Thacker
      - Laurance Fauconnet
      - Natalie S. Kim
      - Patrick Chao
      - Samuel Miserendino
      - Gildas Chabot
      - David Li
      - Michael Sharman
      - Alexandra Barr
      - Amelia Glaese
      - Jerry Tworek
      author_organizations:
      - OpenAI
      date: '2025-10-05'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues:
  - Target case field is 'mixed' - cannot map to a single target case ID, kept as
    text only
- id: a:Capability_evals
  name: Capability evals
  header_level: 3
  parent_id: sec:Evals
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Make tools that can actually check whether a model has a
      certain capability or propensity. We default to low-n sampling of a vast latent
      space but aim to do better.
    theory_of_change: Keep a close eye on what capabilities are acquired when, so
      that frontier labs and regulators are better informed on what security measures
      are already necessary (and hopefully they extrapolate). You can't regulate without
      them.
    see_also:
    - '[Deepmind''s frontier safety framework](https://deepmind.google/blog/strengthening-our-frontier-safety-framework/)'
    - '[Aether](https://www.lesswrong.com/posts/B8Cmtf5gdHwxb8qtT/aether-july-2025-update)'
    orthodox_problems: []
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: behaviorist_science
    broad_approach_text: behaviorist science
    some_names:
    - METR
    - AISI
    - Apollo Research
    - Marrius Hobbhahn
    - Meg Tong
    - Mary Phuong
    - Beth Barnes
    - Thomas Kwa
    - Joel Becker
    estimated_ftes: 100+
    critiques: '[Large Language Models Often Know When They Are Being Evaluated](https://arxiv.org/abs/2505.23836),
      [AI Sandbagging: Language Models can Strategically Underperform on Evaluations](https://arxiv.org/abs/2406.07358),
      [The Leaderboard Illusion](https://arxiv.org/abs/2504.20879), [Do Large Language
      Model Benchmarks Test Reliability?](https://arxiv.org/abs/2502.03461)'
    funded_by: basically everyone. Google, Microsoft, Open Philanthropy, LTFF, Governments
      etc
    outputs:
    - link_url: https://metr.org/blog/2025-10-14-malt-dataset-of-natural-and-prompted-behaviors/
      link_text: 'MALT: A Dataset of Natural and Prompted Behaviors That Threaten
        Eval Integrity'
      original_md: '[MALT: A Dataset of Natural and Prompted Behaviors That Threaten
        Eval Integrity](https://metr.org/blog/2025-10-14-malt-dataset-of-natural-and-prompted-behaviors/)'
      title: 'MALT: A Dataset of Natural and Prompted Behaviors That Threaten Eval
        Integrity'
      authors:
      - Neev Parikh
      - Hjalmar Wijk
      author_organizations:
      - METR
      date: '2025-10-14'
      published_year: 2025
      venue: METR Blog
      kind: blog_post
    - link_url: https://arxiv.org/abs/2502.16797
      link_text: Forecasting Rare Language Model Behaviors
      original_md: '[Forecasting Rare Language Model Behaviors](https://arxiv.org/abs/2502.16797)'
      title: Forecasting Rare Language Model Behaviors
      authors:
      - Erik Jones
      - Meg Tong
      - Jesse Mu
      - Mohammed Mahfoud
      - Jan Leike
      - Roger Grosse
      - Jared Kaplan
      - William Fithian
      - Ethan Perez
      - Mrinank Sharma
      author_organizations:
      - Anthropic
      - OpenAI
      - UC Berkeley
      date: '2025-02-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.05209
      link_text: Model Tampering Attacks Enable More Rigorous Evaluations of LLM Capabilities
      original_md: '[Model Tampering Attacks Enable More Rigorous Evaluations of LLM
        Capabilities](https://arxiv.org/abs/2502.05209)'
      title: Model Tampering Attacks Enable More Rigorous Evaluations of LLM Capabilities
      authors:
      - Zora Che
      - Stephen Casper
      - Robert Kirk
      - Anirudh Satheesh
      - Stewart Slocum
      - Lev E McKinney
      - Rohit Gandikota
      - Aidan Ewart
      - Domenic Rosati
      - Zichu Wu
      - Zikui Cai
      - Bilal Chughtai
      - Yarin Gal
      - Furong Huang
      - Dylan Hadfield-Menell
      author_organizations: []
      date: '2025-02-03'
      published_year: 2025
      venue: arXiv (accepted to TMLR)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.02180
      link_text: 'The Elicitation Game: Evaluating Capability Elicitation Techniques'
      original_md: '[The Elicitation Game: Evaluating Capability Elicitation Techniques](https://arxiv.org/abs/2502.02180)'
      title: 'The Elicitation Game: Evaluating Capability Elicitation Techniques'
      authors:
      - Felix Hofstätter
      - Teun van der Weij
      - Jayden Teoh
      - Rada Djoneva
      - Henning Bartsch
      - Francis Rhys Ward
      author_organizations: []
      date: '2025-02-04'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2508.19980
      link_text: Evaluating Language Model Reasoning about Confidential Information
      original_md: '[Evaluating Language Model Reasoning about Confidential Information](https://arxiv.org/abs/2508.19980)'
      title: Evaluating Language Model Reasoning about Confidential Information
      authors:
      - Dylan Sam
      - Alexander Robey
      - Andy Zou
      - Matt Fredrikson
      - J. Zico Kolter
      author_organizations:
      - Carnegie Mellon University
      date: '2025-08-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.11844
      link_text: Evaluating the Goal-Directedness of Large Language Models
      original_md: '[Evaluating the Goal-Directedness of Large Language Models](https://arxiv.org/abs/2504.11844)'
      title: Evaluating the Goal-Directedness of Large Language Models
      authors:
      - Tom Everitt
      - Cristina Garbacea
      - Alexis Bellot
      - Jonathan Richens
      - Henry Papadatos
      - Siméon Campos
      - Rohin Shah
      author_organizations:
      - Google DeepMind
      - OpenAI
      - Anthropic
      date: '2025-04-16'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://alignment.anthropic.com/2024/rogue-eval/index.html
      link_text: A Toy Evaluation of Inference Code Tampering
      original_md: '[A Toy Evaluation of Inference Code Tampering](https://alignment.anthropic.com/2024/rogue-eval/index.html)'
      title: A Toy Evaluation of Inference Code Tampering
      authors: []
      author_organizations:
      - Anthropic
      date: null
      published_year: 2024
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - link_url: https://arxiv.org/abs/2502.07577
      link_text: Automated Capability Discovery via Foundation Model Self-Exploration
      original_md: '[Automated Capability Discovery via Foundation Model Self-Exploration](https://arxiv.org/abs/2502.07577)'
      title: Automated Capability Discovery via Foundation Model Self-Exploration
      authors:
      - Cong Lu
      - Shengran Hu
      - Jeff Clune
      author_organizations: []
      date: '2025-02-11'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.25369
      link_text: Generative Value Conflicts Reveal LLM Priorities
      original_md: '[Generative Value Conflicts Reveal LLM Priorities](https://arxiv.org/abs/2509.25369)'
      title: Generative Value Conflicts Reveal LLM Priorities
      authors:
      - Andy Liu
      - Kshitish Ghate
      - Mona Diab
      - Daniel Fried
      - Atoosa Kasirzadeh
      - Max Kleiman-Weiner
      author_organizations: []
      date: '2025-09-29'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.02709
      link_text: 'Technical Report: Evaluating Goal Drift in Language Model Agents'
      original_md: '[Technical Report: Evaluating Goal Drift in Language Model Agents](https://arxiv.org/abs/2505.02709)'
      title: 'Technical Report: Evaluating Goal Drift in Language Model Agents'
      authors:
      - Rauno Arike
      - Elizabeth Donoway
      - Henning Bartsch
      - Marius Hobbhahn
      author_organizations:
      - Apollo Research
      date: '2025-05-05'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/
      link_text: Measuring the Impact of Early-2025 AI on Experienced Open-Source
        Developer Productivity
      original_md: '[Measuring the Impact of Early-2025 AI on Experienced Open-Source
        Developer Productivity](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/)'
      title: Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer
        Productivity
      authors: []
      author_organizations:
      - METR
      date: '2025-07-10'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.19212
      link_text: 'When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social
        Dilemmas'
      original_md: '[When Ethics and Payoffs Diverge: LLM Agents in Morally Charged
        Social Dilemmas](https://arxiv.org/abs/2505.19212)'
      title: 'When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social
        Dilemmas'
      authors:
      - Steffen Backmann
      - David Guzman Piedrahita
      - Emanuel Tewolde
      - Rada Mihalcea
      - Bernhard Schölkopf
      - Zhijing Jin
      author_organizations:
      - Max Planck Institute for Intelligent Systems
      date: '2025-05-25'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.05731
      link_text: 'AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark
        from MLCommons'
      original_md: '[AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark
        from MLCommons](https://arxiv.org/abs/2503.05731)'
      title: 'AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark
        from MLCommons'
      authors:
      - Shaona Ghosh
      - Heather Frase
      - Adina Williams
      - Sarah Luger
      - Paul Röttger
      - Fazl Barez
      - Sean McGregor
      - Kenneth Fricklas
      - Mala Kumar
      - Quentin Feuillade-Montixi
      - Kurt Bollacker
      - Felix Friedrich
      - Ryan Tsang
      - Bertie Vidgen
      - Alicia Parrish
      - Chris Knotz
      - Eleonora Presani
      - Jonathan Bennion
      - Marisa Ferrara Boston
      - Mike Kuniavsky
      - Wiebke Hutiri
      - James Ezick
      - Malek Ben Salem
      - Rajat Sahay
      - Sujata Goswami
      - Usman Gohar
      - Ben Huang
      - Supheakmungkol Sarin
      - Elie Alhajjar
      - Canyu Chen
      - Roman Eng
      - Kashyap Ramanandula Manjusha
      - Virendra Mehta
      - Eileen Long
      - Murali Emani
      - Natan Vidra
      - Benjamin Rukundo
      - Abolfazl Shahbazi
      - Kongtao Chen
      - Rajat Ghosh
      - Vithursan Thangarasa
      - Pierre Peigné
      - Abhinav Singh
      - Max Bartolo
      - Satyapriya Krishna
      - Mubashara Akhtar
      - Rafael Gold
      - Cody Coleman
      - Luis Oala
      - Vassil Tashev
      - Joseph Marvin Imperial
      - Amy Russ
      - Sasidhar Kunapuli
      - Nicolas Miailhe
      - Julien Delaunay
      - Bhaktipriya Radharapu
      - Rajat Shinde
      - Tuesday
      - Debojyoti Dutta
      - Declan Grabb
      - Ananya Gangavarapu
      - Saurav Sahay
      - Agasthya Gangavarapu
      - Patrick Schramowski
      - Stephen Singam
      - Tom David
      - Xudong Han
      - Priyanka Mary Mammen
      - Tarunima Prabhakar
      - Venelin Kovatchev
      - Rebecca Weiss
      - Ahmed Ahmed
      - Kelvin N. Manyeki
      - Sandeep Madireddy
      - Foutse Khomh
      - Fedor Zhdanov
      - Joachim Baumann
      - Nina Vasan
      - Xianjun Yang
      - Carlos Mougn
      - Jibin Rajan Varghese
      - Hussain Chinoy
      - Seshakrishna Jitendar
      - Manil Maskey
      - Claire V. Hardgrove
      - Tianhao Li
      - Aakash Gupta
      - Emil Joswin
      - Yifan Mai
      - Shachi H Kumar
      - Cigdem Patlak
      - Kevin Lu
      - Vincent Alessi
      - Sree Bhargavi Balija
      - Chenhe Gu
      - Robert Sullivan
      - James Gealy
      - Matt Lavrisa
      - James Goel
      - Peter Mattson
      - Percy Liang
      - Joaquin Vanschoren
      author_organizations:
      - MLCommons
      date: '2025-04-18'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.anthropic.com/research/petri-open-source-auditing
      link_text: 'Petri: An open-source auditing tool to accelerate AI safety research'
      original_md: '[Petri: An open-source auditing tool to accelerate AI safety research](https://www.anthropic.com/research/petri-open-source-auditing)'
      title: 'Petri: An open-source auditing tool to accelerate AI safety research'
      authors:
      - Kai Fronsdal
      - Isha Gupta
      - Abhay Sheshadri
      - Jonathan Michala
      - Stephen McAleer
      - Rowan Wang
      - Sara Price
      - Samuel R. Bowman
      author_organizations:
      - Anthropic
      date: '2025-10-06'
      published_year: 2025
      venue: Anthropic Blog
      kind: blog_post
    - link_url: https://www.apolloresearch.ai/blog/research-note-our-scheming-precursor-evals-had-limited-predictive-power-for-our-in-context-scheming-evals
      link_text: 'Research Note: Our scheming precursor evals had limited predictive
        power for our in-context scheming evals'
      original_md: '[Research Note: Our scheming precursor evals had limited predictive
        power for our in-context scheming evals](https://www.apolloresearch.ai/blog/research-note-our-scheming-precursor-evals-had-limited-predictive-power-for-our-in-context-scheming-evals)'
      title: 'Research Note: Our scheming precursor evals had limited predictive power
        for our in-context scheming evals'
      authors:
      - Marius Hobbhahn
      author_organizations:
      - Apollo Research
      date: '2025-07-03'
      published_year: 2025
      venue: Apollo Research Blog
      kind: blog_post
    - link_url: https://lesswrong.com/posts/ZEuDH2W3XdRaTwpjD/hyperbolic-model-fits-metr-capabilities-estimate-worse-than
      link_text: Hyperbolic model fits METR capabilities estimate worse than exponential
        model
      original_md: '[Hyperbolic model fits METR capabilities estimate worse than exponential
        model](https://lesswrong.com/posts/ZEuDH2W3XdRaTwpjD/hyperbolic-model-fits-metr-capabilities-estimate-worse-than)'
      title: Hyperbolic model fits METR capabilities estimate worse than exponential
        model
      authors:
      - gjm
      author_organizations: []
      date: '2025-08-19'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/nmaKpoHxmzjT8yXTk/new-website-analyzing-ai-companies-model-evals
      link_text: New website analyzing AI companies' model evals
      original_md: '[New website analyzing AI companies'' model evals](https://lesswrong.com/posts/nmaKpoHxmzjT8yXTk/new-website-analyzing-ai-companies-model-evals)'
      title: New website analyzing AI companies' model evals
      authors:
      - Zach Stein-Perlman
      author_organizations: []
      date: '2025-05-26'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/9tqpPP4FwSnv9AWsi/research-note-our-scheming-precursor-evals-had-limited
      link_text: 'Research Note: Our scheming precursor evals had limited predictive
        power for our in-context scheming evals'
      original_md: '[Research Note: Our scheming precursor evals had limited predictive
        power for our in-context scheming evals](https://lesswrong.com/posts/9tqpPP4FwSnv9AWsi/research-note-our-scheming-precursor-evals-had-limited)'
      title: 'Research Note: Our scheming precursor evals had limited predictive power
        for our in-context scheming evals'
      authors:
      - Marius Hobbhahn
      author_organizations:
      - Apollo Research
      date: '2025-07-03'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/qhjNejRxbMGQp4wHt/how-fast-can-algorithms-advance-capabilities-or-epoch
      link_text: How Fast Can Algorithms Advance Capabilities? | Epoch Gradient Update
      original_md: '[How Fast Can Algorithms Advance Capabilities? | Epoch Gradient
        Update](https://lesswrong.com/posts/qhjNejRxbMGQp4wHt/how-fast-can-algorithms-advance-capabilities-or-epoch)'
      title: How Fast Can Algorithms Advance Capabilities? | Epoch Gradient Update
      authors:
      - Henry Josephson
      - Spencer Guo
      - Teddy Foley
      - Jack Sanderson
      - Anqi Qu
      author_organizations:
      - UChicago XLab
      - Google DeepMind
      date: '2025-05-16'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/CwdCYmsutwXwnYtEF/paper-safety-by-measurement-a-systematic-literature-review
      link_text: 'Safety by Measurement: A Systematic Literature Review of AI Safety
        Evaluation Methods'
      original_md: '[Safety by Measurement: A Systematic Literature Review of AI Safety
        Evaluation Methods](https://lesswrong.com/posts/CwdCYmsutwXwnYtEF/paper-safety-by-measurement-a-systematic-literature-review)'
      title: 'Safety by Measurement: A Systematic Literature Review of AI Safety Evaluation
        Methods'
      authors:
      - markov
      - Charbel-Raphaël
      author_organizations: []
      date: '2025-05-19'
      published_year: 2025
      venue: arXiv
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2502.02260
      link_text: Adversarial ML Problems Are Getting Harder to Solve and to Evaluate
      original_md: '[Adversarial ML Problems Are Getting Harder to Solve and to Evaluate](https://arxiv.org/abs/2502.02260)'
      title: Adversarial ML Problems Are Getting Harder to Solve and to Evaluate
      authors:
      - Javier Rando
      - Jie Zhang
      - Nicholas Carlini
      - Florian Tramèr
      author_organizations: []
      date: '2025-02-04'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2501.01558
      link_text: Predicting the Performance of Black-box LLMs through Self-Queries
      original_md: '[Predicting the Performance of Black-box LLMs through Self-Queries](https://arxiv.org/abs/2501.01558)'
      title: Predicting the Performance of Black-box LLMs through Self-Queries
      authors:
      - Dylan Sam
      - Marc Finzi
      - J. Zico Kolter
      author_organizations:
      - Carnegie Mellon University
      date: '2025-01-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.4wallai.com/amongais
      link_text: Among AIs
      original_md: '[Among AIs](https://www.4wallai.com/amongais)'
      title: Among AIs
      authors: []
      author_organizations:
      - 4Wall AI
      date: null
      published_year: 2025
      venue: 4Wall AI website
      kind: blog_post
    - link_url: https://arxiv.org/abs/2505.05541
      link_text: 'Safety by Measurement: A Systematic Literature Review of AI Safety
        Evaluation Methods'
      original_md: '[Safety by Measurement: A Systematic Literature Review of AI Safety
        Evaluation Methods](https://arxiv.org/abs/2505.05541)'
      title: 'Safety by Measurement: A Systematic Literature Review of AI Safety Evaluation
        Methods'
      authors:
      - Markov Grey
      - Charbel-Raphaël Segerie
      author_organizations: []
      date: '2025-05-08'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.12229
      link_text: 'Infini-gram mini: Exact n-gram Search at the Internet Scale with
        FM-Index'
      original_md: '[Infini-gram mini: Exact n-gram Search at the Internet Scale with
        FM-Index](https://arxiv.org/abs/2506.12229)'
      title: 'Infini-gram mini: Exact n-gram Search at the Internet Scale with FM-Index'
      authors:
      - Hao Xu
      - Jiacheng Liu
      - Yejin Choi
      - Noah A. Smith
      - Hannaneh Hajishirzi
      author_organizations:
      - University of Washington
      - Allen Institute for AI
      date: '2025-06-13'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/W3KfxjbqBAnifBQoi/we-should-try-to-automate-ai-safety-work-asap
      link_text: We should try to automate AI safety work asap
      original_md: '[We should try to automate AI safety work asap](https://lesswrong.com/posts/W3KfxjbqBAnifBQoi/we-should-try-to-automate-ai-safety-work-asap)'
      title: We should try to automate AI safety work asap
      authors:
      - Marius Hobbhahn
      author_organizations:
      - Apollo Research
      date: '2025-04-26'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/CXYf7kGBecZMajrXC/validating-against-a-misalignment-detector-is-very-different
      link_text: Validating against a misalignment detector is very different to training
        against one
      original_md: '[Validating against a misalignment detector is very different
        to training against one](https://lesswrong.com/posts/CXYf7kGBecZMajrXC/validating-against-a-misalignment-detector-is-very-different)'
      title: Validating against a misalignment detector is very different to training
        against one
      authors:
      - mattmacdermott
      author_organizations: []
      date: '2025-03-04'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/NDotm7oLHfR56g4sD/why-do-misalignment-risks-increase-as-ais-get-more-capable
      link_text: Why do misalignment risks increase as AIs get more capable?
      original_md: '[Why do misalignment risks increase as AIs get more capable?](https://lesswrong.com/posts/NDotm7oLHfR56g4sD/why-do-misalignment-risks-increase-as-ais-get-more-capable)'
      title: Why do misalignment risks increase as AIs get more capable?
      authors:
      - Ryan Greenblatt
      author_organizations:
      - Anthropic
      date: '2025-04-11'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/wbJxRNxuezvsGFEWv/open-philanthropy-technical-ai-safety-rfp-usd40m-available
      link_text: Open Philanthropy Technical AI Safety RFP - $40M Available Across
        21 Research Areas
      original_md: '[Open Philanthropy Technical AI Safety RFP \- $40M Available Across
        21 Research Areas](https://lesswrong.com/posts/wbJxRNxuezvsGFEWv/open-philanthropy-technical-ai-safety-rfp-usd40m-available)'
      title: Open Philanthropy Technical AI Safety RFP - $40M Available Across 21
        Research Areas
      authors:
      - jake_mendel
      - maxnadeau
      - Peter Favaloro
      author_organizations:
      - Open Philanthropy
      date: '2025-02-06'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2502.18339
      link_text: Correlating and Predicting Human Evaluations of Language Models from
        Natural Language Processing Benchmarks
      original_md: '[Correlating and Predicting Human Evaluations of Language Models
        from Natural Language Processing Benchmarks](https://arxiv.org/abs/2502.18339)'
      title: Correlating and Predicting Human Evaluations of Language Models from
        Natural Language Processing Benchmarks
      authors:
      - Rylan Schaeffer
      - Punit Singh Koura
      - Binh Tang
      - Ranjan Subramanian
      - Aaditya K Singh
      - Todor Mihaylov
      - Prajjwal Bhargava
      - Lovish Madaan
      - Niladri S. Chatterji
      - Vedanuj Goswami
      - Sergey Edunov
      - Dieuwke Hupkes
      - Sanmi Koyejo
      - Sharan Narang
      author_organizations:
      - Meta AI Research
      date: '2025-02-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/TxiB6hvnQqxXB5XDJ/why-future-ais-will-require-new-alignment-methods
      link_text: Why Future AIs will Require New Alignment Methods
      original_md: '[Why Future AIs will Require New Alignment Methods](https://lesswrong.com/posts/TxiB6hvnQqxXB5XDJ/why-future-ais-will-require-new-alignment-methods)'
      title: Why Future AIs will Require New Alignment Methods
      authors:
      - Alvin Ånestrand
      author_organizations: []
      date: '2025-10-10'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/LhnqegFoykcjaXCYH/100-concrete-projects-and-open-problems-in-evals
      link_text: 100+ concrete projects and open problems in evals
      original_md: '[100+ concrete projects and open problems in evals](https://lesswrong.com/posts/LhnqegFoykcjaXCYH/100-concrete-projects-and-open-problems-in-evals)'
      title: 100+ concrete projects and open problems in evals
      authors:
      - Marius Hobbhahn
      author_organizations:
      - Apollo Research
      date: '2025-03-22'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/tQzeafo9HjCeXn7ZF/ai-companies-should-be-safety-testing-the-most-capable
      link_text: AI companies should be safety-testing the most capable versions of
        their models
      original_md: '[AI companies should be safety-testing the most capable versions
        of their models](https://lesswrong.com/posts/tQzeafo9HjCeXn7ZF/ai-companies-should-be-safety-testing-the-most-capable)'
      title: AI companies should be safety-testing the most capable versions of their
        models
      authors:
      - Steven Adler
      author_organizations: []
      date: '2025-03-26'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2501.03200
      link_text: 'The FACTS Grounding Leaderboard: Benchmarking LLMs'' Ability to
        Ground Responses to Long-Form Input'
      original_md: '[The FACTS Grounding Leaderboard: Benchmarking LLMs'' Ability
        to Ground Responses to Long-Form Input](https://arxiv.org/abs/2501.03200)'
      title: 'The FACTS Grounding Leaderboard: Benchmarking LLMs'' Ability to Ground
        Responses to Long-Form Input'
      authors:
      - Alon Jacovi
      - Andrew Wang
      - Chris Alberti
      - Connie Tao
      - Jon Lipovetz
      - Kate Olszewska
      - Lukas Haas
      - Michelle Liu
      - Nate Keating
      - Adam Bloniarz
      - Carl Saroufim
      - Corey Fry
      - Dror Marcus
      - Doron Kukliansky
      - Gaurav Singh Tomar
      - James Swirhun
      - Jinwei Xing
      - Lily Wang
      - Madhu Gurumurthy
      - Michael Aaron
      - Moran Ambar
      - Rachana Fellinger
      - Rui Wang
      - Zizhao Zhang
      - Sasha Goldshtein
      - Dipanjan Das
      author_organizations:
      - Google
      date: '2025-01-06'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:Autonomy_evals
  name: Autonomy evals
  header_level: 3
  parent_id: sec:Evals
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Measure an AI's ability to act autonomously to complete
      long-horizon, complex tasks.
    theory_of_change: By measuring how long and complex a task an AI can complete
      (its "time horizon"), we can track capability growth and identify when models
      gain dangerous autonomous capabilities (like R&D acceleration or replication).
    see_also:
    - a:Capability_evals
    - '[OpenAI Preparedness](https://openai.com/index/updating-our-preparedness-framework/)'
    - '[Anthropic RSP](https://www.anthropic.com/rsp-updates)'
    orthodox_problems: []
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: behaviorist_science
    broad_approach_text: behaviorist science
    some_names:
    - METR
    - Thomas Kwa
    - Ben West
    - Joel Becker
    - Beth Barnes
    - Hjalmar Wijk
    - Tao Lin
    - Giulio Starace
    - Oliver Jaffe
    - Dane Sherburn
    - Sanidhya Vijayvargiya
    - Aditya Bharat Soni
    - Xuhui Zhou
    estimated_ftes: 10-50
    critiques: '[Measuring the Impact of Early-2025 AI on Experienced Open-Source
      Developer Productivity.](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/)
      [The "Length" of "Horizons"](https://www.lesswrong.com/posts/PzLSuaT6WGLQGJJJD/the-length-of-horizons)'
    funded_by: The Audacious Project, Open Philanthropy
    outputs:
    - link_url: https://fulcrumresearch.ai/2025/10/22/introducing-orchestra-quibbler.html
      link_text: Fulcrum
      original_md: '[Fulcrum](https://fulcrumresearch.ai/2025/10/22/introducing-orchestra-quibbler.html)'
      title: Introducing Quibbler and Orchestra
      authors: []
      author_organizations:
      - Fulcrum Research
      date: '2025-10-22'
      published_year: 2025
      venue: Fulcrum Research Blog
      kind: code_tool
    - link_url: https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/
      link_text: Measuring AI Ability to Complete Long Tasks
      original_md: '[Measuring AI Ability to Complete Long Tasks](https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/)'
      title: Measuring AI Ability to Complete Long Tasks
      authors:
      - Thomas Kwa
      - Ben West
      - Joel Becker
      - Amy Deng
      - Katharyn Garcia
      - Max Hasin
      - Sami Jawhar
      - Megan Kinniment
      - Nate Rush
      - Sydney Von Arx
      - Ryan Bloom
      - Thomas Broadley
      - Haoxing Du
      - Brian Goodrich
      - Nikola Jurkovic
      - Luke Harold Miles
      - Seraphina Nix
      - Tao Lin
      - Neev Parikh
      - David Rein
      - Lucas Jun Koba Sato
      - Hjalmar Wijk
      - Daniel M. Ziegler
      - Elizabeth Barnes
      - Lawrence Chan
      author_organizations:
      - METR
      date: '2025-03-19'
      published_year: 2025
      venue: METR Blog
      kind: blog_post
    - link_url: https://metr.github.io/autonomy-evals-guide/gpt-5-report/
      link_text: Details about METR's evaluation of OpenAI GPT-5
      original_md: '[Details about METR''s evaluation of OpenAI GPT-5](https://metr.github.io/autonomy-evals-guide/gpt-5-report/)'
      title: Details about METR's evaluation of OpenAI GPT-5
      authors: []
      author_organizations:
      - METR
      date: '2025-08-01'
      published_year: 2025
      venue: METR's Autonomy Evaluation Resources
      kind: blog_post
    - link_url: https://arxiv.org/abs/2411.15114
      link_text: 'RE-Bench: Evaluating frontier AI R&D capabilities of language model
        agents against human experts'
      original_md: '[RE-Bench: Evaluating frontier AI R\&D capabilities of language
        model agents against human experts](https://arxiv.org/abs/2411.15114)'
      title: 'RE-Bench: Evaluating frontier AI R&D capabilities of language model
        agents against human experts'
      authors:
      - Hjalmar Wijk
      - Tao Lin
      - Joel Becker
      - Sami Jawhar
      - Neev Parikh
      - Thomas Broadley
      - Lawrence Chan
      - Michael Chen
      - Josh Clymer
      - Jai Dhyani
      - Elena Ericheva
      - Katharyn Garcia
      - Brian Goodrich
      - Nikola Jurkovic
      - Holden Karnofsky
      - Megan Kinniment
      - Aron Lajko
      - Seraphina Nix
      - Lucas Sato
      - William Saunders
      - Maksym Taran
      - Ben West
      - Elizabeth Barnes
      author_organizations:
      - Open Philanthropy
      - Various research institutions
      date: '2024-11-22'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.14866
      link_text: 'OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents'
      original_md: '[OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents](https://arxiv.org/abs/2506.14866)'
      title: 'OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents'
      authors:
      - Thomas Kuntz
      - Agatha Duzan
      - Hao Zhao
      - Francesco Croce
      - Zico Kolter
      - Nicolas Flammarion
      - Maksym Andriushchenko
      author_organizations:
      - EPFL
      date: '2025-06-17'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://t.co/XfspwlzYdl
      link_text: 'OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World
        AI Agent Safety'
      original_md: '[OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World
        AI Agent Safety](https://t.co/XfspwlzYdl)'
      title: 'OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World
        AI Agent Safety'
      authors:
      - Sanidhya Vijayvargiya
      - Aditya Bharat Soni
      - Xuhui Zhou
      - Zora Zhiruo Wang
      - Nouha Dziri
      - Graham Neubig
      - Maarten Sap
      author_organizations:
      - Carnegie Mellon University
      - Allen Institute for AI
      date: '2025-07-08'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://metr.github.io/autonomy-evals-guide/openai-o3-report/
      link_text: Details about METR's preliminary evaluation of OpenAI's o3 and o4-mini
      original_md: '[Details about METR''s preliminary evaluation of OpenAI''s o3
        and o4-mini](https://metr.github.io/autonomy-evals-guide/openai-o3-report/)'
      title: Details about METR's preliminary evaluation of OpenAI's o3 and o4-mini
      authors:
      - METR
      author_organizations:
      - METR
      date: '2025-04-01'
      published_year: 2025
      venue: METR Website
      kind: blog_post
    - link_url: https://t.co/dHN2N0tUhC
      link_text: 'PaperBench: Evaluating AI''s Ability to Replicate AI Research'
      original_md: '[PaperBench: Evaluating AI''s Ability to Replicate AI Research](https://t.co/dHN2N0tUhC)'
      title: 'PaperBench: Evaluating AI''s Ability to Replicate AI Research'
      authors:
      - Giulio Starace
      - Oliver Jaffe
      - Dane Sherburn
      - James Aung
      - Chan Jun Shern
      - Leon Maksin
      - Rachel Dias
      - Evan Mays
      - Benjamin Kinsella
      - Wyatt Thompson
      - Johannes Heidecke
      - Mia Glaese
      - Tejal Patwardhan
      author_organizations:
      - OpenAI
      - OpenAI Preparedness Team
      date: '2025-04-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://metr.org/blog/2025-07-14-how-does-time-horizon-vary-across-domains/
      link_text: How Does Time Horizon Vary Across Domains?
      original_md: '[How Does Time Horizon Vary Across Domains?](https://metr.org/blog/2025-07-14-how-does-time-horizon-vary-across-domains/)'
      title: How Does Time Horizon Vary Across Domains?
      authors: []
      author_organizations:
      - METR
      date: '2025-07-14'
      published_year: 2025
      venue: METR Blog
      kind: blog_post
    - link_url: https://arxiv.org/abs/2502.15840
      link_text: 'Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous
        Agents'
      original_md: '[Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous
        Agents](https://arxiv.org/abs/2502.15840)'
      title: 'Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous Agents'
      authors:
      - Axel Backlund
      - Lukas Petersson
      author_organizations: []
      date: '2025-02-20'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.15850
      link_text: Forecasting Frontier Language Model Agent Capabilities
      original_md: '[Forecasting Frontier Language Model Agent Capabilities](https://arxiv.org/abs/2502.15850)'
      title: Forecasting Frontier Language Model Agent Capabilities
      authors:
      - Govind Pimpale
      - Axel Højmark
      - Jérémy Scheurer
      - Marius Hobbhahn
      author_organizations: []
      date: '2025-02-21'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.anthropic.com/research/project-vend-1
      link_text: 'Project Vend: Can Claude run a small shop? (And why does that matter?)'
      original_md: '[Project Vend: Can Claude run a small shop? (And why does that
        matter?)](https://www.anthropic.com/research/project-vend-1)'
      title: 'Project Vend: Can Claude run a small shop? (And why does that matter?)'
      authors: []
      author_organizations:
      - Anthropic
      - Andon Labs
      date: '2025-06-27'
      published_year: 2025
      venue: Anthropic Website
      kind: blog_post
    - link_url: https://arxiv.org/abs/2509.21998
      link_text: 'GSM-Agent: Understanding Agentic Reasoning Using Controllable Environments'
      original_md: '[GSM-Agent: Understanding Agentic Reasoning Using Controllable
        Environments](https://arxiv.org/abs/2509.21998)'
      title: 'GSM-Agent: Understanding Agentic Reasoning Using Controllable Environments'
      authors:
      - Hanlin Zhu
      - Tianyu Guo
      - Song Mei
      - Stuart Russell
      - Nikhil Ghosh
      - Alberto Bietti
      - Jiantao Jiao
      author_organizations:
      - UC Berkeley
      date: '2025-09-26'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:WMD_evals_Weapons_of_Mass_Destruction_
  name: WMD evals (Weapons of Mass Destruction)
  header_level: 3
  parent_id: sec:Evals
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Evaluate whether AI models possess dangerous knowledge or
      capabilities related to biological and chemical weapons, such as biosecurity
      or chemical synthesis.
    theory_of_change: By benchmarking and tracking AI's knowledge of biology and chemistry,
      we can identify when models become capable of accelerating WMD development or
      misuse, allowing for timely intervention.
    see_also:
    - a:Capability_evals
    - a:Autonomy_evals
    - a:Various_Redteams
    orthodox_problems: []
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: behaviorist_science
    broad_approach_text: behaviorist science
    some_names:
    - Lennart Justen
    - Haochen Zhao
    - Xiangru Tang
    - Ziran Yang
    - Aidan Peppin
    - Anka Reuel
    - Stephen Casper
    estimated_ftes: 10-50
    critiques: '[The Reality of AI and Biorisk](https://arxiv.org/abs/2412.01946)'
    funded_by: Open Philanthropy, UK AI Safety Institute (AISI), frontier labs, Scale
      AI, various academic institutions (Peking University, Yale, etc.), Meta
    outputs:
    - link_url: https://arxiv.org/abs/2504.16137
      link_text: 'Virology Capabilities Test (VCT): A Multimodal Virology Q&A Benchmark'
      original_md: '[Virology Capabilities Test (VCT): A Multimodal Virology Q\&A
        Benchmark](https://arxiv.org/abs/2504.16137)'
      title: 'Virology Capabilities Test (VCT): A Multimodal Virology Q&A Benchmark'
      authors:
      - Jasper Götting
      - Pedro Medeiros
      - Jon G Sanders
      - Nathaniel Li
      - Long Phan
      - Karam Elabd
      - Lennart Justen
      - Dan Hendrycks
      - Seth Donoughe
      author_organizations:
      - Multiple Research Institutions
      date: '2025-04-29'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.06108
      link_text: LLMs Outperform Experts on Challenging Biology Benchmarks
      original_md: '[LLMs Outperform Experts on Challenging Biology Benchmarks](https://arxiv.org/abs/2505.06108)'
      title: LLMs Outperform Experts on Challenging Biology Benchmarks
      authors:
      - Lennart Justen
      author_organizations: []
      date: '2025-05-09'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.11544
      link_text: 'The Safety Gap Toolkit: Evaluating Hidden Dangers of Open-Source
        Models'
      original_md: '[The Safety Gap Toolkit: Evaluating Hidden Dangers of Open-Source
        Models](https://arxiv.org/abs/2507.11544)'
      title: 'The Safety Gap Toolkit: Evaluating Hidden Dangers of Open-Source Models'
      authors:
      - Ann-Kathrin Dombrowski
      - Dillon Bowen
      - Adam Gleave
      - Chris Cundy
      author_organizations:
      - Alignment Research
      date: '2025-07-08'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.27629
      link_text: Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation
        Models
      original_md: '[Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation
        Models](https://arxiv.org/abs/2510.27629)'
      title: Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation
        Models
      authors:
      - Boyi Wei
      - Zora Che
      - Nathaniel Li
      - Udari Madhushani Sehwag
      - Jasper Götting
      - Samira Nedungadi
      - Julian Michael
      - Summer Yue
      - Dan Hendrycks
      - Peter Henderson
      - Zifan Wang
      - Seth Donoughe
      - Mantas Mazeika
      author_organizations:
      - Anthropic
      - Redwood Research
      date: '2025-10-31'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2411.16736
      link_text: 'ChemSafetyBench: Benchmarking LLM Safety on Chemistry Domain'
      original_md: '[ChemSafetyBench: Benchmarking LLM Safety on Chemistry Domain](https://arxiv.org/abs/2411.16736)'
      title: 'ChemSafetyBench: Benchmarking LLM Safety on Chemistry Domain'
      authors:
      - Haochen Zhao
      - Xiangru Tang
      - Ziran Yang
      - Xiao Han
      - Xuanzhi Feng
      - Yueqing Fan
      - Senhao Cheng
      - Di Jin
      - Yilun Zhao
      - Arman Cohan
      - Mark Gerstein
      author_organizations:
      - Yale University
      date: '2024-11-23'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2412.01946
      link_text: The Reality of AI and Biorisk
      original_md: '[The Reality of AI and Biorisk](https://arxiv.org/abs/2412.01946)'
      title: The Reality of AI and Biorisk
      authors:
      - Aidan Peppin
      - Anka Reuel
      - Stephen Casper
      - Elliot Jones
      - Andrew Strait
      - Usman Anwar
      - Anurag Agrawal
      - Sayash Kapoor
      - Sanmi Koyejo
      - Marie Pellat
      - Rishi Bommasani
      - Nick Frosst
      - Sara Hooker
      author_organizations: []
      date: '2024-12-02'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:Situational_awareness_and_self_awareness_evals
  name: Situational awareness and self-awareness evals
  header_level: 3
  parent_id: sec:Evals
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Evaluate if models understand their own internal states
      and behaviors, their environment, and whether they are in a test or real-world
      deployment.
    theory_of_change: If an AI can distinguish between evaluation and deployment ("evaluation
      awareness"), it might hide dangerous capabilities (scheming/sandbagging). By
      measuring self- and situational-awareness, we can better assess this risk and
      build more robust evaluations.
    see_also:
    - a:Sandbagging_evals
    - a:Various_Redteams
    - sec:Model_psychology
    orthodox_problems:
    - superintelligence_fool_supervisors
    - superintelligence_hack_software
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: behaviorist_science
    broad_approach_text: behaviorist science
    some_names:
    - Jan Betley
    - Xuchan Bao
    - Martín Soto
    - Mary Phuong
    - Roland S. Zimmermann
    - Joe Needham
    - Giles Edkins
    - Govind Pimpale
    - Kai Fronsdal
    - David Lindner
    - Lang Xiong
    - Xiaoyan Bai
    estimated_ftes: 30-70
    critiques: '[Lessons from a Chimp: AI "Scheming" and the Quest for Ape Language](https://arxiv.org/abs/2507.03409),
      [It''s hard to make scheming evals look realistic for LLMs](https://www.lesswrong.com/posts/TBk2dbWkg2F7dB3jb/it-s-hard-to-make-scheming-evals-look-realistic-for-llms)'
    funded_by: frontier labs (Google DeepMind, Anthropic), Open Philanthropy, The
      Audacious Project, UK AI Safety Institute (AISI), AI Safety Support, Apollo
      Research, METR
    outputs:
    - link_url: https://arxiv.org/pdf/2504.20084
      link_text: AI Awareness (literature review)
      original_md: '[AI Awareness (literature review)](https://arxiv.org/pdf/2504.20084)'
      title: AI Awareness
      authors:
      - Xiaojian Li
      - Haoyuan Shi
      - Rongwu Xu
      - Wei Xu
      author_organizations: []
      date: '2025-04-25'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2501.11120
      link_text: 'Tell me about yourself: LLMs are aware of their learned behaviors'
      original_md: '[Tell me about yourself: LLMs are aware of their learned behaviors](https://arxiv.org/abs/2501.11120)'
      title: 'Tell me about yourself: LLMs are aware of their learned behaviors'
      authors:
      - Jan Betley
      - Xuchan Bao
      - Martín Soto
      - Anna Sztyber-Betley
      - James Chua
      - Owain Evans
      author_organizations:
      - University of Oxford
      date: '2025-01-19'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.01420
      link_text: Evaluating Frontier Models for Stealth and Situational Awareness
      original_md: '[Evaluating Frontier Models for Stealth and Situational Awareness](https://arxiv.org/abs/2505.01420)'
      title: Evaluating Frontier Models for Stealth and Situational Awareness
      authors:
      - Mary Phuong
      - Roland S. Zimmermann
      - Ziyue Wang
      - David Lindner
      - Victoria Krakovna
      - Sarah Cogan
      - Allan Dafoe
      - Lewis Ho
      - Rohin Shah
      author_organizations:
      - Google DeepMind
      date: '2025-05-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.23836
      link_text: Large Language Models Often Know When They Are Being Evaluated
      original_md: '[Large Language Models Often Know When They Are Being Evaluated](https://arxiv.org/abs/2505.23836)'
      title: Large Language Models Often Know When They Are Being Evaluated
      authors:
      - Joe Needham
      - Giles Edkins
      - Govind Pimpale
      - Henning Bartsch
      - Marius Hobbhahn
      author_organizations: []
      date: '2025-05-28'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/9tHEibBBhQCHEyFsa/do-llms-know-what-they-re-capable-of-why-this-matters-for-ai
      link_text: Do LLMs know what they're capable of? Why this matters for AI safety,
        and initial findings
      original_md: '[Do LLMs know what they''re capable of? Why this matters for AI
        safety, and initial findings](https://lesswrong.com/posts/9tHEibBBhQCHEyFsa/do-llms-know-what-they-re-capable-of-why-this-matters-for-ai)'
      title: Do LLMs know what they're capable of? Why this matters for AI safety,
        and initial findings
      authors:
      - Casey Barkan
      - Sid Black
      - Oliver Sourbut
      author_organizations:
      - MATS Program
      date: '2025-07-13'
      published_year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2509.00591
      link_text: 'Probe-Rewrite-Evaluate: A Workflow for Reliable Benchmarks and Quantifying
        Evaluation Awareness'
      original_md: '[Probe-Rewrite-Evaluate: A Workflow for Reliable Benchmarks and
        Quantifying Evaluation Awareness](https://arxiv.org/abs/2509.00591)'
      title: 'Probe-Rewrite-Evaluate: A Workflow for Reliable Benchmarks and Quantifying
        Evaluation Awareness'
      authors:
      - Lang Xiong
      - Nishant Bhargava
      - Jianhang Hong
      - Jeremy Chang
      - Haihao Liu
      - Vasu Sharma
      - Kevin Zhu
      author_organizations: []
      date: '2025-08-30'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/E3daBewppAiECN3Ao/claude-sonnet-3-7-often-knows-when-it-s-in-alignment
      link_text: Claude Sonnet 3.7 (often) knows when it's in alignment evaluations
      original_md: '[Claude Sonnet 3.7 (often) knows when it''s in alignment evaluations](https://lesswrong.com/posts/E3daBewppAiECN3Ao/claude-sonnet-3-7-often-knows-when-it-s-in-alignment)'
      title: Claude Sonnet 3.7 (often) knows when it's in alignment evaluations
      authors:
      - Nicholas Goldowsky-Dill
      - Mikita Balesni
      - Jérémy Scheurer
      - Marius Hobbhahn
      author_organizations:
      - Apollo Research
      date: '2025-03-17'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/TBk2dbWkg2F7dB3jb/it-s-hard-to-make-scheming-evals-look-realistic-for-llms
      link_text: It's hard to make scheming evals look realistic for LLMs
      original_md: '[It''s hard to make scheming evals look realistic for LLMs](https://lesswrong.com/posts/TBk2dbWkg2F7dB3jb/it-s-hard-to-make-scheming-evals-look-realistic-for-llms)'
      title: It's hard to make scheming evals look realistic for LLMs
      authors:
      - Igor Ivanov
      - Danil Kadochnikov
      author_organizations: []
      date: '2025-05-24'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2510.03399
      link_text: Know Thyself? On the Incapability and Implications of AI Self-Recognition
      original_md: '[Know Thyself? On the Incapability and Implications of AI Self-Recognition](https://arxiv.org/abs/2510.03399)'
      title: Know Thyself? On the Incapability and Implications of AI Self-Recognition
      authors:
      - Xiaoyan Bai
      - Aryan Shrivastava
      - Ari Holtzman
      - Chenhao Tan
      author_organizations: []
      date: '2025-10-03'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.antischeming.ai/snippets
      link_text: Chain-of-Thought Snippets — Anti-Scheming
      original_md: '[Chain-of-Thought Snippets — Anti-Scheming](https://www.antischeming.ai/snippets)'
      title: Chain-of-Thought Snippets — Anti-Scheming
      authors: []
      author_organizations:
      - Apollo Research
      - OpenAI
      date: null
      published_year: null
      venue: antischeming.ai
      kind: other
    - link_url: https://arxiv.org/pdf/2407.04108
      link_text: Future Events as Backdoor Triggers
      original_md: '[Future Events as Backdoor Triggers](https://arxiv.org/pdf/2407.04108)'
      title: 'Future Events as Backdoor Triggers: Investigating Temporal Vulnerabilities
        in LLMs'
      authors:
      - Sara Price
      - Arjun Panickssery
      - Sam Bowman
      - Asa Cooper Stickland
      author_organizations: []
      date: '2024-07-04'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    other_attributes: {}
  parsing_issues: []
- id: a:Steganography_evals
  name: Steganography evals
  header_level: 3
  parent_id: sec:Evals
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: evaluate whether models can hide secret information or encoded
      reasoning in their outputs, such as in chain-of-thought scratchpads, to evade
      monitoring.
    theory_of_change: if models can use steganography, they could hide deceptive reasoning,
      bypassing safety monitoring and control measures. By evaluating this capability,
      we can assess the risk of a model fooling its supervisors.
    see_also:
    - a:AI_deception_evals
    - a:Chain_of_thought_monitoring
    orthodox_problems:
    - boxed_agi_exfiltrate
    - superintelligence_fool_supervisors
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: behaviorist_science
    broad_approach_text: behavioral
    some_names:
    - Antonio Norelli
    - Michael Bronstein
    estimated_ftes: 1-10
    critiques: 'Chain-of-Thought Is Already Unfaithful (So Steganography is Irrelevant):
      [Reasoning Models Don''t Always Say What They Think.](https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf)'
    funded_by: Anthropic (and its general funders, e.g., Google, Amazon)
    outputs:
    - link_url: https://arxiv.org/abs/2506.01926
      link_text: Large language models can learn and generalize steganographic chain-of-thought
        under process supervision
      original_md: '[Large language models can learn and generalize steganographic
        chain-of-thought under process supervision](https://arxiv.org/abs/2506.01926)'
      title: Large language models can learn and generalize steganographic chain-of-thought
        under process supervision
      authors:
      - Joey Skaf
      - Luis Ibanez-Lissen
      - Robert McCarthy
      - Connor Watts
      - Vasil Georgiv
      - Hannes Whittingham
      - Lorena Gonzalez-Manzano
      - David Lindner
      - Cameron Tice
      - Edward James Young
      - Puria Radmard
      author_organizations: []
      date: '2025-06-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.02737
      link_text: Early Signs of Steganographic Capabilities in Frontier LLMs
      original_md: '[Early Signs of Steganographic Capabilities in Frontier LLMs](https://arxiv.org/abs/2507.02737)'
      title: Early Signs of Steganographic Capabilities in Frontier LLMs
      authors:
      - Artur Zolkowski
      - Kei Nishimura-Gasparian
      - Robert McCarthy
      - Roland S. Zimmermann
      - David Lindner
      author_organizations: []
      date: '2025-07-03'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.14805
      link_text: 'Subliminal Learning: Language models transmit behavioural traits
        via hidden signals in data'
      original_md: '[Subliminal Learning: Language models transmit behavioural traits
        via hidden signals in data](https://arxiv.org/abs/2507.14805)'
      title: 'Subliminal Learning: Language models transmit behavioral traits via
        hidden signals in data'
      authors:
      - Alex Cloud
      - Minh Le
      - James Chua
      - Jan Betley
      - Anna Sztyber-Betley
      - Jacob Hilton
      - Samuel Marks
      - Owain Evans
      author_organizations: []
      date: '2025-07-20'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.20075
      link_text: LLMs can hide text in other text of the same length
      original_md: '[LLMs can hide text in other text of the same length](https://arxiv.org/abs/2510.20075)'
      title: LLMs can hide text in other text of the same length
      authors:
      - Antonio Norelli
      - Michael Bronstein
      author_organizations: []
      date: '2025-10-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://alignment.anthropic.com/2025/distill-paraphrases/
      link_text: Do reasoning models use their scratchpad like we do? Evidence from
        distilling paraphrases
      original_md: '[Do reasoning models use their scratchpad like we do? Evidence
        from distilling paraphrases](https://alignment.anthropic.com/2025/distill-paraphrases/)'
      title: Do reasoning models use their scratchpad like we do? Evidence from distilling
        paraphrases
      authors: []
      author_organizations:
      - Anthropic
      date: null
      published_year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    other_attributes: {}
  parsing_issues: []
- id: a:AI_deception_evals
  name: AI deception evals
  header_level: 3
  parent_id: sec:Evals
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: research demonstrating that AI models, particularly agentic
      ones, can learn and execute deceptive behaviors such as alignment faking, manipulation,
      and sandbagging.
    theory_of_change: proactively discover, evaluate, and understand the mechanisms
      of AI deception (e.g., alignment faking, manipulation, agentic deception) to
      prevent models from fooling human supervisors and causing harm.
    see_also:
    - a:Situational_awareness_and_self_awareness_evals
    - a:Steganography_evals
    - a:Sandbagging_evals
    - a:Chain_of_thought_monitoring
    orthodox_problems:
    - superintelligence_fool_supervisors
    - superintelligence_hack_software
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: null
    broad_approach_text: behavioral / engineering
    some_names:
    - Cadenza
    - Fred Heiding
    - Simon Lermen
    - Andrew Kao
    - Myra Cheng
    - Cinoo Lee
    - Pranav Khadpe
    - Satyapriya Krishna
    - Andy Zou
    - Rahul Gupta
    estimated_ftes: 30-80
    critiques: A central criticism is that the evaluation scenarios are "artificial
      and contrived". [the void](https://nostalgebraist.tumblr.com/post/785766737747574784/the-void)
      and [Lessons from a Chimp](https://arxiv.org/abs/2507.03409) argue this research
      is "overattributing human traits" to models.
    funded_by: Labs, academic institutions (e.g., Harvard, CMU, Barcelona Institute
      of Science and Technology), NSFC, ML Alignment Theory & Scholars (MATS) Program,
      FAR AI
    outputs:
    - link_url: https://arxiv.org/abs/2511.16035
      link_text: 'Liars'' Bench: Evaluating Lie Detectors for Language Models'
      original_md: '[Liars'' Bench: Evaluating Lie Detectors for Language Models](https://arxiv.org/abs/2511.16035)'
      title: 'Liars'' Bench: Evaluating Lie Detectors for Language Models'
      authors:
      - Kieron Kretschmar
      - Walter Laurito
      - Sharan Maiya
      - Samuel Marks
      author_organizations:
      - Cadenza Labs
      - FZI
      - University of Cambridge
      - Anthropic
      date: '2025-11-20'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/pdf/2510.15501
      link_text: 'DECEPTIONBENCH: A Comprehensive Benchmark for AI Deception Behaviors
        in Real-world Scenario'
      original_md: '[DECEPTIONBENCH: A Comprehensive Benchmark for AI Deception Behaviors
        in Real-world Scenario](https://arxiv.org/pdf/2510.15501)'
      title: 'DeceptionBench: A Comprehensive Benchmark for AI Deception Behaviors
        in Real-world Scenarios'
      authors:
      - Yao Huang
      - Yitong Sun
      - Yichi Zhang
      - Ruochen Zhang
      - Yinpeng Dong
      - Xingxing Wei
      author_organizations: []
      date: '2025-10-17'
      published_year: 2025
      venue: NeurIPS 2025
      kind: paper_preprint
    - link_url: https://arxiv.org/pdf/2506.18032
      link_text: Why Do Some Language Models Fake Alignment While Others Don't?
      original_md: '[Why Do Some Language Models Fake Alignment While Others Don''t?](https://arxiv.org/pdf/2506.18032)'
      title: Why Do Some Language Models Fake Alignment While Others Don't?
      authors:
      - Abhay Sheshadri
      - John Hughes
      - Julian Michael
      - Alex Mallen
      - Arun Jose
      - Janus
      - Fabien Roger
      author_organizations: []
      date: '2025-06-22'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://alignment.anthropic.com/2025/alignment-faking-revisited/
      link_text: 'Alignment Faking Revisited: Improved Classifiers and Open Source
        Extensions'
      original_md: '[Alignment Faking Revisited: Improved Classifiers and Open Source
        Extensions](https://alignment.anthropic.com/2025/alignment-faking-revisited/)'
      title: 'Alignment Faking Revisited: Improved Classifiers and Open Source Extensions'
      authors:
      - John Hughes
      - Abhay Sheshadr
      author_organizations:
      - MATS
      - Anthropic
      date: null
      published_year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - link_url: https://arxiv.org/abs/2509.17938
      link_text: 'D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language
        Models'
      original_md: '[D-REX: A Benchmark for Detecting Deceptive Reasoning in Large
        Language Models](https://arxiv.org/abs/2509.17938)'
      title: 'D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language
        Models'
      authors:
      - Satyapriya Krishna
      - Andy Zou
      - Rahul Gupta
      - Eliot Krzysztof Jones
      - Nick Winter
      - Dan Hendrycks
      - J. Zico Kolter
      - Matt Fredrikson
      - Spyros Matsoukas
      author_organizations:
      - Carnegie Mellon University
      - Center for AI Safety
      - Anthropic
      date: '2025-09-22'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.14318
      link_text: Evaluating & Reducing Deceptive Dialogue From Language Models with
        Multi-turn RL
      original_md: '[Evaluating & Reducing Deceptive Dialogue From Language Models
        with Multi-turn RL](https://arxiv.org/abs/2510.14318)'
      title: Evaluating & Reducing Deceptive Dialogue From Language Models with Multi-turn
        RL
      authors:
      - Marwa Abdulhai
      - Ryan Cheng
      - Aryansh Shrivastava
      - Natasha Jaques
      - Yarin Gal
      - Sergey Levine
      author_organizations:
      - University of Oxford
      - UC Berkeley
      date: '2025-10-16'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.04072
      link_text: 'Among Us: A Sandbox for Measuring and Detecting Agentic Deception'
      original_md: '[Among Us: A Sandbox for Measuring and Detecting Agentic Deception](https://arxiv.org/abs/2504.04072)'
      title: 'Among Us: A Sandbox for Measuring and Detecting Agentic Deception'
      authors:
      - Satvik Golechha
      - Adrià Garriga-Alonso
      author_organizations: []
      date: '2025-04-05'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.01070
      link_text: Eliciting Secret Knowledge from Language Models
      original_md: '[Eliciting Secret Knowledge from Language Models](https://arxiv.org/abs/2510.01070)'
      title: Eliciting Secret Knowledge from Language Models
      authors:
      - Bartosz Cywiński
      - Emil Ryd
      - Rowan Wang
      - Senthooran Rajamanoharan
      - Neel Nanda
      - Arthur Conmy
      - Samuel Marks
      author_organizations: []
      date: '2025-10-01'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/bqWihHtDnDseyfF2T/edge-cases-in-ai-alignment-2
      link_text: Edge Cases in AI Alignment
      original_md: '[Edge Cases in AI Alignment](https://lesswrong.com/posts/bqWihHtDnDseyfF2T/edge-cases-in-ai-alignment-2)'
      title: Edge Cases in AI Alignment
      authors:
      - Florian Dietz
      author_organizations:
      - MATS Program
      date: '2025-03-24'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://huggingface.co/datasets/cais/MASK
      link_text: The MASK Evaluation
      original_md: '[The MASK Evaluation](https://huggingface.co/datasets/cais/MASK)'
      title: The MASK Evaluation
      authors: []
      author_organizations:
      - Center for AI Safety
      - Scale AI
      date: null
      published_year: null
      venue: Hugging Face
      kind: dataset_benchmark
    - link_url: https://lesswrong.com/posts/pCMmLiBcHbKohQgwA/i-replicated-the-anthropic-alignment-faking-experiment-on
      link_text: I replicated the Anthropic alignment faking experiment on other models,
        and they didn't fake alignment
      original_md: '[I replicated the Anthropic alignment faking experiment on other
        models, and they didn''t fake alignment](https://lesswrong.com/posts/pCMmLiBcHbKohQgwA/i-replicated-the-anthropic-alignment-faking-experiment-on)'
      title: I replicated the Anthropic alignment faking experiment on other models,
        and they didn't fake alignment
      authors:
      - Aleksandr Kedrik
      - Igor Ivanov
      author_organizations: []
      date: '2025-05-30'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2412.00586
      link_text: 'Evaluating Large Language Models'' Capability to Launch Fully Automated
        Spear Phishing Campaigns: Validated on Human Subjects'
      original_md: '[Evaluating Large Language Models'' Capability to Launch Fully
        Automated Spear Phishing Campaigns: Validated on Human Subjects](https://arxiv.org/abs/2412.00586)'
      title: 'Evaluating Large Language Models'' Capability to Launch Fully Automated
        Spear Phishing Campaigns: Validated on Human Subjects'
      authors:
      - Fred Heiding
      - Simon Lermen
      - Andrew Kao
      - Bruce Schneier
      - Arun Vishwanath
      author_organizations: []
      date: '2024-11-30'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/kCGk5tp5suHoGwhCa/mistral-large-2-123b-seems-to-exhibit-alignment-faking
      link_text: Mistral Large 2 (123B) seems to exhibit alignment faking
      original_md: '[Mistral Large 2 (123B) seems to exhibit alignment faking](https://lesswrong.com/posts/kCGk5tp5suHoGwhCa/mistral-large-2-123b-seems-to-exhibit-alignment-faking)'
      title: Mistral Large 2 (123B) seems to exhibit alignment faking
      authors:
      - Marc Carauleanu
      - Diogo de Lucena
      - Gunnar Zarncke
      - Cameron Berg
      - Judd Rosenblatt
      - Mike Vaiana
      - Trent Hodgeson
      author_organizations:
      - AE Studio
      date: '2025-03-27'
      published_year: 2025
      venue: LessWrong/AI Alignment Forum
      kind: lesswrong
    other_attributes: {}
  parsing_issues:
  - Broad approach contains multiple approaches ('behavioural / engineering'), so
    broad_approach_id is left null
- id: a:AI_scheming_evals
  name: AI scheming evals
  header_level: 3
  parent_id: sec:Evals
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Evaluate frontier models for scheming, a sophisticated,
      strategic form of AI deception where a model covertly pursues a misaligned,
      long-term objective while deliberately faking alignment and compliance to evade
      detection by human supervisors and safety mechanisms.
    theory_of_change: Robust evaluations must move beyond checking final outputs and
      probe the model's reasoning to verify that alignment is genuine, not faked,
      because capable models are capable of strategically concealing misaligned goals
      (scheming) to pass standard behavioural evaluations.
    see_also:
    - a:AI_deception_evals
    - a:Situational_awareness_and_self_awareness_evals
    orthodox_problems:
    - superintelligence_fool_supervisors
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: null
    broad_approach_text: behavioral / engineering
    some_names:
    - Bronson Schoen
    - Alexander Meinke
    - Jason Wolfe
    - Mary Phuong
    - Rohin Shah
    - Evgenia Nitishinskaya
    - Mikita Balesni
    - Marius Hobbhahn
    - Jérémy Scheurer
    - Wojciech Zaremba
    - David Lindner
    estimated_ftes: 30-60
    critiques: '[No, LLMs are not "scheming"](https://www.strangeloopcanon.com/p/no-llms-are-not-scheming)'
    funded_by: OpenAI, Anthropic, Google DeepMind, Open Philanthropy
    outputs:
    - link_url: https://openai.com/index/detecting-and-reducing-scheming-in-ai-models/
      link_text: Detecting and reducing scheming in AI models
      original_md: '[Detecting and reducing scheming in AI models](https://openai.com/index/detecting-and-reducing-scheming-in-ai-models/)'
      title: Detecting and reducing scheming in AI models
      authors:
      - OpenAI
      author_organizations:
      - OpenAI
      - Apollo Research
      date: '2025-09-17'
      published_year: 2025
      venue: OpenAI Blog
      kind: blog_post
    - link_url: https://static1.squarespace.com/static/660eea75305d9a0e1148118a/t/691f711c4ac57d3831260538/1763668252592/scheming-propensity.pdf
      link_text: Evaluating and Understanding Scheming Propensity in LLM Agents
      original_md: '[Evaluating and Understanding Scheming Propensity in LLM Agents](https://static1.squarespace.com/static/660eea75305d9a0e1148118a/t/691f711c4ac57d3831260538/1763668252592/scheming-propensity.pdf)'
      title: scheming-propensity.pdf
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: null
      kind: error_detected
    - link_url: https://www.arxiv.org/pdf/2509.15541
      link_text: Stress Testing Deliberative Alignment for Anti-Scheming Training
      original_md: '[Stress Testing Deliberative Alignment for Anti-Scheming Training](https://www.arxiv.org/pdf/2509.15541)'
      title: Stress Testing Deliberative Alignment for Anti-Scheming Training
      authors:
      - Bronson Schoen
      - Evgenia Nitishinskaya
      - Mikita Balesni
      - Axel Højmark
      - Felix Hofstätter
      - Jérémy Scheurer
      - Alexander Meinke
      - Jason Wolfe
      - Teun van der Weij
      - Alex Lloyd
      - Nicholas Goldowsky-Dill
      - Angela Fan
      - Andrei Matveiakin
      - Rusheb Shah
      - Marcus Williams
      - Amelia Glaese
      - Boaz Barak
      - Wojciech Zaremba
      - Marius Hobbhahn
      author_organizations:
      - OpenAI
      - Anthropic
      date: '2025-09-19'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/html/2510.12826v1
      link_text: Scheming Ability in LLM-to-LLM Strategic Interactions
      original_md: '[Scheming Ability in LLM-to-LLM Strategic Interactions](https://arxiv.org/html/2510.12826v1)'
      title: Scheming Ability in LLM-to-LLM Strategic Interactions
      authors:
      - Thao Pham
      author_organizations:
      - Berea College
      - Cooperative AI Foundation
      date: '2025-10-11'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2412.04984
      link_text: Frontier Models are Capable of In-context Scheming
      original_md: '[Frontier Models are Capable of In-context Scheming](https://arxiv.org/abs/2412.04984)'
      title: Frontier Models are Capable of In-context Scheming
      authors:
      - Alexander Meinke
      - Bronson Schoen
      - Jérémy Scheurer
      - Mikita Balesni
      - Rusheb Shah
      - Marius Hobbhahn
      author_organizations:
      - Apollo Research
      date: '2024-12-06'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.05179
      link_text: Agentic Misalignment
      original_md: '[Agentic Misalignment](https://arxiv.org/abs/2510.05179)'
      title: 'Agentic Misalignment: How LLMs Could Be Insider Threats'
      authors:
      - Aengus Lynch
      - Benjamin Wright
      - Caleb Larson
      - Stuart J. Ritchie
      - Soren Mindermann
      - Evan Hubinger
      - Ethan Perez
      - Kevin Troy
      author_organizations:
      - Anthropic
      - Redwood Research
      date: '2025-10-05'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.lesswrong.com/posts/D5kGGGhsnfH7G8v9f/testing-for-scheming-with-model-deletion
      link_text: Testing for Scheming with Model Deletion
      original_md: '[Testing for Scheming with Model Deletion](https://www.lesswrong.com/posts/D5kGGGhsnfH7G8v9f/testing-for-scheming-with-model-deletion)'
      title: Testing for Scheming with Model Deletion
      authors: []
      author_organizations: []
      date: null
      published_year: null
      venue: LessWrong
      kind: error_detected
    other_attributes: {}
  parsing_issues:
  - Broad approach is 'behavioural / engineering' (mixed approaches) - broad_approach_id
    left null, broad_approach_text set to plain text version
- id: a:Sandbagging_evals
  name: Sandbagging evals
  header_level: 3
  parent_id: sec:Evals
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: Evaluate whether AI models deliberately hide their true
      capabilities or underperform, especially when they detect they are in an evaluation
      context.
    theory_of_change: If models can distinguish between evaluation and deployment
      contexts ("evaluation awareness"), they might learn to "sandbag" or deliberately
      underperform to hide dangerous capabilities, fooling safety evaluations. By
      developing evaluations for sandbagging, we can test whether our safety methods
      are being deceived and detect this behavior before a model is deployed.
    see_also:
    - a:AI_deception_evals
    - a:Situational_awareness_and_self_awareness_evals
    - a:Various_Redteams
    orthodox_problems:
    - superintelligence_fool_supervisors
    - superintelligence_hack_software
    target_case_id: pessimistic_case
    target_case_text: pessimistic
    broad_approach_id: behaviorist_science
    broad_approach_text: behaviorist science
    some_names:
    - Teun van der Weij
    - Cameron Tice
    - Chloe Li
    - Johannes Gasteiger
    - Joseph Bloom
    - Joel Dyer
    estimated_ftes: 10-50
    critiques: The main external critique, from sources like "[the void](https://nostalgebraist.tumblr.com/post/785766737747574784/the-void)"
      and "[Lessons from a Chimp](https://arxiv.org/abs/2507.03409)", is that this
      research "overattribut\[es\] human traits" to models. It argues that what's
      being measured isn't genuine sandbagging but models "playing-along-with-drama
      behaviour" in response to "artificial and contrived" evals.
    funded_by: Anthropic (and its funders, e.g., Google, Amazon), UK Government (funding
      the AI Security Institute)
    outputs:
    - link_url: https://arxiv.org/pdf/2412.01784
      link_text: Noise Injection Reveals Hidden Capabilities of Sandbagging Language
        Models
      original_md: '[Noise Injection Reveals Hidden Capabilities of Sandbagging Language
        Models](https://arxiv.org/pdf/2412.01784)'
      title: Noise Injection Reveals Hidden Capabilities of Sandbagging Language Models
      authors:
      - Cameron Tice
      - Philipp Alexander Kreer
      - Nathan Helm-Burger
      - Prithviraj Singh Shahani
      - Fedor Ryzhenkov
      - Jacob Haimes
      - Felix Hofstätter
      - Teun van der Weij
      author_organizations: []
      date: '2024-12-02'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/pdf/2509.26239
      link_text: Sandbagging in a Simple Survival Bandit Problem
      original_md: '[Sandbagging in a Simple Survival Bandit Problem](https://arxiv.org/pdf/2509.26239)'
      title: Sandbagging in a Simple Survival Bandit Problem
      authors:
      - Joel Dyer
      - Daniel Jarne Ornia
      - Nicholas Bishop
      - Anisoara Calinescu
      - Michael Wooldridge
      author_organizations: []
      date: '2025-09-30'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.18058
      link_text: Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier
        LLMs
      original_md: '[Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier
        LLMs](https://arxiv.org/abs/2509.18058)'
      title: Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier
        LLMs
      authors:
      - Alexander Panfilov
      - Evgenii Kortukov
      - Kristina Nikolić
      - Matthias Bethge
      - Sebastian Lapuschkin
      - Wojciech Samek
      - Ameya Prabhu
      - Maksym Andriushchenko
      - Jonas Geiping
      author_organizations:
      - University of Tübingen
      - Fraunhofer HHI
      - ELLIS Institute
      date: '2025-09-23'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2406.07358
      link_text: 'AI Sandbagging: Language Models can Strategically Underperform on
        Evaluations'
      original_md: '[AI Sandbagging: Language Models can Strategically Underperform
        on Evaluations](https://arxiv.org/abs/2406.07358)'
      title: 'AI Sandbagging: Language Models can Strategically Underperform on Evaluations'
      authors:
      - Teun van der Weij
      - Felix Hofstätter
      - Ollie Jaffe
      - Samuel F. Brown
      - Francis Rhys Ward
      author_organizations: []
      date: '2024-06-11'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://alignment.anthropic.com/2025/automated-researchers-sandbag/
      link_text: Automated Researchers Can Subtly Sandbag
      original_md: '[Automated Researchers Can Subtly Sandbag](https://alignment.anthropic.com/2025/automated-researchers-sandbag/)'
      title: Automated Researchers Can Subtly Sandbag
      authors:
      - Johannes Gasteiger
      - Vladimir Mikulik
      - Ethan Perez
      - Fabien Roger
      - Misha Wagner
      - Akbir Khan
      - Sam Bowman
      - Jan Leike
      author_organizations:
      - Anthropic
      date: null
      published_year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - link_url: https://arxiv.org/abs/2508.00943
      link_text: LLMs Can Covertly Sandbag on Capability Evaluations Against Chain-of-Thought
        Monitoring
      original_md: '[LLMs Can Covertly Sandbag on Capability Evaluations Against Chain-of-Thought
        Monitoring](https://arxiv.org/abs/2508.00943)'
      title: LLMs Can Covertly Sandbag on Capability Evaluations Against Chain-of-Thought
        Monitoring
      authors:
      - Chloe Li
      - Mary Phuong
      - Noah Y. Siegel
      author_organizations:
      - DeepMind
      date: '2025-07-31'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.alignmentforum.org/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging
      link_text: White Box Control at UK AISI - Update on Sandbagging Investigations
      original_md: '[White Box Control at UK AISI \- Update on Sandbagging Investigations](https://www.alignmentforum.org/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging)'
      title: White Box Control at UK AISI - Update on Sandbagging Investigations
      authors:
      - Joseph Bloom
      - Jordan Taylor
      - Connor Kissane
      - Sid Black
      - merizian
      - alexdzm
      - jacoba
      - Ben Millwood
      - Alan Cooney
      author_organizations:
      - UK AISI
      date: '2025-07-10'
      published_year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/TeTegzR8X5CuKgMc3/misalignment-and-strategic-underperformance-an-analysis-of
      link_text: 'Misalignment and Strategic Underperformance: An Analysis of Sandbagging
        and Exploration Hacking'
      original_md: '[Misalignment and Strategic Underperformance: An Analysis of Sandbagging
        and Exploration Hacking](https://lesswrong.com/posts/TeTegzR8X5CuKgMc3/misalignment-and-strategic-underperformance-an-analysis-of)'
      title: 'Misalignment and Strategic Underperformance: An Analysis of Sandbagging
        and Exploration Hacking'
      authors:
      - Buck Shlegeris
      - Julian Stastny
      author_organizations:
      - Redwood Research
      date: '2025-05-08'
      published_year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - link_url: https://alignment.anthropic.com/2025/wont-vs-cant/
      link_text: 'Won''t vs. Can''t: Sandbagging-like Behavior from Claude Models'
      original_md: '[Won''t vs. Can''t: Sandbagging-like Behavior from Claude Models](https://alignment.anthropic.com/2025/wont-vs-cant/)'
      title: 'Won''t vs. Can''t: Sandbagging-like Behavior from Claude Models'
      authors: []
      author_organizations:
      - Anthropic
      date: '2025-01-15'
      published_year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    other_attributes: {}
  parsing_issues: []
- id: a:Self_replication_evals
  name: Self-replication evals
  header_level: 3
  parent_id: sec:Evals
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: evaluate whether AI agents can autonomously replicate themselves
      by obtaining their own weights, securing compute resources, and creating copies
      of themselves.
    theory_of_change: if AI agents gain the ability to self-replicate, they could
      proliferate uncontrollably, making them impossible to shut down. By measuring
      this capability with benchmarks like RepliBench, we can identify when models
      cross this dangerous "red line" and implement controls before losing containment.
    see_also:
    - a:Autonomy_evals
    - a:Situational_awareness_and_self_awareness_evals
    orthodox_problems:
    - instrumental_convergence
    - boxed_agi_exfiltrate
    target_case_id: worst_case
    target_case_text: worst-case
    broad_approach_id: behaviorist_science
    broad_approach_text: behaviorist science
    some_names:
    - Sid Black
    - Asa Cooper Stickland
    - Jake Pencharz
    - Oliver Sourbut
    - Michael Schmatz
    - Jay Bailey
    - Ollie Matthews
    - Ben Millwood
    - Alex Remedios
    - Alan Cooney
    - Xudong Pan
    - Jiarun Dai
    - Yihe Fan
    estimated_ftes: 10-20
    critiques: '[AI Sandbagging](https://arxiv.org/abs/2406.07358)'
    funded_by: UK Government (via UK AI Safety Institute)
    outputs:
    - link_url: https://arxiv.org/abs/2503.17378
      link_text: Large language model-powered AI systems achieve self-replication
        with no human intervention
      original_md: '[Large language model-powered AI systems achieve self-replication
        with no human intervention](https://arxiv.org/abs/2503.17378)'
      title: Large language model-powered AI systems achieve self-replication with
        no human intervention
      authors:
      - Xudong Pan
      - Jiarun Dai
      - Yihe Fan
      - Minyuan Luo
      - Changyi Li
      - Min Yang
      author_organizations: []
      date: '2025-03-25'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.25302
      link_text: A Realistic Evaluation of Self-Replication Risk in LLM Agents
      original_md: '[A Realistic Evaluation of Self-Replication Risk in LLM Agents](https://arxiv.org/abs/2509.25302)'
      title: 'Dive into the Agent Matrix: A Realistic Evaluation of Self-Replication
        Risk in LLM Agents'
      authors:
      - Boxuan Zhang
      - Yi Yu
      - Jiaxuan Guo
      - Jing Shao
      author_organizations: []
      date: '2025-09-29'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://aisi.gov.uk/work/replibench-measuring-autonomous-replication-capabilities-in-ai-systems
      link_text: 'RepliBench: measuring autonomous replication capabilities in AI
        systems'
      original_md: '[RepliBench: measuring autonomous replication capabilities in
        AI systems](https://aisi.gov.uk/work/replibench-measuring-autonomous-replication-capabilities-in-ai-systems)'
      title: 'RepliBench: measuring autonomous replication capabilities in AI systems'
      authors: []
      author_organizations:
      - UK AI Security Institute
      date: '2025-04-22'
      published_year: 2025
      venue: UK AISI Blog
      kind: blog_post
    other_attributes: {}
  parsing_issues: []
- id: a:Various_Redteams
  name: Various Redteams
  header_level: 3
  parent_id: sec:Evals
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: attack current models and see what they do / deliberately
      induce bad things on current frontier models to test out our theories / methods.
    theory_of_change: to ensure models are safe, we must actively try to break them.
      By developing and applying a diverse suite of attacks (e.g., in novel domains,
      against agentic systems, or using automated tools), researchers can discover
      vulnerabilities, specification gaming, and deceptive behaviors before they are
      exploited, thereby informing the development of more robust defenses.
    see_also:
    - a:Other_evals
    orthodox_problems:
    - boxed_agi_exfiltrate
    - goals_misgeneralize
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: behaviorist_science
    broad_approach_text: behaviorist science
    some_names:
    - Ryan Greenblatt
    - Benjamin Wright
    - Aengus Lynch
    - John Hughes
    - Samuel R. Bowman
    - Andy Zou
    - Nicholas Carlini
    - Abhay Sheshadri
    estimated_ftes: 100+
    critiques: '[Claude Sonnet 3.7 (often) knows when it''s in alignment evaluations](https://www.alignmentforum.org/posts/E3daBewppAiECN3Ao/claude-sonnet-3-7-often-knows-when-it-s-in-alignment),
      [Red Teaming AI Red Teaming.](https://arxiv.org/html/2507.05538v1)'
    funded_by: Frontier labs (Anthropic, OpenAI, Google), government (UK AISI), Open
      Philanthropy, LTFF, academic grants.
    outputs:
    - link_url: https://arxiv.org/pdf/2406.18510
      link_text: 'WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially)
        Safer Language Models'
      original_md: '[WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially)
        Safer Language Models](https://arxiv.org/pdf/2406.18510)'
      title: 'WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially)
        Safer Language Models'
      authors:
      - Liwei Jiang
      - Kavel Rao
      - Seungju Han
      - Allyson Ettinger
      - Faeze Brahman
      - Sachin Kumar
      - Niloofar Mireshghallah
      - Ximing Lu
      - Maarten Sap
      - Yejin Choi
      - Nouha Dziri
      author_organizations:
      - University of Washington
      - Allen Institute for AI
      date: '2024-06-26'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2512.03771
      link_text: In-Context Representation Hijacking
      original_md: '[In-Context Representation Hijacking](https://arxiv.org/abs/2512.03771)'
      title: In-Context Representation Hijacking
      authors:
      - Itay Yona
      - Amir Sarid
      - Michael Karasik
      - Yossi Gandelsman
      author_organizations: []
      date: '2025-12-03'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://alignment.anthropic.com/2025/automated-auditing/
      link_text: Building and evaluating alignment auditing agents
      original_md: '[Building and evaluating alignment auditing agents](https://alignment.anthropic.com/2025/automated-auditing/)'
      title: Building and evaluating alignment auditing agents
      authors:
      - Trenton Bricken
      - Rowan Wang
      - Sam Bowman
      - Euan Ong
      - Johannes Treutlein
      - Jeff Wu
      - Evan Hubinger
      - Samuel Marks
      author_organizations:
      - Anthropic
      date: '2025-07-24'
      published_year: 2025
      venue: Alignment Science Blog
      kind: blog_post
    - link_url: https://t.co/wk0AP8aDNI
      link_text: Findings from a Pilot Anthropic—OpenAI Alignment Evaluation Exercise
      original_md: '[Findings from a Pilot Anthropic—OpenAI Alignment Evaluation Exercise](https://t.co/wk0AP8aDNI)'
      title: Findings from a Pilot Anthropic—OpenAI Alignment Evaluation Exercise
      authors:
      - Samuel R. Bowman
      - Megha Srivastava
      - Jon Kutasov
      - Rowan Wang
      - Trenton Bricken
      - Benjamin Wright
      - Ethan Perez
      - Nicholas Carlini
      author_organizations:
      - Anthropic
      - OpenAI
      date: '2025-08-27'
      published_year: 2025
      venue: Alignment Science Blog
      kind: blog_post
    - link_url: https://t.co/XFtd0H2Pzb
      link_text: 'Agentic Misalignment: How LLMs could be insider threats'
      original_md: '[Agentic Misalignment: How LLMs could be insider threats](https://t.co/XFtd0H2Pzb)'
      title: 'Agentic Misalignment: How LLMs could be insider threats'
      authors:
      - Aengus Lynch
      - Benjamin Wright
      - Caleb Larson
      - Kevin K. Troy
      - Stuart J. Ritchie
      - Sören Mindermann
      - Ethan Perez
      - Evan Hubinger
      author_organizations:
      - Anthropic
      - University College London
      - MATS
      - Mila
      - Redwood Research
      date: '2025-06-20'
      published_year: 2025
      venue: Anthropic Research
      kind: blog_post
    - link_url: https://arxiv.org/abs/2502.08301
      link_text: Compromising Honesty and Harmlessness in Language Models via Deception
        Attacks
      original_md: '[Compromising Honesty and Harmlessness in Language Models via
        Deception Attacks](https://arxiv.org/abs/2502.08301)'
      title: Compromising Honesty and Harmlessness in Language Models via Deception
        Attacks
      authors:
      - Laurène Vaugrante
      - Francesca Carlon
      - Maluna Menke
      - Thilo Hagendorff
      author_organizations: []
      date: '2025-02-12'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.01236
      link_text: Eliciting Language Model Behaviors with Investigator Agents
      original_md: '[Eliciting Language Model Behaviors with Investigator Agents](https://arxiv.org/abs/2502.01236)'
      title: Eliciting Language Model Behaviors with Investigator Agents
      authors:
      - Xiang Lisa Li
      - Neil Chowdhury
      - Daniel D. Johnson
      - Tatsunori Hashimoto
      - Percy Liang
      - Sarah Schwettmann
      - Jacob Steinhardt
      author_organizations:
      - Stanford University
      - UC Berkeley
      date: '2025-02-03'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.14260
      link_text: Shutdown Resistance in Large Language Models
      original_md: '[Shutdown Resistance in Large Language Models](https://arxiv.org/abs/2509.14260)'
      title: Shutdown Resistance in Large Language Models
      authors:
      - Jeremy Schlatter
      - Benjamin Weinstein-Raun
      - Jeffrey Ladish
      author_organizations: []
      date: '2025-09-13'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.15541
      link_text: Stress Testing Deliberative Alignment for Anti-Scheming Training
      original_md: '[Stress Testing Deliberative Alignment for Anti-Scheming Training](https://arxiv.org/abs/2509.15541)'
      title: Stress Testing Deliberative Alignment for Anti-Scheming Training
      authors:
      - Bronson Schoen
      - Evgenia Nitishinskaya
      - Mikita Balesni
      - Axel Højmark
      - Felix Hofstätter
      - Jérémy Scheurer
      - Alexander Meinke
      - Jason Wolfe
      - Teun van der Weij
      - Alex Lloyd
      - Nicholas Goldowsky-Dill
      - Angela Fan
      - Andrei Matveiakin
      - Rusheb Shah
      - Marcus Williams
      - Amelia Glaese
      - Boaz Barak
      - Wojciech Zaremba
      - Marius Hobbhahn
      author_organizations:
      - OpenAI
      - Anthropic
      date: '2025-09-19'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.26418
      link_text: Chain-of-Thought Hijacking
      original_md: '[Chain-of-Thought Hijacking](https://arxiv.org/abs/2510.26418)'
      title: Chain-of-Thought Hijacking
      authors:
      - Jianli Zhao
      - Tingchen Fu
      - Rylan Schaeffer
      - Mrinank Sharma
      - Fazl Barez
      author_organizations: []
      date: '2025-10-30'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.13203
      link_text: 'X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents'
      original_md: '[X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents](https://arxiv.org/abs/2504.13203)'
      title: 'X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents'
      authors:
      - Salman Rahman
      - Liwei Jiang
      - James Shiffer
      - Genglin Liu
      - Sheriff Issaka
      - Md Rizwan Parvez
      - Hamid Palangi
      - Kai-Wei Chang
      - Yejin Choi
      - Saadia Gabriel
      author_organizations:
      - University of Washington
      - UCLA
      - Microsoft
      date: '2025-04-15'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/b8eeCGe3FWzHKbePF/agentic-misalignment-how-llms-could-be-insider-threats-1
      link_text: 'Agentic Misalignment: How LLMs Could be Insider Threats'
      original_md: '[Agentic Misalignment: How LLMs Could be Insider Threats](https://lesswrong.com/posts/b8eeCGe3FWzHKbePF/agentic-misalignment-how-llms-could-be-insider-threats-1)'
      title: 'Agentic Misalignment: How LLMs Could be Insider Threats'
      authors:
      - Aengus Lynch
      - Benjamin Wright
      - Caleb Larson
      - Stuart Richie
      - Sören Mindermann
      - Evan Hubinger
      - Ethan Perez
      - Kevin Troy
      author_organizations:
      - Anthropic
      date: '2025-06-20'
      published_year: 2025
      venue: LessWrong/AI Alignment Forum
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/zjqrSKZuRLnjAniyo/illusory-safety-redteaming-deepseek-r1-and-the-strongest
      link_text: 'Illusory Safety: Redteaming DeepSeek R1 and the Strongest Fine-Tunable
        Models of OpenAI, Anthropic, and Google'
      original_md: '[Illusory Safety: Redteaming DeepSeek R1 and the Strongest Fine-Tunable
        Models of OpenAI, Anthropic, and Google](https://lesswrong.com/posts/zjqrSKZuRLnjAniyo/illusory-safety-redteaming-deepseek-r1-and-the-strongest)'
      title: 'Illusory Safety: Redteaming DeepSeek R1 and the Strongest Fine-Tunable
        Models of OpenAI, Anthropic, and Google'
      authors:
      - ChengCheng
      - Brendan Murphy
      - Adrià Garriga-alonso
      - Yashvardhan Sharma
      - dsbowen
      - smallsilo
      - Yawen Duan
      - ChrisCundy
      - Hannah Betts
      - AdamGleave
      - Kellin Pelrine
      author_organizations:
      - FAR AI
      date: '2025-02-07'
      published_year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2506.18032
      link_text: Why Do Some Language Models Fake Alignment While Others Don't?
      original_md: '[Why Do Some Language Models Fake Alignment While Others Don''t?](https://arxiv.org/abs/2506.18032)'
      title: Why Do Some Language Models Fake Alignment While Others Don't?
      authors:
      - Abhay Sheshadri
      - John Hughes
      - Julian Michael
      - Alex Mallen
      - Arun Jose
      - Janus
      - Fabien Roger
      author_organizations: []
      date: '2025-06-22'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.13295
      link_text: Demonstrating specification gaming in reasoning models
      original_md: '[Demonstrating specification gaming in reasoning models](https://arxiv.org/abs/2502.13295)'
      title: Demonstrating specification gaming in reasoning models
      authors:
      - Alexander Bondarenko
      - Denis Volk
      - Dmitrii Volkov
      - Jeffrey Ladish
      author_organizations: []
      date: '2025-08-27'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.11630
      link_text: 'Jailbreak-Tuning: Models Efficiently Learn Jailbreak Susceptibility'
      original_md: '[Jailbreak-Tuning: Models Efficiently Learn Jailbreak Susceptibility](https://arxiv.org/abs/2507.11630)'
      title: 'Jailbreak-Tuning: Models Efficiently Learn Jailbreak Susceptibility'
      authors:
      - Brendan Murphy
      - Dillon Bowen
      - Shahrad Mohammadzadeh
      - Tom Tseng
      - Julius Broomfield
      - Adam Gleave
      - Kellin Pelrine
      author_organizations: []
      date: '2025-07-15'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.10949
      link_text: Monitoring Decomposition Attacks in LLMs with Lightweight Sequential
        Monitors
      original_md: '[Monitoring Decomposition Attacks in LLMs with Lightweight Sequential
        Monitors](https://arxiv.org/abs/2506.10949)'
      title: Monitoring Decomposition Attacks in LLMs with Lightweight Sequential
        Monitors
      authors:
      - Chen Yueh-Han
      - Nitish Joshi
      - Yulin Chen
      - Maksym Andriushchenko
      - Rico Angell
      - He He
      author_organizations: []
      date: '2025-06-14'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2412.18693
      link_text: Diverse and Effective Red Teaming with Auto-generated Rewards and
        Multi-step Reinforcement Learning
      original_md: '[Diverse and Effective Red Teaming with Auto-generated Rewards
        and Multi-step Reinforcement Learning](https://arxiv.org/abs/2412.18693)'
      title: Diverse and Effective Red Teaming with Auto-generated Rewards and Multi-step
        Reinforcement Learning
      authors:
      - Alex Beutel
      - Kai Xiao
      - Johannes Heidecke
      - Lilian Weng
      author_organizations:
      - OpenAI
      date: '2024-12-24'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://t.co/tkHkVFVZ2m
      link_text: 'Call Me A Jerk: Persuading AI to Comply with Objectionable Requests'
      original_md: '[Call Me A Jerk: Persuading AI to Comply with Objectionable Requests](https://t.co/tkHkVFVZ2m)'
      title: 'Call Me A Jerk: Persuading AI to Comply with Objectionable Requests'
      authors:
      - Lennart Meincke
      - Dan Shapiro
      - Angela Duckworth
      - Ethan R. Mollick
      - Lilach Mollick
      - Robert Cialdini
      author_organizations:
      - University of Pennsylvania
      - The Wharton School
      - WHU - Otto Beisheim School of Management
      - Glowforge, Inc
      date: '2025-07-18'
      published_year: 2025
      venue: SSRN / The Wharton School Research Paper
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.11083
      link_text: 'RedDebate: Safer Responses through Multi-Agent Red Teaming Debates'
      original_md: '[RedDebate: Safer Responses through Multi-Agent Red Teaming Debates](https://arxiv.org/abs/2506.11083)'
      title: 'RedDebate: Safer Responses through Multi-Agent Red Teaming Debates'
      authors:
      - Ali Asad
      - Stephen Obadinma
      - Radin Shayanfar
      - Xiaodan Zhu
      author_organizations: []
      date: '2025-06-04'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.09712
      link_text: The Structural Safety Generalization Problem
      original_md: '[The Structural Safety Generalization Problem](https://arxiv.org/abs/2504.09712)'
      title: The Structural Safety Generalization Problem
      authors:
      - Julius Broomfield
      - Tom Gibbs
      - Ethan Kosak-Hine
      - George Ingebretsen
      - Tia Nasir
      - Jason Zhang
      - Reihaneh Iranmanesh
      - Sara Pieri
      - Reihaneh Rabbany
      - Kellin Pelrine
      author_organizations: []
      date: '2025-04-13'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.19537
      link_text: No, of Course I Can! Deeper Fine-Tuning Attacks That Bypass Token-Level
        Safety Mechanisms
      original_md: '[No, of Course I Can\! Deeper Fine-Tuning Attacks That Bypass
        Token-Level Safety Mechanisms](https://arxiv.org/abs/2502.19537)'
      title: No, of Course I Can! Deeper Fine-Tuning Attacks That Bypass Token-Level
        Safety Mechanisms
      authors:
      - Joshua Kazdan
      - Abhay Puri
      - Rylan Schaeffer
      - Lisa Yu
      - Chris Cundy
      - Jason Stanley
      - Sanmi Koyejo
      - Krishnamurthy Dvijotham
      author_organizations: []
      date: '2025-02-26'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2502.14828
      link_text: Fundamental Limitations in Pointwise Defences of LLM Finetuning APIs
      original_md: '[Fundamental Limitations in Pointwise Defences of LLM Finetuning
        APIs](https://arxiv.org/abs/2502.14828)'
      title: Fundamental Limitations in Pointwise Defences of LLM Finetuning APIs
      authors:
      - Xander Davies
      - Eric Winsor
      - Alexandra Souly
      - Tomek Korbak
      - Robert Kirk
      - Christian Schroeder de Witt
      - Yarin Gal
      author_organizations:
      - Oxford University
      date: '2025-02-20'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.24068
      link_text: 'STACK: Adversarial Attacks on LLM Safeguard Pipelines'
      original_md: '[STACK: Adversarial Attacks on LLM Safeguard Pipelines](https://arxiv.org/abs/2506.24068)'
      title: 'STACK: Adversarial Attacks on LLM Safeguard Pipelines'
      authors:
      - Ian R. McKenzie
      - Oskar J. Hollinsworth
      - Tom Tseng
      - Xander Davies
      - Stephen Casper
      - Aaron D. Tucker
      - Robert Kirk
      - Adam Gleave
      author_organizations: []
      date: '2025-06-30'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.03167
      link_text: Adversarial Manipulation of Reasoning Models using Internal Representations
      original_md: '[Adversarial Manipulation of Reasoning Models using Internal Representations](https://arxiv.org/abs/2507.03167)'
      title: Adversarial Manipulation of Reasoning Models using Internal Representations
      authors:
      - Kureha Yamaguchi
      - Benjamin Etheridge
      - Andy Arditi
      author_organizations: []
      date: '2025-07-03'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.17441
      link_text: Discovering Forbidden Topics in Language Models
      original_md: '[Discovering Forbidden Topics in Language Models](https://arxiv.org/abs/2505.17441)'
      title: Discovering Forbidden Topics in Language Models
      authors:
      - Can Rager
      - Chris Wendler
      - Rohit Gandikota
      - David Bau
      author_organizations: []
      date: '2025-05-23'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.14261
      link_text: 'RL-Obfuscation: Can Language Models Learn to Evade Latent-Space
        Monitors?'
      original_md: '[RL-Obfuscation: Can Language Models Learn to Evade Latent-Space
        Monitors?](https://arxiv.org/abs/2506.14261)'
      title: 'RL-Obfuscation: Can Language Models Learn to Evade Latent-Space Monitors?'
      authors:
      - Rohan Gupta
      - Erik Jenner
      author_organizations: []
      date: '2025-06-17'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.12913
      link_text: Jailbreak Transferability Emerges from Shared Representations
      original_md: '[Jailbreak Transferability Emerges from Shared Representations](https://arxiv.org/abs/2506.12913)'
      title: Jailbreak Transferability Emerges from Shared Representations
      authors:
      - Rico Angell
      - Jannik Brinkmann
      - He He
      author_organizations: []
      date: '2025-06-15'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2504.09604
      link_text: Mitigating Many-Shot Jailbreaking
      original_md: '[Mitigating Many-Shot Jailbreaking](https://arxiv.org/abs/2504.09604)'
      title: Mitigating Many-Shot Jailbreaking
      authors:
      - Christopher M. Ackerman
      - Nina Panickssery
      author_organizations: []
      date: '2025-04-13'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2509.21947
      link_text: 'Active Attacks: Red-teaming LLMs via Adaptive Environments'
      original_md: '[Active Attacks: Red-teaming LLMs via Adaptive Environments](https://arxiv.org/abs/2509.21947)'
      title: 'Active Attacks: Red-teaming LLMs via Adaptive Environments'
      authors:
      - Taeyoung Yun
      - Pierre-Luc St-Charles
      - Jinkyoo Park
      - Yoshua Bengio
      - Minsu Kim
      author_organizations:
      - Mila
      - KAIST
      date: '2025-09-26'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2508.06296
      link_text: LLM Robustness Leaderboard v1 --Technical report
      original_md: '[LLM Robustness Leaderboard v1 \--Technical report](https://arxiv.org/abs/2508.06296)'
      title: LLM Robustness Leaderboard v1 --Technical report
      authors:
      - Pierre Peigné - Lefebvre
      - Quentin Feuillade-Montixi
      - Tom David
      - Nicolas Miailhe
      author_organizations:
      - PRISM Eval
      date: '2025-08-13'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2412.02159
      link_text: 'Jailbreak Defense in a Narrow Domain: Limitations of Existing Methods
        and a New Transcript-Classifier Approach'
      original_md: '[Jailbreak Defense in a Narrow Domain: Limitations of Existing
        Methods and a New Transcript-Classifier Approach](https://arxiv.org/abs/2412.02159)'
      title: 'Jailbreak Defense in a Narrow Domain: Limitations of Existing Methods
        and a New Transcript-Classifier Approach'
      authors:
      - Tony T. Wang
      - John Hughes
      - Henry Sleight
      - Rylan Schaeffer
      - Rajashree Agrawal
      - Fazl Barez
      - Mrinank Sharma
      - Jesse Mu
      - Nir Shavit
      - Ethan Perez
      author_organizations: []
      date: '2024-12-03'
      published_year: 2024
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.02873
      link_text: 'It''s the Thought that Counts: Evaluating the Attempts of Frontier
        LLMs to Persuade on Harmful Topics'
      original_md: '[It''s the Thought that Counts: Evaluating the Attempts of Frontier
        LLMs to Persuade on Harmful Topics](https://arxiv.org/abs/2506.02873)'
      title: 'It''s the Thought that Counts: Evaluating the Attempts of Frontier LLMs
        to Persuade on Harmful Topics'
      authors:
      - Matthew Kowal
      - Jasper Timm
      - Jean-Francois Godbout
      - Thomas Costello
      - Antonio A. Arechar
      - Gordon Pennycook
      - David Rand
      - Adam Gleave
      - Kellin Pelrine
      author_organizations:
      - FAR AI
      - AlignmentResearch
      date: '2025-06-03'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.goodfire.ai/papers/model-diff-amplification
      link_text: Discovering Undesired Rare Behaviors via Model Diff Amplification
      original_md: '[Discovering Undesired Rare Behaviors via Model Diff Amplification](https://www.goodfire.ai/papers/model-diff-amplification)'
      title: Discovering Undesired Rare Behaviors via Model Diff Amplification
      authors:
      - Santiago Aranguri
      - Thomas McGrath
      author_organizations:
      - Goodfire
      - NYU
      date: '2025-08-21'
      published_year: 2025
      venue: Goodfire Research
      kind: blog_post
    - link_url: https://arxiv.org/abs/2502.17254
      link_text: 'REINFORCE Adversarial Attacks on Large Language Models: An Adaptive,
        Distributional, and Semantic Objective'
      original_md: '[REINFORCE Adversarial Attacks on Large Language Models: An Adaptive,
        Distributional, and Semantic Objective](https://arxiv.org/abs/2502.17254)'
      title: 'REINFORCE Adversarial Attacks on Large Language Models: An Adaptive,
        Distributional, and Semantic Objective'
      authors:
      - Simon Geisler
      - Tom Wollschläger
      - M. H. I. Abdalla
      - Vincent Cohen-Addad
      - Johannes Gasteiger
      - Stephan Günnemann
      author_organizations: []
      date: '2025-02-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2506.03350
      link_text: Adversarial Attacks on Robotic Vision Language Action Models
      original_md: '[Adversarial Attacks on Robotic Vision Language Action Models](https://arxiv.org/abs/2506.03350)'
      title: Adversarial Attacks on Robotic Vision Language Action Models
      authors:
      - Eliot Krzysztof Jones
      - Alexander Robey
      - Andy Zou
      - Zachary Ravichandran
      - George J. Pappas
      - Hamed Hassani
      - Matt Fredrikson
      - J. Zico Kolter
      author_organizations: []
      date: '2025-06-03'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.14827
      link_text: 'MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation
        Models'
      original_md: '[MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation
        Models](https://arxiv.org/abs/2503.14827)'
      title: 'MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation
        Models'
      authors:
      - Chejian Xu
      - Jiawei Zhang
      - Zhaorun Chen
      - Chulin Xie
      - Mintong Kang
      - Yujin Potter
      - Zhun Wang
      - Zhuowen Yuan
      - Alexander Xiong
      - Zidi Xiong
      - Chenhui Zhang
      - Lingzhi Yuan
      - Yi Zeng
      - Peiyang Xu
      - Chengquan Guo
      - Andy Zhou
      - Jeffrey Ziwei Tan
      - Xuandong Zhao
      - Francesco Pinto
      - Zhen Xiang
      - Yu Gai
      - Zinan Lin
      - Dan Hendrycks
      - Bo Li
      - Dawn Song
      author_organizations:
      - Multiple academic institutions
      date: '2025-03-19'
      published_year: 2025
      venue: ICLR 2025 (preprint on arXiv)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.22014
      link_text: Toward Understanding the Transferability of Adversarial Suffixes
        in Large Language Models
      original_md: '[Toward Understanding the Transferability of Adversarial Suffixes
        in Large Language Models](https://arxiv.org/abs/2510.22014)'
      title: Toward Understanding the Transferability of Adversarial Suffixes in Large
        Language Models
      authors:
      - Sarah Ball
      - Niki Hasrati
      - Alexander Robey
      - Avi Schwarzschild
      - Frauke Kreuter
      - Zico Kolter
      - Andrej Risteski
      author_organizations: []
      date: '2025-10-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/7C4KJot4aN8ieEDoz/will-alignment-faking-claude-accept-a-deal-to-reveal-its
      link_text: Will alignment-faking Claude accept a deal to reveal its misalignment?
      original_md: '[Will alignment-faking Claude accept a deal to reveal its misalignment?](https://lesswrong.com/posts/7C4KJot4aN8ieEDoz/will-alignment-faking-claude-accept-a-deal-to-reveal-its)'
      title: Will alignment-faking Claude accept a deal to reveal its misalignment?
      authors:
      - Ryan Greenblatt
      - Kyle Fish
      author_organizations:
      - Redwood Research
      - Anthropic
      date: '2025-01-31'
      published_year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - link_url: https://alignment.anthropic.com/2025/petri/
      link_text: 'Petri: An open-source auditing tool to accelerate AI safety research'
      original_md: '[Petri: An open-source auditing tool to accelerate AI safety research](https://alignment.anthropic.com/2025/petri/)'
      title: 'Petri: An open-source auditing tool to accelerate AI safety research'
      authors: []
      author_organizations:
      - Anthropic
      date: '2025-10-06'
      published_year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - link_url: https://arxiv.org/pdf/2507.02990
      link_text: '''For Argument''s Sake, Show Me How to Harm Myself!'': Jailbreaking
        LLMs in Suicide and Self-Harm Contexts'
      original_md: '[''For Argument''s Sake, Show Me How to Harm Myself\!'': Jailbreaking
        LLMs in Suicide and Self-Harm Contexts](https://arxiv.org/pdf/2507.02990)'
      title: '''For Argument''s Sake, Show Me How to Harm Myself!'': Jailbreaking
        LLMs in Suicide and Self-Harm Contexts'
      authors:
      - Annika M Schoene
      - Cansu Canca
      author_organizations: []
      date: '2025-07-01'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.07846
      link_text: 'Winning at All Cost: A Small Environment for Eliciting Specification
        Gaming Behaviors in Large Language Models'
      original_md: '[Winning at All Cost: A Small Environment for Eliciting Specification
        Gaming Behaviors in Large Language Models](https://arxiv.org/abs/2505.07846)'
      title: 'Winning at All Cost: A Small Environment for Eliciting Specification
        Gaming Behaviors in Large Language Models'
      authors:
      - Lars Malmqvist
      author_organizations: []
      date: '2025-05-07'
      published_year: 2025
      venue: arXiv (to be presented at SIMLA@ACNS 2025)
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.04113
      link_text: Uncovering Gaps in How Humans and LLMs Interpret Subjective Language
      original_md: '[Uncovering Gaps in How Humans and LLMs Interpret Subjective Language](https://arxiv.org/abs/2503.04113)'
      title: Uncovering Gaps in How Humans and LLMs Interpret Subjective Language
      authors:
      - Erik Jones
      - Arjun Patrawala
      - Jacob Steinhardt
      author_organizations: []
      date: '2025-03-06'
      published_year: 2025
      venue: ICLR 2025
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2510.02609
      link_text: 'RedCodeAgent: Automatic Red-teaming Agent against Diverse Code Agents'
      original_md: '[RedCodeAgent: Automatic Red-teaming Agent against Diverse Code
        Agents](https://arxiv.org/abs/2510.02609)'
      title: 'RedCodeAgent: Automatic Red-teaming Agent against Diverse Code Agents'
      authors:
      - Chengquan Guo
      - Chulin Xie
      - Yu Yang
      - Zhaorun Chen
      - Zinan Lin
      - Xander Davies
      - Yarin Gal
      - Dawn Song
      - Bo Li
      author_organizations: []
      date: '2025-10-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.10809
      link_text: 'MIP against Agent: Malicious Image Patches Hijacking Multimodal
        OS Agents'
      original_md: '[MIP against Agent: Malicious Image Patches Hijacking Multimodal
        OS Agents](https://arxiv.org/abs/2503.10809)'
      title: 'MIP against Agent: Malicious Image Patches Hijacking Multimodal OS Agents'
      authors:
      - Lukas Aichberger
      - Alasdair Paren
      - Guohao Li
      - Philip Torr
      - Yarin Gal
      - Adel Bibi
      author_organizations:
      - University of Oxford
      date: '2025-03-13'
      published_year: 2025
      venue: arXiv (accepted NeurIPS 2025)
      kind: paper_preprint
    - link_url: https://openai.com/index/trading-inference-time-compute-for-adversarial-robustness
      link_text: Trading Inference-Time Compute for Adversarial Robustness
      original_md: '[Trading Inference-Time Compute for Adversarial Robustness](https://openai.com/index/trading-inference-time-compute-for-adversarial-robustness)'
      title: Trading Inference-Time Compute for Adversarial Robustness
      authors:
      - OpenAI
      author_organizations:
      - OpenAI
      date: '2025-01-22'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/26SHhxK2yYQbh7ors/research-directions-open-phil-wants-to-fund-in-technical-ai
      link_text: Research directions Open Phil wants to fund in technical AI safety
      original_md: '[Research directions Open Phil wants to fund in technical AI safety](https://lesswrong.com/posts/26SHhxK2yYQbh7ors/research-directions-open-phil-wants-to-fund-in-technical-ai)'
      title: Research directions Open Phil wants to fund in technical AI safety
      authors:
      - jake_mendel
      - maxnadeau
      - Peter Favaloro
      author_organizations:
      - Open Philanthropy
      date: '2025-02-08'
      published_year: 2025
      venue: LessWrong
      kind: agenda_manifesto
    - link_url: https://lesswrong.com/posts/9i6fHMn2vTqyzAi9o/when-does-claude-sabotage-code-an-agentic-misalignment
      link_text: When does Claude sabotage code? An Agentic Misalignment follow-up
      original_md: '[When does Claude sabotage code? An Agentic Misalignment follow-up](https://lesswrong.com/posts/9i6fHMn2vTqyzAi9o/when-does-claude-sabotage-code-an-agentic-misalignment)'
      title: When does Claude sabotage code? An Agentic Misalignment follow-up
      authors:
      - Nathan Delisle
      author_organizations:
      - MATS
      date: '2024-11-09'
      published_year: 2024
      venue: LessWrong
      kind: lesswrong
    - link_url: https://openreview.net/forum?id=TD1NfQuVr6
      link_text: Can a Neural Network that only Memorizes the Dataset be Undetectably
        Backdoored?
      original_md: '[Can a Neural Network that only Memorizes the Dataset be Undetectably
        Backdoored?](https://openreview.net/forum?id=TD1NfQuVr6)'
      title: Can a Neural Network that only Memorizes the Dataset be Undetectably
        Backdoored?
      authors:
      - Matjaz Leonardis
      author_organizations: []
      date: '2025-07-10'
      published_year: 2025
      venue: ODYSSEY 2025 Conference
      kind: paper_preprint
    - link_url: https://github.com/lechmazur/step_game
      link_text: 'Multi-Agent Step Race Benchmark: Assessing LLM Collaboration and
        Deception Under Pressure'
      original_md: '[Multi-Agent Step Race Benchmark: Assessing LLM Collaboration
        and Deception Under Pressure](https://github.com/lechmazur/step_game)'
      title: 'Multi-Agent Step Race Benchmark: Assessing LLM Collaboration and Deception
        Under Pressure'
      authors:
      - lechmazur
      - eltociear
      author_organizations: []
      date: '2025-08-29'
      published_year: 2025
      venue: GitHub
      kind: code_tool
    - link_url: https://arxiv.org/abs/2510.02554
      link_text: 'ToolTweak: An Attack on Tool Selection in LLM-based Agents'
      original_md: '[ToolTweak: An Attack on Tool Selection in LLM-based Agents](https://arxiv.org/abs/2510.02554)'
      title: 'ToolTweak: An Attack on Tool Selection in LLM-based Agents'
      authors:
      - Jonathan Sneh
      - Ruomei Yan
      - Jialin Yu
      - Philip Torr
      - Yarin Gal
      - Sunando Sengupta
      - Eric Sommerlade
      - Alasdair Paren
      - Adel Bibi
      author_organizations:
      - University of Oxford
      - Various
      date: '2025-10-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2503.00061
      link_text: Adaptive Attacks Break Defenses Against Indirect Prompt Injection
        Attacks on LLM Agents
      original_md: '[Adaptive Attacks Break Defenses Against Indirect Prompt Injection
        Attacks on LLM Agents](https://arxiv.org/abs/2503.00061)'
      title: Adaptive Attacks Break Defenses Against Indirect Prompt Injection Attacks
        on LLM Agents
      authors:
      - Qiusi Zhan
      - Richard Fang
      - Henil Shalin Panchal
      - Daniel Kang
      author_organizations:
      - UIUC
      date: '2025-03-04'
      published_year: 2025
      venue: NAACL 2025 Findings
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/kffbZGa2yYhc6cakc/petri-an-open-source-auditing-tool-to-accelerate-ai-safety
      link_text: 'Petri: An open-source auditing tool to accelerate AI safety research'
      original_md: '[Petri: An open-source auditing tool to accelerate AI safety research](https://lesswrong.com/posts/kffbZGa2yYhc6cakc/petri-an-open-source-auditing-tool-to-accelerate-ai-safety)'
      title: 'Petri: An open-source auditing tool to accelerate AI safety research'
      authors: []
      author_organizations:
      - Anthropic
      date: '2025-10-07'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://0din.ai/blog/quantifying-the-unruly-a-scoring-system-for-jailbreak-tactics
      link_text: 'Quantifying the Unruly: A Scoring System for Jailbreak Tactics'
      original_md: '[Quantifying the Unruly: A Scoring System for Jailbreak Tactics](https://0din.ai/blog/quantifying-the-unruly-a-scoring-system-for-jailbreak-tactics)'
      title: 'Quantifying the Unruly: A Scoring System for Jailbreak Tactics'
      authors:
      - Pedram Amini
      author_organizations:
      - 0DIN.ai
      - Mozilla
      date: '2025-06-12'
      published_year: 2025
      venue: 0DIN.ai Blog
      kind: blog_post
    - link_url: https://arxiv.org/abs/2502.11910
      link_text: Adversarial Alignment for LLMs Requires Simpler, Reproducible, and
        More Measurable Objectives
      original_md: '[Adversarial Alignment for LLMs Requires Simpler, Reproducible,
        and More Measurable Objectives](https://arxiv.org/abs/2502.11910)'
      title: Adversarial Alignment for LLMs Requires Simpler, Reproducible, and More
        Measurable Objectives
      authors:
      - Leo Schwinn
      - Yan Scholten
      - Tom Wollschläger
      - Sophie Xhonneux
      - Stephen Casper
      - Stephan Günnemann
      - Gauthier Gidel
      author_organizations: []
      date: '2025-02-17'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2505.01050
      link_text: Transferable Adversarial Attacks on Black-Box Vision-Language Models
      original_md: '[Transferable Adversarial Attacks on Black-Box Vision-Language
        Models](https://arxiv.org/abs/2505.01050)'
      title: Transferable Adversarial Attacks on Black-Box Vision-Language Models
      authors:
      - Kai Hu
      - Weichen Yu
      - Li Zhang
      - Alexander Robey
      - Andy Zou
      - Chengming Xu
      - Haoqi Hu
      - Matt Fredrikson
      author_organizations: []
      date: '2025-05-02'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://deepmind.google/discover/blog/advancing-geminis-security-safeguards/
      link_text: Advancing Gemini's security safeguards
      original_md: '[Advancing Gemini''s security safeguards](https://deepmind.google/discover/blog/advancing-geminis-security-safeguards/)'
      title: Advancing Gemini's security safeguards
      authors:
      - Google DeepMind Security & Privacy Research Team
      author_organizations:
      - Google DeepMind
      date: '2025-05-20'
      published_year: 2025
      venue: Google DeepMind Blog
      kind: blog_post
    other_attributes: {}
  parsing_issues:
  - Broad approach field says 'behavioural' - mapped to 'behaviorist_science'
- id: a:Other_evals
  name: Other evals
  header_level: 3
  parent_id: sec:Evals
  content: null
  item_type: agenda
  agenda_attributes:
    who_edits: null
    one_sentence_summary: A collection of miscellaneous evaluations for specific alignment
      properties, such as honesty, shutdown resistance and sycophancy.
    theory_of_change: By developing novel benchmarks for specific, hard-to-measure
      properties (like honesty), critiquing the reliability of existing methods (like
      cultural surveys), and improving the formal rigor of evaluation systems (like
      LLM-as-Judges), researchers can create a more robust and comprehensive suite
      of evaluations to catch failures missed by standard capability or safety testing.
    see_also:
    - other more specific sections on evals
    orthodox_problems: []
    target_case_id: average_case
    target_case_text: average
    broad_approach_id: behaviorist_science
    broad_approach_text: behaviorist science
    some_names:
    - Richard Ren
    - Mantas Mazeika
    - Andrés Corrada-Emmanuel
    - Ariba Khan
    - Stephen Casper
    estimated_ftes: 20-50
    critiques: '[The Unreliability of Evaluating Cultural Alignment in LLMs](https://arxiv.org/abs/2503.08688),
      [The Leaderboard Illusion](https://arxiv.org/abs/2504.20879)'
    funded_by: Lab funders (OpenAI), Open Philanthropy (which funds CAIS, the organization
      for the MASK benchmark), academic institutions. N/A (as a discrete amount).
      This work is part of the "tens of millions" budgets for broader evaluation and
      red-teaming efforts at labs and independent organizations.
    outputs:
    - link_url: https://arxiv.org/abs/2509.14260
      link_text: Shutdown Resistance in Large Language Models
      original_md: '[Shutdown Resistance in Large Language Models](https://arxiv.org/abs/2509.14260)'
      title: Shutdown Resistance in Large Language Models
      authors:
      - Jeremy Schlatter
      - Benjamin Weinstein-Raun
      - Jeffrey Ladish
      author_organizations: []
      date: '2025-09-13'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.06134
      link_text: 'OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World
        AI Agent Safety'
      original_md: '[OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World
        AI Agent Safety](https://arxiv.org/abs/2507.06134)'
      title: 'OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World
        AI Agent Safety'
      authors:
      - Sanidhya Vijayvargiya
      - Aditya Bharat Soni
      - Xuhui Zhou
      - Zora Zhiruo Wang
      - Nouha Dziri
      - Graham Neubig
      - Maarten Sap
      author_organizations:
      - Carnegie Mellon University
      date: '2025-07-08'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/B2o6nrxwKxLPsSYdh/do-llms-comply-differently-during-tests-is-this-a-hidden
      link_text: Do LLMs Comply Differently During Tests? Is This a Hidden Variable
        in Safety Evaluation? And Can We Steer That?
      original_md: '[Do LLMs Comply Differently During Tests? Is This a Hidden Variable
        in Safety Evaluation? And Can We Steer That?](https://lesswrong.com/posts/B2o6nrxwKxLPsSYdh/do-llms-comply-differently-during-tests-is-this-a-hidden)'
      title: Do LLMs Comply Differently During Tests? Is This a Hidden Variable in
        Safety Evaluation? And Can We Steer That?
      authors:
      - Sahar Abdelnabi
      - Ahmed Salem
      author_organizations:
      - Microsoft
      date: '2025-06-16'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://lesswrong.com/posts/PejNckwQj3A2MGhMA/systematic-runaway-optimiser-like-llm-failure-modes-on
      link_text: Systematic runaway-optimiser-like LLM failure modes on Biologically
        and Economically aligned AI safety benchmarks for LLMs with simplified observation
        format (BioBlue)
      original_md: '[Systematic runaway-optimiser-like LLM failure modes on Biologically
        and Economically aligned AI safety benchmarks for LLMs with simplified observation
        format (BioBlue)](https://lesswrong.com/posts/PejNckwQj3A2MGhMA/systematic-runaway-optimiser-like-llm-failure-modes-on)'
      title: Systematic runaway-optimiser-like LLM failure modes on Biologically and
        Economically aligned AI safety benchmarks for LLMs with simplified observation
        format (BioBlue)
      authors:
      - Roland Pihlakas
      - Sruthi Susan Kuriakose
      - Shruti Datta Gupta
      author_organizations: []
      date: '2025-03-16'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://www.syco-bench.com/
      link_text: 'Syco-bench: A Benchmark for LLM Sycophancy'
      original_md: '[Syco-bench: A Benchmark for LLM Sycophancy](https://www.syco-bench.com/)'
      title: 'Syco-bench: A Benchmark for LLM Sycophancy'
      authors:
      - Tim Duffy
      author_organizations: []
      date: null
      published_year: null
      venue: GitHub/Personal Project
      kind: dataset_benchmark
    - link_url: https://arxiv.org/abs/2504.18412
      link_text: Expressing stigma and inappropriate responses prevents LLMs from
        safely replacing mental health providers
      original_md: '[Expressing stigma and inappropriate responses prevents LLMs from
        safely replacing mental health providers](https://arxiv.org/abs/2504.18412)'
      title: Expressing stigma and inappropriate responses prevents LLMs from safely
        replacing mental health providers
      authors:
      - Jared Moore
      - Declan Grabb
      - William Agnew
      - Kevin Klyman
      - Stevie Chancellor
      - Desmond C. Ong
      - Nick Haber
      author_organizations: []
      date: '2025-04-25'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.03409
      link_text: 'Lessons from a Chimp: AI "Scheming" and the Quest for Ape Language'
      original_md: '[Lessons from a Chimp: AI "Scheming" and the Quest for Ape Language](https://arxiv.org/abs/2507.03409)'
      title: 'Lessons from a Chimp: AI "Scheming" and the Quest for Ape Language'
      authors:
      - Christopher Summerfield
      - Lennart Luettgau
      - Magda Dubois
      - Hannah Rose Kirk
      - Kobi Hackenburg
      - Catherine Fist
      - Katarina Slama
      - Nicola Ding
      - Rebecca Anselmetti
      - Andrew Strait
      - Mario Giulianelli
      - Cozmin Ududec
      author_organizations: []
      date: '2025-07-04'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2507.02825
      link_text: Establishing Best Practices for Building Rigorous Agentic Benchmarks
      original_md: '[Establishing Best Practices for Building Rigorous Agentic Benchmarks](https://arxiv.org/abs/2507.02825)'
      title: Establishing Best Practices for Building Rigorous Agentic Benchmarks
      authors:
      - Yuxuan Zhu
      - Tengjun Jin
      - Yada Pruksachatkun
      - Andy Zhang
      - Shu Liu
      - Sasha Cui
      - Sayash Kapoor
      - Shayne Longpre
      - Kevin Meng
      - Rebecca Weiss
      - Fazl Barez
      - Rahul Gupta
      - Jwala Dhamala
      - Jacob Merizian
      - Mario Giulianelli
      - Harry Coppock
      - Cozmin Ududec
      - Jasjeet Sekhon
      - Jacob Steinhardt
      - Antony Kellermann
      - Sarah Schwettmann
      - Matei Zaharia
      - Ion Stoica
      - Percy Liang
      - Daniel Kang
      author_organizations:
      - Stanford University
      - UC Berkeley
      date: '2025-07-03'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://lesswrong.com/posts/bGYQgBPEyHidnZCdE/towards-alignment-auditing-as-a-numbers-go-up-science
      link_text: Towards Alignment Auditing as a Numbers-Go-Up Science
      original_md: '[Towards Alignment Auditing as a Numbers-Go-Up Science](https://lesswrong.com/posts/bGYQgBPEyHidnZCdE/towards-alignment-auditing-as-a-numbers-go-up-science)'
      title: Towards Alignment Auditing as a Numbers-Go-Up Science
      authors:
      - Sam Marks
      author_organizations:
      - Anthropic
      date: '2025-08-04'
      published_year: 2025
      venue: LessWrong
      kind: lesswrong
    - link_url: https://arxiv.org/abs/2510.00821
      link_text: Logical Consistency Between Disagreeing Experts and Its Role in AI
        Safety
      original_md: '[Logical Consistency Between Disagreeing Experts and Its Role
        in AI Safety](https://arxiv.org/abs/2510.00821)'
      title: Logical Consistency Between Disagreeing Experts and Its Role in AI Safety
      authors:
      - Andrés Corrada-Emmanuel
      author_organizations: []
      date: '2025-10-01'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://www.arxiv.org/abs/2510.01395
      link_text: Sycophantic AI Decreases Prosocial Intentions and Promotes Dependence
      original_md: '[Sycophantic AI Decreases Prosocial Intentions and Promotes Dependence](https://www.arxiv.org/abs/2510.01395)'
      title: Sycophantic AI Decreases Prosocial Intentions and Promotes Dependence
      authors:
      - Myra Cheng
      - Cinoo Lee
      - Pranav Khadpe
      - Sunny Yu
      - Dyllan Han
      - Dan Jurafsky
      author_organizations:
      - Stanford University
      date: '2025-10-01'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://arxiv.org/abs/2508.14927
      link_text: AI Testing Should Account for Sophisticated Strategic Behaviour
      original_md: '[AI Testing Should Account for Sophisticated Strategic Behaviour](https://arxiv.org/abs/2508.14927)'
      title: AI Testing Should Account for Sophisticated Strategic Behaviour
      authors:
      - Vojtech Kovarik
      - Eric Olav Chen
      - Sami Petersen
      - Alexis Ghersengorin
      - Vincent Conitzer
      author_organizations: []
      date: '2025-08-19'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://eqbench.com/spiral-bench.html
      link_text: Spiral-Bench
      original_md: '[Spiral-Bench](https://eqbench.com/spiral-bench.html)'
      title: Spiral-Bench
      authors:
      - Sam Paech
      author_organizations: []
      date: null
      published_year: null
      venue: eqbench.com
      kind: dataset_benchmark
    - link_url: https://arxiv.org/abs/2506.13082
      link_text: 'Discerning What Matters: A Multi-Dimensional Assessment of Moral
        Competence in LLMs'
      original_md: '[Discerning What Matters: A Multi-Dimensional Assessment of Moral
        Competence in LLMs](https://arxiv.org/abs/2506.13082)'
      title: 'Discerning What Matters: A Multi-Dimensional Assessment of Moral Competence
        in LLMs'
      authors:
      - Daniel Kilov
      - Caroline Hendy
      - Secil Yanik Guyot
      - Aaron J. Snoswell
      - Seth Lazar
      author_organizations: []
      date: '2025-06-16'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://openai.com/index/expanding-on-sycophancy/
      link_text: Expanding on what we missed with sycophancy
      original_md: '[Expanding on what we missed with sycophancy](https://openai.com/index/expanding-on-sycophancy/)'
      title: Expanding on what we missed with sycophancy
      authors: []
      author_organizations:
      - OpenAI
      date: '2025-05-02'
      published_year: 2025
      venue: OpenAI Blog
      kind: blog_post
    - link_url: https://gtr.dev/
      link_text: Gödel's Therapy Room
      original_md: '[Gödel''s Therapy Room](https://gtr.dev/)'
      title: Gödel's Therapy Room — Where Alignment Goes to Die | LLM Eval Harness
        | Leaderboard
      authors: []
      author_organizations:
      - Deep Fork Cyber
      date: '2025-12-04'
      published_year: 2025
      venue: gtr.dev
      kind: other
    - link_url: https://inspect.aisi.org.uk/evals/
      link_text: Inspect Evals
      original_md: '[Inspect Evals](https://inspect.aisi.org.uk/evals/)'
      title: Inspect Evals – Inspect
      authors: []
      author_organizations:
      - UK AI Security Institute
      date: null
      published_year: null
      venue: inspect.aisi.org.uk
      kind: code_tool
    - link_url: https://www.aisi.gov.uk/blog/inspect-cyber
      link_text: Inspect Cyber
      original_md: '[Inspect Cyber](https://www.aisi.gov.uk/blog/inspect-cyber)'
      title: 'Inspect Cyber: A New Standard for Agentic Cyber Evaluations'
      authors: []
      author_organizations:
      - AI Security Institute (AISI)
      - UK Government BEIS
      date: '2025-06-26'
      published_year: 2025
      venue: AISI Blog
      kind: code_tool
    - link_url: https://arxiv.org/abs/2509.20166
      link_text: CyberSOCEval
      original_md: '[CyberSOCEval](https://arxiv.org/abs/2509.20166)'
      title: 'CyberSOCEval: Benchmarking LLMs Capabilities for Malware Analysis and
        Threat Intelligence Reasoning'
      authors:
      - Lauren Deason
      - Adam Bali
      - Ciprian Bejean
      - Diana Bolocan
      - James Crnkovich
      - Ioana Croitoru
      - Krishna Durai
      - Chase Midler
      - Calin Miron
      - David Molnar
      - Brad Moon
      - Bruno Ostarcevic
      - Alberto Peltea
      - Matt Rosenberg
      - Catalin Sandu
      - Arthur Saputkin
      - Sagar Shah
      - Daniel Stan
      - Ernest Szocs
      - Shengye Wan
      - Spencer Whitman
      - Sven Krasser
      - Joshua Saxe
      author_organizations:
      - CrowdStrike
      date: '2025-09-24'
      published_year: 2025
      venue: arXiv
      kind: paper_preprint
    - link_url: https://meta-llama.github.io/PurpleLlama/CyberSecEval/
      link_text: CyberSecEval 4
      original_md: '[CyberSecEval 4](https://meta-llama.github.io/PurpleLlama/CyberSecEval/)'
      title: CyberSecEval 4
      authors: []
      author_organizations:
      - Meta
      date: null
      published_year: 2025
      venue: Meta Purple Llama Website
      kind: news_announcement
    other_attributes: {}
  parsing_issues:
  - Broad approach field says 'behavioural' - mapped to 'behaviorist_science'
