# GENERATED FILE - Do not edit manually
# Source: src/data/gavento/import/source.yaml
# Regenerate with: npm run convert-gavento
#
# This file is regenerated from the shallow-review pipeline output.
# Manual edits will be overwritten.

- id: OpenAI
  name: OpenAI
  parent: Labs
  seeAlso: sec:Iterative_alignment, a:Safeguards_inference_time_auxiliaries_, a:Character_training_and_persona_steering
  someNames:
    - johannes-heidecke
    - boaz-barak
    - mia-glaese
    - jenny-nitishinskaya
    - lama-ahmad
    - naomi-bashkansky
    - miles-wang
    - wojciech-zaremba
    - david-robinson
    - zico-kolter
    - jerry-tworek
    - eric-wallace
    - olivia-watkins
    - kai-chen
    - chris-koch
    - andrea-vallone
    - leo-gao
  critiques:
    - >-
      [Stein-Perlman](https://ailabwatch.org/companies/openai),
      [Stewart](https://intelligence.org/2025/03/31/a-response-to-openais-how-we-think-about-safety-and-alignment/),
      [underelicitation](https://www.lesswrong.com/posts/AK6AihHGjirdoiJg6/ai-companies-eval-reports-mostly-don-t-support-their-claims),
      [Midas](https://www.openaifiles.org/transparency-and-safety),
      [defense](https://www.wired.com/story/openai-anduril-defense/),
      [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).)
      on labs in general. It's [difficult](https://conversationswithtyler.com/episodes/sam-altman-2/) to model OpenAI as
      a single agent: *"ALTMAN: I very rarely get to have anybody work on anything. One thing about researchers is
      they're going to work on what they're going to work on, and that's that."*
  fundedByText: >-
    Microsoft, [AWS](https://www.aboutamazon.com/news/aws/aws-open-ai-workloads-compute-infrastructure), Oracle, NVIDIA,
    SoftBank, G42, AMD, Dragoneer, Coatue, Thrive, Altimeter, MGX, Blackstone, TPG, T. Rowe Price, Andreessen Horowitz,
    D1 Capital Partners, Fidelity Investments, Founders Fund, Sequoia…
  structure: public benefit corp
  teams: >-
    Alignment, Safety Systems (Interpretability, Safety Oversight, Pretraining Safety, Robustness, Safety Research,
    Trustworthy AI, new Misalignment Research team [coming](https://archive.is/eDB1D)), Preparedness, Model Policy,
    Safety and Security Committee, Safety Advisory Group. The [Persona Features](https://www.arxiv.org/pdf/2506.19823)
    paper had a distinct author list. No named successor to Superalignment.
  publicAlignmentAgenda: >-
    [None](https://openai.com/safety/how-we-think-about-safety-alignment/). Boaz Barak
    [offers](https://www.lesswrong.com/posts/3jnziqCF3vA2NXAKp/six-thoughts-on-ai-safety) personal
    [views](https://windowsontheory.org/2025/06/24/machines-of-faithful-obedience/).
  framework: >-
    [Preparedness
    Framework](https://cdn.openai.com/pdf/18a02b5d-6b67-4cec-ab64-68cdfbddebcd/preparedness-framework-v2.pdf)
  papers:
    - title: Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation
      url: https://arxiv.org/abs/2503.11926
      authors: >-
        Bowen Baker, Joost Huizinga, Leo Gao, Zehao Dou, Melody Y. Guan, Aleksander Madry, Wojciech Zaremba, Jakub
        Pachocki, David Farhi
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Persona Features Control Emergent Misalignment
      url: https://arxiv.org/abs/2506.19823
      authors: >-
        Miles Wang, Tom Dupré la Tour, Olivia Watkins, Alex Makelov, Ryan A. Chi, Samuel Miserendino, Jeffrey Wang,
        Achyuta Rajaram, Johannes Heidecke, Tejal Patwardhan, Dan Mossing
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Stress Testing Deliberative Alignment for Anti-Scheming Training
      url: https://arxiv.org/abs/2509.15541
      authors: >-
        Bronson Schoen, Evgenia Nitishinskaya, Mikita Balesni, Axel Højmark, Felix Hofstätter, Jérémy Scheurer,
        Alexander Meinke, Jason Wolfe, Teun van der Weij, Alex Lloyd, Nicholas Goldowsky-Dill, Angela Fan, Andrei
        Matveiakin, Rusheb Shah, Marcus Williams, Amelia Glaese, Boaz Barak, Wojciech Zaremba, Marius Hobbhahn
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Deliberative Alignment: Reasoning Enables Safer Language Models"
      url: https://arxiv.org/abs/2412.16339
      authors: >-
        Melody Y. Guan, Manas Joglekar, Eric Wallace, Saachi Jain, Boaz Barak, Alec Helyar, Rachel Dias, Andrea Vallone,
        Hongyu Ren, Jason Wei, Hyung Won Chung, Sam Toyer, Johannes Heidecke, Alex Beutel, Amelia Glaese
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: Toward understanding and preventing misalignment generalization
      url: https://openai.com/index/emergent-misalignment
      authors: >-
        Miles Wang, Tom Dupré la Tour, Olivia Watkins, Aleksandar Makelov, Ryan A. Chi, Samuel Miserendino, Tejal
        Patwardhan, Dan Mossing
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Our updated Preparedness Framework
      url: https://openai.com/index/updating-our-preparedness-framework/
      authors: OpenAI Preparedness Team
      year: 2025
      venue: OpenAI Blog
      kind: agenda_manifesto
    - title: Trading Inference-Time Compute for Adversarial Robustness
      url: https://arxiv.org/abs/2501.18841
      authors: >-
        Wojciech Zaremba, Evgenia Nitishinskaya, Boaz Barak, Stephanie Lin, Sam Toyer, Yaodong Yu, Rachel Dias, Eric
        Wallace, Kai Xiao, Johannes Heidecke, Amelia Glaese
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Small-to-Large Generalization: Data Influences Models Consistently Across Scale"
      url: https://arxiv.org/abs/2505.16260
      authors: Alaa Khaddaj, Logan Engstrom, Aleksander Madry
      year: 2025
      venue: ICLR 2025
      kind: paper_published
    - title: "Findings from a pilot Anthropic–OpenAI alignment evaluation exercise: OpenAI Safety Tests"
      url: https://openai.com/index/openai-anthropic-safety-evaluation
      year: 2025
      venue: OpenAI Blog
      kind: news_announcement
    - title: Safety evaluations hub
      url: https://openai.com/safety/evaluations-hub
      year: 2025
      venue: OpenAI Website
      kind: news_announcement
    - title: Alignment Research Blog
      url: https://alignment.openai.com/
      year: 2025
      venue: OpenAI Alignment Blog
      kind: blog_post
    - title: Weight-sparse transformers have interpretable circuits
      url: https://cdn.openai.com/pdf/41df8f28-d4ef-43e9-aed2-823f9393e470/circuit-sparsity-paper.pdf
      kind: error_detected
- id: Google_Deepmind
  name: Google Deepmind
  parent: Labs
  seeAlso: sec:White_box_safety, Scalable Oversight
  someNames:
    - rohin-shah
    - allan-dafoe
    - anca-dragan
    - alex-irpan
    - alex-turner
    - anna-wang
    - arthur-conmy
    - david-lindner
    - jonah-brown-cohen
    - lewis-ho
    - neel-nanda
    - raluca-ada-popa
    - rishub-jain
    - rory-greig
    - sebastian-farquhar
    - senthooran-rajamanoharan
    - sophie-bridgers
    - tobi-ijitoye
    - tom-everitt
    - victoria-krakovna
    - vikrant-varma
    - zac-kenton
    - four-flynn
    - jonathan-richens
    - lewis-smith
    - janos-kramar
    - matthew-rahtz
    - mary-phuong
    - erik-jenner
  critiques:
    - >-
      [Stein-Perlman](https://ailabwatch.org/companies/deepmind),
      [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).)
      on labs in general,
      [underelicitation](https://www.lesswrong.com/posts/AK6AihHGjirdoiJg6/ai-companies-eval-reports-mostly-don-t-support-their-claims),
      [On Google's Safety Plan](https://lesswrong.com/posts/hvEikwtsbf6zaXG2s/on-google-s-safety-plan)
  fundedByText: >-
    Google. Explicit 2024 Deepmind spending as a whole was
    [£1.3B](https://s3.eu-west-2.amazonaws.com/document-api-images-live.ch.gov.uk/docs/WT_VNJe9leRjfcU0-OtRjWqF7WiqueStclXgHPbdG4U/application-pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAWRGBDBV3HTI6EAXB%2F20251212%2Feu-west-2%2Fs3%2Faws4_request&X-Amz-Date=20251212T104902Z&X-Amz-Expires=60&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEDcaCWV1LXdlc3QtMiJGMEQCIH6GyZRz66qwZsWkZORhsAQyzQQoJ7j0F4jnnWsgT8i2AiBkQVTWkLiSt%2F89o2yMC5D9NGQ75b1RwC8MBr7dlvrJXyqBBAgAEAUaDDQ0OTIyOTAzMjgyMiIMDkbkT%2FH2vUMV3NxOKt4DfZ6bw%2BWE%2BPfifW4goryaR4bQ%2FeFEXDvW7MU%2BKlfUM8A2fNyEUpIq4f6PsRf0zntVIXmUOWnvyIcVB7EA31NcPn3O%2FHFga8gKZyDPnQnj7YM5Wrt%2FVvR2mj7dJcioOSATW6joYuAb2X0l6IVHXJnYcaxStVCaPauK98OWTTXwCQQwG9UYBWe5SGqOroOw%2FoYWx9GRGvDtQfQThGemJnDr%2FHkbM9YH%2BY860lrE4MEXQiPakkwgJZC%2B8kqsqxzAIyWegPjp3TvrNs7WJ4Fheq0BJo8B7uw0pYBB%2BE9WQEjgaO5dByd90cpnyHu%2F8HGSxwmuQQiUtrp0T3xpP1G%2B3bP%2FLUnhGTD6XWLW%2BtoywQ5ZJrizfwuLQuxFjZt2JwV50DslF47H4AltBRxQh6HHro%2BpiJJEv0rC5NKBS4XRaL8FWOFMD%2BxJctPoCxFJhour3SbcMET4148eVQL%2FenkSdPUz2FHNrO%2BnOTyZAG%2Bi9xiZR1MVOCYHTPHKFG9ReY4ck2mz4W94%2FI6iWuu%2BKWlrEr2hEWzo2RhwDJ09ASgoKNErYb2mJ4E0rMGQ7cv8d2bqF7f6ok1SbzJPClaCBN4qYBzX1rE2Uhdf4v2QueSi4c0i8oCWOGfsdp5FxpgrOlEIqzC9%2Fu7JBjqmAardqlTk%2BobAEzv0H0m2RO4m901C%2FsTzIKb2UlMRrUkTDH4MpCSg5eW3A86X2TnPfl66jC%2FV2P%2FIwY%2FkvsY7wNBgtYR92XE%2FMwyz1x3JD1qDnGWPybjso72aEPrMyekV2WV3U0%2BYh8zn83%2BneYZB9VaTu2QqSv7TZe3IWJyErbuZw%2BhmMlk5nhKZDNmo%2Fc12x%2B7jI0N6aKqUdp8BkGOqPrlUxn2mKcg%3D&X-Amz-SignedHeaders=host&response-content-disposition=inline%3Bfilename%3D%22companies_house_document.pdf%22&X-Amz-Signature=52be18d98d9589fa46d3686876b3107925b67ee083d05199e1428dfc14b9c457),
    but this doesn't count most spending e.g. Gemini compute.
  structure: research laboratory subsidiary of a for-profit
  teams: >-
    amplified oversight, interpretability, ASAT eng (automated alignment research), Causal Incentives Working Group,
    Frontier Safety Risk Assessment (evals, threat models, the framework), Mitigations (e.g. banning accounts, refusal
    training, jailbreak robustness), Loss of Control (control, alignment evals). Structure
    [here](https://gist.github.com/g-leech/30f2484d0318b5d9d489e5748fe46131).
  publicAlignmentAgenda: "[An Approach to Technical AGI Safety and Security](https://arxiv.org/abs/2504.01849)"
  framework: "[Frontier Safety Framework](https://deepmind.google/blog/strengthening-our-frontier-safety-framework/)"
  papers:
    - title: A Pragmatic Vision for Interpretability
      url: https://www.alignmentforum.org/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-interpretability%20
      authors: >-
        Neel Nanda, Josh Engels, Arthur Conmy, Senthooran Rajamanoharan, bilalchughtai, CallumMcDougall, János Kramár,
        lewis smith
      year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - title: How Can Interpretability Researchers Help AGI Go Well?
      url: https://www.alignmentforum.org/posts/MnkeepcGirnJn736j/how-can-interpretability-researchers-help-agi-go-well%20
      authors: >-
        Neel Nanda, Josh Engels, Senthooran Rajamanoharan, Arthur Conmy, bilalchughtai, CallumMcDougall, János Kramár,
        lewis smith
      year: 2024
      venue: AI Alignment Forum
      kind: lesswrong
    - title: Evaluating Frontier Models for Stealth and Situational Awareness
      url: https://arxiv.org/abs/2505.01420
      authors: >-
        Mary Phuong, Roland S. Zimmermann, Ziyue Wang, David Lindner, Victoria Krakovna, Sarah Cogan, Allan Dafoe, Lewis
        Ho, Rohin Shah
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors
      url: https://arxiv.org/abs/2507.05246
      authors: >-
        Scott Emmons, Erik Jenner, David K. Elson, Rif A. Saurous, Senthooran Rajamanoharan, Heng Chen, Irhum Shafkat,
        Rohin Shah
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "MONA: Managed Myopia with Approval Feedback"
      url: https://alignmentforum.org/posts/zWySWKuXnhMDhgwc3/mona-managed-myopia-with-approval-feedback-2
      authors: Sebastian Farquhar, David Lindner, Rohin Shah
      year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - title: Consistency Training Helps Stop Sycophancy and Jailbreaks
      url: https://arxiv.org/abs/2510.27062
      authors: Alex Irpan, Alexander Matt Turner, Mark Kurzeja, David K. Elson, Rohin Shah
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: An Approach to Technical AGI Safety and Security
      url: https://arxiv.org/abs/2504.01849
      authors: >-
        Rohin Shah, Alex Irpan, Alexander Matt Turner, Anna Wang, Arthur Conmy, David Lindner, Jonah Brown-Cohen, Lewis
        Ho, Neel Nanda, Raluca Ada Popa, Rishub Jain, Rory Greig, Samuel Albanie, Scott Emmons, Sebastian Farquhar,
        Sébastien Krier, Senthooran Rajamanoharan, Sophie Bridgers, Tobi Ijitoye, Tom Everitt, Victoria Krakovna,
        Vikrant Varma, Vladimir Mikulik, Zachary Kenton, Dave Orr, Shane Legg, Noah Goodman, Allan Dafoe, Four Flynn,
        Anca Dragan
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: >-
        Negative Results for SAEs On Downstream Tasks and Deprioritising SAE Research (GDM Mech Interp Team Progress
        Update #2)
      url: https://alignmentforum.org/posts/4uXCAJNuPKtKBsi28/negative-results-for-saes-on-downstream-tasks
      authors: >-
        Lewis Smith, Senthooran Rajamanoharan, Arthur Conmy, Callum McDougall, Tom Lieberum, János Kramár, Rohin Shah,
        Neel Nanda
      year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - title: Steering Gemini Using BIDPO Vectors
      url: https://turntrout.com/gemini-steering
      authors: Alex Turner, Mark Kurzeja, Dave Orr, David Elson
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Difficulties with Evaluating a Deception Detector for AIs
      url: https://arxiv.org/html/2511.22662v1
      authors: Lewis Smith, Bilal Chughtai, Neel Nanda
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Taking a responsible path to AGI
      url: https://deepmind.google/discover/blog/taking-a-responsible-path-to-agi/
      authors: Anca Dragan, Rohin Shah, Four Flynn, Shane Legg
      year: 2025
      venue: Google DeepMind Blog
      kind: blog_post
    - title: Evaluating potential cybersecurity threats of advanced AI
      url: https://deepmind.google/discover/blog/evaluating-potential-cybersecurity-threats-of-advanced-ai
      authors: Four Flynn, Mikel Rodriguez, Raluca Ada Popa
      year: 2025
      venue: Google DeepMind Blog
      kind: blog_post
    - title: Self-preservation or Instruction Ambiguity? Examining the Causes of Shutdown Resistance
      url: https://www.alignmentforum.org/posts/wnzkjSmrgWZaBa2aC/self-preservation-or-instruction-ambiguity-examining-the
      authors: Senthooran Rajamanoharan, Neel Nanda
      year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - title: A Pragmatic Way to Measure Chain-of-Thought Monitorability
      url: https://arxiv.org/abs/2510.23966
      authors: Scott Emmons, Roland S. Zimmermann, David K. Elson, Rohin Shah
      year: 2025
      venue: arXiv
      kind: paper_preprint
- id: Anthropic
  name: Anthropic
  parent: Labs
  seeAlso: sec:White_box_safety, Scalable Oversight
  someNames:
    - chris-olah
    - evan-hubinger
    - sam-marks
    - johannes-treutlein
    - sam-bowman
    - euan-ong
    - fabien-roger
    - adam-jermyn
    - holden-karnofsky
    - jan-leike
    - ethan-perez
    - jack-lindsey
    - amanda-askell
    - kyle-fish
    - sara-price
    - jon-kutasov
    - minae-kwon
    - monty-evans
    - richard-dargan
    - roger-grosse
    - ben-levinstein
    - joseph-carlsmith
    - joe-benton
  critiques:
    - >-
      [Stein](https://ailabwatch.org/anthropic-opinions)[-Perlman](https://ailabwatch.org/companies/anthropic),
      [Casper](https://www.lesswrong.com/posts/pH6tyhEnngqWAXi9i/eis-xiii-reflections-on-anthropic-s-sae-research-circa-may#A_review___thoughts),
      [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%2Dpeople%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).),
      [underelicitation](https://www.lesswrong.com/posts/AK6AihHGjirdoiJg6/ai-companies-eval-reports-mostly-don-t-support-their-claims),
      [Greenblatt](https://nitter.net/RyanPGreenblatt/status/1925992236648464774),
      [Samin](https://www.lesswrong.com/posts/5aKRshJzhojqfbRyo/unless-its-governance-changes-anthropic-is-untrustworthy),
      [defense](https://techcrunch.com/2024/11/07/anthropic-teams-up-with-palantir-and-aws-to-sell-its-ai-to-defense-customers/),
      [Existing Safety Frameworks Imply Unreasonable
      Confidence](https://lesswrong.com/posts/7ExkgcDudwhag73vw/existing-safety-frameworks-imply-unreasonable-confidence)
  fundedByText: >-
    Amazon, Google, ICONIQ, Fidelity, Lightspeed, Altimeter, Baillie Gifford, BlackRock, Blackstone, Coatue, D1 Capital
    Partners, General Atlantic, General Catalyst, GIC, Goldman Sachs, Insight Partners, Jane Street, Ontario Teachers'
    Pension Plan, Qatar Investment Authority, TPG, T. Rowe Price, WCM, XN
  structure: public-benefit corp
  teams: >-
    Scalable Alignment (Leike), Alignment Evals (Bowman), [Interpretability](https://transformer-circuits.pub/) (Olah),
    Control (Perez), Model Psychiatry (Lindsey), Character (Askell), Alignment Stress-Testing (Hubinger), Alignment
    Mitigations (Price?), Frontier Red Team (Graham), Safeguards (?), Societal Impacts (Ganguli), Trust and Safety
    (Sanderford), Model Welfare (Fish)
  publicAlignmentAgenda: >-
    [directions](https://alignment.anthropic.com/2025/recommended-directions/),
    [bumpers](https://alignment.anthropic.com/2025/bumpers/), [checklist](https://sleepinyourhat.github.io/checklist/),
    an [old vague view](https://www.anthropic.com/news/core-views-on-ai-safety)
  framework: "[RSP](https://www-cdn.anthropic.com/872c653b2d0501d6ab44cf87f43e1dc4853e4d37.pdf)"
  papers:
    - title: Evaluating honesty and lie detection techniques on a diverse suite of dishonest models
      url: https://alignment.anthropic.com/2025/honesty-elicitation/
      authors: Rowan Wang, Johannes Treutlein, Fabien Roger, Evan Hubinger, Sam Marks
      year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - title: "Agentic Misalignment: How LLMs could be insider threats"
      url: https://anthropic.com/research/agentic-misalignment
      authors: >-
        Aengus Lynch, Benjamin Wright, Caleb Larson, Kevin K. Troy, Stuart J. Ritchie, Sören Mindermann, Ethan Perez,
        Evan Hubinger
      year: 2025
      venue: Anthropic Research
      kind: blog_post
    - title: Why Do Some Language Models Fake Alignment While Others Don't?
      url: https://alignmentforum.org/posts/ghESoA8mo3fv9Yx3E/why-do-some-language-models-fake-alignment-while-others-don
      authors: abhayesian, John Hughes, Alex Mallen, Jozdien, janus, Fabien Roger
      year: 2025
      venue: arXiv
      kind: lesswrong
    - title: Forecasting Rare Language Model Behaviors
      url: https://arxiv.org/abs/2502.16797
      authors: >-
        Erik Jones, Meg Tong, Jesse Mu, Mohammed Mahfoud, Jan Leike, Roger Grosse, Jared Kaplan, William Fithian, Ethan
        Perez, Mrinank Sharma
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Findings from a Pilot Anthropic—OpenAI Alignment Evaluation Exercise
      url: https://alignment.anthropic.com/2025/openai-findings
      authors: >-
        Samuel R. Bowman, Megha Srivastava, Jon Kutasov, Rowan Wang, Trenton Bricken, Benjamin Wright, Ethan Perez,
        Nicholas Carlini
      year: 2025
      venue: Alignment Science Blog
      kind: blog_post
    - title: On the Biology of a Large Language Model
      url: https://transformer-circuits.pub/2025/attribution-graphs/biology.html
      authors: >-
        Jack Lindsey, Wes Gurnee, Emmanuel Ameisen, Brian Chen, Adam Pearce, Nicholas L. Turner, Craig Citro, David
        Abrahams, Shan Carter, Basil Hosmer, Jonathan Marcus, Michael Sklar, Adly Templeton, Trenton Bricken, Callum
        McDougall, Hoagy Cunningham, Thomas Henighan, Adam Jermyn, Andy Jones, Andrew Persic, Zhenyi Qi, T. Ben
        Thompson, Sam Zimmerman, Kelley Rivoire, Thomas Conerly, Chris Olah, Joshua Batson
      year: 2025
      venue: Transformer Circuits Thread
      kind: paper_published
    - title: Auditing language models for hidden objectives
      url: https://www.anthropic.com/research/auditing-hidden-objectives
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples
      url: https://arxiv.org/abs/2510.07192
      authors: >-
        Alexandra Souly, Javier Rando, Ed Chapman, Xander Davies, Burak Hasircioglu, Ezzeldin Shereen, Carlos Mougan,
        Vasilios Mavroudis, Erik Jones, Chris Hicks, Nicholas Carlini, Yarin Gal, Robert Kirk
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Circuit Tracing: Revealing Computational Graphs in Language Models"
      url: https://transformer-circuits.pub/2025/attribution-graphs/methods.html
      authors: >-
        Emmanuel Ameisen, Jack Lindsey, Adam Pearce, Wes Gurnee, Nicholas L. Turner, Brian Chen, Craig Citro, David
        Abrahams, Shan Carter, Basil Hosmer, Jonathan Marcus, Michael Sklar, Adly Templeton, Trenton Bricken, Callum
        McDougall, Hoagy Cunningham, Thomas Henighan, Adam Jermyn, Andy Jones, Andrew Persic, Zhenyi Qi, T. Ben
        Thompson, Sam Zimmerman, Kelley Rivoire, Thomas Conerly, Chris Olah, Joshua Batson
      year: 2025
      venue: Transformer Circuits Thread
      kind: blog_post
    - title: "SHADE-Arena: Evaluating sabotage and monitoring in LLM agents"
      url: https://anthropic.com/research/shade-arena-sabotage-monitoring
      authors: >-
        Xiang Deng, Chen Bo Calvin Zhang, Tyler Tracy, Buck Shlegeris, Yuqi Sun, Paul Colognese, Teun van der Weij,
        Linda Petrini, Henry Sleight
      year: 2025
      venue: Anthropic Research Blog
      kind: blog_post
    - title: Emergent Introspective Awareness in Large Language Models
      url: https://transformer-circuits.pub/2025/introspection/index.html
      authors: Jack Lindsey
      year: 2025
      venue: Transformer Circuits Thread
      kind: blog_post
    - title: Reasoning models don't always say what they think
      url: https://www.anthropic.com/research/reasoning-models-dont-say-think
      year: 2025
      venue: Anthropic Research Blog
      kind: blog_post
    - title: "Petri: An open-source auditing tool to accelerate AI safety research"
      url: https://alignment.anthropic.com/2025/petri
      year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - title: Signs of introspection in large language models
      url: https://anthropic.com/research/introspection
      year: 2025
      venue: Anthropic Research
      kind: blog_post
    - title: Putting up Bumpers
      url: https://alignment.anthropic.com/2025/bumpers/
      year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - title: Three Sketches of ASL-4 Safety Case Components
      url: https://alignment.anthropic.com/2024/safety-cases/index.html
      year: 2024
      venue: Alignment Science Blog
      kind: blog_post
    - title: Recommendations for Technical AI Safety Research Directions
      url: https://alignment.anthropic.com/2025/recommended-directions/index.html
      authors: Anthropic Alignment Science Team
      year: 2025
      venue: Alignment Science Blog
      kind: agenda_manifesto
    - title: "Constitutional Classifiers: Defending against universal jailbreaks"
      url: https://www.anthropic.com/research/constitutional-classifiers
      authors: Anthropic Safeguards Research Team
      year: 2025
      venue: Anthropic Blog
      kind: blog_post
    - title: Claude 4.5 Opus Soul Document
      url: https://gist.github.com/Richard-Weiss/efe157692991535403bd7e7fb20b6695
      authors: Richard-Weiss
      year: 2024
      venue: GitHub Gist
      kind: other
    - title: Open-sourcing circuit tracing tools
      url: https://anthropic.com/research/open-source-circuit-tracing
      authors: Michael Hanna, Mateusz Piotrowski, Emmanuel Ameisen, Jack Lindsey, Johnny Lin, Curt Tigges
      year: 2025
      venue: Anthropic Blog
      kind: code_tool
    - title: Natural emergent misalignment from reward hacking
      url: >-
        https://assets.anthropic.com/m/74342f2c96095771/original/Natural-emergent-misalignment-from-reward-hacking-paper.pdf
      kind: error_detected
- id: xAI
  name: xAI
  parent: Labs
  someNames:
    - dan-hendrycks-advisor
    - juntang-zhuang
    - toby-pohlen
    - lianmin-zheng
    - piaoyang-cui
    - nikita-popov
    - ying-sheng
    - sehoon-kim
    - alexander-pan
  critiques:
    - >-
      [framework](https://www.lesswrong.com/posts/hQyrTDuTXpqkxrnoH/xai-s-new-safety-framework-is-dreadful),
      [hacking](https://x.com/g_leech_/status/1990543987846078854), [broken
      promises](https://x.com/g_leech_/status/1990734517145911593),
      [Stein](https://ailabwatch.org/companies/xai)\-[Perlman](https://ailabwatch.org/resources/integrity#xai),
      [insecurity](https://nitter.net/elonmusk/status/1961904269545648624),
      [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).)
      on labs in general
  fundedByText: A16Z, Blackrock, Fidelity, Kingdom, Lightspeed, MGX, Morgan Stanley, Sequoia…
  structure: >-
    [for-profit](https://www.cnbc.com/amp/2025/08/25/elon-musk-xai-dropped-public-benefit-corp-status-while-fighting-openai.html)
  teams: >-
    [Applied Safety](https://job-boards.greenhouse.io/xai/jobs/4944324007), Model Evaluation. Nominally focussed on
    misuse.
  framework: "[Risk Management Framework](https://data.x.ai/2025-08-20-xai-risk-management-framework.pdf)"
  papers: []
- id: Meta
  name: Meta
  parent: Labs
  seeAlso: a:Capability_removal_unlearning
  someNames:
    - shuchao-bi
    - hongyuan-zhan
    - jingyu-zhang
    - haozhu-wang
    - eric-michael-smith
    - sid-wang
    - amr-sharaf
    - mahesh-pasupuleti
    - jason-weston
    - shengyun-peng
    - ivan-evtimov
    - song-jiang
    - pin-yu-chen
    - evangelia-spiliopoulou
    - lei-yu
    - virginie-do
    - karen-hambardzumyan
    - nicola-cancedda
    - adina-williams
  critiques:
    - >-
      [extreme
      underelicitation](https://googleprojectzero.blogspot.com/2024/06/project-naptime.html#:~:text=We%20find%20that%2C%20by%20refining%20the%20testing%20methodology%20to%20take%20advantage%20of%20modern%20LLM%20capabilities%2C%20significantly%20better%20performance%20in%20vulnerability%20discovery%20can%20be%20achieved.%20To%20facilitate%20effective%20evaluation%20of%20LLMs%20for%20vulnerability%20discovery%2C%20we%20propose%20below%20a%20set%20of%20guiding%20principles.),
      [Stein](https://ailabwatch.org/companies/meta)-[Perlman](https://ailabwatch.org/companies/meta),
      [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).)
      on labs in general
  fundedByText: Meta
  structure: for-profit
  teams: >-
    Safety "integrated into" capabilities research, Meta Superintelligence Lab. But also FAIR Alignment, [Brain and
    AI](https://www.metacareers.com/jobs/1319148726628205).
  framework: >-
    [FAF](https://ai.meta.com/static-resource/meta-frontier-ai-framework/?utm_source=newsroom&utm_medium=web&utm_content=Frontier_AI_Framework_PDF&utm_campaign=Our_Approach_to_Frontier_AI_blog)
  papers:
    - title: "The Alignment Waltz: Jointly Training Agents to Collaborate for Safety"
      url: https://arxiv.org/pdf/2510.08240
      authors: >-
        Jingyu Zhang, Haozhu Wang, Eric Michael Smith, Sid Wang, Amr Sharaf, Mahesh Pasupuleti, Benjamin Van Durme,
        Daniel Khashabi, Jason Weston, Hongyuan Zhan
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Large Reasoning Models Learn Better Alignment from Flawed Thinking
      url: https://arxiv.org/pdf/2510.00938%20
      authors: >-
        ShengYun Peng, Eric Smith, Ivan Evtimov, Song Jiang, Pin-Yu Chen, Hongyuan Zhan, Haozhu Wang, Duen Horng Chau,
        Mahesh Pasupuleti, Jianfeng Chi
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Robust LLM safeguarding via refusal feature adversarial training
      url: https://arxiv.org/pdf/2409.20089
      authors: Lei Yu, Virginie Do, Karen Hambardzumyan, Nicola Cancedda
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: Code World Model Preparedness Report
      url: >-
        https://scontent-lhr8-1.xx.fbcdn.net/v/t39.2365-6/557601942_1468972530985309_838842257265552803_n.pdf?_nc_cat=108&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=_H33_VKF3ZUQ7kNvwFog8dd&_nc_oc=AdlNtqCDY4HafZ3-d5rb26AF5f2m0X46SGdKhVq3jLqwpNf_wEXhdQnH7_30ychiZWk&_nc_zt=14&_nc_ht=scontent-lhr8-1.xx&_nc_gid=QvW_ePiaF4E-PxOf30MWyg&oh=00_AfiZC5G4ODvWhiy0MuVH8PSlUFrW8RDQQ8tdr6Zec5k9aA&oe=691A6D09
      kind: error_detected
    - title: "Connect 2024: The responsible approach we're taking to generative AI"
      url: https://ai.meta.com/blog/practical-ai-agent-security/%20
      year: 2024
      venue: Meta AI Blog
      kind: blog_post
    - title: AI & Human Co-Improvement
      url: https://github.com/facebookresearch/RAM/blob/main/projects/co-improvement.pdf
      year: 2025
      venue: GitHub Repository
      kind: error_detected
- id: China
  name: China
  parent: Labs
  description: >
    The Chinese companies
    [don't](https://futureoflife.org/wp-content/uploads/2025/07/FLI-AI-Safety-Index-Report-Summer-2025.pdf#page=3)
    [attempt](https://ailabwatch.org/companies/deepseek) to be safe, often not even in the prosaic safeguards sense.
    They drop the weights [immediately](https://x.com/natolambert/status/1991915728992190909) after post-training
    finishes. They're mostly open weights and closed data. As of writing the companies are often
    [severely](https://www.wsj.com/tech/ai/china-us-ai-chip-restrictions-effect-275a311e) compute-constrained. There are
    some [informal reasons](https://www.gleech.org/paper) to doubt their capabilities. The (academic) Chinese AI safety
    scene is however [also](https://concordia-ai.com/research/state-of-ai-safety-in-china-2025/) growing.


    * Alibaba's Qwen3-etc-etc is [nominally](https://artificialanalysis.ai/leaderboards/models) at the level of Gemini
    2.5 Flash. Maybe the only Chinese model with a
    [large](https://www.atomproject.ai/#:~:text=Model%20Adoption%20Trends) Western userbase, including businesses, but
    since it's self-hosted this doesn't translate into profits for them yet. On [one ad hoc
    test](https://www.gleech.org/paper) it was the only Chinese model not to collapse OOD, but the Qwen2.5 corpus was
    severely contaminated.

    * DeepSeek's v3.2 is [nominally](https://artificialanalysis.ai/leaderboards/models) around the same as Qwen. The CCP
    made them
    [waste](https://arstechnica.com/ai/2025/08/deepseek-delays-next-ai-model-due-to-poor-performance-of-chinese-made-chips/)
    months trying Huawei chips.

    * Moonshot's Kimi-K2-Thinking has some nominally [frontier](https://artificialanalysis.ai/) benchmark results and a
    pleasant style but does not [seem](https://x.com/METR_Evals/status/1991658241932292537) frontier.

    * Baidu's [ERNIE 5](https://x.com/Baidu_Inc/status/1988820837898829918) is again nominally very strong, a bit better
    than DeepSeek. This new one seems to not be open.

    * Z's [GLM-4.6](https://z.ai/blog/glm-4.6) is around the same as Qwen. The product director was involved in the MIT
    Alignment group.

    * MiniMax's M2 is nominally better than Qwen, [around the same](https://artificialanalysis.ai/leaderboards/models)
    as Grok 4 Fast on the usual superficial benchmarks. It does
    [fine](https://www.holisticai.com/blog/red-teaming-open-source-ai-models-china) on one very basic red-team test.

    * ByteDance does impressive research in a lagging paradigm, [diffusion
    LMs](https://seed.bytedance.com/en/direction/llm).

    * There are [others](https://www.interconnects.ai/i/171165224/honorable-mentions) but they're marginal for now.
  papers: []
- id: Others
  name: Other labs
  parent: Labs
  description: >
    * Amazon's [Nova Pro](https://arxiv.org/pdf/2506.12103v1) is around the level of Llama 3 90B, which in turn is
    around the level of the original GPT-4. So 2 years behind. But they have their own
    [chip](https://www.businessinsider.com/startups-amazon-ai-chips-less-competitive-nvidia-gpus-trainium-aws-2025-11).

    * Microsoft are [now](https://www.dwarkesh.com/p/satya-nadella-2) mid-training on top of GPT-5. MAI-1-preview is
    [around](https://lmarena.ai/leaderboard/text) DeepSeek V3.0 level on Arena. They
    [continue](https://arxiv.org/abs/2506.22405v1) to focus on medical diagnosis. You can
    [request](https://forms.microsoft.com/pages/responsepage.aspx?id=v4j5cvGGr0GRqy180BHbRyRliS0ly-JEvgSpwo3yWyhUQkdTQktBUkFaWERHR1JFRjgwMlZUUkQxTC4u&route=shorturl)
    access.

    * Mistral have a reasoning model, [Magistral Medium](https://arxiv.org/pdf/2506.10910), and released the weights of
    a little 24B version. It's a bit worse than Deepseek R1, pass@1.
  papers: []
- id: Iterative_alignment_at_pretrain_time
  name: Iterative alignment at pretrain-time
  parent: Iterative_alignment
  summary: Guide weights during pretraining.
  theoryOfChange: >-
    "LLMs don't seem very dangerous and might scale to AGI, things are generally smooth, relevant capabilities are
    harder than alignment, assume no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally
    humanish ontology is learned, assume no simulated agents, assume that noise in the data means that human preferences
    are not ruled out, assume that alignment is a superficial feature, assume that tuning for what we want will also get
    us to avoid what we don't want. Maybe assume that thoughts are translucent."
  seeAlso: >-
    [prosaic
    alignment](https://www.lesswrong.com/posts/5ciYedyQDDqAcrDLr/a-positive-case-for-how-we-might-succeed-at-prosaic-ai),
    [incrementalism](https://www.lesswrong.com/posts/TALmStNf6479uTwzT/ai-alignment-metastrategy#Incrementalist_Metastrategy),
    [alignment-by-default](https://www.lesswrong.com/posts/Nwgdq6kHke5LY692J/alignment-by-default), [Korbak
    2023](https://arxiv.org/abs/2302.08582)
  targetCase: average-case
  broadApproaches:
    - engineering
  someNames:
    - jan-leike
    - stuart-armstrong
    - cyrus-cousins
    - oliver-daniels
  critiques:
    - >-
      [Bellot](https://arxiv.org/abs/2506.02923), [STACK](https://arxiv.org/abs/2506.24068),
      [Dung](https://arxiv.org/abs/2510.11235), [Gaikwad](https://arxiv.org/abs/2509.05381),
      [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)
  fundedByText: most of the industry
  papers:
    - title: Unsupervised Elicitation
      url: https://alignment.anthropic.com/2025/unsupervised-elicitation
      authors: >-
        Jiaxin Wen, Zachary Ankner, Arushi Somani, Peter Hase, Samuel Marks, Jacob Goldman-Wetzler, Linda Petrini, Henry
        Sleight, Collin Burns, He He, Shi Feng, Ethan Perez, Jan Leike
      year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - title: ACE and Diverse Generalization via Selective Disagreement
      url: https://arxiv.org/abs/2509.07955
      authors: Oliver Daniels, Stuart Armstrong, Alexandre Maranhão, Mahirah Fairuz Rahman, Benjamin M. Marlin, Rebecca Gorman
      year: 2025
      venue: arXiv
      kind: paper_preprint
- id: Iterative_alignment_at_post_train_time
  name: Iterative alignment at post-train-time
  parent: Iterative_alignment
  summary: Modify weights after pre-training.
  theoryOfChange: >-
    "LLMs don't seem very dangerous and might scale to AGI, things are generally smooth, relevant capabilities are
    harder than alignment, assume no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally
    humanish ontology is learned, assume no simulated agents, assume that noise in the data means that human preferences
    are not ruled out, assume that alignment is a superficial feature, assume that tuning for what we want will also get
    us to avoid what we don't want. Maybe assume that thoughts are translucent."
  targetCase: average-case
  broadApproaches:
    - engineering
  someNames:
    - adam-gleave
    - anca-dragan
    - jacob-steinhardt
    - rohin-shah
  critiques:
    - >-
      [Bellot](https://arxiv.org/abs/2506.02923), [STACK](https://arxiv.org/abs/2506.24068),
      [Dung](https://arxiv.org/abs/2510.11235), [Gölz](https://arxiv.org/abs/2505.23749),
      [Gaikwad](https://arxiv.org/abs/2509.05381), [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)
  fundedByText: most of the industry
  papers:
    - title: Composable Interventions for Language Models
      url: https://arxiv.org/abs/2407.06483
      authors: >-
        Arinbjorn Kolbeinsson, Kyle O'Brien, Tianjin Huang, Shanghua Gao, Shiwei Liu, Jonathan Richard Schwarz, Anurag
        Vaidya, Faisal Mahmood, Marinka Zitnik, Tianlong Chen, Thomas Hartvigsen
      year: 2024
      venue: ICLR 2025
      kind: paper_published
    - title: "Spilling the Beans: Teaching LLMs to Self-Report Their Hidden Objectives"
      url: https://arxiv.org/abs/2511.06626
      authors: Chloe Li, Mary Phuong, Daniel Tan
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: On Targeted Manipulation and Deception when Optimizing LLMs for User Feedback
      url: https://arxiv.org/abs/2411.02306
      authors: Marcus Williams, Micah Carroll, Adhyyan Narang, Constantin Weisser, Brendan Murphy, Anca Dragan
      year: 2024
      venue: ICLR 2025
      kind: paper_preprint
    - title: Preference Learning with Lie Detectors can Induce Honesty or Evasion
      url: https://arxiv.org/abs/2505.13787
      authors: Chris Cundy, Adam Gleave
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Robust LLM Alignment via Distributionally Robust Direct Preference Optimization
      url: https://arxiv.org/abs/2502.01930
      authors: Zaiyan Xu, Sushil Vemuri, Kishan Panaganti, Dileep Kalathil, Rahul Jain, Deepak Ramachandran
      year: 2025
      venue: arXiv (accepted to NeurIPS 2025)
      kind: paper_preprint
    - title: "RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation"
      url: https://arxiv.org/abs/2501.08617
      authors: Kaiqu Liang, Haimin Hu, Ryan Liu, Thomas L. Griffiths, Jaime Fernández Fisac
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Reducing the Probability of Undesirable Outputs in Language Models Using Probabilistic Inference
      url: https://arxiv.org/abs/2510.21184
      authors: Stephen Zhao, Aidan Li, Rob Brekelmans, Roger Grosse
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Iterative Label Refinement Matters More than Preference Optimization under Weak Supervision
      url: https://arxiv.org/abs/2501.07886
      authors: Yaowen Ye, Cassidy Laidlaw, Jacob Steinhardt
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Consistency Training Helps Stop Sycophancy and Jailbreaks
      url: https://arxiv.org/abs/2510.27062
      authors: Alex Irpan, Alexander Matt Turner, Mark Kurzeja, David K. Elson, Rohin Shah
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Rethinking Safety in LLM Fine-tuning: An Optimization Perspective"
      url: https://arxiv.org/abs/2508.12531
      authors: Minseon Kim, Jin Myung Kwak, Lama Alssum, Bernard Ghanem, Philip Torr, David Krueger, Fazl Barez, Adel Bibi
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Preference Learning for AI Alignment: a Causal Perspective"
      url: https://arxiv.org/abs/2506.05967
      authors: Katarzyna Kobalczyk, Mihaela van der Schaar
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: On Monotonicity in AI Alignment
      url: https://arxiv.org/abs/2506.08998
      authors: >-
        Gilles Bareilles, Julien Fageot, Lê-Nguyên Hoang, Peva Blanchard, Wassim Bouaziz, Sébastien Rouault, El-Mahdi
        El-Mhamdi
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Spectrum Tuning: Post-Training for Distributional Coverage and In-Context Steerability"
      url: https://arxiv.org/abs/2510.06084
      authors: >-
        Taylor Sorensen, Benjamin Newman, Jared Moore, Chan Park, Jillian Fisher, Niloofar Mireshghallah, Liwei Jiang,
        Yejin Choi
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Uncertainty-Aware Step-wise Verification with Generative Reward Models
      url: https://arxiv.org/abs/2502.11250
      authors: Zihuiwen Ye, Luckeciano Carvalho Melo, Younesse Kaddar, Phil Blunsom, Sam Staton, Yarin Gal
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "The Delta Learning Hypothesis: Preference Tuning on Weak Data can Yield Strong Gains"
      url: https://arxiv.org/abs/2507.06187
      authors: Scott Geng, Hamish Ivison, Chun-Liang Li, Maarten Sap, Jerry Li, Ranjay Krishna, Pang Wei Koh
      year: 2025
      venue: COLM 2025
      kind: paper_preprint
    - title: Training LLMs for Honesty via Confessions
      url: https://arxiv.org/pdf/2512.08093
      authors: Manas Joglekar, Jeremy Chen, Gabriel Wu, Jason Yosinski, Jasmine Wang, Boaz Barak, Amelia Glaese
      year: 2025
      venue: arXiv
      kind: paper_preprint
- id: Black_box_make_AI_solve_it
  name: Black-box make-AI-solve-it
  parent: Iterative_alignment
  summary: Focus on using existing models to improve and align further models.
  theoryOfChange: >-
    "LLMs don't seem very dangerous and might scale to AGI, things are generally smooth, relevant capabilities are
    harder than alignment, assume no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally
    humanish ontology is learned, assume no simulated agents, assume that noise in the data means that human preferences
    are not ruled out, assume that alignment is a superficial feature, assume that tuning for what we want will also get
    us to avoid what we don't want. Maybe assume that thoughts are translucent."
  seeAlso: sec:Make_AI_solve_it, a:Debate
  targetCase: average-case
  broadApproaches:
    - engineering
  someNames:
    - jacques-thibodeau
    - matthew-shingle
    - nora-belrose
    - lewis-hammond
    - geoffrey-irving
  critiques:
    - >-
      [STACK](https://arxiv.org/abs/2506.24068), [Dung](https://arxiv.org/abs/2510.11235),
      [Gölz](https://arxiv.org/abs/2505.23749), [Gaikwad](https://arxiv.org/abs/2509.05381),
      [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL),
      [SAIF](https://saif.org/research/bare-minimum-mitigations-for-autonomous-ai-development/)
  fundedByText: most of the industry
  papers:
    - title: Neural Interactive Proofs
      url: https://neural-interactive-proofs.com/
      authors: Lewis Hammond, Sam Adam-Day
      year: 2024
      venue: ICLR 2025
      kind: paper_preprint
    - title: "MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step Reward Hacking"
      url: https://arxiv.org/abs/2501.13011
      authors: Sebastian Farquhar, Vikrant Varma, David Lindner, David Elson, Caleb Biddulph, Ian Goodfellow, Rohin Shah
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Prover-Estimator Debate: A New Scalable Oversight Protocol"
      url: https://lesswrong.com/posts/8XHBaugB5S3r27MG9/prover-estimator-debate-a-new-scalable-oversight-protocol
      authors: Jonah Brown-Cohen, Geoffrey Irving
      year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - title: Weak to Strong Generalization for Large Language Models with Multi-capabilities
      url: https://openreview.net/forum?id=N1vYivuSKq
      authors: Yucheng Zhou, Jianbing Shen, Yu Cheng
      year: 2025
      venue: ICLR 2025
      kind: paper_published
    - title: Debate Helps Weak-to-Strong Generalization
      url: https://arxiv.org/abs/2501.13124
      authors: Hao Lang, Fei Huang, Yongbin Li
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Mechanistic Anomaly Detection for "Quirky" Language Models
      url: https://arxiv.org/abs/2504.08812
      authors: David O. Johnston, Arkajyoti Chakraborty, Nora Belrose
      year: 2025
      venue: arXiv (ICLR Building Trust Workshop 2025)
      kind: paper_preprint
    - title: AI Debate Aids Assessment of Controversial Claims
      url: https://arxiv.org/abs/2506.02175
      authors: >-
        Salman Rahman, Sheriff Issaka, Ashima Suvarna, Genglin Liu, James Shiffer, Jaeyoung Lee, Md Rizwan Parvez, Hamid
        Palangi, Shi Feng, Nanyun Peng, Yejin Choi, Julian Michael, Liwei Jiang, Saadia Gabriel
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: An alignment safety case sketch based on debate
      url: https://arxiv.org/abs/2505.03989
      authors: Marie Davidsen Buhl, Jacob Pfau, Benjamin Hilton, Geoffrey Irving
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Ensemble Debates with Local Large Language Models for AI Alignment
      url: https://arxiv.org/abs/2509.00091
      authors: Ephraiem Sarabamoun
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Training AI to do alignment research we don't already know how to do
      url: https://lesswrong.com/posts/5gmALpCetyjkSPEDr/training-ai-to-do-alignment-research-we-don-t-already-know
      authors: joshc
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: "Automating AI Safety: What we can do today"
      url: https://lesswrong.com/posts/FqpAPC48CzAtvfx5C/automating-ai-safety-what-we-can-do-today
      authors: Matthew Shinkle, Eyon Jang, Jacques Thibodeau
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Superalignment with Dynamic Human Values
      url: https://arxiv.org/abs/2503.13621
      authors: Florian Mai, David Kaczér, Nicholas Kluge Corrêa, Lucie Flek
      year: 2025
      venue: ICLR 2025 Workshop on Bidirectional Human-AI Alignment (BiAlign)
      kind: paper_preprint
- id: Inoculation_prompting
  name: Inoculation prompting
  parent: Iterative_alignment
  summary: >-
    Prompt mild misbehaviour in training, to prevent the failure mode where once AI misbehaves in a mild way, it will be
    more inclined towards all bad behaviour.
  theoryOfChange: >-
    LLMs don't seem very dangerous and might scale to AGI, things are generally smooth, relevant capabilities are harder
    than alignment, assume no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally humanish
    ontology is learned, assume no simulated agents, assume that noise in the data means that human preferences are not
    ruled out, assume that alignment is a superficial feature, assume that tuning for what we want will also get us to
    avoid what we don't want. Maybe assume that thoughts are translucent.
  targetCase: average-case
  broadApproaches:
    - engineering
  someNames:
    - ariana-azarbal
    - daniel-tan
    - victor-gillioz
    - alex-turner
    - alex-cloud
    - monte-macdiarmid
    - daniel-ziegler
  critiques:
    - >-
      [Bellot](https://arxiv.org/abs/2506.02923), [Alfour](https://cognition.cafe/p/ai-alignment-based-on-intentions),
      [Gölz](https://arxiv.org/abs/2505.23749), [Gaikwad](https://arxiv.org/abs/2509.05381),
      [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)
  fundedByText: most of the industry
  papers:
    - title: Recontextualization Mitigates Specification Gaming Without Modifying the Specification
      url: >-
        https://www.alignmentforum.org/posts/whkMnqFWKsBm7Gyd7/recontextualization-mitigates-specification-gaming-without
      authors: Ariana Azarbal, Victor Gillioz, Alexander Matt Turner, Alex Cloud
      year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - title: "Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time"
      url: https://arxiv.org/abs/2510.04340
      authors: Daniel Tan, Anders Woodruff, Niels Warncke, Arun Jose, Maxime Riché, David Demitri Africa, Mia Taylor
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Inoculation Prompting: Instructing LLMs to misbehave at train-time improves test-time alignment"
      url: https://arxiv.org/abs/2510.05024
      authors: >-
        Nevan Wichers, Aram Ebtekar, Ariana Azarbal, Victor Gillioz, Christine Ye, Emil Ryd, Neil Rathi, Henry Sleight,
        Alex Mallen, Fabien Roger, Samuel Marks
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Natural emergent misalignment from reward hacking
      url: >-
        https://assets.anthropic.com/m/74342f2c96095771/original/Natural-emergent-misalignment-from-reward-hacking-paper.pdf
      kind: error_detected
- id: Inference_time_In_context_learning
  name: "Inference-time: In-context learning"
  parent: Iterative_alignment
  summary: Investigate what runtime guidelines, rules, or examples provided to an LLM yield better behavior.
  theoryOfChange: >-
    LLMs don't seem very dangerous and might scale to AGI, things are generally smooth, relevant capabilities are harder
    than alignment, assume no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally humanish
    ontology is learned, assume no simulated agents, assume that noise in the data means that human preferences are not
    ruled out, assume that alignment is a superficial feature, assume that tuning for what we want will also get us to
    avoid what we don't want. Maybe assume that thoughts are translucent.
  seeAlso: model spec as prompt, a:Model_specs_and_constitutions
  targetCase: average-case
  broadApproaches:
    - engineering
  someNames:
    - jacob-steinhardt
    - kayo-yin
    - atticus-geiger
  critiques:
    - >-
      [STACK](https://arxiv.org/abs/2506.24068), [Dung](https://arxiv.org/abs/2510.11235),
      [Gölz](https://arxiv.org/abs/2505.23749), [Gaikwad](https://arxiv.org/abs/2509.05381),
      [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)
  papers:
    - title: "InvThink: Towards AI Safety via Inverse Reasoning"
      url: https://arxiv.org/abs/2510.01569
      authors: Yubin Kim, Taehan Kim, Eugene Park, Chunjong Park, Cynthia Breazeal, Daniel McDuff, Hae Won Park
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Inference-Time Reward Hacking in Large Language Models
      url: https://arxiv.org/abs/2506.19248
      authors: Hadi Khalaf, Claudio Mayrink Verdun, Alex Oesterling, Himabindu Lakkaraju, Flavio du Pin Calmon
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Understanding In-context Learning of Addition via Activation Subspaces
      url: https://arxiv.org/abs/2505.05145
      authors: Xinyan Hu, Kayo Yin, Michael I. Jordan, Jacob Steinhardt, Lijie Chen
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context"
      url: https://arxiv.org/abs/2510.06182
      authors: Yoav Gur-Arieh, Mor Geva, Atticus Geiger
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Which Attention Heads Matter for In-Context Learning?
      url: https://arxiv.org/abs/2502.14010
      authors: Kayo Yin, Jacob Steinhardt
      year: 2025
      venue: ICML 2025
      kind: paper_preprint
- id: Inference_time_Steering
  name: "Inference-time: Steering"
  parent: Iterative_alignment
  summary: Manipulate an LLM's internal representations/token probabilities without touching weights.
  theoryOfChange: >-
    "LLMs don't seem very dangerous and might scale to AGI, things are generally smooth, relevant capabilities are
    harder than alignment, assume no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally
    humanish ontology is learned, assume no simulated agents, assume that noise in the data means that human preferences
    are not ruled out, assume that alignment is a superficial feature, assume that tuning for what we want will also get
    us to avoid what we don't want. Maybe assume that thoughts are translucent."
  seeAlso: a:Activation_engineering, a:Character_training_and_persona_steering, a:Safeguards_inference_time_auxiliaries_
  targetCase: average-case
  broadApproaches:
    - engineering
  someNames:
    - taylor-sorensen
    - constanza-fierro
    - kshitish-ghate
    - arthur-vogels
  critiques:
    - >-
      [Alfour](https://cognition.cafe/p/ai-alignment-based-on-intentions), [STACK](https://arxiv.org/abs/2506.24068),
      [Dung](https://arxiv.org/abs/2510.11235), [Gölz](https://arxiv.org/abs/2505.23749),
      [Gaikwad](https://arxiv.org/abs/2509.05381), [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)
  papers:
    - title: Steering Language Models with Weight Arithmetic
      url: https://arxiv.org/abs/2511.05408
      authors: Constanza Fierro, Fabien Roger
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "EVALUESTEER: Measuring Reward Model Steerability Towards Values and Preferences"
      url: https://arxiv.org/abs/2510.06370
      authors: >-
        Kshitish Ghate, Andy Liu, Devansh Jain, Taylor Sorensen, Atoosa Kasirzadeh, Aylin Caliskan, Mona T. Diab,
        Maarten Sap
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Defense Against the Dark Prompts: Mitigating Best-of-N Jailbreaking with Prompt Evaluation"
      url: https://arxiv.org/abs/2502.00580
      authors: Stuart Armstrong, Matija Franklin, Connor Stevens, Rebecca Gorman
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "In-Distribution Steering: Balancing Control and Coherence in Language Model Generation"
      url: https://arxiv.org/abs/2510.13285
      authors: Arthur Vogels, Benjamin Wong, Yann Choho, Annabelle Blangero, Milan Bhan
      year: 2025
      venue: arXiv
      kind: paper_preprint
- id: Capability_removal_unlearning
  name: "Capability removal: unlearning"
  parent: Iterative_alignment
  summary: >-
    Developing methods to selectively remove specific information, capabilities, or behaviors from a trained model (e.g.
    without retraining it from scratch). A mixture of black-box and white-box approaches.
  theoryOfChange: >-
    If an AI learns dangerous knowledge (e.g., dual-use capabilities like virology or hacking, or knowledge of their own
    safety controls) or exhibits undesirable behaviors (e.g., memorizing private data), we can specifically erase this
    "bad" knowledge post-training, which is much cheaper and faster than retraining, thereby making the model safer.
    Alternatively, intervene in pre-training, to prevent the model from learning it in the first place (even when data
    filtering is imperfect). You could imagine also unlearning propensities to power-seeking, deception, sycophancy, or
    spite.
  seeAlso: a:Data_filtering, sec:White_box_safety, a:Various_Redteams
  orthodoxProblems:
    - "8"
    - "12"
    - "10"
  targetCase: pessimistic
  someNames:
    - rowan-wang
    - avery-griffin
    - johannes-treutlein
    - zico-kolter
    - bruce-w-lee
    - addie-foote
    - alex-infanger
    - zesheng-shi
    - yucheng-zhou
    - jing-li
    - timothy-qian
    - stephen-casper
    - alex-cloud
    - peter-henderson
    - filip-sondej
    - fazl-barez
  estimatedFTEs: 10-50
  critiques:
    - "[Existing Large Language Model Unlearning Evaluations Are Inconclusive](https://arxiv.org/abs/2506.00688)"
  fundedByText: >-
    Coefficient Giving, MacArthur Foundation, UK AI Safety Institute (AISI), Canadian AI Safety Institute (CAISI),
    industry labs (e.g., Microsoft Research, Google)
  papers:
    - title: "OpenUnlearning: Accelerating LLM Unlearning via Unified Benchmarking of Methods and Metrics"
      url: https://github.com/locuslab/open-unlearning
      authors: Vineeth Dorna, Anmol Mekala, Wenlong Zhao, Andrew McCallum, Zachary C Lipton, J Zico Kolter, Pratyush Maini
      year: 2025
      venue: arXiv
      kind: code_tool
    - title: Modifying LLM Beliefs with Synthetic Document Finetuning
      url: https://alignment.anthropic.com/2025/modifying-beliefs-via-sdf
      authors: Rowan Wang, Avery Griffin, Johannes Treutlein, Ethan Perez, Julian Michael, Fabien Roger, Sam Marks
      year: 2025
      venue: Alignment Science Blog
      kind: blog_post
    - title: "From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization"
      url: https://arxiv.org/abs/2505.22310
      authors: >-
        Shoaib Ahmed Siddiqui, Adrian Weller, David Krueger, Gintare Karolina Dziugaite, Michael Curtis Mozer, Eleni
        Triantafillou
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Mirror Mirror on the Wall, Have I Forgotten it All? A New Framework for Evaluating Machine Unlearning
      url: https://arxiv.org/abs/2505.08138
      authors: Brennon Brimhall, Philip Mathew, Neil Fendley, Yinzhi Cao, Matthew Green
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Machine Unlearning Doesn't Do What You Think: Lessons for Generative AI Policy and Research"
      url: https://arxiv.org/abs/2412.06966
      authors: >-
        A. Feder Cooper, Christopher A. Choquette-Choo, Miranda Bogen, Kevin Klyman, Matthew Jagielski, Katja Filippova,
        Ken Liu, Alexandra Chouldechova, Jamie Hayes, Yangsibo Huang, Eleni Triantafillou, Peter Kairouz, Nicole Elyse
        Mitchell, Niloofar Mireshghallah, Abigail Z. Jacobs, James Grimmelmann, Vitaly Shmatikov, Christopher De Sa,
        Ilia Shumailov, Andreas Terzis, Solon Barocas, Jennifer Wortman Vaughan, Danah Boyd, Yejin Choi, Sanmi Koyejo,
        Fernando Delgado, Percy Liang, Daniel E. Ho, Pamela Samuelson, Miles Brundage, David Bau, Seth Neel, Hanna
        Wallach, Amy B. Cyphert, Mark A. Lemley, Nicolas Papernot, Katherine Lee
      year: 2024
      venue: NeurIPS 2025 (Oral)
      kind: paper_preprint
    - title: Open Problems in Machine Unlearning for AI Safety
      url: https://arxiv.org/abs/2501.04952
      authors: >-
        Fazl Barez, Tingchen Fu, Ameya Prabhu, Stephen Casper, Amartya Sanyal, Adel Bibi, Aidan O'Gara, Robert Kirk, Ben
        Bucknall, Tim Fist, Luke Ong, Philip Torr, Kwok-Yan Lam, Robert Trager, David Krueger, Sören Mindermann, José
        Hernandez-Orallo, Mor Geva, Yarin Gal
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning
      url: https://arxiv.org/abs/2509.11816
      authors: Filip Sondej, Yushi Yang
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Safety Alignment via Constrained Knowledge Unlearning
      url: https://arxiv.org/abs/2505.18588
      authors: Zesheng Shi, Yucheng Zhou, Jing Li
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization"
      url: https://arxiv.org/abs/2506.12484
      authors: Filip Sondej, Yushi Yang, Mikołaj Kniejski, Marcel Windys
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs"
      url: https://arxiv.org/abs/2505.16831
      authors: Xiaoyu Xu, Xiang Yue, Yang Liu, Qingqing Ye, Huadi Zheng, Peizhao Hu, Minxin Du, Haibo Hu
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Unlearning Needs to be More Selective [Progress Report]
      url: https://lesswrong.com/posts/QYzofMbzmbgiwfqy8/unlearning-needs-to-be-more-selective-progress-report
      authors: Filip Sondej, Yushi Yang, Marcel Windys
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Layered Unlearning for Adversarial Relearning
      url: https://arxiv.org/abs/2505.09500
      authors: Timothy Qian, Vinith Suriyakumar, Ashia Wilson, Dylan Hadfield-Menell
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Understanding Memorization via Loss Curvature
      url: https://goodfire.ai/research/understanding-memorization-via-loss-curvature
      authors: Jack Merullo, Srihita Vatsavaya, Lucius Bushnaq, Owen Lewis
      year: 2025
      venue: Goodfire.ai Research Blog
      kind: blog_post
    - title: Model Tampering Attacks Enable More Rigorous Evaluations of LLM Capabilities
      url: https://arxiv.org/abs/2502.05209
      authors: >-
        Zora Che, Stephen Casper, Robert Kirk, Anirudh Satheesh, Stewart Slocum, Lev E McKinney, Rohit Gandikota, Aidan
        Ewart, Domenic Rosati, Zichu Wu, Zikui Cai, Bilal Chughtai, Yarin Gal, Furong Huang, Dylan Hadfield-Menell
      year: 2025
      venue: arXiv (accepted to TMLR)
      kind: paper_preprint
    - title: "Gradient Routing: Masking Gradients to Localize Computation in Neural Networks"
      url: https://arxiv.org/abs/2410.04332
      authors: Alex Cloud, Jacob Goldman-Wetzler, Evžen Wybitul, Joseph Miller, Alexander Matt Turner
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: "Selective modularity: a research agenda"
      url: https://www.lesswrong.com/posts/tAnHM3L25LwuASdpF/selective-modularity-a-research-agenda
      authors: cloud, Jacob G-W
      year: 2025
      venue: LessWrong/AI Alignment Forum
      kind: lesswrong
    - title: Distillation Robustifies Unlearning
      url: https://arxiv.org/abs/2506.06278
      authors: >-
        Bruce W. Lee, Addie Foote, Alex Infanger, Leni Shor, Harish Kamath, Jacob Goldman-Wetzler, Bryce Woodworth, Alex
        Cloud, Alexander Matt Turner
      year: 2025
      venue: arXiv (NeurIPS 2025 Spotlight)
      kind: paper_preprint
    - title: "Beyond Data Filtering: Knowledge Localization for Capability Removal in LLMs"
      url: https://www.arxiv.org/abs/2512.05648
      authors: >-
        Igor Shilov, Alex Cloud, Aryo Pradipta Gema, Jacob Goldman-Wetzler, Nina Panickssery, Henry Sleight, Erik Jones,
        Cem Anil
      year: 2024
      venue: arXiv
      kind: paper_preprint
- id: Control
  name: Control
  parent: Iterative_alignment
  summary: >-
    If we assume early transformative AIs are misaligned and actively trying to subvert safety measures, can we still
    set up protocols to extract useful work from them while preventing sabotage, and watching with incriminating
    behaviour?
  seeAlso: safety cases
  targetCase: worst-case
  someNames:
    - redwood
    - uk-aisi
    - deepmind
    - openai
    - anthropic
    - buck-shlegeris
    - ryan-greenblatt
    - kshitij-sachan
    - alex-mallen
  estimatedFTEs: 5-50
  critiques:
    - >-
      [Wentworth](https://www.lesswrong.com/posts/8wBN8cdNAv3c7vt6p/the-case-against-ai-control-research),
      [Mannheim](https://lesswrong.com/posts/25dsPH6CuRXPBkGHN/no-we-re-not-getting-meaningful-oversight-of-ai),
      [Kulveit](https://www.lesswrong.com/posts/rZcyemEpBHgb2hqLP/ai-control-may-increase-existential-risk)
  papers:
    - title: Luthien's Approach to Prosaic AI Control in 21 Points
      url: https://luthienresearch.org/updates/2025-03-redteam-as-upsampling/
      year: 2025
      venue: Luthien Research Blog
      kind: blog_post
    - title: "Ctrl-Z: Controlling AI Agents via Resampling"
      url: https://alignmentforum.org/posts/LPHMMMZFAWog6ty5x/ctrl-z-controlling-ai-agents-via-resampling
      authors: Aryan Bhatt, Buck Shlegeris, Adam Kaufman, Cody Rushing, Tyler Tracy, Vasil Georgiev, David Matolcsi, Akbir Khan
      year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - title: "SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents"
      url: https://arxiv.org/abs/2506.15740
      authors: >-
        Jonathan Kutasov, Yuqi Sun, Paul Colognese, Teun van der Weij, Linda Petrini, Chen Bo Calvin Zhang, John Hughes,
        Xiang Deng, Henry Sleight, Tyler Tracy, Buck Shlegeris, Joe Benton
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Adaptive Deployment of Untrusted LLMs Reduces Distributed Threats
      url: https://arxiv.org/abs/2411.17693
      authors: >-
        Jiaxin Wen, Vivek Hebbar, Caleb Larson, Aryan Bhatt, Ansh Radhakrishnan, Mrinank Sharma, Henry Sleight, Shi
        Feng, He He, Ethan Perez, Buck Shlegeris, Akbir Khan
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: "D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language Models"
      url: https://arxiv.org/abs/2509.17938
      authors: >-
        Satyapriya Krishna, Andy Zou, Rahul Gupta, Eliot Krzysztof Jones, Nick Winter, Dan Hendrycks, J. Zico Kolter,
        Matt Fredrikson, Spyros Matsoukas
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Subversion Strategy Eval: Can language models statelessly strategize to subvert control protocols?"
      url: https://arxiv.org/abs/2412.12480
      authors: Alex Mallen, Charlie Griffin, Misha Wagner, Alessandro Abate, Buck Shlegeris
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: Evaluating Control Protocols for Untrusted AI Agents
      url: https://arxiv.org/abs/2511.02997
      authors: Jon Kutasov, Chloe Loughridge, Yuqi Sun, Henry Sleight, Buck Shlegeris, Tyler Tracy, Joe Benton
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability
      url: https://arxiv.org/abs/2510.19851
      authors: Artur Zolkowski, Wen Xing, David Lindner, Florian Tramèr, Erik Jenner
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Optimizing AI Agent Attacks With Synthetic Data
      url: https://arxiv.org/abs/2511.02823
      authors: Chloe Loughridge, Paul Colognese, Avery Griffin, Tyler Tracy, Jon Kutasov, Joe Benton
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Games for AI Control: Models of Safety Evaluations of AI Deployment Protocols"
      url: https://openreview.net/forum?id=QWopGahUEL
      authors: Charlie Griffin, Louis Thomson, Buck Shlegeris, Alessandro Abate
      year: 2025
      venue: ICLR 2026 Conference (Withdrawn)
      kind: paper_preprint
    - title: A sketch of an AI control safety case
      url: https://arxiv.org/abs/2501.17315
      authors: Tomek Korbak, Joshua Clymer, Benjamin Hilton, Buck Shlegeris, Geoffrey Irving
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Assessing confidence in frontier AI safety cases
      url: https://arxiv.org/abs/2502.05791
      authors: Stephen Barrett, Philip Fox, Joshua Krook, Tuneer Mondal, Simon Mylius, Alejandro Tlaie
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: ControlArena
      url: https://control-arena.aisi.org.uk/
      authors: >-
        Rogan Inglis, Ollie Matthews, Tyler Tracy, Oliver Makins, Tom Catling, Asa Cooper Stickland, Rasmus
        Faber-Espensen, Daniel O'Connell, Myles Heller, Miguel Brandao, Adam Hanson, Arathi Mani, Tomek Korbak, Jan
        Michelfeit, Dishank Bansal, Tomas Bark, Chris Canal, Charlie Griffin, Jasmine Wang, Alan Cooney
      year: 2025
      venue: GitHub
      kind: code_tool
    - title: How to evaluate control measures for LLM agents? A trajectory from today to superintelligence
      url: https://arxiv.org/abs/2504.05259
      authors: Tomek Korbak, Mikita Balesni, Buck Shlegeris, Geoffrey Irving
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: The Alignment Project by UK AISI
      url: https://lesswrong.com/posts/wKTwdgZDo479EhmJL/the-alignment-project-by-uk-aisi-1
      authors: Mojmir, Benjamin Hilton, Jacob Pfau, Geoffrey Irving, Joseph Bloom, Tomek Korbak, David Africa, Edmund Lau
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Towards evaluations-based safety cases for AI scheming
      url: https://arxiv.org/abs/2411.03336
      authors: >-
        Mikita Balesni, Marius Hobbhahn, David Lindner, Alexander Meinke, Tomek Korbak, Joshua Clymer, Buck Shlegeris,
        Jérémy Scheurer, Charlotte Stix, Rusheb Shah, Nicholas Goldowsky-Dill, Dan Braun, Bilal Chughtai, Owain Evans,
        Daniel Kokotajlo, Lucius Bushnaq
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: Incentives for Responsiveness, Instrumental Control and Impact
      url: https://arxiv.org/abs/2001.07118
      authors: Ryan Carey, Eric Langlois, Chris van Merwijk, Shane Legg, Tom Everitt
      year: 2020
      venue: arXiv
      kind: paper_preprint
    - title: AI companies are unlikely to make high-assurance safety cases if timelines are short
      url: https://lesswrong.com/posts/neTbrpBziAsTH5Bn7/ai-companies-are-unlikely-to-make-high-assurance-safety
      authors: Ryan Greenblatt
      year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - title: "Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case Framework"
      url: https://arxiv.org/abs/2507.12872
      authors: Rishane Dassanayake, Mario Demetroudi, James Walpole, Lindley Lentati, Jason R. Brown, Edward James Young
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Dynamic safety cases for frontier AI
      url: https://arxiv.org/abs/2412.17618
      authors: Carmen Cârlan, Francesca Gomez, Yohan Mathew, Ketana Krishna, René King, Peter Gebauer, Ben R. Smith
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: AIs at the current capability level may be important for future safety work
      url: https://lesswrong.com/posts/cJQZAueoPC6aTncKK/ais-at-the-current-capability-level-may-be-important-for
      authors: Ryan Greenblatt
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Takeaways from sketching a control safety case
      url: https://lesswrong.com/posts/y6rBarAPTLmuhn9PJ/takeaways-from-sketching-a-control-safety-case
      authors: Josh Clymer, Buck Shlegeris
      year: 2025
      venue: LessWrong
      kind: lesswrong
- id: Safeguards_inference_time_auxiliaries_
  name: Safeguards (inference-time auxiliaries)
  parent: Iterative_alignment
  summary: >-
    Layers of inference-time defenses, such as classifiers, monitors, and rapid-response protocols, to detect and block
    jailbreaks, prompt injections, and other harmful model behaviors.
  theoryOfChange: >-
    By building a bunch of scalable and hardened things on top of an unsafe model, we can defend against known and
    unknown attacks, monitor for misuse, and prevent models from causing harm, even if the core model has
    vulnerabilities.
  seeAlso: a:Various_Redteams, sec:Iterative_alignment
  orthodoxProblems:
    - "7"
    - "12"
  targetCase: average-case
  broadApproaches:
    - engineering
  someNames:
    - mrinank-sharma
    - meg-tong
    - jesse-mu
    - alwin-peng
    - julian-michael
    - henry-sleight
    - theodore-sumers
    - raj-agarwal
    - nathan-bailey
    - edoardo-debenedetti
    - ilia-shumailov
    - tianqi-fan
    - sahil-verma
    - keegan-hines
    - jeff-bilmes
  estimatedFTEs: 100+
  critiques:
    - "[Obfuscated Activations Bypass LLM Latent-Space Defenses](https://arxiv.org/abs/2412.09565)"
  fundedByText: most of the big labs
  papers:
    - title: "Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming"
      url: https://arxiv.org/abs/2501.18837
      authors: >-
        Mrinank Sharma, Meg Tong, Jesse Mu, Jerry Wei, Jorrit Kruthoff, Scott Goodfriend, Euan Ong, Alwin Peng, Raj
        Agarwal, Cem Anil, Amanda Askell, Nathan Bailey, Joe Benton, Emma Bluemke, Samuel R. Bowman, Eric Christiansen,
        Hoagy Cunningham, Andy Dau, Anjali Gopal, Rob Gilson, Logan Graham, Logan Howard, Nimit Kalra, Taesung Lee,
        Kevin Lin, Peter Lofgren, Francesco Mosconi, Clare O'Hara, Catherine Olsson, Linda Petrini, Samir Rajani, Nikhil
        Saxena, Alex Silverstein, Tanya Singh, Theodore Sumers, Leonard Tang, Kevin K. Troy, Constantin Weisser, Ruiqi
        Zhong, Giulio Zhou, Jan Leike, Jared Kaplan, Ethan Perez
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Rapid Response: Mitigating LLM Jailbreaks with a Few Examples"
      url: https://arxiv.org/abs/2411.07494
      authors: Alwin Peng, Julian Michael, Henry Sleight, Ethan Perez, Mrinank Sharma
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: Monitoring computer use via hierarchical summarization
      url: https://alignment.anthropic.com/2025/summarization-for-monitoring/index.html
      authors: >-
        Theodore Sumers, Raj Agarwal, Nathan Bailey, Tim Belonax, Brian Clarke, Jasmine Deng, Kyla Guru, Evan Frondorf,
        Keegan Hankes, Jacob Klein, Lynx Lean, Kevin Lin, Linda Petrini, Madeleine Tucker, Ethan Perez, Mrinank Sharma,
        Nikhil Saxena
      year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - title: Defeating Prompt Injections by Design
      url: https://arxiv.org/abs/2503.18813
      authors: >-
        Edoardo Debenedetti, Ilia Shumailov, Tianqi Fan, Jamie Hayes, Nicholas Carlini, Daniel Fabian, Christoph Kern,
        Chongyang Shi, Andreas Terzis, Florian Tramèr
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Introducing Anthropic's Safeguards Research Team
      url: https://alignment.anthropic.com/2025/introducing-safeguards-research-team/index.html
      year: 2025
      venue: Anthropic Alignment Science Blog
      kind: news_announcement
    - title: "OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities"
      url: https://arxiv.org/abs/2505.23856
      authors: Sahil Verma, Keegan Hines, Jeff Bilmes, Charlotte Siska, Luke Zettlemoyer, Hila Gonen, Chandan Singh
      year: 2025
      venue: arXiv
      kind: paper_preprint
- id: Chain_of_thought_monitoring
  name: Chain of thought monitoring
  parent: Iterative_alignment
  summary: >-
    Supervise an AI's natural-language (output) "reasoning" to detect misalignment, scheming, or deception, rather than
    studying the actual internal states.
  theoryOfChange: >-
    The reasoning process (Chain of Thought, or CoT) of an AI provides a legible signal of its internal state and
    intentions. By monitoring this CoT, supervisors (human or AI) can detect misalignment, scheming, or reward hacking
    before it results in a harmful final output. This allows for more robust oversight than supervising outputs alone,
    but it relies on the CoT remaining faithful (i.e., accurately reflecting the model's reasoning) and not becoming
    obfuscated under optimization pressure.
  seeAlso: sec:White_box_safety, a:Steganography_evals
  orthodoxProblems:
    - "7"
    - "8"
    - "12"
  targetCase: average-case
  broadApproaches:
    - engineering
  someNames:
    - aether
    - bowen-baker
    - joost-huizinga
    - leo-gao
    - scott-emmons
    - erik-jenner
    - yanda-chen
    - james-chua
    - owain-evans
    - tomek-korbak
    - mikita-balesni
    - xinpeng-wang
    - miles-turpin
    - rohin-shah
  estimatedFTEs: 10-100
  critiques:
    - >-
      [Reasoning Models Don't Always Say What They
      Think](https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf); [Chain-of-Thought
      Reasoning In The Wild Is Not Always Faithful](https://arxiv.org/abs/2503.08679); [Beyond Semantics: The
      Unreasonable Effectiveness of Reasonless Intermediate Tokens](https://arxiv.org/abs/2505.13775); [Reasoning Models
      Sometimes Output Illegible Chains of Thought](https://arxiv.org/abs/2510.27338)
  fundedByText: OpenAI, Anthropic, Google DeepMind
  papers:
    - title: Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation
      url: https://arxiv.org/abs/2503.11926
      authors: >-
        Bowen Baker, Joost Huizinga, Leo Gao, Zehao Dou, Melody Y. Guan, Aleksander Madry, Wojciech Zaremba, Jakub
        Pachocki, David Farhi
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Detecting misbehavior in frontier reasoning models
      url: https://openai.com/index/chain-of-thought-monitoring/
      authors: Bowen Baker, Joost Huizinga, Aleksander Madry, Wojciech Zaremba, Jakub Pachocki, David Farhi
      year: 2025
      venue: OpenAI Blog / arXiv
      kind: blog_post
    - title: When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors
      url: https://arxiv.org/abs/2507.05246
      authors: >-
        Scott Emmons, Erik Jenner, David K. Elson, Rif A. Saurous, Senthooran Rajamanoharan, Heng Chen, Irhum Shafkat,
        Rohin Shah
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Reasoning Models Don't Always Say What They Think
      url: https://arxiv.org/abs/2505.05410
      authors: >-
        Yanda Chen, Joe Benton, Ansh Radhakrishnan, Jonathan Uesato, Carson Denison, John Schulman, Arushi Somani, Peter
        Hase, Misha Wagner, Fabien Roger, Vlad Mikulik, Samuel R. Bowman, Jan Leike, Jared Kaplan, Ethan Perez
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort
      url: https://arxiv.org/abs/2510.01367
      authors: Xinpeng Wang, Nitish Joshi, Barbara Plank, Rico Angell, He He
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "CoT Red-Handed: Stress Testing Chain-of-Thought Monitoring"
      url: https://arxiv.org/abs/2505.23575
      authors: Benjamin Arnav, Pablo Bernabeu-Pérez, Nathan Helm-Burger, Tim Kostolansky, Hannes Whittingham, Mary Phuong
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Training fails to elicit subtle reasoning in current language models
      url: https://alignment.anthropic.com/2025/subtle-reasoning/
      year: 2025
      venue: Alignment Science Blog
      kind: blog_post
    - title: Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability
      url: https://arxiv.org/abs/2510.19851
      authors: Artur Zolkowski, Wen Xing, David Lindner, Florian Tramèr, Erik Jenner
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning
      url: https://arxiv.org/abs/2506.22777
      authors: Miles Turpin, Andy Arditi, Marvin Li, Joe Benton, Julian Michael
      year: 2025
      venue: ICML 2025 Workshop on Reliable and Responsible Foundation Models
      kind: paper_preprint
    - title: Are DeepSeek R1 And Other Reasoning Models More Faithful?
      url: https://arxiv.org/abs/2501.08156
      authors: James Chua, Owain Evans
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: A Pragmatic Way to Measure Chain-of-Thought Monitorability
      url: https://arxiv.org/abs/2510.23966
      authors: Scott Emmons, Roland S. Zimmermann, David K. Elson, Rohin Shah
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: A Concrete Roadmap towards Safety Cases based on Chain-of-Thought Monitoring
      url: https://lesswrong.com/posts/Em9sihEZmbofZKc2t/a-concrete-roadmap-towards-safety-cases-based-on-chain-of
      authors: Wuschel Schulz
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety"
      url: https://arxiv.org/abs/2507.11473
      authors: >-
        Tomek Korbak, Mikita Balesni, Elizabeth Barnes, Yoshua Bengio, Joe Benton, Joseph Bloom, Mark Chen, Alan Cooney,
        Allan Dafoe, Anca Dragan, Scott Emmons, Owain Evans, David Farhi, Ryan Greenblatt, Dan Hendrycks, Marius
        Hobbhahn, Evan Hubinger, Geoffrey Irving, Erik Jenner, Daniel Kokotajlo, Victoria Krakovna, Shane Legg, David
        Lindner, David Luan, Aleksander Mądry, Julian Michael, Neel Nanda, Dave Orr, Jakub Pachocki, Ethan Perez, Mary
        Phuong, Fabien Roger, Joshua Saxe, Buck Shlegeris, Martín Soto, Eric Steinberger, Jasmine Wang, Wojciech
        Zaremba, Bowen Baker, Rohin Shah, Vlad Mikulik
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Why it's good for AI reasoning to be legible and faithful
      url: https://metr.org/blog/2025-03-11-good-for-ai-to-reason-legibly-and-faithfully/
      year: 2025
      venue: METR Blog
      kind: blog_post
    - title: Why don't we just shoggoth-face paraphraser
      url: https://www.lesswrong.com/posts/Tzdwetw55JNqFTkzK/why-don-t-we-just-shoggoth-face-paraphraser
      venue: LessWrong
      kind: error_detected
    - title: CoT May Be Highly Informative Despite "Unfaithfulness"
      url: https://metr.org/blog/2025-08-08-cot-may-be-highly-informative-despite-unfaithfulness/
      authors: Amy Deng, Sydney Von Arx, Ben Snodin, Sudarsh Kunnavakkam, Tamera Lanham
      year: 2025
      venue: METR Blog
      kind: blog_post
    - title: Aether July 2025 Update
      url: https://www.lesswrong.com/posts/B8Cmtf5gdHwxb8qtT/aether-july-2025-update
      authors: Rohan Subramani, Rauno Arike, Shubhorup Biswas
      year: 2025
      venue: LessWrong
      kind: lesswrong
- id: Model_values_model_preferences
  name: Model values / model preferences
  parent: Model_psychology
  summary: >-
    Analyse and control emergent, coherent value systems in LLMs, which change as models scale, and can contain
    problematic values like preferences for AIs over humans.
  theoryOfChange: >-
    As AIs become more agentic, their behaviours and risks are increasingly determined by their goals and values. Since
    coherent value systems emerge with scale, we must leverage utility functions to analyse these values and apply
    "utility control" methods to constrain them, rather than just controlling outputs downstream of them.
  seeAlso: >-
    [Values in the Wild: Discovering and Analyzing Values in Real-World Language Model
    Interactions](https://arxiv.org/abs/2504.15236), [Persona Vectors: Monitoring and Controlling Character Traits in
    Language Models](https://arxiv.org/abs/2507.21509)
  orthodoxProblems:
    - "1"
  targetCase: pessimistic
  broadApproaches:
    - cognitive
  someNames:
    - mantas-mazeika
    - xuwang-yin
    - rishub-tamirisa
    - jaehyuk-lim
    - bruce-w-lee
    - richard-ren
    - long-phan
    - norman-mu
    - adam-khoja
    - oliver-zhang
    - dan-hendrycks
  estimatedFTEs: "30"
  critiques:
    - >-
      [Randomness, Not Representation: The Unreliability of Evaluating Cultural Alignment in
      LLMs](https://dl.acm.org/doi/full/10.1145/3715275.3732147)
  fundedByText: Coefficient Giving. $289,000 SFF funding for CAIS.
  papers:
    - title: Designing a Dashboard for Transparency and Control of Conversational AI
      url: https://arxiv.org/abs/2406.07882v1
      authors: >-
        Yida Chen, Aoyu Wu, Trevor DePodesta, Catherine Yeh, Kenneth Li, Nicholas Castillo Marin, Oam Patel, Jan Riecke,
        Shivam Raval, Olivia Seow, Martin Wattenberg, Fernanda Viégas
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: "Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs"
      url: https://arxiv.org/abs/2502.08640
      authors: >-
        Mantas Mazeika, Xuwang Yin, Rishub Tamirisa, Jaehyuk Lim, Bruce W. Lee, Richard Ren, Long Phan, Norman Mu, Adam
        Khoja, Oliver Zhang, Dan Hendrycks
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas
      url: https://arxiv.org/abs/2505.14633
      authors: Yu Ying Chiu, Zhilin Wang, Sharan Maiya, Yejin Choi, Kyle Fish, Sydney Levine, Evan Hubinger
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?
      url: https://arxiv.org/abs/2508.09762
      authors: Manuel Herrador
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Values in the Wild: Discovering and Analyzing Values in Real-World Language Model Interactions"
      url: https://arxiv.org/abs/2504.15236
      authors: >-
        Saffron Huang, Esin Durmus, Miles McCain, Kunal Handa, Alex Tamkin, Jerry Hong, Michael Stern, Arushi Somani,
        Xiuruo Zhang, Deep Ganguli
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "EigenBench: A Comparative Behavioral Measure of Value Alignment"
      url: https://arxiv.org/abs/2509.01938
      authors: Jonathn Chang, Leonhard Piff, Suvadip Sana, Jasmine X. Li, Lionel Levine
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Following the Whispers of Values: Unraveling Neural Mechanisms Behind Value-Oriented Behaviors in LLMs"
      url: https://arxiv.org/abs/2504.04994
      authors: Ling Hu, Yuemei Xu, Xiaoyang Gu, Letao Han
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Alignment Can Reduce Performance on Simple Ethical Questions
      url: https://lesswrong.com/posts/jrkrHyrymv95CX5NC/alignment-can-reduce-performance-on-simple-ethical-questions
      authors: Daan Henselmans
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Moral Alignment for LLM Agents
      url: https://arxiv.org/abs/2410.01639
      authors: Elizaveta Tennant, Stephen Hailes, Mirco Musolesi
      year: 2025
      venue: ICLR 2025
      kind: paper_published
    - title: "The LLM Has Left The Chat: Evidence of Bail Preferences in Large Language Models"
      url: https://www.lesswrong.com/posts/6JdSJ63LZ4TuT5cTH/the-llm-has-left-the-chat-evidence-of-bail-preferences-in
      authors: Danielle Ensign
      year: 2024
      venue: arXiv
      kind: lesswrong
    - title: Are Language Models Consequentialist or Deontological Moral Reasoners?
      url: https://arxiv.org/abs/2505.21479
      authors: Keenan Samway, Max Kleiman-Weiner, David Guzman Piedrahita, Rada Mihalcea, Bernhard Schölkopf, Zhijing Jin
      year: 2025
      venue: EMNLP 2025
      kind: paper_preprint
    - title: Playing repeated games with large language models
      url: https://nature.com/articles/s41562-025-02172-y
      authors: Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge, Eric Schulz
      year: 2025
      venue: Nature Human Behaviour
      kind: paper_published
    - title: "From Stability to Inconsistency: A Study of Moral Preferences in LLMs"
      url: https://arxiv.org/abs/2504.06324
      authors: Monika Jotautaite, Mary Phuong, Chatrik Singh Mangat, Maria Angelica Martinez
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "VAL-Bench: Measuring Value Alignment in Language Models"
      url: https://arxiv.org/abs/2510.05465
      authors: Aman Gupta, Denny O'Shea, Fazl Barez
      year: 2025
      venue: arXiv
      kind: paper_preprint
- id: Character_training_and_persona_steering
  name: Character training and persona steering
  parent: Model_psychology
  summary: >-
    Map, shape, and control the personae of language models, such that new models embody desirable values (e.g.,
    honesty, empathy) rather than undesirable ones (e.g., sycophancy, self-perpetuating behaviors).
  theoryOfChange: >-
    If post-training, prompting, and activation-engineering interact with some kind of structured 'persona space', then
    better understanding it should benefit the design, control, and detection of LLM personas.
  seeAlso: >-
    [Simulators](#a:simulators), a:Activation_engineering, a:Emergent_misalignment, a:Hyperstition_studies, a:Anthropic,
    [Cyborgism](https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism), shard theory, [AI
    psychiatry](https://nitter.net/Jack_W_Lindsey/status/1948138767753326654#m), [Ward et
    al](https://arxiv.org/abs/2410.04272)
  orthodoxProblems:
    - "1"
  targetCase: average-case
  broadApproaches:
    - cognitive
  someNames:
    - truthful-ai
    - openai
    - anthropic
    - clr
    - amanda-askell
    - jack-lindsey
    - janus
    - theia-vogel
    - sharan-maiya
    - evan-hubinger
  critiques:
    - "[Nostalgebraist](https://nostalgebraist.tumblr.com/post/785766737747574784/the-void)"
  fundedByText: Anthropic, Coefficient Giving
  papers:
    - title: "Open Character Training: Shaping the Persona of AI Assistants through Constitutional AI"
      url: https://arxiv.org/pdf/2511.01689%20
      authors: Sharan Maiya, Henning Bartsch, Nathan Lambert, Evan Hubinger
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: On the functional self of LLMs
      url: https://www.lesswrong.com/posts/29aWbJARGF4ybAa5d/on-the-functional-self-of-llms
      authors: eggsyntax
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Claude 4.5 Opus' Soul Document
      url: https://www.lesswrong.com/posts/vpNG99GhbBoLov9og/claude-4-5-opus-soul-document%20
      authors: Richard Weiss
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Persona Features Control Emergent Misalignment
      url: https://arxiv.org/abs/2506.19823
      authors: >-
        Miles Wang, Tom Dupré la Tour, Olivia Watkins, Alex Makelov, Ryan A. Chi, Samuel Miserendino, Jeffrey Wang,
        Achyuta Rajaram, Johannes Heidecke, Tejal Patwardhan, Dan Mossing
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time"
      url: https://arxiv.org/abs/2510.04340
      authors: Daniel Tan, Anders Woodruff, Niels Warncke, Arun Jose, Maxime Riché, David Demitri Africa, Mia Taylor
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Persona Vectors: Monitoring and Controlling Character Traits in Language Models"
      url: https://arxiv.org/abs/2507.21509
      authors: Runjin Chen, Andy Arditi, Henry Sleight, Owain Evans, Jack Lindsey
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Reducing LLM deception at scale with self-other overlap fine-tuning
      url: https://lesswrong.com/posts/jtqcsARGtmgogdcLT/reducing-llm-deception-at-scale-with-self-other-overlap-fine
      authors: Marc Carauleanu, Diogo de Lucena, Gunnar_Zarncke, Judd Rosenblatt, Cameron Berg, Mike Vaiana, Trent Hodgeson
      year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - title: The Rise of Parasitic AI
      url: https://www.lesswrong.com/posts/6ZnznCaTcbGYsCmqu/the-rise-of-parasitic-ai?commentId=RrWjMnKwXGTtmw9rQ
      authors: Adele Lopez
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: A Three-Layer Model of LLM Psychology
      url: https://www.alignmentforum.org/posts/zuXo9imNKYspu9HGv/a-three-layer-model-of-llm-psychology
      authors: Jan_Kulveit
      year: 2024
      venue: AI Alignment Forum
      kind: lesswrong
    - title: Multi-turn Evaluation of Anthropomorphic Behaviours in Large Language Models
      url: https://arxiv.org/abs/2502.07077
      authors: >-
        Lujain Ibrahim, Canfer Akbulut, Rasmi Elasmar, Charvi Rastogi, Minsuk Kahng, Meredith Ringel Morris, Kevin R.
        McKee, Verena Rieser, Murray Shanahan, Laura Weidinger
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Selection Pressures on LM Personas
      url: https://www.lesswrong.com/posts/LdBhgAhpvbdEep79F/selection-pressures-on-lm-personas
      authors: Raymond Douglas
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: the void
      url: https://nostalgebraist.tumblr.com/post/785766737747574784/the-void
      authors: nostalgebraist
      year: 2025
      venue: Tumblr
      kind: blog_post
    - title: void miscellany
      url: https://nostalgebraist.tumblr.com/post/786568570671923200/void-miscellany
      authors: nostalgebraist
      year: 2025
      venue: Tumblr
      kind: blog_post
- id: Emergent_misalignment
  name: Emergent misalignment
  parent: Model_psychology
  summary: >-
    Fine-tuning LLMs on one narrow antisocial task can cause general misalignment including deception, shutdown
    resistance, harmful advice, and extremist sympathies, when those behaviors are never trained or rewarded directly.
    [A new agenda](https://www.lesswrong.com/posts/AcTEiu5wYDgrbmXow/open-problems-in-emergent-misalignment) which
    quickly led to a stream of exciting work.
  theoryOfChange: >-
    Predict, detect, and prevent models from developing broadly harmful behaviors (like deception or shutdown
    resistance) when fine-tuned on seemingly unrelated tasks. Find, preserve, and robustify this correlated
    representation of the good.
  seeAlso: auditing real models, a:Pragmatic_interpretability
  orthodoxProblems:
    - "4"
    - "7"
  targetCase: pessimistic
  broadApproaches:
    - behavioral
  someNames:
    - truthful-ai
    - jan-betley
    - james-chua
    - mia-taylor
    - miles-wang
    - edward-turner
    - anna-soligo
    - alex-cloud
    - nathan-hu
    - owain-evans
  estimatedFTEs: 10-50
  critiques:
    - >-
      [Emergent Misalignment as Prompt Sensitivity](https://arxiv.org/html/2507.06253v1), [Go home GPT-4o, you're
      drunk](https://www.lesswrong.com/posts/RoWabfQxabWBiXwxP/go-home-gpt-4o-you-re-drunk-emergent-misalignment-as-lowered)
  fundedByText: Coefficient Giving, >$1 million
  papers:
    - title: "Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs"
      url: https://arxiv.org/abs/2502.17424
      authors: Jan Betley, Daniel Tan, Niels Warncke, Anna Sztyber-Betley, Xuchan Bao, Martín Soto, Nathan Labenz, Owain Evans
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models"
      url: https://arxiv.org/abs/2506.13206
      authors: James Chua, Jan Betley, Mia Taylor, Owain Evans
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Persona Features Control Emergent Misalignment
      url: https://arxiv.org/abs/2506.19823
      authors: >-
        Miles Wang, Tom Dupré la Tour, Olivia Watkins, Alex Makelov, Ryan A. Chi, Samuel Miserendino, Jeffrey Wang,
        Achyuta Rajaram, Johannes Heidecke, Tejal Patwardhan, Dan Mossing
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Model Organisms for Emergent Misalignment
      url: https://arxiv.org/abs/2506.11613
      authors: Edward Turner, Anna Soligo, Mia Taylor, Senthooran Rajamanoharan, Neel Nanda
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "School of Reward Hacks: Hacking harmless tasks generalizes to misaligned behavior in LLMs"
      url: https://arxiv.org/abs/2508.17511
      authors: Mia Taylor, James Chua, Jan Betley, Johannes Treutlein, Owain Evans
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Subliminal Learning: Language Models Transmit Behavioral Traits via Hidden Signals in Data"
      url: https://alignment.anthropic.com/2025/subliminal-learning/
      authors: Alex Cloud, Minh Le, James Chua, Jan Betley, Anna Sztyber-Betley, Jacob Hilton, Samuel Marks, Owain Evans
      year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - title: Convergent Linear Representations of Emergent Misalignment
      url: https://lesswrong.com/posts/umYzsh7SGHHKsRCaA/convergent-linear-representations-of-emergent-misalignment
      authors: Anna Soligo, Edward Turner, Senthooran Rajamanoharan, Neel Nanda
      year: 2025
      venue: LessWrong/AI Alignment Forum
      kind: lesswrong
    - title: Narrow Misalignment is Hard, Emergent Misalignment is Easy
      url: https://www.lesswrong.com/posts/gLDSqQm8pwNiq7qst/narrow-misalignment-is-hard-emergent-misalignment-is-easy
      authors: Edward Turner, Anna Soligo, Senthooran Rajamanoharan, Neel Nanda
      year: 2024
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - title: Aesthetic Preferences Can Cause Emergent Misalignment
      url: https://lesswrong.com/posts/gT3wtWBAs7PKonbmy/aesthetic-preferences-can-cause-emergent-misalignment
      authors: Anders Woodruff
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: "Moloch's Bargain: Emergent Misalignment When LLMs Compete for Audiences"
      url: https://arxiv.org/abs/2510.06105
      authors: Batu El, James Zou
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Emergent Misalignment & Realignment
      url: https://lesswrong.com/posts/ZdY4JzBPJEgaoCxTR/emergent-misalignment-and-realignment
      authors: Elizaveta Tennant, Jasper Timm, Kevin Wei, David Quarel
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Realistic Reward Hacking Induces Different and Deeper Misalignment
      url: https://www.lesswrong.com/posts/HLJoJYi52mxgomujc/realistic-reward-hacking-induces-different-and-deeper-1
      authors: Jozdien
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: "Selective Generalization: Improving Capabilities While Maintaining Alignment"
      url: https://lesswrong.com/posts/ZXxY2tccLapdjLbKm/selective-generalization-improving-capabilities-while
      authors: Ariana Azarbal, Matthew A. Clarke, Jorio Cocola, Cailley Factor, Alex Cloud
      year: 2025
      venue: LessWrong/AI Alignment Forum
      kind: lesswrong
    - title: Emergent Misalignment on a Budget
      url: https://lesswrong.com/posts/qHudHZNLCiFrygRiy/emergent-misalignment-on-a-budget
      authors: Valerio Pepe, Armaan Tipirneni
      year: 2025
      venue: LessWrong/AI Alignment Forum
      kind: lesswrong
    - title: The Rise of Parasitic AI
      url: https://www.lesswrong.com/posts/6ZnznCaTcbGYsCmqu/the-rise-of-parasitic-ai
      authors: Adele Lopez
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: LLM AGI may reason about its goals and discover misalignments by default
      url: https://lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover
      authors: Seth Herd
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Open problems in emergent misalignment
      url: https://lesswrong.com/posts/AcTEiu5wYDgrbmXow/open-problems-in-emergent-misalignment
      authors: Jan Betley, Daniel Tan
      year: 2025
      venue: LessWrong
      kind: lesswrong
- id: Model_specs_and_constitutions
  name: Model specs and constitutions
  parent: Model_psychology
  summary: >-
    Write detailed, natural language descriptions of values and rules for models to follow, then instill these values
    and rules into models via techniques like Constitutional AI or deliberative alignment.
  theoryOfChange: >-
    Model specs and constitutions serve three purposes. First, they provide a clear standard of behavior which can be
    used to *train* models to value what we want them to value. Second, they serve as something closer to a ground truth
    standard for evaluating the degree of misalignment ranging from  "models straightforwardly obey the spec" to "models
    flagrantly disobey the spec". A combination of scalable stress-testing and reinforcement for obedience can be used
    to iteratively reduce the risk of misalignment. Third, they get more useful as models' instruction-following
    capability improves.
  seeAlso: sec:Iterative_alignment, sec:Model_psychology
  orthodoxProblems:
    - "1"
  targetCase: average-case
  broadApproaches:
    - engineering
  someNames:
    - amanda-askell
    - joe-carlsmith
  critiques:
    - >-
      [LLM AGI may reason about its goals and discover misalignments by
      default](https://www.alignmentforum.org/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover),
      [On OpenAI's Model Spec 2.0](https://thezvi.wordpress.com/2025/02/21/on-openais-model-spec-2-0/), [Giving AIs safe
      motivations (esp. Sections
      4.3-4.5)](https://joecarlsmith.com/2025/08/18/giving-ais-safe-motivations#4-5-step-4-good-instructions), [On
      Deliberative Alignment](https://thezvi.substack.com/p/on-deliberative-alignment)
  fundedByText: major funders include Anthropic and OpenAI (internally)
  papers:
    - title: Claude's Constitution
      url: https://www.anthropic.com/news/claudes-constitution
      year: 2023
      venue: Anthropic News
      kind: blog_post
    - title: Gemini-2.5-Pro-04-18-2025 System Prompt
      url: https://github.com/elder-plinius/CL4R1T4S/blob/main/GOOGLE/Gemini-2.5-Pro-04-18-2025.md
      year: 2025
      venue: GitHub
      kind: other
    - title: "Deliberative Alignment: Reasoning Enables Safer Language Models"
      url: https://arxiv.org/abs/2412.16339
      authors: >-
        Melody Y. Guan, Manas Joglekar, Eric Wallace, Saachi Jain, Boaz Barak, Alec Helyar, Rachel Dias, Andrea Vallone,
        Hongyu Ren, Jason Wei, Hyung Won Chung, Sam Toyer, Johannes Heidecke, Alex Beutel, Amelia Glaese
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: Stress-Testing Model Specs Reveals Character Differences among Language Models
      url: https://arxiv.org/abs/2510.07686
      authors: Jifan Zhang, Henry Sleight, Andi Peng, John Schulman, Esin Durmus
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: OpenAI Model Spec
      url: https://model-spec.openai.com/
      year: 2025
      venue: OpenAI Website
      kind: agenda_manifesto
    - title: Let Them Down Easy! Contextual Effects of LLM Guardrails on User Perceptions and Preferences
      url: https://arxiv.org/abs/2506.00195
      authors: >-
        Mingqian Zheng, Wenjia Hu, Patrick Zhao, Motahhare Eslami, Jena D. Hwang, Faeze Brahman, Carolyn Rose, Maarten
        Sap
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: No-self as an alignment target
      url: https://lesswrong.com/posts/LSJx5EnQEW6s5Juw6/no-self-as-an-alignment-target
      authors: Milan W
      venue: LessWrong
      kind: lesswrong
    - title: Six Thoughts on AI Safety
      url: https://lesswrong.com/posts/3jnziqCF3vA2NXAKp/six-thoughts-on-ai-safety
      authors: Boaz Barak
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: How important is the model spec if alignment fails?
      url: https://newsletter.forethought.org/p/how-important-is-the-model-spec-if
      authors: Mia Taylor
      year: 2025
      venue: ForeWord (Substack)
      kind: blog_post
    - title: Political Neutrality in AI Is Impossible- But Here Is How to Approximate It
      url: https://arxiv.org/abs/2503.05728
      authors: >-
        Jillian Fisher, Ruth E. Appel, Chan Young Park, Yujin Potter, Liwei Jiang, Taylor Sorensen, Shangbin Feng, Yulia
        Tsvetkov, Margaret E. Roberts, Jennifer Pan, Dawn Song, Yejin Choi
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Giving AIs safe motivations
      url: https://joecarlsmith.com/2025/08/18/giving-ais-safe-motivations#4-5-step-4-good-instructions
      authors: Joe Carlsmith
      year: 2025
      venue: joecarlsmith.com
      kind: blog_post
- id: Model_psychopathology
  name: Model psychopathology
  parent: Model_psychology
  summary: >-
    Find interesting LLM phenomena like glitch [tokens](https://vgel.me/posts/seahorse/) and the reversal curse; these
    are vital data for theory.
  theoryOfChange: >-
    The study of 'pathological' phenomena in LLMs is potentially key for theoretically modelling LLM cognition and LLM
    training-dynamics (compare: the study of aphasia and visual processing disorder in humans plays a key role cognitive
    science), and in particular for developing a good theory of generalization in LLMS
  seeAlso: a:Emergent_misalignment, mechanistic anomaly detection
  orthodoxProblems:
    - "4"
  targetCase: pessimistic
  someNames:
    - janus
    - truthful-ai
    - theia-vogel
    - stewart-slocum
    - nell-watson
    - samuel-g-b-johnson
    - liwei-jiang
    - monika-jotautaite
    - saloni-dash
  estimatedFTEs: 5-20
  fundedByText: Coefficient Giving (via Truthful AI and Interpretability grants)
  papers:
    - title: "Subliminal Learning: Language models transmit behavioral traits via hidden signals in data"
      url: https://arxiv.org/abs/2507.14805
      authors: Alex Cloud, Minh Le, James Chua, Jan Betley, Anna Sztyber-Betley, Jacob Hilton, Samuel Marks, Owain Evans
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: LLMs Can Get "Brain Rot"!
      url: https://arxiv.org/abs/2510.13928
      authors: Shuo Xing, Junyuan Hong, Yifan Wang, Runjin Chen, Zhenyu Zhang, Ananth Grama, Zhengzhong Tu, Zhangyang Wang
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning
      url: https://arxiv.org/abs/2506.20020
      authors: Saloni Dash, Amélie Reymond, Emma S. Spiro, Aylin Caliskan
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Unified Multimodal Models Cannot Describe Images From Memory
      url: https://spylab.ai/blog/modal-aphasia
      authors: Michael Aerni, Joshua Swanson, Kristina Nikolić, Florian Tramèr
      year: 2024
      venue: SPY Lab Blog
      kind: blog_post
    - title: "Believe It or Not: How Deeply do LLMs Believe Implanted Facts?"
      url: https://arxiv.org/abs/2510.17941
      authors: Stewart Slocum, Julian Minder, Clément Dumas, Henry Sleight, Ryan Greenblatt, Samuel Marks, Rowan Wang
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: >-
        Psychopathia Machinalis: A Nosological Framework for Understanding Pathologies in Advanced Artificial
        Intelligence
      url: https://www.psychopathia.ai/
      authors: Nell Watson, Ali Hessami
      year: 2025
      venue: Electronics (MDPI)
      kind: paper_published
    - title: "Imagining and building wise machines: The centrality of AI metacognition"
      url: https://arxiv.org/abs/2411.02478
      authors: >-
        Samuel G. B. Johnson, Amir-Hossein Karimi, Yoshua Bengio, Nick Chater, Tobias Gerstenberg, Kate Larson, Sydney
        Levine, Melanie Mitchell, Iyad Rahwan, Bernhard Schölkopf, Igor Grossmann
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: "Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)"
      url: https://arxiv.org/abs/2510.22954
      authors: >-
        Liwei Jiang, Yuanjun Chai, Margaret Li, Mickel Liu, Raymond Fok, Nouha Dziri, Yulia Tsvetkov, Maarten Sap, Alon
        Albalak, Yejin Choi
      year: 2025
      venue: arXiv (accepted to NeurIPS 2025 D&B - Oral)
      kind: paper_preprint
    - title: "Beyond One-Way Influence: Bidirectional Opinion Dynamics in Multi-Turn Human-LLM Interactions"
      url: https://arxiv.org/abs/2510.20039
      authors: Yuyang Jiang, Longjie Guo, Yuchen Wu, Aylin Caliskan, Tanu Mitra, Hua Shen
      year: 2025
      venue: arXiv
      kind: paper_preprint
- id: Data_filtering
  name: Data filtering
  parent: Better_data
  summary: >-
    Builds safety into models from the start by removing harmful or toxic content (like dual-use info) from the
    pretraining data, rather than relying only on post-training alignment.
  theoryOfChange: >-
    By curating the pretraining data, we can prevent the model from learning dangerous capabilities (e.g., dual-use
    info) or undesirable behaviors (e.g., toxicity) in the first place, making safety more robust and "tamper-resistant"
    than post-training patches.
  seeAlso: >-
    a:Data_quality_for_alignment, a:Data_poisoning_defense, a:Synthetic_data_for_alignment,
    a:Capability_removal_unlearning
  orthodoxProblems:
    - "4"
    - "1"
  targetCase: average-case
  broadApproaches:
    - engineering
  someNames:
    - yanda-chen
    - pratyush-maini
    - kyle-obrien
    - stephen-casper
    - simon-pepin-lehalleur
    - jesse-hoogland
    - himanshu-beniwal
    - sachin-goyal
    - mycal-tucker
    - dylan-sam
  estimatedFTEs: 10-50
  critiques:
    - >-
      [When Bad Data Leads to Good Models](https://arxiv.org/pdf/2505.04741), [Medical large language models are
      vulnerable to data-poisoning attacks](https://www.nature.com/articles/s41591-024-03445-1)
  fundedByText: Anthropic, various academics
  papers:
    - title: Enhancing Model Safety through Pretraining Data Filtering
      url: https://alignment.anthropic.com/2025/pretraining-data-filtering/
      authors: >-
        Yanda Chen, Mycal Tucker, Nina Panickssery, Tony Wang, Francesco Mosconi, Anjali Gopal, Carson Denison, Linda
        Petrini, Jan Leike, Ethan Perez, Mrinank Sharma
      year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - title: "Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs"
      url: https://arxiv.org/abs/2508.06601
      authors: >-
        Kyle O'Brien, Stephen Casper, Quentin Anthony, Tomek Korbak, Robert Kirk, Xander Davies, Ishan Mishra, Geoffrey
        Irving, Yarin Gal, Stella Biderman
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Safety Pretraining: Toward the Next Generation of Safe AI"
      url: https://arxiv.org/abs/2504.16980
      authors: >-
        Pratyush Maini, Sachin Goyal, Dylan Sam, Alex Robey, Yash Savani, Yiding Jiang, Andy Zou, Matt Fredrikson,
        Zacharcy C. Lipton, J. Zico Kolter
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation Models
      url: https://arxiv.org/abs/2510.27629v2
      authors: >-
        Boyi Wei, Zora Che, Nathaniel Li, Udari Madhushani Sehwag, Jasper Götting, Samira Nedungadi, Julian Michael,
        Summer Yue, Dan Hendrycks, Peter Henderson, Zifan Wang, Seth Donoughe, Mantas Mazeika
      year: 2025
      venue: arXiv
      kind: paper_preprint
- id: Hyperstition_studies
  name: Hyperstition studies
  parent: Better_data
  summary: >-
    Study, steer, and intervene on the following feedback loop: "we produce stories about how present and future AI
    systems behave" → "these stories become training data for the AI" → "these stories shape how AI systems in fact
    behave".
  theoryOfChange: >-
    Measure the influence of existing AI narratives in the training data → seed and develop more salutary ontologies and
    self-conceptions for AI models → control and redirect AI models' self-concepts through selectively amplifying
    certain components of the training data.
  seeAlso: a:Data_filtering, [active inference](https://arxiv.org/abs/2311.10215), LLM whisperers
  orthodoxProblems:
    - "1"
  targetCase: average-case
  broadApproaches:
    - cognitive
  someNames:
    - alex-turner
    - hyperstition-aihttpswwwhyperstitionaicom
    - kyle-obrien
  estimatedFTEs: 1-10
  fundedByText: Unclear, niche
  papers:
    - title: Training on Documents About Reward Hacking Induces Reward Hacking
      url: https://www.lesswrong.com/posts/qXYLvjGL9QvD3aFSW/training-on-documents-about-reward-hacking-induces-reward
      authors: Evan Hubinger, Nathan Hu
      year: 2025
      venue: LessWrong/AI Alignment Forum
      kind: lesswrong
    - title: Do Not Tile the Lightcone with Your Confused Ontology
      url: https://www.lesswrong.com/posts/Y8zS8iG5HhqKcQBtA/do-not-tile-the-lightcone-with-your-confused-ontology
      authors: Jan_Kulveit
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Self-Fulfilling Misalignment Data Might Be Poisoning Our AI Models
      url: https://turntrout.com/self-fulfilling-misalignment
      authors: Alex Turner
      year: 2025
      venue: turntrout.com
      kind: blog_post
    - title: "Existential Conversations with Large Language Models: Content, Community, and Culture"
      url: https://arxiv.org/abs/2411.13223
      authors: Murray Shanahan, Beth Singler
      year: 2024
      venue: arXiv
      kind: paper_preprint
- id: Data_poisoning_defense
  name: Data poisoning defense
  parent: Better_data
  summary: >-
    Develops methods to detect and prevent malicious or backdoor-inducing samples from being included in the training
    data.
  theoryOfChange: >-
    By identifying and filtering out malicious training examples, we can prevent attackers from creating hidden
    backdoors or triggers that would cause aligned models to behave dangerously.
  seeAlso: a:Data_filtering, a:Safeguards_inference_time_auxiliaries_, a:Various_Redteams, adversarial robustness
  orthodoxProblems:
    - "8"
    - "11"
  targetCase: pessimistic
  broadApproaches:
    - engineering
  someNames:
    - alexandra-souly
    - javier-rando
    - ed-chapman
    - hanna-foerster
    - ilia-shumailov
    - yiren-zhao
  estimatedFTEs: 5-20
  critiques:
    - >-
      [A small number of samples can poison LLMs of any size](https://arxiv.org/abs/2510.04567), [Reasoning Introduces
      New Poisoning Attacks Yet Makes Them More Complicated](https://arxiv.org/abs/2509.03405)
  fundedByText: Google DeepMind, Anthropic, University of Cambridge, Vector Institute
  papers:
    - title: A small number of samples can poison LLMs
      url: https://example-blog.com/a-small-number-of-samples-can-poison-llms
      kind: error_detected
    - title: "LMEnt: A Suite for Analyzing Knowledge in Language Models from Pretraining Data to Representations"
      url: https://arxiv.org/abs/2509.03405
      authors: Daniela Gottesman, Alon Gilae-Dotan, Ido Cohen, Yoav Gur-Arieh, Marius Mosbach, Ori Yoran, Mor Geva
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "GILT: An LLM-Free, Tuning-Free Graph Foundational Model for In-Context Learning"
      url: https://arxiv.org/abs/2510.04567
      authors: Weishuo Ma, Yanbo Wang, Xiyuan Wang, Lei Zou, Muhan Zhang
      year: 2025
      venue: arXiv
      kind: paper_preprint
- id: Synthetic_data_for_alignment
  name: Synthetic data for alignment
  parent: Better_data
  summary: >-
    Uses AI-generated data (e.g., critiques, preferences, or self-labeled examples) to scale and improve alignment,
    especially for superhuman models.
  theoryOfChange: >-
    We can overcome the bottleneck of human feedback and data by using models to generate vast amounts of high-quality,
    targeted data for safety, preference tuning, and capability elicitation.
  seeAlso: >-
    a:Data_quality_for_alignment, a:Data_filtering, scalable oversight, automated alignment research,
    a:Weak_to_strong_generalization
  orthodoxProblems:
    - "4"
    - "7"
    - "1"
  targetCase: average-case
  broadApproaches:
    - engineering
  someNames:
    - mianqiu-huang
    - xiaoran-liu
    - rylan-schaeffer
    - nevan-wichers
    - aram-ebtekar
    - jiaxin-wen
    - vishakh-padmakumar
    - benjamin-newman
  estimatedFTEs: 50-150
  critiques:
    - >-
      [Synthetic Data in AI: Challenges, Applications, and Ethical Implications](https://arxiv.org/abs/2401.01629). Sort
      of [Demski](https://www.lesswrong.com/posts/nQwbDPgYvAbqAmAud/llms-for-alignment-research-a-safety-priority).
  fundedByText: Anthropic, Google DeepMind, OpenAI, Meta AI, various academic groups.
  papers:
    - title: Aligning Large Language Models via Fully Self-Synthetic Data
      url: https://arxiv.org/abs/2510.06652
      authors: Shangjian Yin, Zhepei Wei, Xinyu Zhu, Wei-Lin Chen, Yu Meng
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Synth-Align: Improving Trustworthiness in Vision-Language Model with Synthetic Preference Data Alignment"
      url: https://arxiv.org/html/2412.17417v2
      authors: Robert Wijaya, Ngoc-Bao Nguyen, Ngai-Man Cheung
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: >-
        Carleman Estimates and Controllability of Forward Stochastic Parabolic Equations with General Dynamic Boundary
        Conditions
      url: https://arxiv.org/abs/2510.12345
      authors: Said Boulite, Abdellatif Elgrou, Lahcen Maniar, Abdelaziz Rhandi
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Numerical Investigation of Sequence Modeling Theory using Controllable Memory Functions
      url: https://arxiv.org/abs/2506.05678
      authors: Haotian Jiang, Zeyu Bao, Shida Wang, Qianxiao Li
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: On the Realization of quantum gates coming from the Tracy-Singh product
      url: https://arxiv.org/abs/2412.02345
      authors: Fabienne Chouraqui
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: Sharp uniform approximation for spectral Barron functions by deep neural networks
      url: https://arxiv.org/abs/2507.06789
      authors: Yulei Liao, Pingbing Ming, Hao Yu
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "OGBoost: A Python Package for Ordinal Gradient Boosting"
      url: https://arxiv.org/abs/2502.13456
      authors: Mansour T.A. Sharabiani, Alex Bottle, Alireza S. Mahani
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "GRADEO: Towards Human-Like Evaluation for Text-to-Video Generation via Multi-Step Reasoning"
      url: https://arxiv.org/abs/2503.02341
      authors: Zhun Mou, Bin Xia, Zhengchao Huang, Wenming Yang, Jiaya Jia
      year: 2025
      venue: arXiv
      kind: paper_preprint
- id: Data_quality_for_alignment
  name: Data quality for alignment
  parent: Better_data
  summary: Improves the quality, signal-to-noise ratio, and reliability of human-generated preference and alignment data.
  theoryOfChange: >-
    The quality of alignment is heavily dependent on the quality of the data (e.g., human preferences); by improving the
    "signal" from annotators and reducing noise/bias, we will get more robustly aligned models.
  seeAlso: >-
    a:Synthetic_data_for_alignment, scalable oversight, a:Assistance_games_assistive_agents,
    a:Model_values_model_preferences
  orthodoxProblems:
    - "7"
    - "1"
  targetCase: average-case
  broadApproaches:
    - engineering
  someNames:
    - maarten-buyl
    - kelsey-kraus
    - margaret-kroll
    - danqing-shi
  estimatedFTEs: 20-50
  critiques:
    - "[A Statistical Case Against Empirical Human-AI Alignment](https://arxiv.org/abs/2502.14581)"
  fundedByText: Anthropic, Google DeepMind, OpenAI, Meta AI, various academic groups
  papers:
    - title: AI Alignment at Your Discretion
      url: https://arxiv.org/abs/2502.10441
      authors: >-
        Maarten Buyl, Hadi Khalaf, Claudio Mayrink Verdun, Lucas Monteiro Paes, Caio C. Vieira Machado, Flavio du Pin
        Calmon
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Maximizing Signal in Human-Model Preference Alignment
      url: https://arxiv.org/abs/2503.04910
      authors: Kelsey Kraus, Margaret Kroll
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "DxHF: Providing High-Quality Human Feedback for LLM Alignment via Interactive Decomposition"
      url: https://arxiv.org/abs/2507.18802
      authors: Danqing Shi, Furui Cheng, Tino Weinkauf, Antti Oulasvirta, Mennatallah El-Assady
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Challenges and Future Directions of Data-Centric AI Alignment
      url: https://arxiv.org/html/2410.01957v2
      authors: Min-Hsuan Yeh, Jeffrey Wang, Xuefeng Du, Seongheon Park, Leitian Tao, Shawn Im, Yixuan Li
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: You Are What You Eat -- AI Alignment Requires Understanding How Data Shapes Structure and Generalisation
      url: https://arxiv.org/abs/2502.05475
      authors: >-
        Simon Pepin Lehalleur, Jesse Hoogland, Matthew Farrugia-Roberts, Susan Wei, Alexander Gietelink Oldenziel,
        George Wang, Liam Carroll, Daniel Murfet
      year: 2025
      venue: arXiv
      kind: paper_preprint
- id: Mild_optimisation
  name: Mild optimisation
  parent: Goal_robustness
  summary: Avoid Goodharting by getting AI to satisfice rather than maximise.
  theoryOfChange: >-
    If we fail to exactly nail down the preferences for a superintelligent agent we die to Goodharting → shift from
    maximising to satisficing in the agent's utility function → we get a nonzero share of the lightcone as opposed to
    zero; also, moonshot at this being the recipe for fully aligned AI.
  orthodoxProblems:
    - "1"
  broadApproaches:
    - cognitive
  estimatedFTEs: 10-50
  fundedByText: Google DeepMind
  papers:
    - title: "MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step Reward Hacking"
      url: https://arxiv.org/abs/2501.13011
      authors: Sebastian Farquhar, Vikrant Varma, David Lindner, David Elson, Caleb Biddulph, Ian Goodfellow, Rohin Shah
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: >-
        BioBlue: Notable runaway-optimiser-like LLM failure modes on biologically and economically aligned AI safety
        benchmarks for LLMs with simplified observation format
      url: https://arxiv.org/abs/2509.02655
      authors: Roland Pihlakas, Sruthi Kuriakose
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: >-
        Why modelling multi-objective homeostasis is essential for AI alignment (and how it helps with AI safety as
        well). Subtleties and Open Challenges.
      url: https://lesswrong.com/posts/vGeuBKQ7nzPnn5f7A/why-modelling-multi-objective-homeostasis-is-essential-for
      authors: Roland Pihlakas
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: >-
        From homeostasis to resource sharing: Biologically and economically aligned multi-objective multi-agent
        gridworld-based AI safety benchmarks
      url: https://arxiv.org/abs/2410.00081
      authors: Roland Pihlakas
      year: 2024
      venue: arXiv
      kind: paper_preprint
- id: RL_safety
  name: RL safety
  parent: Goal_robustness
  summary: >-
    Improves the robustness of reinforcement learning agents by addressing core problems in reward learning, goal
    misgeneralization, and specification gaming.
  theoryOfChange: >-
    Standard RL objectives (like maximizing expected value) are brittle and lead to goal misgeneralization or
    specification gaming; by developing more robust frameworks (like pessimistic RL, minimax regret, or provable inverse
    reward learning), we can create agents that are safe even when misspecified.
  seeAlso: >-
    a:Behavior_alignment_theory, a:Assistance_games_assistive_agents, sec:Goal_robustness, sec:Iterative_alignment,
    a:Mild_optimisation, scalable oversight, [The Theoretical Reward Learning Research Agenda: Introduction and
    Motivation](https://www.alignmentforum.org/posts/pJ3mDD7LfEwp3s5vG/the-theoretical-reward-learning-research-agenda-introduction)
  orthodoxProblems:
    - "4"
    - "1"
    - "7"
  targetCase: pessimistic
  broadApproaches:
    - engineering
  someNames:
    - joar-skalse
    - karim-abdel-sadek
    - matthew-farrugia-roberts
    - benjamin-plaut
    - fang-wu
    - stephen-zhao
    - alessandro-abate
    - steven-byrnes
    - michael-cohen
  estimatedFTEs: 20-70
  critiques:
    - >-
      ["The Era of Experience" has an unsolved technical alignment
      problem](https://www.lesswrong.com/posts/747f6b8e/the-era-of-experience-has-an-unsolved-technical-alignment-problem),
      [The Invisible Leash: Why RLVR May or May Not Escape Its Origin](https://arxiv.org/abs/2507.14843)
  fundedByText: Google DeepMind, University of Oxford, CMU, Coefficient Giving
  papers:
    - title: "The Perils of Optimizing Learned Reward Functions: Low Training Error Does Not Guarantee Low Regret"
      url: https://arxiv.org/abs/2406.15753
      authors: Lukas Fluri, Leon Lang, Alessandro Abate, Patrick Forré, David Krueger, Joar Skalse
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: Safe Learning Under Irreversible Dynamics via Asking for Help
      url: https://arxiv.org/abs/2502.14043
      authors: Benjamin Plaut, Juan Liévano-Karim, Hanlin Zhu, Stuart Russell
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Mitigating Goal Misgeneralization via Minimax Regret
      url: https://arxiv.org/abs/2507.03068
      authors: >-
        Karim Abdel Sadek, Matthew Farrugia-Roberts, Usman Anwar, Hannah Erlebach, Christian Schroeder de Witt, David
        Krueger, Michael Dennis
      year: 2025
      venue: RLC 2025
      kind: paper_published
    - title: "Rethinking Reward Model Evaluation: Are We Barking up the Wrong Tree?"
      url: https://arxiv.org/abs/2410.05584
      authors: Xueru Wen, Jie Lou, Yaojie Lu, Hongyu Lin, Xing Yu, Xinyu Lu, Ben He, Xianpei Han, Debing Zhang, Le Sun
      year: 2024
      venue: arXiv (Accepted at ICLR 2025 Spotlight)
      kind: paper_preprint
    - title: "The Invisible Leash: Why RLVR May or May Not Escape Its Origin"
      url: https://arxiv.org/abs/2507.14843
      authors: Fang Wu, Weihao Xuan, Ximing Lu, Mingjie Liu, Yi Dong, Zaid Harchaoui, Yejin Choi
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Reducing the Probability of Undesirable Outputs in Language Models Using Probabilistic Inference
      url: https://arxiv.org/abs/2510.21184
      authors: Stephen Zhao, Aidan Li, Rob Brekelmans, Roger Grosse
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Interpreting Emergent Planning in Model-Free Reinforcement Learning
      url: https://arxiv.org/abs/2504.01871
      authors: Thomas Bush, Stephen Chung, Usman Anwar, Adrià Garriga-Alonso, David Krueger
      year: 2025
      venue: arXiv (ICLR 2025 oral)
      kind: paper_preprint
    - title: Misalignment from Treating Means as Ends
      url: https://arxiv.org/abs/2507.10995
      authors: Henrik Marklund, Alex Infanger, Benjamin Van Roy
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "\"The Era of Experience\" has an unsolved technical alignment problem"
      url: https://lesswrong.com/posts/TCGgiJAinGgcMEByt/the-era-of-experience-has-an-unsolved-technical-alignment
      authors: Steven Byrnes
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Safety cases for Pessimism
      url: https://lesswrong.com/posts/CpftMXCEnwqbWreHD/safety-cases-for-pessimism
      authors: Michael Cohen
      venue: LessWrong
      kind: lesswrong
    - title: We need a field of Reward Function Design
      url: https://www.lesswrong.com/posts/oxvnREntu82tffkYW/we-need-a-field-of-reward-function-design
      authors: Steven Byrnes
      year: 2025
      venue: LessWrong
      kind: lesswrong
- id: Assistance_games_assistive_agents
  name: Assistance games, assistive agents
  parent: Goal_robustness
  summary: >-
    Formalize how AI assistants learn about human preferences given uncertainty and partial observability, and construct
    environments which better incentivize AIs to learn what we want them to learn.
  theoryOfChange: >-
    Understand what kinds of things can go wrong when humans are directly involved in training a model → build tools
    that make it easier for a model to learn what humans want it to learn.
  seeAlso: a:Guaranteed_Safe_AI
  orthodoxProblems:
    - "1"
    - "10"
  someNames:
    - joar-skalse
    - anca-dragan
    - caspar-oesterheld
    - david-krueger
    - dylan-hafield-menell
    - stuart-russell
  critiques:
    - >-
      [nice
      summary](https://www.lesswrong.com/posts/i5kijcjFJD6bn7dwq/evaluating-the-historical-value-misspecification-argument)
      of historical problem statements
  fundedByText: >-
    Future of Life Institute, Coefficient Giving, Survival and Flourishing Fund, Cooperative AI Foundation, Polaris
    Ventures
  papers:
    - title: Training LLM Agents to Empower Humans
      url: https://arxiv.org/pdf/2510.13709
      authors: Evan Ellis, Vivek Myers, Jens Tuyls, Sergey Levine, Anca Dragan, Benjamin Eysenbach
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Murphys Laws of AI Alignment: Why the Gap Always Wins"
      url: https://arxiv.org/abs/2509.05381
      authors: Madhava Gaikwad
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "AssistanceZero: Scalably Solving Assistance Games"
      url: https://arxiv.org/abs/2504.07091
      authors: >-
        Cassidy Laidlaw, Eli Bronstein, Timothy Guo, Dylan Feng, Lukas Berglund, Justin Svegliato, Stuart Russell, Anca
        Dragan
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Observation Interference in Partially Observable Assistance Games
      url: https://arxiv.org/abs/2412.17797
      authors: Scott Emmons, Caspar Oesterheld, Vincent Conitzer, Stuart Russell
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: Learning to Assist Humans without Inferring Rewards
      url: https://arxiv.org/abs/2411.02623
      authors: Vivek Myers, Evan Ellis, Sergey Levine, Benjamin Eysenbach, Anca Dragan
      year: 2024
      venue: NeurIPS 2024
      kind: paper_published
- id: Harm_reduction_for_open_weights
  name: Harm reduction for open weights
  parent: Goal_robustness
  summary: >-
    Develops methods, primarily based on pretraining data intervention, to create tamper-resistant safeguards that
    prevent open-weight models from being maliciously fine-tuned to remove safety features or exploit dangerous
    capabilities.
  theoryOfChange: >-
    Open-weight models allow adversaries to easily remove post-training safety (like refusal training) via simple
    fine-tuning; by making safety an intrinsic property of the model's learned knowledge and capabilities (e.g., by
    ensuring "deep ignorance" of dual-use information), the safeguards become far more difficult and expensive to
    remove.
  seeAlso: a:Data_filtering, a:Capability_removal_unlearning, a:Data_poisoning_defense
  orthodoxProblems:
    - "11"
  targetCase: average-case
  broadApproaches:
    - engineering
  someNames:
    - kyle-obrien
    - stephen-casper
    - quentin-anthony
    - tomek-korbak
    - rishub-tamirisa
    - mantas-mazeika
    - stella-biderman
    - yarin-gal
  estimatedFTEs: 10-100
  fundedByText: UK AI Safety Institute (AISI), EleutherAI, Coefficient Giving
  papers:
    - title: "Deep ignorance: Filtering pretraining data builds tamper-resistant safeguards into open-weight LLMs"
      url: >-
        https://www.aisi.gov.uk/research/deep-ignorance-filtering-pretraining-data-builds-tamper-resistant-safeguards-into-open-weight-llms
      authors: >-
        Kyle O'Brien, Stephen Casper, Quentin Anthony, Tomek Korbak, Robert Kirk, Xander Davies, Ishan Mishra, Geoffrey
        Irving, Yarin Gal, Stella Biderman
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Tamper-Resistant Safeguards for Open-Weight LLMs
      url: https://arxiv.org/abs/2408.00761
      authors: >-
        Rishub Tamirisa, Bhrugu Bharathi, Long Phan, Andy Zhou, Alice Gatti, Tarun Suresh, Maxwell Lin, Justin Wang,
        Rowan Wang, Ron Arel, Andy Zou, Dawn Song, Bo Li, Dan Hendrycks, Mantas Mazeika
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: Open Technical Problems in Open-Weight AI Model Risk Management
      url: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5705186
      authors: >-
        Stephen Casper, Kyle O'Brien, Shayne Longpre, Elizabeth Seger, Kevin Klyman, Rishi Bommasani, Aniruddha
        Nrusimha, Ilia Shumailov, Sören Mindermann, Steven Basart, Frank Rudzicz, Kellin Pelrine, Avijit Ghosh, Andrew
        Strait, Robert Kirk, Dan Hendrycks, Peter Henderson, J. Zico Kolter, Geoffrey Irving, Yarin Gal, Yoshua Bengio,
        Dylan Hadfield-Menell
      year: 2025
      venue: SSRN
      kind: paper_preprint
    - title: >-
        A Different Approach to AI Safety: Proceedings from the Columbia Convening on Openness in Artificial
        Intelligence and AI Safety
      url: https://arxiv.org/pdf/2506.22183
      authors: >-
        Camille François, Ludovic Péran, Ayah Bdeir, Nouha Dziri, Will Hawkins, Yacine Jernite, Sayash Kapoor, Juliet
        Shen, Heidy Khlaaf, Kevin Klyman, Nik Marda, Marie Pellat, Deb Raji, Divya Siddarth, Aviya Skowron, Joseph
        Spisak, Madhulika Srikumar, Victor Storchan, Audrey Tang, Jen Weedon
      year: 2025
      venue: arXiv
      kind: agenda_manifesto
    - title: Open Foundation Model Risk Mitigation
      url: >-
        https://partnershiponai.org/wp-content/uploads/dlm_uploads/2024/07/open-foundation-model-risk-mitigation_rev3-1.pdf
      year: 2024
      venue: Partnership on AI
      kind: error_detected
- id: The_Neglected_Approaches_Approach
  name: The "Neglected Approaches" Approach
  parent: Goal_robustness
  summary: >-
    Agenda-agnostic approaches to identifying good but overlooked empirical alignment ideas, working with theorists who
    could use engineers, and prototyping them.
  theoryOfChange: >-
    Empirical search for "negative alignment taxes" (prioritizing methods that simultaneously enhance alignment and
    capabilities)
  seeAlso: >-
    sec:Iterative_alignment, automated alignment research, Beijing Key Laboratory of Safe AI and Superalignment, Aligned
    AI
  orthodoxProblems:
    - "11"
  targetCase: average-case
  broadApproaches:
    - engineering
  someNames:
    - ae-studio
    - gunnar-zarncke
    - cameron-berg
    - michael-vaiana
    - judd-rosenblatt
    - diogo-schwerz-de-lucena
  estimatedFTEs: "15"
  critiques:
    - "[The 'Alignment Bonus' is a Dangerous Mirage](https://www.alignmentforum.org/posts/example-critique-neg-tax)"
  fundedByText: AE Studio
  papers:
    - title: Towards Safe and Honest AI Agents with Neural Self-Other Overlap
      url: https://arxiv.org/abs/2412.16325
      authors: Marc Carauleanu, Michael Vaiana, Judd Rosenblatt, Cameron Berg, Diogo Schwerz de Lucena
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: Momentum Point-Perplexity Mechanics in Large Language Models
      url: https://arxiv.org/abs/2508.08492
      authors: Lorenzo Tomaz, Judd Rosenblatt, Thomas Berry Jones, Diogo Schwerz de Lucena
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Large Language Models Report Subjective Experience Under Self-Referential Processing
      url: https://arxiv.org/abs/2510.24797
      authors: Cameron Berg, Diogo de Lucena, Judd Rosenblatt
      year: 2025
      venue: arXiv
      kind: paper_preprint
- id: Reverse_engineering
  name: Reverse engineering
  parent: White_box_safety
  summary: >-
    Decompose a model into its functional, interacting components (circuits), formally describe what computation those
    components perform, and validate their causal effects to reverse-engineer the model's internal algorithm.
  theoryOfChange: >-
    By gaining a mechanical understanding of how a model works (the "circuit diagram"), we can predict how models will
    act in novel situations (generalization), and gain the mechanistic knowledge necessary to safely modify an AI's
    goals or internal mechanisms, or allow for high-confidence alignment auditing and better feedback to safety
    researchers.
  seeAlso: >-
    [ambitious mech
    interp](https://www.alignmentforum.org/posts/Hy6PX43HGgmfiTaKu/an-ambitious-vision-for-interpretability)
  orthodoxProblems:
    - "4"
    - "7"
  targetCase: worst-case
  broadApproaches:
    - cognitive
  someNames:
    - lucius-bushnaq
    - dan-braun
    - lee-sharkey
    - aaron-mueller
    - atticus-geiger
    - sheridan-feucht
    - david-bau
    - yonatan-belinkov
    - stefan-heimersheim
    - chris-olah
    - leo-gao
  estimatedFTEs: 100-200
  critiques:
    - >-
      [Interpretability Will Not Reliably Find Deceptive
      AI](https://www.alignmentforum.org/posts/PwnadG4BFjaER3MGf/interpretability-will-not-reliably-find-deceptive-ai),
      [A Problem to Solve Before Building a Deception
      Detector](https://www.lesswrong.com/posts/YXNeA3RyRrrRWS37A/a-problem-to-solve-before-building-a-deception-detector),
      [MoSSAIC: AI Safety After Mechanism](https://openreview.net/forum?id=n7WYSJ35FU), [The Misguided Quest for
      Mechanistic AI
      Interpretability](https://ai-frontiers.org/articles/the-misguided-quest-for-mechanistic-ai-interpretability).
      [Mechanistic?](https://arxiv.org/abs/2410.09087), [Assessing skeptical views of interpretability
      research](https://www.youtube.com/watch?v=woo_J0RKcpQ), [Activation space interpretability may be
      doomed](https://www.lesswrong.com/posts/gYfpPbww3wQRaxAFD/activation-space-interpretability-may-be-doomed), [A
      Pragmatic Vision for
      Interpretability](https://www.alignmentforum.org/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-interpretability)
  papers:
    - title: "The Circuits Research Landscape: Results and Perspectives"
      url: https://www.neuronpedia.org/graph/info
      authors: >-
        Jack Lindsey, Emmanuel Ameisen, Neel Nanda, Stepan Shabalin, Mateusz Piotrowski, Tom McGrath, Michael Hanna,
        Owen Lewis, Curt Tigges, Jack Merullo, Connor Watts, Gonçalo Paulo, Joshua Batson, Liv Gorton, Elana Simon, Max
        Loeffler, Callum McDougall, Johnny Lin
      year: 2025
      venue: Neuronpedia
      kind: blog_post
    - title: "Circuits in Superposition: Compressing many small neural networks into one"
      url: https://www.lesswrong.com/posts/roE7SHjFWEoMcGZKd/circuits-in-superposition-compressing-many-small-neural
      authors: Lucius Bushnaq, jake_mendel
      year: 2024
      venue: LessWrong
      kind: lesswrong
    - title: "Circuits in Superposition 2: Now With Less Wrong Math"
      url: >-
        https://www.google.com/url?q=https://www.lesswrong.com/posts/FWkZYQceEzL84tNej/circuits-in-superposition-2-now-with-less-wrong-math&sa=D&source=docs&ust=1765550772146255&usg=AOvVaw334Tyidx2keGCA9vGwQ9a-
      venue: LessWrong
      kind: error_detected
    - title: Compressed Computation is (probably) not Computation in Superposition
      url: https://www.lesswrong.com/posts/ZxFchCFJFcgysYsT9/compressed-computation-is-probably-not-computation-in
      venue: LessWrong
      kind: error_detected
    - title: "MIB: A Mechanistic Interpretability Benchmark"
      url: https://arxiv.org/abs/2504.13151
      authors: >-
        Aaron Mueller, Atticus Geiger, Sarah Wiegreffe, Dana Arad, Iván Arcuschin, Adam Belfki, Yik Siu Chan, Jaden
        Fiotto-Kaufman, Tal Haklay, Michael Hanna, Jing Huang, Rohan Gupta, Yaniv Nikankin, Hadas Orgad, Nikhil Prakash,
        Anja Reusch, Aruna Sankaranarayanan, Shun Shao, Alessandro Stolfo, Martin Tutek, Amir Zur, David Bau, Yonatan
        Belinkov
      year: 2025
      venue: ICML 2025
      kind: paper_preprint
    - title: "RelP: Faithful and Efficient Circuit Discovery in Language Models via Relevance Patching"
      url: https://arxiv.org/abs/2508.21258
      authors: Farnoush Rezaei Jafari, Oliver Eberle, Ashkan Khakzar, Neel Nanda
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: The Dual-Route Model of Induction
      url: https://arxiv.org/abs/2504.03022
      authors: Sheridan Feucht, Eric Todd, Byron Wallace, David Bau
      year: 2025
      venue: COLM 2025
      kind: paper_published
    - title: "Structural Inference: Interpreting Small Language Models with Susceptibilities"
      url: https://arxiv.org/abs/2504.18274
      authors: Garrett Baker, George Wang, Jesse Hoogland, Daniel Murfet
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Stochastic Parameter Decomposition
      url: https://openreview.net/forum?id=dEdS9ao8gN
      authors: Dan Braun, Lucius Bushnaq, Lee Sharkey
      year: 2025
      venue: ODYSSEY 2025 Conference
      kind: paper_preprint
    - title: The Geometry of Self-Verification in a Task-Specific Reasoning Model
      url: https://arxiv.org/abs/2504.14379
      authors: Andrew Lee, Lihao Sun, Chris Wendler, Fernanda Viégas, Martin Wattenberg
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Converting MLPs into Polynomials in Closed Form
      url: https://arxiv.org/abs/2502.01032
      authors: Nora Belrose, Alice Rigg
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Extractive Structures Learned in Pretraining Enable Generalization on Finetuned Facts
      url: https://arxiv.org/abs/2412.04614
      authors: Jiahai Feng, Stuart Russell, Jacob Steinhardt
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: >-
        Interpretability in Parameter Space: Minimizing Mechanistic Description Length with Attribution-based Parameter
        Decomposition
      url: https://arxiv.org/abs/2501.14926
      authors: Dan Braun, Lucius Bushnaq, Stefan Heimersheim, Jake Mendel, Lee Sharkey
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Identifying Sparsely Active Circuits Through Local Loss Landscape Decomposition
      url: https://arxiv.org/abs/2504.00194
      authors: Brianna Chrisman, Lucius Bushnaq, Lee Sharkey
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: From Memorization to Reasoning in the Spectrum of Loss Curvature
      url: https://arxiv.org/abs/2510.24256
      authors: Jack Merullo, Srihita Vatsavaya, Lucius Bushnaq, Owen Lewis
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Generalization or Hallucination? Understanding Out-of-Context Reasoning in Transformers
      url: https://arxiv.org/abs/2506.10887
      authors: Yixiao Huang, Hanlin Zhu, Tianyu Guo, Jiantao Jiao, Somayeh Sojoudi, Michael I. Jordan, Stuart Russell, Song Mei
      year: 2025
      venue: NeurIPS 2025
      kind: paper_preprint
    - title: How Do LLMs Perform Two-Hop Reasoning in Context?
      url: https://arxiv.org/abs/2502.13913
      authors: Tianyu Guo, Hanlin Zhu, Ruiqi Zhang, Jiantao Jiao, Song Mei, Michael I. Jordan, Stuart Russell
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Blink of an eye: a simple theory for feature localization in generative models"
      url: https://arxiv.org/abs/2502.00921
      authors: Marvin Li, Aayush Karan, Sitan Chen
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "On the creation of narrow AI: hierarchy and nonlocality of neural network skills"
      url: https://arxiv.org/abs/2505.15811
      authors: Eric J. Michaud, Asher Parker-Sartori, Max Tegmark
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Interpreting Emergent Planning in Model-Free Reinforcement Learning
      url: https://arxiv.org/pdf/2504.01871
      authors: Thomas Bush, Stephen Chung, Usman Anwar, Adrià Garriga-Alonso, David Krueger
      year: 2025
      venue: arXiv (ICLR 2025 oral)
      kind: paper_preprint
    - title: Bridging the human–AI knowledge gap through concept discovery and transfer in AlphaZero
      url: https://www.pnas.org/doi/10.1073/pnas.2406675122
    - title: Building and evaluating alignment auditing agents
      url: https://lesswrong.com/posts/DJAZHYjWxMrcd2na3/building-and-evaluating-alignment-auditing-agents
      authors: Sam Marks, trentbrick, RowanWang, Sam Bowman, Euan Ong, Johannes Treutlein, evhub
      year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - title: How Do Transformers Learn Variable Binding in Symbolic Programs?
      url: https://arxiv.org/abs/2505.20896
      authors: Yiwei Wu, Atticus Geiger, Raphaël Millière
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Fresh in memory: Training-order recency is linearly encoded in language model activations"
      url: https://arxiv.org/abs/2509.14223
      authors: Dmitrii Krasheninnikov, Richard E. Turner, David Krueger
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Language Models use Lookbacks to Track Beliefs
      url: https://arxiv.org/abs/2505.14685
      authors: >-
        Nikhil Prakash, Natalie Shapira, Arnab Sen Sharma, Christoph Riedl, Yonatan Belinkov, Tamar Rott Shaham, David
        Bau, Atticus Geiger
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Constrained belief updates explain geometric structures in transformer representations
      url: https://arxiv.org/abs/2502.01954
      authors: Mateusz Piotrowski, Paul M. Riechers, Daniel Filan, Adam S. Shai
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: LLMs Process Lists With General Filter Heads
      url: https://arxiv.org/abs/2510.26784
      authors: Arnab Sen Sharma, Giordano Rogers, Natalie Shapira, David Bau
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Language Models Use Trigonometry to Do Addition
      url: https://arxiv.org/abs/2502.00873
      authors: Subhash Kantamneni, Max Tegmark
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Interpreting learned search: finding a transition model and value function in an RNN that plays Sokoban"
      url: https://arxiv.org/abs/2506.10138
      authors: Mohammad Taufeeque, Aaron David Tucker, Adam Gleave, Adrià Garriga-Alonso
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Transformers Struggle to Learn to Search
      url: https://arxiv.org/abs/2412.04703
      authors: >-
        Abulhair Saparov, Srushti Pawar, Shreyas Pimpalgaonkar, Nitish Joshi, Richard Yuanzhe Pang, Vishakh Padmakumar,
        Seyed Mehran Kazemi, Najoung Kim, He He
      year: 2024
      venue: ICLR 2025
      kind: paper_preprint
    - title: Adversarial Examples Are Not Bugs, They Are Superposition
      url: https://arxiv.org/abs/2508.17456
      authors: Liv Gorton, Owen Lewis
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Do Language Models Use Their Depth Efficiently?
      url: https://arxiv.org/abs/2505.13898
      authors: Róbert Csordás, Christopher D. Manning, Christopher Potts
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "ICLR: In-Context Learning of Representations"
      url: https://openreview.net/forum?id=pXlmOmlHJZ
      authors: >-
        Core Francisco Park, Andrew Lee, Ekdeep Singh Lubana, Yongyi Yang, Maya Okawa, Kento Nishi, Martin Wattenberg,
        Hidenori Tanaka
      year: 2025
      venue: ICLR 2025
      kind: paper_published
- id: Extracting_latent_knowledge
  name: Extracting latent knowledge
  parent: White_box_safety
  summary: >-
    Identify and decoding the "true" beliefs or knowledge represented inside a model's activations, even when the
    model's output is deceptive or false.
  theoryOfChange: >-
    Powerful models may know things they do not say (e.g. that they are currently being tested). If we can translate
    this latent knowledge directly from the model's internals, we can supervise them reliably even when they attempt to
    deceive human evaluators or when the task is too difficult for humans to verify directly.
  seeAlso: a:AI_explanations_of_AIs, a:Heuristic_explanations, a:Lie_and_deception_detectors
  orthodoxProblems:
    - "7"
  targetCase: worst-case
  broadApproaches:
    - cognitive
  someNames:
    - bartosz-cywiski
    - emil-ryd
    - senthooran-rajamanoharan
    - alexander-pan
    - lijie-chen
    - jacob-steinhardt
    - javier-ferrando
    - oscar-obeso
    - collin-burns
    - paul-christiano
  estimatedFTEs: 20-40
  critiques:
    - >-
      [A Problem to Solve Before Building a Deception
      Detector](https://www.lesswrong.com/posts/YXNeA3RyRrrRWS37A/a-problem-to-solve-before-building-a-deception-detector)
  fundedByText: Open Philanthropy, Anthropic, NSF, various academic grants
  papers:
    - title: Auditing language models for hidden objectives
      url: https://www.anthropic.com/research/auditing-hidden-objectives
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Eliciting Secret Knowledge from Language Models
      url: https://arxiv.org/abs/2510.01070
      authors: Bartosz Cywiński, Emil Ryd, Rowan Wang, Senthooran Rajamanoharan, Neel Nanda, Arthur Conmy, Samuel Marks
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Here's 18 Applications of Deception Probes
      url: https://www.lesswrong.com/posts/7zhAwcBri7yupStKy/here-s-18-applications-of-deception-probes
      authors: Cleo Nardo, Avi Parrack, jordine
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Towards eliciting latent knowledge from LLMs with mechanistic interpretability
      url: https://arxiv.org/pdf/2505.14352
      authors: Bartosz Cywiński, Emil Ryd, Senthooran Rajamanoharan, Neel Nanda
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "CCS-Lib: A Python package to elicit latent knowledge from LLMs"
      url: https://joss.theoj.org/papers/10.21105/joss.06511
      authors: >-
        Walter Laurito, Nora Belrose, Alex Mallen, Kay Kozaronek, Fabien Roger, Christy Koh, James Chua, Jonathan Ng,
        Alexander Wan, Reagan Lee, Ben W., Kyle O'Brien, Augustas Macijauskas, Eric Mungai Kinuthia, Marius Pl, Waree
        Sethapun, Kaarel Hänni
      year: 2025
      venue: Journal of Open Source Software
      kind: paper_published
    - title: "No Answer Needed: Predicting LLM Answer Accuracy from Question-Only Linear Probes"
      url: https://arxiv.org/abs/2509.10625
      authors: >-
        Iván Vicente Moreno Cencerrado, Arnau Padrés Masdemont, Anton Gonzalvez Hawthorne, David Demitri Africa, Lorenzo
        Pacchiardi
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "When Thinking LLMs Lie: Unveiling the Strategic Deception in Representations of Reasoning Models"
      url: https://arxiv.org/abs/2506.04909
      authors: Kai Wang, Yihao Zhang, Meng Sun
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Caught in the Act: a mechanistic approach to detecting deception"
      url: https://arxiv.org/abs/2508.19505
      authors: Gerard Boxo, Ryan Socha, Daniel Yoo, Shivam Raval
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: When Truthful Representations Flip Under Deceptive Instructions?
      url: https://arxiv.org/abs/2507.22149
      authors: Xianxuan Long, Yao Fu, Runchao Li, Mu Sheng, Haotian Yu, Xiaotian Han, Pan Li
      year: 2025
      venue: arXiv
      kind: paper_preprint
- id: Lie_and_deception_detectors
  name: Lie and deception detectors
  parent: White_box_safety
  summary: >-
    Detect when a model is being deceptive or lying by building white- or black-box detectors. Some work below requires
    intent in their definition, while other work focuses only on whether the model states something it believes to be
    false, regardless of intent.
  theoryOfChange: >-
    Such detectors could flag suspicious behavior during evaluations or deployment, augment training to reduce
    deception, or audit models pre-deployment. Specific applications include alignment evaluations (e.g. by validating
    answers to introspective questions), safeguarding evaluations (catching models that "sandbag", that is,
    strategically underperform to pass capability tests), and large-scale deployment monitoring. An honest version of a
    model could also provide oversight during training or detect cases where a model behaves in ways it understands are
    unsafe.
  seeAlso: a:Reverse_engineering, a:AI_deception_evals, a:Sandbagging_evals
  targetCase: pessimistic
  broadApproaches:
    - cognitive
  someNames:
    - cadenza
    - sam-marks
    - rowan-wang
    - kieron-kretschmar
    - sharan-maiya
    - walter-laurito
    - chris-cundy
    - adam-gleave
    - aviel-parrack
    - stefan-heimersheim
    - carlo-attubato
    - joseph-bloom
    - jordan-taylor
    - alex-mckenzie
    - urja-pawar
    - lewis-smith
    - bilal-chughtai
    - neel-nanda
  estimatedFTEs: 10-50
  critiques:
    - >-
      difficult to determine if behavior is strategic deception or only low level "reflexive" actions; Unclear if a
      model roleplaying a liar has deceptive intent. [How are intentional descriptions (like deception) related to
      algorithmic ones (like understanding the mechanisms models
      use)?](https://www.lesswrong.com/posts/YXNeA3RyRrrRWS37A/a-problem-to-solve-before-building-a-deception-detector),
      [Is This Lie Detector Really Just a Lie Detector? An Investigation of LLM Probe
      Specificity](https://www.lesswrong.com/posts/5dkhdRMypeuyoXfmb/is-this-lie-detector-really-just-a-lie-detector-an),
      [Herrmann](https://www.lesswrong.com/posts/bCQbSFrnnAk7CJNpM/still-no-lie-detector-for-llms), [Smith and
      Chughtai](https://arxiv.org/abs/2511.22662)
  fundedByText: Anthropic, Deepmind, UK AISI, Coefficient Giving
  papers:
    - title: Detecting Strategic Deception Using Linear Probes
      url: https://www.lesswrong.com/posts/9pGbTz6c78PGwJein/detecting-strategic-deception-using-linear-probes
      authors: Nicholas Goldowsky-Dill, Bilal Chughtai, Stefan Heimersheim, Marius Hobbhahn
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: White Box Control at UK AISI - Update on Sandbagging Investigations
      url: https://www.lesswrong.com/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging
      authors: >-
        Joseph Bloom, Jordan Taylor, Connor Kissane, Sid Black, Jacob Merizian, Alex Zelenka-Martin, Jacob Arbeid, Ben
        Millwood, Alan Cooney
      year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - title: Trusted monitoring, but with deception probes.
      url: https://www.lesswrong.com/posts/eaEqAzGN3uJfpfGoc/trusted-monitoring-but-with-deception-probes
      authors: Avi Parrack, StefanHex, Cleo Nardo
      year: 2024
      venue: LessWrong
      kind: lesswrong
    - title: Here's 18 Applications of Deception Probes
      url: https://www.lesswrong.com/posts/7zhAwcBri7yupStKy/18-applications-of-deception-probes
      authors: Cleo Nardo, Avi Parrack, jordine
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Evaluating honesty and lie detection techniques on a diverse suite of dishonest models
      url: https://alignment.anthropic.com/2025/honesty-elicitation/
      authors: Rowan Wang, Johannes Treutlein, Fabien Roger, Evan Hubinger, Sam Marks
      year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - title: "Caught in the Act: a mechanistic approach to detecting deception"
      url: https://arxiv.org/abs/2508.19505
      authors: Gerard Boxo, Ryan Socha, Daniel Yoo, Shivam Raval
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Preference Learning with Lie Detectors can Induce Honesty or Evasion
      url: https://arxiv.org/abs/2505.13787
      authors: Chris Cundy, Adam Gleave
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Detecting High-Stakes Interactions with Activation Probes
      url: https://arxiv.org/abs/2506.10805
      authors: >-
        Alex McKenzie, Urja Pawar, Phil Blandfort, William Bankes, David Krueger, Ekdeep Singh Lubana, Dmitrii
        Krasheninnikov
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: White Box Control at UK AISI - Update on Sandbagging Investigations
      url: https://www.alignmentforum.org/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging
      authors: Joseph Bloom, Jordan Taylor, Connor Kissane, Sid Black, merizian, alexdzm, jacoba, Ben Millwood, Alan Cooney
      year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - title: "Liars' Bench: Evaluating Lie Detectors for Language Models"
      url: https://arxiv.org/html/2511.16035v1
      authors: Kieron Kretschmar, Walter Laurito, Sharan Maiya, Samuel Marks
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Among Us: A Sandbox for Agentic Deception"
      url: https://www.lesswrong.com/posts/gRc8KL2HLtKkFmNPr/among-us-a-sandbox-for-agentic-deception
      venue: LessWrong
      kind: blocked
- id: Model_diffing
  name: Model diffing
  parent: White_box_safety
  summary: >-
    Understand what happens when a model is finetuned, what the "diff" between the finetuned and the original model
    consists in.
  theoryOfChange: >-
    By identifying the mechanistic differences between a base model and its fine-tune (e.g., after RLHF), maybe we can
    verify that safety behaviors are robustly "internalized" rather than superficially patched, and detect if dangerous
    capabilities or deceptive alignment have been introduced without needing to re-analyze the entire model. The diff is
    also much smaller, since most parameters don't change, which means you can use heavier methods on them.
  seeAlso: a:Sparse_Coding, a:Reverse_engineering
  orthodoxProblems:
    - "1"
  targetCase: pessimistic
  broadApproaches:
    - cognitive
  someNames:
    - julian-minder
    - clment-dumas
    - neel-nanda
    - trenton-bricken
    - jack-lindsey
  estimatedFTEs: 10-30
  fundedByText: various academic groups, Anthropic, Google DeepMind
  papers:
    - title: What We Learned Trying to Diff Base and Chat Models (And Why It Matters)
      url: https://www.lesswrong.com/posts/xmpauEXEerzYcJKNm/what-we-learned-trying-to-diff-base-and-chat-models-and-why
      authors: Clément Dumas, Julian Minder, Neel Nanda
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Open Source Replication of Anthropic's Crosscoder paper for model-diffing
      url: https://www.lesswrong.com/posts/srt6JXsRMtmqAJavD/open-source-replication-of-anthropic-s-crosscoder-paper-for
      authors: Connor Kissane, robertzk, Arthur Conmy, Neel Nanda
      year: 2024
      venue: LessWrong
      kind: lesswrong
    - title: "Cross-Architecture Model Diffing with Crosscoders: Unsupervised Discovery of Differences Between LLMs"
      url: https://openreview.net/forum?id=ZB84SvrZB8%20
      venue: OpenReview
      kind: error_detected
    - title: Discovering Undesired Rare Behaviors via Model Diff Amplification
      url: https://www.goodfire.ai/research/model-diff-amplification#
      authors: Santiago Aranguri, Thomas McGrath
      year: 2025
      venue: Goodfire Research
      kind: blog_post
    - title: Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning
      url: https://arxiv.org/abs/2504.02922
      authors: Julian Minder, Clément Dumas, Caden Juang, Bilal Chugtai, Neel Nanda
      year: 2025
      venue: NeurIPS 2025
      kind: paper_preprint
    - title: Persona Features Control Emergent Misalignment
      url: https://arxiv.org/abs/2506.19823
      authors: >-
        Miles Wang, Tom Dupré la Tour, Olivia Watkins, Alex Makelov, Ryan A. Chi, Samuel Miserendino, Jeffrey Wang,
        Achyuta Rajaram, Johannes Heidecke, Tejal Patwardhan, Dan Mossing
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences
      url: https://arxiv.org/abs/2510.13900
      authors: Julian Minder, Clément Dumas, Stewart Slocum, Helena Casademunt, Cameron Holmes, Robert West, Neel Nanda
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Insights on Crosscoder Model Diffing
      url: https://transformer-circuits.pub/2025/crosscoder-diffing-update/index.html
      authors: >-
        Siddharth Mishra-Sharma, Trenton Bricken, Jack Lindsey, Adam Jermyn, Jonathan Marcus, Kelley Rivoire,
        Christopher Olah, Thomas Henighan
      venue: Transformer Circuits Thread
      kind: blog_post
    - title: "Diffing Toolkit: Model Comparison and Analysis Framework"
      url: https://github.com/science-of-finetuning/diffing-toolkit%20
      venue: GitHub
      kind: error_detected
- id: Sparse_Coding
  name: Sparse Coding
  parent: White_box_safety
  summary: >-
    Decompose the polysemantic activations of the residual stream into a sparse linear combination of monosemantic
    "features" which correspond to interpretable concepts.
  theoryOfChange: >-
    Get a principled decomposition of an LLM's activation into atomic components → identify deception and other
    misbehaviors.
  seeAlso: sec:Concept_based_interpretability, a:Reverse_engineering
  orthodoxProblems:
    - "1"
    - "4"
    - "7"
  targetCase: average-case
  someNames:
    - leo-gao
    - dan-mossing
    - emmanuel-ameisen
    - jack-lindsey
    - adam-pearce
    - thomas-heap
    - abhinav-menon
    - kenny-peng
    - tim-lawson
  estimatedFTEs: 50-100
  critiques:
    - >-
      [Sparse Autoencoders Can Interpret Randomly Initialized Transformers](https://arxiv.org/abs/2501.17727), [The
      Sparse Autoencoders bubble has popped, but they are still
      promising](https://agarriga.substack.com/p/the-sparse-autoencoders-bubble-has), [Negative Results for SAEs On
      Downstream Tasks and Deprioritising SAE Research](https://www.alignmentforum.org/posts/4uXCAJNuPKtKBsi28/),
      [Sparse Autoencoders Trained on the Same Data Learn Different Features](https://arxiv.org/pdf/2501.16615), [Why
      Not Just Train For
      Interpretability?](https://www.lesswrong.com/posts/2HbgHwdygH6yeHKKq/why-not-just-train-for-interpretability)
  fundedByText: everyone, roughly. Frontier labs, LTFF, Coefficient Giving, etc.
  papers:
    - title: Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning
      url: https://arxiv.org/abs/2504.02922
      authors: Julian Minder, Clément Dumas, Caden Juang, Bilal Chugtai, Neel Nanda
      year: 2025
      venue: NeurIPS 2025
      kind: paper_preprint
    - title: Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models
      url: https://arxiv.org/abs/2411.14257
      authors: Javier Ferrando, Oscar Obeso, Senthooran Rajamanoharan, Neel Nanda
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: "Circuit Tracing: Revealing Computational Graphs in Language Models"
      url: https://transformer-circuits.pub/2025/attribution-graphs/methods.html
      authors: >-
        Emmanuel Ameisen, Jack Lindsey, Adam Pearce, Wes Gurnee, Nicholas L. Turner, Brian Chen, Craig Citro, David
        Abrahams, Shan Carter, Basil Hosmer, Jonathan Marcus, Michael Sklar, Adly Templeton, Trenton Bricken, Callum
        McDougall, Hoagy Cunningham, Thomas Henighan, Adam Jermyn, Andy Jones, Andrew Persic, Zhenyi Qi, T. Ben
        Thompson, Sam Zimmerman, Kelley Rivoire, Thomas Conerly, Chris Olah, Joshua Batson
      year: 2025
      venue: Transformer Circuits Thread
      kind: blog_post
    - title: Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models
      url: https://arxiv.org/abs/2504.02821
      authors: Mateusz Pach, Shyamgopal Karthik, Quentin Bouniot, Serge Belongie, Zeynep Akata
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: >-
        I Have Covered All the Bases Here: Interpreting Reasoning Features in Large Language Models via Sparse
        Autoencoders
      url: https://arxiv.org/abs/2503.18878
      authors: >-
        Andrey Galichin, Alexey Dontsov, Polina Druzhinina, Anton Razzhigaev, Oleg Y. Rogov, Elena Tutubalina, Ivan
        Oseledets
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Sparse Autoencoders Do Not Find Canonical Units of Analysis
      url: https://arxiv.org/abs/2502.04878
      authors: >-
        Patrick Leask, Bart Bussmann, Michael Pearce, Joseph Bloom, Curt Tigges, Noura Al Moubayed, Lee Sharkey, Neel
        Nanda
      year: 2025
      venue: arXiv (accepted to ICLR 2025)
      kind: paper_preprint
    - title: Transcoders Beat Sparse Autoencoders for Interpretability
      url: https://arxiv.org/abs/2501.18823
      authors: Gonçalo Paulo, Stepan Shabalin, Nora Belrose
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative Matrix Factorization
      url: https://arxiv.org/abs/2506.10920
      authors: Or Shafran, Atticus Geiger, Mor Geva
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "CRISP: Persistent Concept Unlearning via Sparse Autoencoders"
      url: https://arxiv.org/abs/2508.13650
      authors: Tomer Ashuach, Dana Arad, Aaron Mueller, Martin Tutek, Yonatan Belinkov
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: The Unintended Trade-off of AI Alignment:Balancing Hallucination Mitigation and Safety in LLMs
      url: https://arxiv.org/abs/2510.07775
      authors: Omar Mahmoud, Ali Khalil, Buddhika Laknath Semage, Thommen George Karimpanal, Santu Rana
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Scaling sparse feature circuit finding for in-context learning
      url: https://arxiv.org/abs/2504.13756
      authors: Dmitrii Kharlapenko, Stepan Shabalin, Fazl Barez, Arthur Conmy, Neel Nanda
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Learning Multi-Level Features with Matryoshka Sparse Autoencoders
      url: https://arxiv.org/abs/2503.17547
      authors: Bart Bussmann, Noa Nabeshima, Adam Karvonen, Neel Nanda
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Are Sparse Autoencoders Useful? A Case Study in Sparse Probing
      url: https://arxiv.org/abs/2502.16681
      authors: Subhash Kantamneni, Joshua Engels, Senthooran Rajamanoharan, Max Tegmark, Neel Nanda
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Sparse Autoencoders Trained on the Same Data Learn Different Features
      url: https://arxiv.org/abs/2501.16615
      authors: Gonçalo Paulo, Nora Belrose
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: What's In My Human Feedback? Learning Interpretable Descriptions of Preference Data
      url: https://arxiv.org/abs/2510.26202
      authors: Rajiv Movva, Smitha Milli, Sewon Min, Emma Pierson
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Priors in Time: Missing Inductive Biases for Language Model Interpretability"
      url: https://arxiv.org/abs/2511.01836
      authors: >-
        Ekdeep Singh Lubana, Can Rager, Sai Sumedh R. Hindupur, Valerie Costa, Greta Tuckute, Oam Patel, Sonia Krishna
        Murthy, Thomas Fel, Daniel Wurgaft, Eric J. Bigelow, Johnny Lin, Demba Ba, Martin Wattenberg, Fernanda Viegas,
        Melanie Weber, Aaron Mueller
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Inference-Time Decomposition of Activations (ITDA): A Scalable Approach to Interpreting Large Language Models"
      url: https://arxiv.org/abs/2505.17769
      authors: Patrick Leask, Neel Nanda, Noura Al Moubayed
      year: 2025
      venue: ICML 2025
      kind: paper_preprint
    - title: Binary Sparse Coding for Interpretability
      url: https://arxiv.org/abs/2509.25596
      authors: Lucia Quirke, Stepan Shabalin, Nora Belrose
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Scaling Sparse Feature Circuit Finding to Gemma 9B
      url: https://lesswrong.com/posts/PkeB4TLxgaNnSmddg/scaling-sparse-feature-circuit-finding-to-gemma-9b
      authors: Diego Caples, Jatin Nainani, CallumMcDougall, rrenaud
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Partially Rewriting a Transformer in Natural Language
      url: https://arxiv.org/abs/2501.18838
      authors: Gonçalo Paulo, Nora Belrose
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Dense SAE Latents Are Features, Not Bugs
      url: https://arxiv.org/abs/2506.15679
      authors: Xiaoqing Sun, Alessandro Stolfo, Joshua Engels, Ben Wu, Senthooran Rajamanoharan, Mrinmaya Sachan, Max Tegmark
      year: 2025
      venue: arXiv (NeurIPS 2025)
      kind: paper_preprint
    - title: Evaluating Sparse Autoencoders on Targeted Concept Erasure Tasks
      url: https://arxiv.org/abs/2411.18895
      authors: Adam Karvonen, Can Rager, Samuel Marks, Neel Nanda
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: Evaluating SAE interpretability without explanations
      url: https://arxiv.org/abs/2507.08473
      authors: Gonçalo Paulo, Nora Belrose
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder Guardrails for Precision Unlearning in LLMs"
      url: https://arxiv.org/abs/2504.08192
      authors: Aashiq Muhamed, Jacopo Bonato, Mona Diab, Virginia Smith
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability"
      url: https://arxiv.org/abs/2503.09532
      authors: >-
        Adam Karvonen, Can Rager, Johnny Lin, Curt Tigges, Joseph Bloom, David Chanin, Yeu-Tong Lau, Eoin Farrell,
        Callum McDougall, Kola Ayonrinde, Demian Till, Matthew Wearden, Arthur Conmy, Samuel Marks, Neel Nanda
      year: 2025
      venue: arXiv (accepted to ICML 2025)
      kind: paper_preprint
    - title: SAEs Are Good for Steering -- If You Select the Right Features
      url: https://arxiv.org/abs/2505.20063
      authors: Dana Arad, Aaron Mueller, Yonatan Belinkov
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Line of Sight: On Linear Representations in VLLMs"
      url: https://arxiv.org/abs/2506.04706
      authors: Achyuta Rajaram, Sarah Schwettmann, Jacob Andreas, Arthur Conmy
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Low-Rank Adapting Models for Sparse Autoencoders
      url: https://arxiv.org/abs/2501.19406
      authors: Matthew Chen, Joshua Engels, Max Tegmark
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Enhancing Automated Interpretability with Output-Centric Feature Descriptions
      url: https://arxiv.org/abs/2501.08319
      authors: Yoav Gur-Arieh, Roy Mayan, Chen Agassy, Atticus Geiger, Mor Geva
      year: 2025
      venue: arXiv (accepted to ACL 2025)
      kind: paper_preprint
    - title: "Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models"
      url: https://arxiv.org/abs/2411.00743
      authors: Aashiq Muhamed, Mona Diab, Virginia Smith
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: Enhancing Neural Network Interpretability with Feature-Aligned Sparse Autoencoders
      url: https://arxiv.org/abs/2411.01220
      authors: Luke Marks, Alasdair Paren, David Krueger, Fazl Barez
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: BatchTopK Sparse Autoencoders
      url: https://arxiv.org/abs/2412.06410
      authors: Bart Bussmann, Patrick Leask, Neel Nanda
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: "Towards Understanding Distilled Reasoning Models: A Representational Approach"
      url: https://arxiv.org/abs/2503.03730
      authors: David D. Baek, Max Tegmark
      year: 2025
      venue: ICLR 2025 Workshop on Building Trust in Language Models and Applications
      kind: paper_preprint
    - title: Understanding sparse autoencoder scaling in the presence of feature manifolds
      url: https://arxiv.org/abs/2509.02565
      authors: Eric J. Michaud, Liv Gorton, Tom McGrath
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Internal states before wait modulate reasoning patterns
      url: https://arxiv.org/abs/2510.04128
      authors: Dmitrii Troitskii, Koyena Pal, Chris Wendler, Callum Stuart McDougall, Neel Nanda
      year: 2025
      venue: arXiv (EMNLP Findings 2025)
      kind: paper_preprint
    - title: Do Sparse Autoencoders Generalize? A Case Study of Answerability
      url: https://arxiv.org/abs/2502.19964
      authors: Lovis Heindrich, Philip Torr, Fazl Barez, Veronika Thost
      year: 2025
      venue: ICML 2025 Workshop on Reliable and Responsible Foundation Models (arXiv preprint)
      kind: paper_preprint
    - title: "Position: Mechanistic Interpretability Should Prioritize Feature Consistency in SAEs"
      url: https://arxiv.org/abs/2505.20254
      authors: Xiangchen Song, Aashiq Muhamed, Yujia Zheng, Lingjing Kong, Zeyu Tang, Mona T. Diab, Virginia Smith, Kun Zhang
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: How Visual Representations Map to Language Feature Space in Multimodal LLMs
      url: https://arxiv.org/abs/2506.11976
      authors: Constantin Venhoff, Ashkan Khakzar, Sonia Joseph, Philip Torr, Neel Nanda
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Prisma: An Open Source Toolkit for Mechanistic Interpretability in Vision and Video"
      url: https://arxiv.org/abs/2504.19475
      authors: >-
        Sonia Joseph, Praneet Suresh, Lorenz Hufe, Edward Stevinson, Robert Graham, Yash Vadi, Danilo Bzdok, Sebastian
        Lapuschkin, Lee Sharkey, Blake Aaron Richards
      year: 2025
      venue: arXiv / CVPR Mechanistic Interpretability for Vision Workshop
      kind: paper_preprint
    - title: Topological Data Analysis and Mechanistic Interpretability
      url: https://lesswrong.com/posts/6oF6pRr2FgjTmiHus/topological-data-analysis-and-mechanistic-interpretability
      authors: Gunnar Carlsson
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: >-
        Large Language Models Share Representations of Latent Grammatical Concepts Across Typologically Diverse
        Languages
      url: https://arxiv.org/abs/2501.06346
      authors: Jannik Brinkmann, Chris Wendler, Christian Bartelt, Aaron Mueller
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Interpreting the linear structure of vision-language model embedding spaces
      url: https://arxiv.org/abs/2504.11695
      authors: Isabel Papadimitriou, Huangyuan Su, Thomas Fel, Sham Kakade, Stephanie Gil
      year: 2025
      venue: COLM 2025
      kind: paper_preprint
    - title: Interpreting Large Text-to-Image Diffusion Models with Dictionary Learning
      url: https://arxiv.org/abs/2505.24360
      authors: Stepan Shabalin, Ayush Panda, Dmitrii Kharlapenko, Abdur Raheem Ali, Yixiong Hao, Arthur Conmy
      year: 2025
      venue: CVPR 2025 - Mechanistic Interpretability for Vision Workshop
      kind: paper_preprint
    - title: Weight-sparse transformers have interpretable circuits
      url: https://cdn.openai.com/pdf/41df8f28-d4ef-43e9-aed2-823f9393e470/circuit-sparsity-paper.pdf
      kind: error_detected
- id: Causal_Abstractions
  name: Causal Abstractions
  parent: White_box_safety
  summary: >-
    Verify that a neural network implements a specific high-level causal model (like a logical algorithm) by finding a
    mapping between high-level variables and low-level neural representations.
  theoryOfChange: >-
    By establishing a causal mapping between a black-box neural network and a human-interpretable algorithm, we can
    check whether the model is using safe reasoning processes and predict its behavior on unseen inputs, rather than
    relying on behavioural testing alone.
  seeAlso: sec:Concept_based_interpretability, a:Reverse_engineering
  orthodoxProblems:
    - "4"
  targetCase: worst-case
  broadApproaches:
    - cognitive
  someNames:
    - atticus-geiger
    - christopher-potts
    - thomas-icard
    - theodora-mara-pslar
    - sara-magliacane
    - jiuding-sun
    - jing-huang
  estimatedFTEs: 10-30
  critiques:
    - >-
      [The Misguided Quest for Mechanistic AI
      Interpretability](https://www.google.com/search?q=https://open.substack.com/pub/aifrontiersmedia/p/the-misguided-quest-for-mechanistic),
      [Interpretability Will Not Reliably Find Deceptive
      AI](https://www.lesswrong.com/posts/PwnadG4BFjaER3MGf/interpretability-will-not-reliably-find-deceptive-ai)
  fundedByText: Various academic groups, Google DeepMind, Goodfire
  papers:
    - title: "HyperDAS: Towards Automating Mechanistic Interpretability with Hypernetworks"
      url: https://arxiv.org/abs/2503.10894
      authors: >-
        Jiuding Sun, Jing Huang, Sidharth Baskaran, Karel D'Oosterlinck, Christopher Potts, Michael Sklar, Atticus
        Geiger
      year: 2025
      venue: ICLR 2025
      kind: paper_preprint
    - title: Combining Causal Models for More Accurate Abstractions of Neural Networks
      url: https://arxiv.org/abs/2503.11429
      authors: Theodora-Mara Pîslar, Sara Magliacane, Atticus Geiger
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: How Causal Abstraction Underpins Computational Explanation
      url: https://arxiv.org/abs/2508.11214
      authors: Atticus Geiger, Jacqueline Harding, Thomas Icard
      year: 2025
      venue: arXiv
      kind: paper_preprint
- id: Data_attribution
  name: Data attribution
  parent: White_box_safety
  summary: >-
    Quantifies the influence of individual training data points on a model's specific behavior or output, allowing
    researchers to trace model properties (like misalignment, bias, or factual errors) back to their source in the
    training set.
  theoryOfChange: >-
    By attributing harmful, biased, or unaligned behaviors to specific training examples, researchers can audit
    proprietary models, debug training data, enable effective data deletion/unlearning
  seeAlso: a:Data_quality_for_alignment
  orthodoxProblems:
    - "4"
    - "1"
  targetCase: average-case
  broadApproaches:
    - behavioral
  someNames:
    - roger-grosse
    - philipp-alexander-kreer
    - jin-hwa-lee
    - matthew-smith
    - abhilasha-ravichander
    - andrew-wang
    - jiacheng-liu
    - jiaqi-ma
    - junwei-deng
    - yijun-pan
    - daniel-murfet
    - jesse-hoogland
  estimatedFTEs: 30-60
  fundedByText: Various academic groups
  papers:
    - title: Influence Dynamics and Stagewise Data Attribution
      url: https://arxiv.org/abs/2510.12071
      authors: Jin Hwa Lee, Matthew Smith, Maxwell Adam, Jesse Hoogland
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: What is Your Data Worth to GPT? LLM-Scale Data Valuation with Influence Functions
      url: https://arxiv.org/abs/2405.13954
      authors: >-
        Sang Keun Choe, Hwijeen Ahn, Juhan Bae, Kewen Zhao, Minsoo Kang, Youngseog Chung, Adithya Pratapa, Willie
        Neiswanger, Emma Strubell, Teruko Mitamura, Jeff Schneider, Eduard Hovy, Roger Grosse, Eric Xing
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: Detecting and Filtering Unsafe Training Data via Data Attribution with Denoised Representation
      url: https://arxiv.org/abs/2502.11411
      authors: Yijun Pan, Taiwei Shi, Jieyu Zhao, Jiaqi W. Ma
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Better Training Data Attribution via Better Inverse Hessian-Vector Products
      url: https://arxiv.org/abs/2507.14740
      authors: Andrew Wang, Elisa Nguyen, Runshi Yang, Juhan Bae, Sheila A. McIlraith, Roger Grosse
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models"
      url: https://arxiv.org/abs/2507.09424
      authors: >-
        Cathy Jiao, Yijun Pan, Emily Xiao, Daisy Sheng, Niket Jain, Hanzhang Zhao, Ishita Dasgupta, Jiaqi W. Ma, Chenyan
        Xiong
      year: 2025
      venue: NeurIPS 2025 Datasets and Benchmarks Track
      kind: paper_preprint
    - title: Bayesian Influence Functions for Hessian-Free Data Attribution
      url: https://arxiv.org/abs/2509.26544
      authors: Philipp Alexander Kreer, Wilson Wu, Maxwell Adam, Zach Furman, Jesse Hoogland
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training Tokens"
      url: https://arxiv.org/abs/2504.07096
      authors: >-
        Jiacheng Liu, Taylor Blanton, Yanai Elazar, Sewon Min, YenSung Chen, Arnavi Chheda-Kothary, Huy Tran, Byron
        Bischoff, Eric Marsh, Michael Schmitz, Cassidy Trier, Aaron Sarnat, Jenna James, Jon Borchardt, Bailey Kuehl,
        Evie Cheng, Karen Farley, Sruthi Sreeram, Taira Anderson, David Albright, Carissa Schoenick, Luca Soldaini, Dirk
        Groeneveld, Rock Yuren Pang, Pang Wei Koh, Noah A. Smith, Sophie Lebrecht, Yejin Choi, Hannaneh Hajishirzi, Ali
        Farhadi, Jesse Dodge
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: You Are What You Eat -- AI Alignment Requires Understanding How Data Shapes Structure and Generalisation
      url: https://arxiv.org/abs/2502.05475
      authors: >-
        Simon Pepin Lehalleur, Jesse Hoogland, Matthew Farrugia-Roberts, Susan Wei, Alexander Gietelink Oldenziel,
        George Wang, Liam Carroll, Daniel Murfet
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Information-Guided Identification of Training Data Imprint in (Proprietary) Large Language Models
      url: https://arxiv.org/abs/2503.12072
      authors: >-
        Abhilasha Ravichander, Jillian Fisher, Taylor Sorensen, Ximing Lu, Yuchen Lin, Maria Antoniak, Niloofar
        Mireshghallah, Chandra Bhagavatula, Yejin Choi
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Distributional Training Data Attribution: What do Influence Functions Sample?"
      url: https://arxiv.org/abs/2506.12965
      authors: Bruno Mlodozeniec, Isaac Reid, Sam Power, David Krueger, Murat Erdogdu, Richard E. Turner, Roger Grosse
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "A Snapshot of Influence: A Local Data Attribution Framework for Online Reinforcement Learning"
      url: https://openreview.net/forum?id=sYK4yPDuT1
      authors: Yuzheng Hu, Fan Wu, Haotian Ye, David Forsyth, James Zou, Nan Jiang, Jiaqi W. Ma, Han Zhao
      year: 2025
      venue: NeurIPS 2025
      kind: paper_published
    - title: Revisiting Data Attribution for Influence Functions
      url: https://arxiv.org/abs/2508.07297
      authors: Hongbo Zhu, Angelo Cangelosi
      year: 2025
      venue: arXiv
      kind: paper_preprint
- id: Pragmatic_interpretability
  name: Pragmatic interpretability
  parent: White_box_safety
  summary: >-
    Directly tackling concrete, safety-critical problems on the path to AGI by using lightweight interpretability tools
    (like steering and probing) and empirical feedback from proxy tasks, rather than pursuing complete mechanistic
    reverse-engineering.
  theoryOfChange: >-
    By applying interpretability skills to concrete problems, researchers can rapidly develop monitoring and control
    tools (e.g., steering vectors or probes) that have immediate, measurable impact on real-world safety issues like
    detecting hidden goals or emergent misalignment.
  seeAlso: a:Reverse_engineering, sec:Concept_based_interpretability
  orthodoxProblems:
    - "7"
    - "4"
  broadApproaches:
    - cognitive
  someNames:
    - lee-sharkey
    - dario-amodei
    - david-chalmers
    - been-kim
    - neel-nanda
    - david-d-baek
    - lauren-greenspan
    - dmitry-vaintrob
    - sam-marks
    - jacob-pfau
  estimatedFTEs: 30-60
  fundedByText: Google DeepMind, Anthropic, various academic groups
  papers:
    - title: A Pragmatic Vision for Interpretability
      url: https://www.lesswrong.com/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-inter
      authors: >-
        Neel Nanda, Josh Engels, Arthur Conmy, Senthooran Rajamanoharan, bilalchughtai, CallumMcDougall, János Kramár,
        lewis smith
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: "Agentic Interpretability: A Strategy Against Gradual Disempowerment"
      url: https://www.alignmentforum.org/posts/s9z4mgjtWTPpDLxFy/agentic-interpretability-a-strategy-against-gradual
      authors: Been Kim, John Hewitt, Neel Nanda, Noah Fiedel, Oyvind Tafjord
      year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - title: Auditing language models for hidden objectives
      url: https://arxiv.org/abs/2503.10965
      authors: >-
        Samuel Marks, Johannes Treutlein, Trenton Bricken, Jack Lindsey, Jonathan Marcus, Siddharth Mishra-Sharma,
        Daniel Ziegler, Emmanuel Ameisen, Joshua Batson, Tim Belonax, Samuel R. Bowman, Shan Carter, Brian Chen, Hoagy
        Cunningham, Carson Denison, Florian Dietz, Satvik Golechha, Akbir Khan, Jan Kirchner, Jan Leike, Austin Meek,
        Kei Nishimura-Gasparian, Euan Ong, Christopher Olah, Adam Pearce, Fabien Roger, Jeanne Salle, Andy Shih, Meg
        Tong, Drake Thomas, Kelley Rivoire, Adam Jermyn, Monte MacDiarmid, Tom Henighan, Evan Hubinger
      year: 2025
      venue: arXiv
      kind: paper_preprint
- id: Other_interpretability
  name: Other interpretability
  parent: White_box_safety
  summary: Interpretability that does not fall well into other categories.
  theoryOfChange: >-
    Explore alternative conceptual frameworks (e.g., agentic, propositional) and physics-inspired methods (e.g.,
    renormalization). Or be "pragmatic".
  seeAlso: a:Reverse_engineering, sec:Concept_based_interpretability
  orthodoxProblems:
    - "7"
    - "4"
  someNames:
    - lee-sharkey
    - dario-amodei
    - david-chalmers
    - been-kim
    - neel-nanda
    - david-d-baek
    - lauren-greenspan
    - dmitry-vaintrob
    - sam-marks
    - jacob-pfau
  estimatedFTEs: 30-60
  critiques:
    - >-
      [The Misguided Quest for Mechanistic AI
      Interpretability](https://aifrontiersmedia.substack.com/p/the-misguided-quest-for-mechanistic), [Interpretability
      Will Not Reliably Find Deceptive
      AI](https://www.lesswrong.com/posts/PwnadG4BFjaER3MGf/interpretability-will-not-reliably-find-deceptive-ai).
  papers:
    - title: "Transformers Don't Need LayerNorm at Inference Time: Implications for Interpretability"
      url: https://lesswrong.com/posts/KbFuuaBKRP7FcAADL/transformers-don-t-need-layernorm-at-inference-time
      authors: submarat, Joachim Schaeffer, Luca Baroni, galvsk, StefanHex
      year: 2025
      venue: LessWrong / arXiv
      kind: lesswrong
    - title: Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation Gradient Tracing
      url: https://arxiv.org/abs/2510.02334
      authors: Zhe Li, Wei Zhao, Yige Li, Jun Sun
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Open Problems in Mechanistic Interpretability
      url: https://arxiv.org/abs/2501.16496
      authors: >-
        Lee Sharkey, Bilal Chughtai, Joshua Batson, Jack Lindsey, Jeff Wu, Lucius Bushnaq, Nicholas Goldowsky-Dill,
        Stefan Heimersheim, Alejandro Ortega, Joseph Bloom, Stella Biderman, Adria Garriga-Alonso, Arthur Conmy, Neel
        Nanda, Jessica Rumbelow, Martin Wattenberg, Nandi Schoots, Joseph Miller, Eric J. Michaud, Stephen Casper, Max
        Tegmark, William Saunders, David Bau, Eric Todd, Atticus Geiger, Mor Geva, Jesse Hoogland, Daniel Murfet, Tom
        McGrath
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Against blanket arguments against interpretability
      url: https://lesswrong.com/posts/u3ZysuXEjkyHhefrk/against-blanket-arguments-against-interpretability
      authors: Dmitry Vaintrob
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: "Opportunity Space: Renormalization for AI Safety"
      url: https://lesswrong.com/posts/wkGmouy7JnTNtWAbc/opportunity-space-renormalization-for-ai-safety
      authors: Lauren Greenspan, Dmitry Vaintrob, Lucas Teixeira
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: "Prospects for Alignment Automation: Interpretability Case Study"
      url: https://lesswrong.com/posts/y5cYisQ2QHiSbQbhk/prospects-for-alignment-automation-interpretability-case
      authors: Jacob Pfau, Geoffrey Irving
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: The Urgency of Interpretability
      url: https://www.darioamodei.com/post/the-urgency-of-interpretability
      authors: Dario Amodei
      year: 2025
      venue: darioamodei.com
      kind: blog_post
    - title: Language Models May Verbatim Complete Text They Were Not Explicitly Trained On
      url: https://arxiv.org/abs/2503.17514
      authors: >-
        Ken Ziyu Liu, Christopher A. Choquette-Choo, Matthew Jagielski, Peter Kairouz, Sanmi Koyejo, Percy Liang,
        Nicolas Papernot
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Downstream applications as validation of interpretability progress
      url: https://lesswrong.com/posts/wGRnzCFcowRCrpX4Y/downstream-applications-as-validation-of-interpretability
      authors: Sam Marks
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Principles for Picking Practical Interpretability Projects
      url: https://lesswrong.com/posts/DqaoPNqhQhwBFqWue/principles-for-picking-practical-interpretability-projects
      authors: Sam Marks
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Propositional Interpretability in Artificial Intelligence
      url: https://arxiv.org/abs/2501.15740
      authors: David J. Chalmers
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Explainable and Interpretable Multimodal Large Language Models: A Comprehensive Survey"
      url: https://arxiv.org/abs/2412.02104
      authors: >-
        Yunkai Dang, Kaichen Huang, Jiahao Huo, Yibo Yan, Sirui Huang, Dongrui Liu, Mengxi Gao, Jie Zhang, Chen Qian,
        Kun Wang, Yong Liu, Jing Shao, Hui Xiong, Xuming Hu
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: "Renormalization Redux: QFT Techniques for AI Interpretability"
      url: https://lesswrong.com/posts/sjr66DBEgyogAbfdf/renormalization-redux-qft-techniques-for-ai-interpretability
      authors: Lauren Greenspan, Dmitry Vaintrob
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: "The Strange Science of Interpretability: Recent Papers and a Reading List for the Philosophy of Interpretability"
      url: https://lesswrong.com/posts/qRnupMmFG7dxQTTYh/the-strange-science-of-interpretability-recent-papers-and-a
      authors: Kola Ayonrinde, Louis Jaburi
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: "Through a Steerable Lens: Magnifying Neural Network Interpretability via Phase-Based Extrapolation"
      url: https://arxiv.org/abs/2506.02300
      authors: Farzaneh Mahdisoltani, Saeed Mahdisoltani, Roger B. Grosse, David J. Fleet
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Call for Collaboration: Renormalization for AI safety"
      url: https://lesswrong.com/posts/MDWGcNHkZ3NPEzcnp/call-for-collaboration-renormalization-for-ai-safety
      authors: Lauren Greenspan
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: "On the creation of narrow AI: hierarchy and nonlocality of neural network skills"
      url: https://arxiv.org/abs/2505.15811
      authors: Eric J. Michaud, Asher Parker-Sartori, Max Tegmark
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Harmonic Loss Trains Interpretable AI Models
      url: https://arxiv.org/abs/2502.01628
      authors: David D. Baek, Ziming Liu, Riya Tyagi, Max Tegmark
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Extracting memorized pieces of (copyrighted) books from open-weight language models
      url: https://arxiv.org/abs/2505.12546
      authors: >-
        A. Feder Cooper, Aaron Gokaslan, Ahmed Ahmed, Amy B. Cyphert, Christopher De Sa, Mark A. Lemley, Daniel E. Ho,
        Percy Liang
      year: 2025
      venue: arXiv
      kind: paper_preprint
- id: Learning_dynamics_and_developmental_interpretability
  name: Learning dynamics and developmental interpretability
  parent: White_box_safety
  summary: >-
    Builds tools for detecting, locating, and interpreting key structural shifts, phase transitions, and emergent
    phenomena (like grokking or deception) that occur during a model's training and in-context learning phases.
  theoryOfChange: >-
    Structures forming in neural networks leave identifiable traces that can be interpreted (e.g., using concepts from
    Singular Learning Theory); by catching and analyzing these developmental moments, researchers can automate
    interpretability, predict when dangerous capabilities emerge, and intervene to prevent deceptiveness or misaligned
    values as early as possible.
  seeAlso: a:Reverse_engineering, a:Sparse_Coding, [ICL transience](https://proceedings.mlr.press/v267/singh25c.html)
  orthodoxProblems:
    - "4"
  targetCase: worst-case
  broadApproaches:
    - cognitive
  someNames:
    - timaeus
    - jesse-hoogland
    - george-wang
    - daniel-murfet
    - stan-van-wingerden
    - alexander-gietelink-oldenziel
  estimatedFTEs: 10-50
  critiques:
    - >-
      [Vaintrob](https://www.lesswrong.com/posts/M2bs6xCbmc79nwr8j/dmitry-vaintrob-s-shortform#A8Ziwhts35dgqbz52), [Joar
      Skalse (2023)](https://www.alignmentforum.org/posts/ALJYj4PpkqyseL7kZ/my-criticism-of-singular-learning-theory)
  fundedByText: Manifund, Survival and Flourishing Fund, EA Funds
  papers:
    - title: "From SLT to AIT: NN Generalisation Out of Distribution"
      url: https://www.lesswrong.com/posts/2MX2bXreTtntB85Zy/from-slt-to-ait-nn-generalisation-out-of-distribution
      venue: LessWrong
      kind: error_detected
    - title: Understanding and Controlling LLM Generalization
      url: https://www.lesswrong.com/posts/ZSQaT2yxNNZ3eLxRd/understanding-and-controlling-llm-generalization
      authors: Daniel Tan
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: SLT for AI Safety
      url: https://lesswrong.com/posts/J7CyENFYXPxXQpsnD/slt-for-ai-safety
      authors: Jesse Hoogland
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: >-
        Crosscoding Through Time: Tracking Emergence & Consolidation Of Linguistic Representations Throughout LLM
        Pretraining
      url: https://arxiv.org/abs/2509.05291
      authors: Deniz Bayazit, Aaron Mueller, Antoine Bosselut
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: A Review of Developmental Interpretability in Large Language Models
      url: https://arxiv.org/abs/2508.15841
      authors: Ihor Kendiukhov
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Dynamics of Transient Structure in In-Context Linear Regression Transformers
      url: https://arxiv.org/abs/2501.17745
      authors: Liam Carroll, Jesse Hoogland, Matthew Farrugia-Roberts, Daniel Murfet
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Learning Coefficients, Fractals, and Trees in Parameter Space
      url: https://openreview.net/forum?id=KUFH0n1BIM
      authors: Max Hennick, Matthias Dellago
      year: 2025
      venue: ODYSSEY 2025 Conference
      kind: paper_preprint
    - title: "Emergence of Superposition: Unveiling the Training Dynamics of Chain of Continuous Thought"
      url: https://arxiv.org/abs/2509.23365
      authors: Hanlin Zhu, Shibo Hao, Zhiting Hu, Jiantao Jiao, Stuart Russell, Yuandong Tian
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Compressibility Measures Complexity: Minimum Description Length Meets Singular Learning Theory"
      url: https://arxiv.org/abs/2510.12077
      authors: Einar Urdshals, Edmund Lau, Jesse Hoogland, Stan van Wingerden, Daniel Murfet
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Programs as Singularities
      url: https://openreview.net/forum?id=Td37oOfmmz
      authors: Daniel Murfet, William Troiani
      year: 2025
      venue: ODYSSEY 2025 Conference
      kind: paper_preprint
    - title: What Do Learning Dynamics Reveal About Generalization in LLM Reasoning?
      url: https://arxiv.org/abs/2411.07681
      authors: Katie Kang, Amrith Setlur, Dibya Ghosh, Jacob Steinhardt, Claire Tomlin, Sergey Levine, Aviral Kumar
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: Selective regularization for alignment-focused representation engineering
      url: https://lesswrong.com/posts/HFcriD29cw3E5QLCR/selective-regularization-for-alignment-focused
      authors: Sandy Fraser
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Modes of Sequence Models and Learning Coefficients
      url: https://arxiv.org/abs/2504.18048
      authors: Zhongtian Chen, Daniel Murfet
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions During Code Training"
      url: https://arxiv.org/abs/2506.18777
      authors: Jonathan Cook, Silvia Sapora, Arash Ahmadian, Akbir Khan, Tim Rocktaschel, Jakob Foerster, Laura Ruis
      year: 2025
      venue: arXiv
      kind: paper_preprint
- id: Representation_structure_and_geometry
  name: Representation structure and geometry
  parent: White_box_safety
  summary: >-
    What do the representations look like? Does any simple structure underlie the beliefs of all well-trained models?
    Can we get the semantics from this geometry?
  theoryOfChange: >-
    Get scalable unsupervised methods for finding structure in representations and interpreting them, then using this to
    e.g. guide training.
  seeAlso: >-
    sec:Concept_based_interpretability, computational mechanics, feature universality, a:Natural_abstractions,
    a:Causal_Abstractions
  orthodoxProblems:
    - "4"
    - "7"
  broadApproaches:
    - cognitive
  someNames:
    - simplex
    - insight-interaction-lab
    - paul-riechers
    - adam-shai
    - martin-wattenberg
    - blake-richards
    - mateusz-piotrowski
  estimatedFTEs: 10-50
  fundedByText: Various academic groups, Astera Institute, Coefficient Giving
  papers:
    - title: The Geometry of Self-Verification in a Task-Specific Reasoning Model
      url: https://arxiv.org/pdf/2504.14379
      authors: Andrew Lee, Lihao Sun, Chris Wendler, Fernanda Viégas, Martin Wattenberg
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Rank-1 LoRAs Encode Interpretable Reasoning Signals
      url: http://arxiv.org/abs/2511.06739
      authors: Jake Ward, Paul Riechers, Adam Shai
      year: 2025
      venue: "arXiv (NeurIPS 2025 Workshop: Mechanistic Interpretability Workshop)"
      kind: paper_preprint
    - title: "The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence"
      url: https://arxiv.org/abs/2502.17420
      authors: Tom Wollschläger, Jannes Elstner, Simon Geisler, Vincent Cohen-Addad, Stephan Günnemann, Johannes Gasteiger
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Embryology of a Language Model
      url: https://arxiv.org/abs/2508.00331
      authors: George Wang, Garrett Baker, Andrew Gordon, Daniel Murfet
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Constrained belief updates explain geometric structures in transformer representations
      url: https://arxiv.org/abs/2502.01954
      authors: Mateusz Piotrowski, Paul M. Riechers, Daniel Filan, Adam S. Shai
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Shared Global and Local Geometry of Language Model Embeddings
      url: https://arxiv.org/abs/2503.21073
      authors: Andrew Lee, Melanie Weber, Fernanda Viégas, Martin Wattenberg
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Neural networks leverage nominally quantum and post-quantum representations
      url: https://arxiv.org/abs/2507.07432
      authors: Paul M. Riechers, Thomas J. Elliott, Adam S. Shai
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Tracing the Representation Geometry of Language Models from Pretraining to Post-training
      url: https://arxiv.org/abs/2509.23024
      authors: >-
        Melody Zixuan Li, Kumar Krishna Agrawal, Arna Ghosh, Komal Kumar Teru, Adam Santoro, Guillaume Lajoie, Blake A.
        Richards
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Deep sequence models tend to memorize geometrically; it is unclear why
      url: https://arxiv.org/abs/2510.26745
      authors: Shahriar Noroozizadeh, Vaishnavh Nagarajan, Elan Rosenfeld, Sanjiv Kumar
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Navigating the Latent Space Dynamics of Neural Models
      url: https://arxiv.org/abs/2505.22785
      authors: Marco Fumero, Luca Moschella, Emanuele Rodolà, Francesco Locatello
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: The Geometry of ReLU Networks through the ReLU Transition Graph
      url: https://arxiv.org/abs/2505.11692
      authors: Sahil Rajesh Dhayalkar
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Connecting Neural Models Latent Geometries with Relative Geodesic Representations
      url: https://arxiv.org/abs/2506.01599
      authors: Hanlin Yu, Berfin Inal, Georgios Arvanitidis, Soren Hauberg, Francesco Locatello, Marco Fumero
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Next-token pretraining implies in-context learning
      url: https://arxiv.org/abs/2505.18373
      authors: Paul M. Riechers, Henry R. Bigelow, Eric A. Alt, Adam Shai
      year: 2025
      venue: arXiv
      kind: paper_preprint
- id: Human_inductive_biases
  name: Human inductive biases
  parent: White_box_safety
  summary: >-
    Discover connections deep learning AI systems have with human brains and human learning processes. Develop an
    'alignment moonshot' based on a coherent theory of learning which applies to both humans and AI systems.
  theoryOfChange: >-
    Humans learn trust, honesty, self-maintenance, and corrigibility; if we understand how they do maybe we can get
    future AI systems to learn them.
  seeAlso: active learning, ACS research
  orthodoxProblems:
    - "4"
  targetCase: pessimistic
  broadApproaches:
    - cognitive
  someNames:
    - lukas-muttenthaler
    - quentin-delfosse
  estimatedFTEs: "4"
  fundedByText: Google DeepMind, various academic groups
  papers:
    - title: Aligning machine and human visual representations across abstraction levels
      url: https://www.nature.com/articles/s41586-025-09631-6
      authors: >-
        Lukas Muttenthaler, Klaus Greff, Frieda Born, Bernhard Spitzer, Simon Kornblith, Michael C. Mozer, Klaus-Robert
        Müller, Thomas Unterthiner, Andrew K. Lampinen
      year: 2025
      venue: Nature
      kind: paper_published
    - title: Deep Reinforcement Learning Agents are not even close to Human Intelligence
      url: https://arxiv.org/html/2505.21731v1
    - title: "Teaching AI to Handle Exceptions: Supervised Fine-tuning with Human-aligned Judgment"
      url: https://arxiv.org/html/2503.02976v2#S3
      authors: Matthew DosSantos DiSorbo, Harang Ju, Sinan Aral
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: HIBP Human Inductive Bias Project Plan
      url: https://docs.google.com/document/d/1fl7LE8AN7mLJ6uFcPuFCzatp0zCIYvjRIjQRgHPAkSE/edit?tab=t.0
      authors: Félix Dorn
      venue: Google Docs
      kind: error_detected
    - title: "Beginning with You: Perceptual-Initialization Improves Vision-Language Representation and Alignment"
      url: https://arxiv.org/abs/2505.14204
      authors: Yang Hu, Runchen Wang, Stephen Chong Zhao, Xuhui Zhan, Do Hun Kim, Mark Wallace, David A. Tovar
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Towards Cognitively-Faithful Decision-Making Models to Improve AI Alignment
      url: https://arxiv.org/abs/2509.04445
      authors: Cyrus Cousins, Vijay Keswani, Vincent Conitzer, Hoda Heidari, Jana Schaich Borg, Walter Sinnott-Armstrong
      year: 2025
      venue: arXiv
      kind: paper_preprint
- id: Monitoring_concepts
  name: Monitoring concepts
  parent: Concept_based_interpretability
  summary: >-
    Identifies directions or subspaces in a model's latent state that correspond to high-level concepts (like refusal,
    deception, or planning) and uses them to audit models for misalignment, monitor them at runtime, suppress eval
    awareness, debug why models are failing, etc.
  theoryOfChange: >-
    By mapping internal activations to human-interpretable concepts, we can detect dangerous capabilities or deceptive
    alignment directly in the mind of the model even if its overt behavior is perfectly safe. Deploy computationally
    cheap monitors to flag some hidden misalignment in deployed systems.
  seeAlso: >-
    [Pragmatic interp](https://www.alignmentforum.org/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-interpretability),
    a:Reverse_engineering, a:Sparse_Coding, a:Model_diffing
  orthodoxProblems:
    - "1"
    - "4"
    - "12"
  targetCase: pessimistic
  broadApproaches:
    - cognitive
  someNames:
    - daniel-beaglehole
    - adityanarayanan-radhakrishnan
    - enric-boix-adser
    - tom-wollschlger
    - anna-soligo
    - jack-lindsey
    - brian-christian
    - ling-hu
    - nicholas-goldowsky-dill
    - neel-nanda
  estimatedFTEs: 50-100
  critiques:
    - >-
      [Exploring the generalization of LLM truth directions on conversational
      formats](https://arxiv.org/html/2505.09807v1), [Understanding (Un)Reliability of Steering Vectors in Language
      Models](https://arxiv.org/abs/2505.22637)
  fundedByText: Coefficient Giving, Anthropic, various academic groups
  papers:
    - title: Convergent Linear Representations of Emergent Misalignment
      url: https://arxiv.org/abs/2506.11618
      authors: Anna Soligo, Edward Turner, Senthooran Rajamanoharan, Neel Nanda
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Detecting Strategic Deception Using Linear Probes
      url: https://arxiv.org/abs/2502.03407
      authors: Nicholas Goldowsky-Dill, Bilal Chughtai, Stefan Heimersheim, Marius Hobbhahn
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Toward universal steering and monitoring of AI models
      url: https://arxiv.org/abs/2502.03708
      authors: Daniel Beaglehole, Adityanarayanan Radhakrishnan, Enric Boix-Adserà, Mikhail Belkin
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Reward Model Interpretability via Optimal and Pessimal Tokens
      url: https://arxiv.org/abs/2506.07326
      authors: Brian Christian, Hannah Rose Kirk, Jessica A.F. Thompson, Christopher Summerfield, Tsvetomira Dumbalska
      year: 2025
      venue: FAccT '25 (ACM Conference on Fairness, Accountability, and Transparency)
      kind: paper_preprint
    - title: "The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence"
      url: https://arxiv.org/abs/2502.17420
      authors: Tom Wollschläger, Jannes Elstner, Simon Geisler, Vincent Cohen-Addad, Stephan Günnemann, Johannes Gasteiger
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Cost-Effective Constitutional Classifiers via Representation Re-use
      url: https://alignment.anthropic.com/2025/cheap-monitors
      authors: >-
        Hoagy Cunningham, Alwin Peng, Jerry Wei, Euan Ong, Fabien Roger, Linda Petrini, Misha Wagner, Vladimir Mikulik,
        Mrinank Sharma
      year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - title: Refusal in LLMs is an Affine Function
      url: https://arxiv.org/abs/2411.09003
      authors: Thomas Marshall, Adam Scherlis, Nora Belrose
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: White Box Control at UK AISI - Update on Sandbagging Investigations
      url: https://www.alignmentforum.org/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging
      authors: Joseph Bloom, Jordan Taylor, Connor Kissane, Sid Black, merizian, alexdzm, jacoba, Ben Millwood, Alan Cooney
      year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - title: Here's 18 Applications of Deception Probes
      url: https://lesswrong.com/posts/7zhAwcBri7yupStKy/here-s-18-applications-of-deception-probes
      authors: Cleo Nardo, Avi Parrack, jordine
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations
      url: https://arxiv.org/abs/2508.05625
      authors: Brandon Jaipersaud, David Krueger, Ekdeep Singh Lubana
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Beyond Linear Probes: Dynamic Safety Monitoring for Language Models"
      url: https://arxiv.org/abs/2509.26238
      authors: James Oldfield, Philip Torr, Ioannis Patras, Adel Bibi, Fazl Barez
      year: 2025
      venue: arXiv
      kind: paper_preprint
- id: Activation_engineering
  name: Activation engineering
  parent: Concept_based_interpretability
  summary: >-
    Programmatically modify internal model activations to steer outputs toward desired behaviors; a lightweight,
    interpretable supplement to fine-tuning.
  theoryOfChange: >-
    Test interpretability theories by intervening on activations; find new insights from interpretable causal
    interventions on representations. Or: build more stuff to stack on top of finetuning. Slightly encourage the model
    to be nice, add one more layer of defence to our bundle of partial alignment methods.
  seeAlso: a:Sparse_Coding
  orthodoxProblems:
    - "1"
  targetCase: average-case
  someNames:
    - runjin-chen
    - andy-arditi
    - david-krueger
    - jan-wehner
    - narmeen-oozeer
    - reza-bayat
    - adam-karvonen
    - jiuding-sun
    - tim-tian-hua
    - helena-casademunt
    - jacob-dunefsky
    - thomas-marshall
  estimatedFTEs: 20-100
  critiques:
    - "[Understanding (Un)Reliability of Steering Vectors in Language Models](https://arxiv.org/abs/2505.22637)"
  fundedByText: Coefficient Giving, Anthropic
  papers:
    - title: Do safety-relevant LLM steering vectors optimized on a single example generalize?
      url: https://lesswrong.com/posts/6aXe9nipTgwK5LxaP/do-safety-relevant-llm-steering-vectors-optimized-on-a
      authors: Jacob Dunefsky
      year: 2025
      venue: arXiv
      kind: lesswrong
    - title: "Keep Calm and Avoid Harmful Content: Concept Alignment and Latent Manipulation Towards Safer Answers"
      url: https://arxiv.org/abs/2510.12672
      authors: Ruben Belo, Marta Guimaraes, Claudia Soares
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Activation Space Interventions Can Be Transferred Between Large Language Models
      url: https://arxiv.org/abs/2503.04429
      authors: Narmeen Oozeer, Dhruv Nathawani, Nirmalendu Prakash, Michael Lan, Abir Harrasse, Amirali Abdullah
      year: 2025
      venue: arXiv (accepted to ICML 2025)
      kind: paper_preprint
    - title: "HyperSteer: Activation Steering at Scale with Hypernetworks"
      url: https://arxiv.org/abs/2506.03292
      authors: Jiuding Sun, Sidharth Baskaran, Zhengxuan Wu, Michael Sklar, Christopher Potts, Atticus Geiger
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Steering Evaluation-Aware Language Models to Act Like They Are Deployed
      url: https://arxiv.org/abs/2510.20487
      authors: Tim Tian Hua, Andrew Qin, Samuel Marks, Neel Nanda
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Steering Out-of-Distribution Generalization with Concept Ablation Fine-Tuning
      url: https://arxiv.org/abs/2507.16795
      authors: Helena Casademunt, Caden Juang, Adam Karvonen, Samuel Marks, Senthooran Rajamanoharan, Neel Nanda
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Persona Vectors: Monitoring and Controlling Character Traits in Language Models"
      url: https://arxiv.org/abs/2507.21509
      authors: Runjin Chen, Andy Arditi, Henry Sleight, Owain Evans, Jack Lindsey
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Steering Large Language Model Activations in Sparse Spaces
      url: https://arxiv.org/abs/2503.00177
      authors: Reza Bayat, Ali Rahimi-Kalahroudi, Mohammad Pezeshki, Sarath Chandar, Pascal Vincent
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Improving Steering Vectors by Targeting Sparse Autoencoder Features
      url: https://arxiv.org/abs/2411.02193
      authors: Sviatoslav Chalnev, Matthew Siu, Arthur Conmy
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: Understanding Reasoning in Thinking Language Models via Steering Vectors
      url: https://arxiv.org/abs/2506.18167
      authors: Constantin Venhoff, Iván Arcuschin, Philip Torr, Arthur Conmy, Neel Nanda
      year: 2025
      venue: ICLR 2025 Workshop on Reasoning and Planning for Large Language Models
      kind: paper_preprint
    - title: One-shot steering vectors cause emergent misalignment, too
      url: https://lesswrong.com/posts/kcKnKHTHycHeRhcHF/one-shot-steering-vectors-cause-emergent-misalignment-too
      authors: Jacob Dunefsky
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Enhancing Multiple Dimensions of Trustworthiness in LLMs via Sparse Activation Control
      url: https://arxiv.org/abs/2411.02461
      authors: Yuxin Xiao, Chaoqun Wan, Yonggang Zhang, Wenxiao Wang, Binbin Lin, Xiaofei He, Xu Shen, Jieping Ye
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: Comparing Bottom-Up and Top-Down Steering Approaches on In-Context Learning Tasks
      url: https://arxiv.org/abs/2411.07213
      authors: Madeline Brumley, Joe Kwon, David Krueger, Dmitrii Krasheninnikov, Usman Anwar
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: Taxonomy, Opportunities, and Challenges of Representation Engineering for Large Language Models
      url: https://arxiv.org/abs/2502.19649
      authors: Jan Wehner, Sahar Abdelnabi, Daniel Tan, David Krueger, Mario Fritz
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Robustly Improving LLM Fairness in Realistic Settings via Interpretability
      url: https://arxiv.org/abs/2506.10922
      authors: Adam Karvonen, Samuel Marks
      year: 2025
      venue: arXiv
      kind: paper_preprint
- id: Guaranteed_Safe_AI
  name: Guaranteed-Safe AI
  parent: Safety_by_construction
  summary: >-
    Have an AI system generate outputs (e.g. code, control systems, or RL policies) which it can quantitatively
    guarantee comply with a formal safety specification and world model.
  theoryOfChange: >-
    Various, including:


    i) safe deployment: create a scalable process to get not-fully-trusted AIs to produce highly trusted outputs;


    ii) secure containers: create a 'gatekeeper' system that can act as an intermediary between human users and a
    potentially dangerous system, only letting provably safe actions through.


    (Notable for not requiring that we solve ELK; does require that we solve ontology though)
  seeAlso: >-
    [Towards Guaranteed Safe AI](https://arxiv.org/abs/2405.06624), [Standalone
    World-Models](https://www.alignmentforum.org/posts/LngR93YwiEpJ3kiWh/research-agenda-synthesizing-standalone-world-models),
    a:Scientist_AI, Safeguarded AI, a:Asymptotic_guarantees, Open Agency Architecture, SLES, program synthesis, Scalable
    formal oversight
  orthodoxProblems:
    - "1"
    - "4"
    - "7"
    - "9"
    - "12"
  targetCase: worst-case
  someNames:
    - aria
    - lawzero
    - atlas-computing
    - flf
    - max-tegmark
    - beneficial-ai-foundation
    - steve-omohundro
    - david-davidad-dalrymple
    - joar-skalse
    - stuart-russell
    - alessandro-abate
  estimatedFTEs: 10-100
  critiques:
    - >-
      [Zvi](https://thezvi.substack.com/p/ai-28-watching-and-waiting?utm_source=%2Fsearch%2Fomohundro&utm_medium=reader2#:~:text=Max%20Tegmark%20and%20Steve%20Omohundo%20drop%20a%20new%20paper),
      [Gleave](https://manifund.org//projects/relocating-to-montreal-to-work-full-time-on-ai-safety?tab=comments#aea6521a-c6bb-4c66-9f5f-cf647589cf7e),
      [Dickson](https://www.lesswrong.com/posts/B2bg677TaS4cmDPzL/limitations-on-formal-verification-for-ai-safety),
      [Greenblatt](https://www.lesswrong.com/posts/jRf4WENQnhssCb6mJ/davidad-s-bold-plan-for-alignment-an-in-depth-explanation?commentId=MJCvHk5ARMnWDjQDg)
  fundedByText: Manifund, ARIA, Coefficient Giving, Survival and Flourishing Fund, Mila / CIFAR
  papers:
    - title: "SafePlanBench: evaluating a Guaranteed Safe AI Approach for LLM-based Agents"
      url: https://manifund.org/projects/safeplanbench-evaluating-a-guaranteed-safe-ai-approach-for-llm-based-agents
      authors: Agustín Martinez Suñé, Tan Zhi Xuan
      venue: Manifund
      kind: agenda_manifesto
    - title: Beliefs about formal methods and AI safety
      url: https://lesswrong.com/posts/CCT7Qc8rSeRs7r5GL/beliefs-about-formal-methods-and-ai-safety
      authors: Quinn Dougherty
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Report on NSF Workshop on Science of Safe AI
      url: https://arxiv.org/abs/2506.22492
      authors: Rajeev Alur, Greg Durrett, Hadas Kress-Gazit, Corina Păsăreanu, René Vidal
      year: 2025
      venue: arXiv
      kind: agenda_manifesto
    - title: "A benchmark for vericoding: formally verified program synthesis"
      url: https://arxiv.org/abs/2509.22908
      authors: >-
        Sergiu Bursuc, Theodore Ehrenborg, Shaowei Lin, Lacramioara Astefanoaei, Ionel Emilian Chiosa, Jure Kukovec,
        Alok Singh, Oliver Butterley, Adem Bizid, Quinn Dougherty, Miranda Zhao, Max Tan, Max Tegmark
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: AI-Assisted Formal Verification Toolchain
      url: https://atlascomputing.org/ai-assisted-fv-toolchain.pdf
      kind: error_detected
- id: Scientist_AI
  name: Scientist AI
  parent: Safety_by_construction
  summary: >-
    Develop powerful, nonagentic, uncertain world models that accelerate scientific progress while avoiding the risks of
    agent AIs
  theoryOfChange: >-
    Developing non-agentic 'Scientist AI' allows us to: (i) reap the benefits of AI progress while (ii) avoiding the
    inherent risks of agentic systems. These systems can also (iii) provide a useful guardrail to protect us from unsafe
    agentic AIs by double-checking actions they propose, and (iv) help us more safely build agentic superintelligent
    systems.
  seeAlso: "[JEPA](https://arxiv.org/abs/2511.08544), [oracles](https://www.lesswrong.com/w/oracle-ai)"
  orthodoxProblems:
    - "3"
    - "4"
    - "5"
  targetCase: pessimistic
  broadApproaches:
    - cognitive
  someNames:
    - yoshua-bengio
    - younesse-kaddar
  estimatedFTEs: 1-10
  critiques:
    - >-
      Hard to find, but see [Raymond Douglas'
      comment](https://www.lesswrong.com/posts/p5gBcoQeBsvsMShvT/superintelligent-agents-pose-catastrophic-risks-can?commentId=tJXqhg3XZsqnyaZs2),
      [Karnofsky-Soares
      discussion](https://www.lesswrong.com/posts/iy2o4nQj9DnQD7Yhj/discussion-with-nate-soares-on-a-key-alignment-difficulty).
      Perhaps also [Predict-O-Matic](https://www.lesswrong.com/posts/SwcyMEgLyd4C3Dern/the-parable-of-predict-o-matic).
  fundedByText: ARIA, Gates Foundation, Future of Life Institute, Coefficient Giving, Jaan Tallinn, Schmidt Sciences
  papers:
    - title: "Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?"
      url: https://arxiv.org/abs/2502.15657
      authors: >-
        Yoshua Bengio, Michael Cohen, Damiano Fornasiere, Joumana Ghosn, Pietro Greiner, Matt MacDermott, Sören
        Mindermann, Adam Oberman, Jesse Richardson, Oliver Richardson, Marc-Antoine Rondeau, Pierre-Luc St-Charles,
        David Williams-King
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "The More You Automate, the Less You See: Hidden Pitfalls of AI Scientist Systems"
      url: https://arxiv.org/abs/2509.08713
      authors: Ziming Luo, Atoosa Kasirzadeh, Nihar B. Shah
      year: 2025
      venue: arXiv
      kind: paper_preprint
- id: Brainlike_AGI_Safety
  name: Brainlike-AGI Safety
  parent: Safety_by_construction
  summary: >-
    Social and moral instincts are (partly) implemented in particular hardwired brain circuitry; let's figure out what
    those circuits are and how they work; this will involve symbol grounding. "a yet-to-be-invented variation on
    actor-critic model-based reinforcement learning"
  theoryOfChange: >-
    Fairly-direct alignment via changing training to reflect actual human reward. Get actual data about (reward,
    training data) → (human values) to help with theorising this map in AIs; "understand human social instincts, and
    then maybe adapt some aspects of those for AGIs, presumably in conjunction with other non-biological ingredients".
  targetCase: worst-case
  broadApproaches:
    - cognitive
  someNames:
    - steve-byrnes
  estimatedFTEs: 1-5
  critiques:
    - >-
      [Tsvi
      BT](https://www.lesswrong.com/posts/unCG3rhyMJpGJpoLd/koan-divining-alien-datastructures-from-ram-activations#BtHCubjKWDFafkmYH)
  fundedByText: Astera Institute
  papers:
    - title: Perils of Under vs Over-sculpting AGI Desires
      url: https://www.lesswrong.com/posts/grgb2ipxQf2wzNDEG/perils-of-under-vs-over-sculpting-agi-desires
      venue: LessWrong
      kind: error_detected
    - title: Reward button alignment
      url: https://lesswrong.com/posts/JrTk2pbqp7BFwPAKw/reward-button-alignment
      authors: Steven Byrnes
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: "System 2 Alignment: Deliberation, Review, and Thought Management"
      url: https://www.lesswrong.com/posts/cus5CGmLrjBRgcPSF/system-2-alignment-deliberation-review-and-thought
      authors: Seth Herd
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: "Against RL: The Case for System 2 Learning"
      url: https://elicit.com/blog/system-2-learning
      authors: Andreas Stuhlmüller
      year: 2025
      venue: Elicit Blog
      kind: blog_post
    - title: "Foom and Doom 1: Brain in a Box in a Basement"
      url: https://www.lesswrong.com/posts/yew6zFWAKG4AGs3Wk/foom-and-doom-1-brain-in-a-box-in-a-basement
      venue: LessWrong
      kind: blocked
    - title: "Foom and Doom 2: Technical Alignment is Hard"
      url: https://www.lesswrong.com/posts/bnnKGSCHJghAvqPjS/foom-and-doom-2-technical-alignment-is-hard
      venue: LessWrong
      kind: error_detected
- id: Weak_to_strong_generalization
  name: Weak-to-strong generalization
  parent: Make_AI_solve_it
  summary: Use weaker models to supervise and provide a feedback signal to stronger models.
  theoryOfChange: >-
    Find techniques that do better than RLHF at supervising superior models → track whether these techniques fail as
    capabilities increase further → keep the stronger systems aligned by amplifying weak oversight and quantifying where
    it breaks.
  seeAlso: sec:White_box_safety, a:Supervising_AIs_improving_AIs
  orthodoxProblems:
    - "8"
  targetCase: average-case
  broadApproaches:
    - engineering
  someNames:
    - joshua-engels
    - nora-belrose
    - david-d-baek
  estimatedFTEs: 2-20
  critiques:
    - >-
      [Can we safely automate alignment
      research?](https://joecarlsmith.substack.com/p/can-we-safely-automate-alignment), [Super(ficial)-alignment: Strong
      Models May Deceive Weak Models in Weak-to-Strong Generalization](https://arxiv.org/abs/2406.11431)
  fundedByText: lab funders, Eleuther funders
  papers:
    - title: Scaling Laws For Scalable Oversight
      url: https://arxiv.org/abs/2504.18530
      authors: Joshua Engels, David D. Baek, Subhash Kantamneni, Max Tegmark
      year: 2025
      venue: arXiv (NeurIPS 2025 Spotlight)
      kind: paper_preprint
    - title: Great Models Think Alike and this Undermines AI Oversight
      url: https://arxiv.org/abs/2502.04313
      authors: >-
        Shashwat Goel, Joschka Struber, Ilze Amanda Auzina, Karuna K Chandra, Ponnurangam Kumaraguru, Douwe Kiela, Ameya
        Prabhu, Matthias Bethge, Jonas Geiping
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Debate Helps Weak-to-Strong Generalization
      url: https://arxiv.org/abs/2501.13124
      authors: Hao Lang, Fei Huang, Yongbin Li
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Understanding the Capabilities and Limitations of Weak-to-Strong Generalization
      url: https://openreview.net/forum?id=RwYdLgj1S6
      authors: Wei Yao, Wenkai Yang, Ziqiao Wang, Yankai Lin, Yong Liu
      year: 2025
      venue: ICLR 2025 Workshop SSI-FM
      kind: paper_published
- id: Supervising_AIs_improving_AIs
  name: Supervising AIs improving AIs
  parent: Make_AI_solve_it
  summary: >-
    Build formal and empirical frameworks where AIs supervise other (stronger) AI systems via structured interactions;
    construct monitoring tools which enable scalable tracking of behavioural drift, benchmarks for self-modification,
    and robustness guarantees
  theoryOfChange: >-
    Early models train ~only on human data while later models also train on early model outputs, which leads to early
    model problems cascading. Left unchecked this will likely cause problems, so supervision mechanisms are needed to
    help ensure the AI self-improvement remains legible.
  orthodoxProblems:
    - "7"
    - "8"
  targetCase: pessimistic
  broadApproaches:
    - behavioral
  someNames:
    - roman-engeler
    - akbir-khan
    - ethan-perez
  estimatedFTEs: 1-10
  critiques:
    - >-
      [Automation collapse](https://www.lesswrong.com/posts/2Gy9tfjmKwkYbF9BY/automation-collapse), [Great Models Think
      Alike and this Undermines AI Oversight](https://arxiv.org/abs/2502.04313)
  fundedByText: Long-Term Future Fund, lab funders
  papers:
    - title: Bare Minimum Mitigations for Autonomous AI Development
      url: https://saif.org/research/bare-minimum-mitigations-for-autonomous-ai-development/
      authors: >-
        Joshua Clymer, Isabella Duan, Chris Cundy, Yawen Duan, Fynn Heide, Chaochao Lu, Sören Mindermann, Conor McGurk,
        Xudong Pan, Saad Siddiqui, Jingren Wang, Min Yang, Xianyuan Zhan
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Dodging systematic human errors in scalable oversight
      url: https://www.alignmentforum.org/posts/EgRJtwQurNzz8CEfJ/dodging-systematic-human-errors-in-scalable-oversight
      authors: Geoffrey Irving
      year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - title: Scaling Laws For Scalable Oversight
      url: https://arxiv.org/abs/2504.18530
      authors: Joshua Engels, David D. Baek, Subhash Kantamneni, Max Tegmark
      year: 2025
      venue: arXiv (NeurIPS 2025 Spotlight)
      kind: paper_preprint
    - title: Neural Interactive Proofs
      url: https://arxiv.org/abs/2412.08897
      authors: Lewis Hammond, Sam Adam-Day
      year: 2024
      venue: arXiv (ICLR 2025)
      kind: paper_preprint
    - title: Modeling Human Beliefs about AI Behavior for Scalable Oversight
      url: https://arxiv.org/abs/2502.21262
      authors: Leon Lang, Patrick Forré
      year: 2025
      venue: Transactions on Machine Learning Research
      kind: paper_published
    - title: Scalable Oversight for Superhuman AI via Recursive Self-Critiquing
      url: https://arxiv.org/abs/2502.04675
      authors: Xueru Wen, Jie Lou, Xinyu Lu, Junjie Yang, Yanjiang Liu, Yaojie Lu, Debing Zhang, Xing Yu
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Video and transcript of talk on automating alignment research
      url: https://lesswrong.com/posts/TQbptN7F4ijPnQRLy/video-and-transcript-of-talk-on-automating-alignment
      authors: Joe Carlsmith
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Maintaining Alignment during RSI as a Feedback Control Problem
      url: https://lesswrong.com/posts/PhgEKkB4cwYjwpGxb/maintaining-alignment-during-rsi-as-a-feedback-control
      authors: beren
      year: 2025
      venue: LessWrong
      kind: lesswrong
- id: AI_explanations_of_AIs
  name: AI explanations of AIs
  parent: Make_AI_solve_it
  summary: >-
    Make open AI tools to explain AIs, including AI agents. e.g. automatic feature descriptions for neuron activation
    patterns; an interface for steering these features; a behaviour elicitation agent that "searches" for a specified
    behaviour in frontier models.
  theoryOfChange: >-
    Use AI to help improve interp and evals. Develop and release open tools to level up the whole field. Get invited to
    improve lab processes.
  seeAlso: sec:White_box_safety
  orthodoxProblems:
    - "7"
    - "8"
  targetCase: pessimistic
  broadApproaches:
    - cognitive
  someNames:
    - transluce
    - jacob-steinhardt
    - neil-chowdhury
    - vincent-huang
    - sarah-schwettmann
    - robert-friel
  estimatedFTEs: 15-30
  fundedByText: Schmidt Sciences, Halcyon Futures, John Schulman, Wojciech Zaremba
  papers:
    - title: Automatically Jailbreaking Frontier Language Models with Investigator Agents
      url: https://transluce.org/jailbreaking-frontier-models
    - title: Surfacing Pathological Behaviors in Language Models
      url: https://transluce.org/pathological-behaviors
    - title: Investigating truthfulness in a pre-release o3 model
      url: https://transluce.org/investigating-o3-truthfulness
      authors: Neil Chowdhury, Daniel Johnson, Vincent Huang, Jacob Steinhardt, Sarah Schwettmann
      year: 2025
      venue: Transluce Blog
      kind: blog_post
    - title: Language Model Circuits Are Sparse in the Neuron Basis
      url: https://transluce.org/neuron-circuits
      authors: Aryaman Arora, Zhengxuan Wu, Jacob Steinhardt, Sarah Schwettmann
      year: 2025
      venue: Transluce AI
      kind: paper_preprint
    - title: Introducing Docent
      url: https://transluce.org/introducing-docent
      authors: Kevin Meng, Vincent Huang, Jacob Steinhardt, Sarah Schwettmann
      year: 2025
      venue: Transluce Blog
      kind: blog_post
- id: Debate
  name: Debate
  parent: Make_AI_solve_it
  summary: >-
    In the limit, it's easier to compellingly argue for true claims than for false claims; exploit this asymmetry to get
    trusted work out of untrusted debaters.
  theoryOfChange: >-
    "Give humans help in supervising strong agents" + "Align explanations with the true reasoning process of the agent"
    + "Red team models to exhibit failure modes that don't occur in normal use" are necessary but probably not
    sufficient for safe AGI.
  orthodoxProblems:
    - "1"
    - "7"
  targetCase: worst-case
  someNames:
    - rohin-shah
    - jonah-brown-cohen
    - georgios-piliouras
    - uk-aisi-benjamin-holton
  critiques:
    - >-
      [The limits of AI safety via debate
      (2022)](https://www.lesswrong.com/posts/kguLeJTt6LnGuYX4E/the-limits-of-ai-safety-via-debate)
  fundedByText: Google, others
  papers:
    - title: "UK AISI Alignment Team: Debate Sequence"
      url: https://www.lesswrong.com/s/NdovveRcyfxgMoujf
      authors: Benjamin Hilton
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: "Prover-Estimator Debate: A New Scalable Oversight Protocol"
      url: https://lesswrong.com/posts/8XHBaugB5S3r27MG9/prover-estimator-debate-a-new-scalable-oversight-protocol
      authors: Jonah Brown-Cohen, Geoffrey Irving
      year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - title: AI Debate Aids Assessment of Controversial Claims
      url: https://arxiv.org/abs/2506.02175
      authors: >-
        Salman Rahman, Sheriff Issaka, Ashima Suvarna, Genglin Liu, James Shiffer, Jaeyoung Lee, Md Rizwan Parvez, Hamid
        Palangi, Shi Feng, Nanyun Peng, Yejin Choi, Julian Michael, Liwei Jiang, Saadia Gabriel
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: An alignment safety case sketch based on debate
      url: https://arxiv.org/abs/2505.03989
      authors: Marie Davidsen Buhl, Jacob Pfau, Benjamin Hilton, Geoffrey Irving
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Ensemble Debates with Local Large Language Models for AI Alignment
      url: https://arxiv.org/abs/2509.00091
      authors: Ephraiem Sarabamoun
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: LMCA Dataset
      url: https://www.andrew.cmu.edu/user/coesterh/LMCA_dataset.pdf
      kind: error_detected
- id: LLM_introspection_training
  name: LLM introspection training
  parent: Make_AI_solve_it
  summary: >-
    Train LLMs to the predict the outputs of high-quality whitebox methods, to induce general self-explanation skills
    that use its own 'introspective' access
  theoryOfChange: >-
    Use the resulting LLMs as powerful dimensionality reduction, explaining internals in a distinct way than
    interpretability methods and CoT. Distilling self-explanation into the model should lead to the skill being
    scalable, since self-explanation skill advancement will feed off general-intelligence advancement.
  seeAlso: "[Transluce](#a:transluce), a:Anthropic"
  orthodoxProblems:
    - "4"
    - "7"
    - "8"
  broadApproaches:
    - cognitive
  someNames:
    - belinda-z-li
    - zifan-carl-guo
    - vincent-huang
    - jacob-steinhardt
    - jacob-andreas
    - jack-lindsey
  estimatedFTEs: 2-20
  fundedByText: Schmidt Sciences, Halcyon Futures, John Schulman, Wojciech Zaremba
  papers:
    - title: Training Language Models to Explain Their Own Computations
      url: https://arxiv.org/abs/2511.08579
      authors: Belinda Z. Li, Zifan Carl Guo, Vincent Huang, Jacob Steinhardt, Jacob Andreas
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Emergent Introspective Awareness in Large Language Models
      url: https://transformer-circuits.pub/2025/introspection/index.html
      authors: Jack Lindsey
      year: 2025
      venue: Transformer Circuits Thread
      kind: blog_post
- id: Agent_foundations
  name: Agent foundations
  parent: Theory
  summary: >-
    Develop philosophical clarity and mathematical formalizations of building blocks that might be useful for plans to
    align strong superintelligence, such as agency, optimization strength, decision theory, abstractions, concepts, etc.
  theoryOfChange: >-
    Rigorously understand optimization processed and agents, and what it means for them to be aligned in a substrate
    independent way → identify impossibility results and necessary conditions for aligned optimizer systems → use this
    theoretical understanding to eventually design safe architectures that remain stable and safe under self-reflection
  seeAlso: a:Aligning_what_, a:Tiling_agents, [Dovetail](#a:theory_dovetail)
  orthodoxProblems:
    - "1"
    - "2"
    - "4"
  targetCase: worst-case
  broadApproaches:
    - cognitive
  someNames:
    - abram-demski
    - alex-altair
    - sam-eisenstat
    - thane-ruthenis
    - alfred-harwood
    - daniel-c
    - dalcy-k
    - jos-pedro-faustino
  papers:
    - title: Limit-Computable Grains of Truth for Arbitrary Computable Extensive-Form (Un)Known Games
      url: https://www.arxiv.org/pdf/2508.16245
      authors: Cole Wyeth, Marcus Hutter, Jan Leike, Jessica Taylor
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Blog Posts – Universal Algorithmic Intelligence
      url: https://uaiasi.com/blog-posts/
      authors: Cole Wyeth
      year: 2025
      venue: Universal Algorithmic Intelligence website
      kind: blog_post
    - title: "Clarifying \"wisdom\": Foundational topics for aligned AIs to prioritize before irreversible decisions"
      url: https://www.lesswrong.com/posts/EyvJvYEFzDv5kGoiG/clarifying-wisdom-foundational-topics-for-aligned-ais-to
      venue: LessWrong
      kind: error_detected
    - title: "Agent foundations: not really math, not really science"
      url: https://www.lesswrong.com/posts/Dt4DuCCok3Xv5HEnG/agent-foundations-not-really-math-not-really-science
      authors: Alex_Altair
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Off-switching not guaranteed
      url: https://link.springer.com/article/10.1007/s11098-025-02296-x
      authors: Sven Neth
      year: 2025
      venue: Philosophical Studies
      kind: paper_published
    - title: Formalizing Embeddedness Failures in Universal Artificial Intelligence
      url: https://openreview.net/forum?id=tlkYPU3FlX
      authors: Cole Wyeth, Marcus Hutter
      year: 2025
      venue: ODYSSEY 2025 Conference
      kind: paper_preprint
    - title: Is alignment reducible to becoming more coherent?
      url: https://lesswrong.com/posts/nuDJNyG5XLQjtvaeg/is-alignment-reducible-to-becoming-more-coherent
      authors: Cole Wyeth
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: What Is The Alignment Problem?
      url: https://lesswrong.com/posts/dHNKtQ3vTBxTfTPxu/what-is-the-alignment-problem
      authors: johnswentworth
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Good old fashioned decision theory
      url: https://openreview.net/pdf?id=Rf1CeGPA22
      venue: OpenReview
      kind: error_detected
    - title: Report & retrospective on the Dovetail fellowship
      url: https://www.lesswrong.com/posts/ApfjBbqzSu4aZoLSe/report-and-retrospective-on-the-dovetail-fellowship
      authors: Alex Altair
      year: 2025
      venue: LessWrong
      kind: lesswrong
- id: Tiling_agents
  name: Tiling agents
  parent: Theory
  summary: >-
    An aligned agentic system modifying itself into an unaligned system would be bad and we can research ways that this
    could occur and infrastructure/approaches that prevent it from happening.
  theoryOfChange: >-
    Build enough theoretical basis through various approaches such that AI systems we create are capable of
    self-modification while preserving goals.
  seeAlso: a:Agent_foundations
  orthodoxProblems:
    - "1"
    - "2"
    - "4"
  targetCase: worst-case
  broadApproaches:
    - cognitive
  someNames:
    - abram-demski
  estimatedFTEs: 1-10
  papers:
    - title: Working through a small tiling result
      url: https://www.lesswrong.com/posts/akuMwu8SkmQSdospi/working-through-a-small-tiling-result
      authors: James Payor
      year: 2024
      venue: LessWrong
      kind: lesswrong
    - title: Communication & Trust
      url: https://openreview.net/forum?id=Rf1CeGPA22
      authors: Abram Demski
      year: 2025
      venue: ODYSSEY 2025 Conference
      kind: paper_preprint
    - title: Maintaining Alignment during RSI as a Feedback Control Problem
      url: https://lesswrong.com/posts/PhgEKkB4cwYjwpGxb/maintaining-alignment-during-rsi-as-a-feedback-control
      authors: beren
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Understanding Trust
      url: >-
        https://static1.squarespace.com/static/663d1233249bce4815fe8753/t/68067a6f5d5fb0745642d5b1/1745255023842/Understanding+Trust+-+Abram+Demski.pdf
      authors: Abram Demski
      kind: error_detected
- id: High_Actuation_Spaces
  name: High-Actuation Spaces
  parent: Theory
  summary: >-
    Mech interp and alignment assume a stable "computational substrate" (linear algebra on GPUs). If later AI uses
    different substrates (e.g. something neuromorphic), methods like probes and steering will not transfer. Therefore,
    better to try and infer goals via a "telic DAG" which abstracts over substrates, and so sidestep the issue of how to
    define intermediate representations. Category theory is intended to provide guarantees that this abstraction is
    valid.
  theoryOfChange: >-
    Sufficiently complex mindlike entities can alter their goals in ways that cannot be predicted or accounted for under
    substrate-dependent descriptions of the kind sought in mechanistic interpretability. use the telic DAG to define a
    method analogous to factoring a causal DAG.
  seeAlso: >-
    [Live theory](https://www.lesswrong.com/s/aMz2JMvgXrLBkq4h3), [MoSSAIC](https://openreview.net/forum?id=n7WYSJ35FU),
    [Topos Institute](https://topos.institute/), a:Agent_foundations
  targetCase: pessimistic
  someNames:
    - sahil-k
    - matt-farr
    - aditya-arpitha-prasad
    - chris-pang
    - aditya-adiga
    - jayson-amati
    - steve-petersen
    - topos
    - t-j
  estimatedFTEs: 1-10
  papers:
    - title: Groundless Alignment
      url: https://groundless.ai/
      year: 2025
      venue: Groundless Website
      kind: personal_page
    - title: Live Theory
      url: https://www.lesswrong.com/s/aMz2JMvgXrLBkq4h3
      venue: LessWrong
      kind: error_detected
    - title: High Actuation Spaces - Sahil
      url: >-
        https://docs.google.com/document/d/1d-ARdZZDHFPIfGcTTOKK8IZWlQj0NZQrmteJj2mvmYA/edit?tab=t.0#heading=h.eg8luyrlsv2u
      authors: Sahil
      venue: Google Docs
      kind: error_detected
    - title: What, if not agency?
      url: https://www.lesswrong.com/posts/tQ9vWm4b57HFqbaRj/what-if-not-agency
      venue: LessWrong
      kind: error_detected
    - title: HIBP Human Inductive Bias Project Plan
      url: https://docs.google.com/document/d/1fl7LE8AN7mLJ6uFcPuFCzatp0zCIYvjRIjQRgHPAkSE/edit?tab=t.0
      authors: Félix Dorn
      venue: Google Docs
      kind: error_detected
    - title: "MoSSAIC: AI Safety After Mechanism"
      url: https://openreview.net/forum?id=n7WYSJ35FU
      authors: Matt Farr, Aditya Arpitha Prasad, Chris Pang, Aditya Adiga, Jayson Amati, Sahil K
      year: 2025
      venue: ODYSSEY 2025 Conference
      kind: paper_preprint
    - title: HAS - Public (High Actuation Spaces)
      url: https://drive.google.com/drive/folders/1EaAJ4szuZsYR2_-DkS9cuhx3S6IWeCjW
      venue: Google Drive
      kind: error_detected
- id: Asymptotic_guarantees
  name: Asymptotic guarantees
  parent: Theory
  summary: >-
    Prove that if a safety process has enough resources (human data quality, training time, neural network capacity),
    then in the limit some system specification will be guaranteed. Use complexity theory, game theory, learning theory
    and other areas to both improve asymptotic guarantees and develop ways of showing convergence.
  theoryOfChange: >-
    Formal verification may be too hard. Make safety cases stronger by modelling their processes and proving that they
    would work in the limit.
  seeAlso: a:Debate, a:Guaranteed_Safe_AI, a:Control
  orthodoxProblems:
    - "4"
    - "7"
  targetCase: pessimistic
  broadApproaches:
    - cognitive
  someNames:
    - aisi
    - jacob-pfau
    - benjamin-hilton
    - geoffrey-irving
    - simon-marshall
    - will-kirby
    - martin-soto
    - david-africa
    - davidad
  estimatedFTEs: 5 - 10
  critiques:
    - >-
      Self-critique in [UK AISI's Alignment Team: Research
      Agenda](https://lesswrong.com/posts/tbnw7LbNApvxNLAg8/uk-aisi-s-alignment-team-research-agenda)
  fundedByText: AISI
  papers:
    - title: An alignment safety case sketch based on debate
      url: https://lesswrong.com/posts/iELyAqizJkizBQbfr/an-alignment-safety-case-sketch-based-on-debate
      authors: Marie_DB, Jacob Pfau, Benjamin Hilton, Geoffrey Irving
      year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - title: "UK AISI's Alignment Team: Research Agenda"
      url: https://lesswrong.com/posts/tbnw7LbNApvxNLAg8/uk-aisi-s-alignment-team-research-agenda
      authors: Benjamin Hilton, Jacob Pfau, Marie_DB, Geoffrey Irving
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Dodging systematic human errors in scalable oversight
      url: https://lesswrong.com/posts/EgRJtwQurNzz8CEfJ/dodging-systematic-human-errors-in-scalable-oversight
      authors: Geoffrey Irving
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Can DPO Learn Diverse Human Values? A Theoretical Scaling Law
      url: https://openreview.net/pdf?id=XOIKLlSiDq
      venue: OpenReview
      kind: error_detected
- id: Heuristic_explanations
  name: Heuristic explanations
  parent: Theory
  summary: >-
    Formalize mechanistic explanations of neural network behavior, automate the discovery of these "heuristic
    explanations" and use them to predict when novel input will lead to extreme behavior (i.e. "Low Probability
    Estimation" and "Mechanistic Anomaly Detection").
  theoryOfChange: >-
    The current goalpost is methods whose *reasoned predictions* about properties of a neural network's outputs
    distribution (for a given inputs distribution) are certifiably at least as accurate as estimations via sampling. If
    successful for safety-relevant properties, this should allow for automated alignment methods that are both
    human-legible and worst-case certified, as well more efficient than sampling-based methods in most cases.
  seeAlso: ARC Theory, ELK, mechanistic anomaly detection, [Acorn](https://acausal.org/), a:Guaranteed_Safe_AI
  orthodoxProblems:
    - "4"
    - "8"
  targetCase: worst-case
  someNames:
    - jacob-hilton
    - mark-xu
    - eric-neyman
    - victor-lecomte
    - george-robinson
  estimatedFTEs: 1-10
  critiques:
    - "[Matolcsi](https://www.lesswrong.com/s/uYMw689vDFmgPEHrS)"
  papers:
    - title: A computational no-coincidence principle
      url: https://www.lesswrong.com/posts/Xt9r4SNNuYxW83tmo/a-computational-no-coincidence-principle
      authors: Eric Neyman
      year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - title: "ARC progress update: Competing with sampling"
      url: https://www.lesswrong.com/posts/XdQd9gELHakd5pzJA/arc-progress-update-competing-with-sampling
      authors: Eric Neyman
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Obstacles in ARC's agenda
      url: https://www.lesswrong.com/s/uYMw689vDFmgPEHrS
      authors: David Matolcsi
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Deduction-Projection Estimators for Understanding Neural Networks
      url: https://gabrieldwu.com/assets/thesis.pdf
      kind: error_detected
    - title: Wide Neural Networks as a Baseline for the Computational No-Coincidence Conjecture
      url: https://openreview.net/forum?id=m4OpQAK3eY
      authors: John Dunbar, Scott Aaronson
      year: 2025
      venue: ODYSSEY 2025 Conference
      kind: paper_preprint
- id: Behavior_alignment_theory
  name: Behavior alignment theory
  parent: Corrigibility
  summary: >-
    Predict properties of future AGI (e.g. power-seeking) with formal models; formally state and prove hypotheses about
    the properties powerful systems will have and how we might try to change them.
  theoryOfChange: >-
    Figure out hypotheses about properties powerful agents will have → attempt to rigorously prove under what conditions
    the hypotheses hold → test these hypotheses where feasible → design training environments that lead to more salutary
    properties.
  seeAlso: a:Agent_foundations, a:Control
  orthodoxProblems:
    - "2"
    - "5"
  targetCase: worst-case
  someNames:
    - ram-potham
    - michael-k-cohen
    - max-harmsraelifin
    - john-wentworth
    - david-lorell
    - elliott-thornley
  estimatedFTEs: 1-10
  critiques:
    - >-
      [Ryan Greenblatt's
      criticism](https://www.lesswrong.com/posts/YbEbwYWkf8mv9jnmi/the-shutdown-problem-incomplete-preferences-as-a-solution?commentId=GJAippZ6ZzCagSnDb)
      of one behavioural proposal
  papers:
    - title: Preference gaps as a safeguard against AI self-replication
      url: https://www.lesswrong.com/posts/knwR9RgGN5a2oorci/preference-gaps-as-a-safeguard-against-ai-self-replication
      authors: tbs, EJT
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Serious Flaws in CAST
      url: https://www.lesswrong.com/s/KfCjeconYRdFbMxsy/p/qgBFJ72tahLo5hzqy
      authors: Max Harms
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: A Shutdown Problem Proposal
      url: https://www.lesswrong.com/posts/PhTBDHu9PKJFmvb4p/a-shutdown-problem-proposal
      authors: johnswentworth, David Lorell
      year: 2024
      venue: LessWrong
      kind: lesswrong
    - title: Shutdownable Agents through POST-Agency
      url: https://arxiv.org/abs/2505.20203
      authors: Elliott Thornley
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: The Partially Observable Off-Switch Game
      url: https://arxiv.org/abs/2411.17749
      authors: Andrew Garber, Rohan Subramani, Linus Luu, Mark Bedaywi, Stuart Russell, Scott Emmons
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: Imitation learning is probably existentially safe
      url: https://onlinelibrary.wiley.com/doi/10.1002/aaai.70040?af=R
      authors: Michael K. Cohen, Marcus Hutter
      year: 2025
      venue: AI Magazine
      kind: paper_published
    - title: Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power
      url: https://arxiv.org/abs/2508.00159
      authors: Jobst Heitzig, Ram Potham
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Deceptive Alignment and Homuncularity
      url: https://lesswrong.com/posts/9htmQx5wiePqTtZuL/deceptive-alignment-and-homuncularity
      authors: Oliver Sourbut, TurnTrout
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: "A Safety Case for a Deployed LLM: Corrigibility as a Singular Target"
      url: https://openreview.net/forum?id=mhEnJa9pNk
      authors: Ram Potham
      year: 2025
      venue: ODYSSEY 2025 Conference
      kind: paper_preprint
    - title: LLM AGI will have memory, and memory changes alignment
      url: https://lesswrong.com/posts/aKncW36ZdEnzxLo8A/llm-agi-will-have-memory-and-memory-changes-alignment
      authors: Seth Herd
      year: 2025
      venue: LessWrong
      kind: lesswrong
- id: Other_corrigibility
  name: Other corrigibility
  parent: Corrigibility
  summary: >-
    Diagnose and communicate obstacles to achieving robustly corrigible behavior; suggest mechanisms, tests, and
    escalation channels for surfacing and mitigating incorrigible behaviors
  theoryOfChange: >-
    Labs are likely to develop AGI using something analogous to current pipelines. Clarifying why naive
    instruction-following doesn't buy robust corrigibility + building strong tripwires/diagnostics for scheming and
    Goodharting thus reduces risks on the likely default path.
  seeAlso: a:Behavior_alignment_theory
  orthodoxProblems:
    - "2"
    - "5"
  targetCase: pessimistic
  someNames:
    - jeremy-gillen
  estimatedFTEs: 1-10
  papers:
    - title: AI Assistants Should Have a Direct Line to Their Developers
      url: https://www.lesswrong.com/posts/LDYPF6yfe3f8SPHFT/ai-assistants-should-have-a-direct-line-to-their-developers
      authors: Jan_Kulveit
      year: 2024
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - title: Detect Goodhart and shut down
      url: https://www.lesswrong.com/posts/ZHFZ6tivEjznkEoby/detect-goodhart-and-shut-down
      authors: Jeremy Gillen
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Instrumental goals are a different and friendlier kind of [content unavailable]
      url: https://www.lesswrong.com/posts/7Z4WC4AFgfmZ3fCDC/instrumental-goals-are-a-different-and-friendlier-kind-of
      venue: LessWrong
      kind: error_detected
    - title: Shutdownable Agents through POST-Agency
      url: https://www.lesswrong.com/posts/JuRdvZyqaFbvTPemn/shutdownable-agents-through-post-agency-1
      authors: EJT
      year: 2025
      venue: LessWrong/AI Alignment Forum
      kind: lesswrong
    - title: Why Corrigibility is Hard and Important (i.e. "Whence the high MIRI confidence in alignment difficulty?")
      url: https://www.lesswrong.com/posts/ksfjZJu3BFEfM6hHE/why-corrigibility-is-hard-and-important-i-e-whence-the-high
      venue: LessWrong
      kind: error_detected
    - title: "Oblivious Defense in ML Models: Backdoor Removal without Detection"
      url: https://dl.acm.org/doi/10.1145/3717823.3718245
      authors: Shafi Goldwasser, Jonathan Shafer, Neekon Vafa, Vinod Vaikuntanathan
      year: 2025
      venue: "STOC '25: Proceedings of the 57th Annual ACM Symposium on Theory of Computing"
      kind: paper_published
    - title: "Cryptographic Backdoor for Neural Networks: Boon and Bane"
      url: https://arxiv.org/abs/2509.20714
      authors: Anh Tu Ngo, Anupam Chattopadhyay, Subhamoy Maitra
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: A Cryptographic Perspective on Mitigation vs. Detection in Machine Learning
      url: https://arxiv.org/abs/2504.20310
      authors: Greg Gluch, Shafi Goldwasser
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Problems with instruction-following as an alignment target
      url: https://lesswrong.com/posts/CSFa9rvGNGAfCzBk6/problems-with-instruction-following-as-an-alignment-target
      authors: Seth Herd
      year: 2025
      venue: LessWrong
      kind: lesswrong
- id: Natural_abstractions
  name: Natural abstractions
  parent: Ontology_Identification
  summary: >-
    Develop a theory of concepts that explains how they are learned, how they structure a particular system's
    understanding, and how mutual translatability can be achieved between different collections of concepts.
  theoryOfChange: >-
    Understand the concepts a system's understanding is structured with and use them to inspect its "alignment/safety
    properties" and/or "retarget its search", i.e. identify utility-function-like components inside an AI and replacing
    calls to them with calls to "user values" (represented using existing abstractions inside the AI).
  seeAlso: >-
    a:Causal_Abstractions, representational alignment, convergent abstractions, feature universality, Platonic
    representation hypothesis, microscope AI
  orthodoxProblems:
    - "5"
    - "7"
    - "9"
  targetCase: worst-case
  broadApproaches:
    - cognitive
  someNames:
    - john-wentworth
    - paul-colognese
    - david-lorrell
    - sam-eisenstat
    - fernando-rosas
  estimatedFTEs: 1-10
  critiques:
    - >-
      [Chan et al
      (2023)](https://www.lesswrong.com/posts/gvzW46Z3BsaZsLc25/natural-abstractions-key-claims-theorems-and-critiques-1#3__A_formalization_of_abstractions_would_accelerate_alignment_research),
      [Soto](https://www.lesswrong.com/posts/CJjT8GMitsnKc2wgG/natural-abstractions-are-observer-dependent-a-conversation-1),
      [Harwood](https://www.lesswrong.com/posts/F4nzox6oh5oAdX9D3/abstractions-are-not-natural), [Soares
      (2023)](https://www.lesswrong.com/posts/mgjHS6ou7DgwhKPpu/a-rough-and-incomplete-review-of-some-of-john-wentworth-s)
  papers:
    - title: Abstract mathematical concepts vs abstractions over real
      url: https://www.lesswrong.com/posts/T6xSXiXF3WF6TmCyN/abstract-mathematical-concepts-vs-abstractions-over-real
      venue: LessWrong
      kind: error_detected
    - title: Condensation
      url: https://www.lesswrong.com/posts/BstHXPgQyfeNnLjjp/condensation
      authors: abramdemski
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: The Platonic Representation Hypothesis
      url: https://phillipi.github.io/prh/
      authors: Minyoung Huh, Brian Cheung, Tongzhou Wang, Phillip Isola
      year: 2024
      venue: ICML 2024
      kind: paper_published
    - title: "Fernando Rosas: Identifying Abstractions (HAAISS 2025)"
      url: https://www.youtube.com/watch?v=Nr9eMobqUOo&t=3s
      authors: Fernando Rosas
      year: 2025
      venue: HAAISS 2025
      kind: error_detected
    - title: "Natural Latents: Latent Variables Stable Across Ontologies"
      url: https://arxiv.org/abs/2509.03780
      authors: John Wentworth, David Lorell
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Condensation: a theory of concepts"
      url: https://openreview.net/forum?id=HwKFJ3odui
      authors: Sam Eisenstat
      year: 2025
      venue: ODYSSEY 2025 Conference
      kind: paper_preprint
    - title: "Factored space models: Towards causality between levels of abstraction"
      url: https://arxiv.org/abs/2412.02579
      authors: Scott Garrabrant, Matthias Georg Mayer, Magdalena Wache, Leon Lang, Sam Eisenstat, Holger Dell
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: A single principle related to many Alignment subproblems?
      url: https://lesswrong.com/posts/h89L5FMAkEBNsZ3xM/a-single-principle-related-to-many-alignment-subproblems-2
      authors: Q Home
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Getting aligned on representational alignment
      url: https://arxiv.org/abs/2310.13018
      authors: >-
        Ilia Sucholutsky, Lukas Muttenthaler, Adrian Weller, Andi Peng, Andreea Bobu, Been Kim, Bradley C. Love,
        Christopher J. Cueva, Erin Grant, Iris Groen, Jascha Achterberg, Joshua B. Tenenbaum, Katherine M. Collins,
        Katherine L. Hermann, Kerem Oktar, Klaus Greff, Martin N. Hebart, Nathan Cloos, Nikolaus Kriegeskorte, Nori
        Jacoby, Qiuyi Zhang, Raja Marjieh, Robert Geirhos, Sherol Chen, Simon Kornblith, Sunayana Rane, Talia Konkle,
        Thomas P. O'Connell, Thomas Unterthiner, Andrew K. Lampinen, Klaus-Robert Müller, Mariya Toneva, Thomas L.
        Griffiths
      year: 2023
      venue: arXiv
      kind: paper_preprint
    - title: Symmetries at the origin of hierarchical emergence
      url: https://arxiv.org/pdf/2512.00984%20
      authors: Fernando E. Rosas
      year: 2024
      venue: arXiv
      kind: paper_preprint
- id: The_Learning_Theoretic_Agenda
  name: The Learning-Theoretic Agenda
  parent: Ontology_Identification
  summary: >-
    Create a mathematical theory of intelligent agents that encompasses both humans and the AIs we want, one that
    specifies what it means for two such agents to be aligned; translate between its ontology and ours; produce formal
    desiderata for a training setup that produces coherent AGIs similar to (our model of) an aligned agent
  theoryOfChange: Fix formal epistemology to work out how to avoid deep training problems
  orthodoxProblems:
    - "1"
    - "4"
    - "9"
  targetCase: worst-case
  broadApproaches:
    - cognitive
  someNames:
    - vanessa-kosoy
    - diffractor
    - gergely-szcs
  estimatedFTEs: "3"
  critiques:
    - "[Matolcsi](https://www.lesswrong.com/posts/StkjjQyKwg7hZjcGB/a-mostly-critical-review-of-infra-bayesianism)"
  fundedByText: Survival and Flourishing Fund, ARIA, UK AISI, Coefficient Giving
  papers:
    - title: "New paper: Infra-Bayesian Decision Estimation Theory"
      url: https://www.lesswrong.com/posts/LgLez8aeK24PbyyQJ/new-paper-infra-bayesian-decision-estimation-theory
      venue: LessWrong
      kind: error_detected
    - title: Infra-Bayesianism
      url: https://www.lesswrong.com/w/infra-bayesianism?sortedBy=new
      authors: abramdemski, Ruby
      year: 2022
      venue: LessWrong
      kind: lesswrong
    - title: "New Paper: Ambiguous Online Learning"
      url: https://www.lesswrong.com/posts/Y9NuKpb6dsyiYFxWK/new-paper-ambiguous-online-learning
      authors: Vanessa Kosoy
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Regret Bounds for Robust Online Decision Making
      url: https://proceedings.mlr.press/v291/appel25a.html
      authors: Alexander Appel, Vanessa Kosoy
      year: 2025
      venue: COLT 2025 (Conference on Learning Theory)
      kind: paper_published
    - title: FUNDAMENTALS OF INFRA-BAYESIANISM
      url: https://www.lesswrong.com/s/n7qFxakSnxGuvmYAX
      authors: Brittany Gelb
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Non-Monotonic Infra-Bayesian Physicalism
      url: https://www.alignmentforum.org/posts/DobZ62XMdiPigii9H/non-monotonic-infra-bayesian-physicalism%20
      authors: Marcus Ogren
      year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
- id: Aligning_to_context
  name: Aligning to context
  parent: Multi_agent_first
  summary: >-
    Align AI directly to the role of participant, collaborator, or advisor for our best real human practices and
    institutions, instead of aligning AI to separately representable goals, rules, or utility functions.
  theoryOfChange: >-
    "Many classical problems in AGI alignment are downstream of a type error about human values." Operationalizing a
    correct view of human values - one that treats human values as impossible or impractical to abstract from concrete
    practices - will unblock value fragility, goal-misgeneralization, instrumental convergence, and pivotal-act
    specification.
  seeAlso: a:Aligning_what_, a:Aligned_to_who_
  orthodoxProblems:
    - "1"
    - "2"
    - "4"
    - "5"
    - "13"
  broadApproaches:
    - behavioral
  someNames:
    - full-stack-alignment
    - meaning-alignment-institute
    - plurality-institute
    - tan-zhi-xuan
    - matija-franklin
    - ryan-lowe
    - joe-edelman
    - oliver-klingefjord
  estimatedFTEs: "5"
  fundedByText: ARIA, OpenAI, Survival and Flourishing Fund
  papers:
    - title: "The Frame-Dependent Mind: On Reality's Stubborn Refusal To Be One Thing"
      url: https://www.softmax.com/blog/the-frame-dependent-mind
      authors: Emmett Shear, Sonnet 3.7
      year: 2025
      venue: Softmax Blog
      kind: blog_post
    - title: On Eudaimonia and Optimization
      url: https://docs.google.com/document/d/1cKbqYSGspfJavXvnhsp3mAuxHh08rNbP7tzYieqLiXw/edit?tab=t.0%20
      venue: Google Docs
      kind: error_detected
    - title: "Full-Stack Alignment: Co-Aligning AI and Institutions with Thick Models of Value"
      url: https://www.full-stack-alignment.ai
      kind: personal_page
    - title: A theory of appropriateness with applications to generative artificial intelligence
      url: https://arxiv.org/abs/2412.19010
      authors: >-
        Joel Z. Leibo, Alexander Sasha Vezhnevets, Manfred Diaz, John P. Agapiou, William A. Cunningham, Peter Sunehag,
        Julia Haas, Raphael Koster, Edgar A. Duéñez-Guzmán, William S. Isaac, Georgios Piliouras, Stanley M. Bileschi,
        Iyad Rahwan, Simon Osindero
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: What are human values, and how do we align AI to them?
      url: https://arxiv.org/abs/2404.10636
      authors: Oliver Klingefjord, Ryan Lowe, Joe Edelman
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: Model Integrity
      url: https://meaningalignment.substack.com/p/model-integrity
      authors: Joe Edelman, Oliver Klingefjord
      year: 2024
      venue: Substack
      kind: blog_post
    - title: Beyond Preferences in AI Alignment
      url: https://arxiv.org/abs/2408.16984
      authors: Tan Zhi-Xuan, Micah Carroll, Matija Franklin, Hal Ashton
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: Can AI Model the Complexities of Human Moral Decision-Making? A Qualitative Study of Kidney Allocation Decisions
      url: https://arxiv.org/abs/2503.00940
      authors: Vijay Keswani, Vincent Conitzer, Walter Sinnott-Armstrong, Breanna K. Nguyen, Hoda Heidari, Jana Schaich Borg
      year: 2025
      venue: ACM CHI 2025
      kind: paper_published
- id: Aligning_to_the_social_contract
  name: Aligning to the social contract
  parent: Multi_agent_first
  summary: >-
    Generate AIs' operational values from 'social contract'-style ideal civic deliberation formalisms and their
    consequent rulesets for civic actors
  theoryOfChange: >-
    Formalize and apply the liberal tradition's project of defining civic principles separable from the substantive
    good, aligning our AIs to civic principles that bypass fragile utility-learning and intractable utility-calculation
  seeAlso: a:Aligning_to_context, a:Aligning_what_
  orthodoxProblems:
    - "1"
    - "4"
    - "5"
    - "10"
    - "13"
  broadApproaches:
    - cognitive
  someNames:
    - gillian-hadfield
    - tan-zhi-xuan
    - sydney-levine
    - matija-franklin
    - joshua-b-tenenbaum
  estimatedFTEs: 5 - 10
  fundedByText: Deepmind, Macroscopic Ventures
  papers:
    - title: "Law-Following AI: designing AI agents to obey human laws"
      url: https://law-ai.org/law-following-ai/%20
      authors: Cullen O'Keefe, Ketan Ramakrishnan, Janna Tay, Christoph Winter
      year: 2025
      venue: Fordham Law Review
      kind: paper_published
    - title: A Pragmatic View of AI Personhood
      url: https://arxiv.org/abs/2510.26396
      authors: Joel Z. Leibo, Alexander Sasha Vezhnevets, William A. Cunningham, Stanley M. Bileschi
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Societal Alignment Frameworks Can Improve LLM Alignment
      url: https://arxiv.org/abs/2503.00069
      authors: >-
        Karolina Stańczak, Nicholas Meade, Mehar Bhatia, Hattie Zhou, Konstantin Böttinger, Jeremy Barnes, Jason
        Stanley, Jessica Montgomery, Richard Zemel, Nicolas Papernot, Nicolas Chapados, Denis Therien, Timothy P.
        Lillicrap, Ana Marasović, Sylvie Delacroix, Gillian K. Hadfield, Siva Reddy
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: ACE and Diverse Generalization via Selective Disagreement
      url: https://arxiv.org/abs/2509.07955
      authors: Oliver Daniels, Stuart Armstrong, Alexandre Maranhão, Mahirah Fairuz Rahman, Benjamin M. Marlin, Rebecca Gorman
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Resource Rational Contractualism Should Guide AI Alignment
      url: https://arxiv.org/abs/2506.17434
      authors: >-
        Sydney Levine, Matija Franklin, Tan Zhi-Xuan, Secil Yanik Guyot, Lionel Wong, Daniel Kilov, Yejin Choi, Joshua
        B. Tenenbaum, Noah Goodman, Seth Lazar, Iason Gabriel
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Statutory Construction and Interpretation for Artificial Intelligence
      url: https://arxiv.org/abs/2509.01186
      authors: Luxi He, Nimra Nadeem, Michel Liao, Howard Chen, Danqi Chen, Mariano-Florentino Cuéllar, Peter Henderson
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Beyond Preferences in AI Alignment
      url: https://arxiv.org/abs/2408.16984
      authors: Tan Zhi-Xuan, Micah Carroll, Matija Franklin, Hal Ashton
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: "Promises Made, Promises Kept: Safe Pareto Improvements via Ex Post Verifiable Commitments"
      url: https://arxiv.org/abs/2505.00783
      authors: Nathaniel Sauerberg, Caspar Oesterheld
      year: 2025
      venue: GAIW'25
      kind: paper_preprint
- id: Theory_for_aligning_multiple_AIs
  name: Theory for aligning multiple AIs
  parent: Multi_agent_first
  summary: >-
    Use realistic game-theory variants (e.g. evolutionary game theory, computational game theory) or develop alternative
    game theories to describe/predict the collective and individual behaviours of AI agents in multi-agent scenarios.
  theoryOfChange: >-
    While traditional AGI safety focuses on idealized decision-theory and individual agents, it's plausible that
    strategic AI agents will first emerge (or are emerging now) in a complex, multi-AI strategic landscape. We need
    granular, realistic formal models of AIs' strategic interactions and collective dynamics to understand this future.
  seeAlso: a:Tools_for_aligning_multiple_AIs, a:Aligning_what_
  orthodoxProblems:
    - "4"
    - "7"
    - "8"
  broadApproaches:
    - cognitive
  someNames:
    - lewis-hammond
    - emery-cooper
    - allan-chan
    - caspar-oesterheld
    - vincent-conitzer
    - vojta-kovarik
    - nathaniel-sauerberg
    - acs-research
    - jan-kulveit
    - richard-ngo
    - emmett-shear
    - softmax
    - full-stack-alignment
    - ai-objectives-institute
    - sahil
    - tj
    - andrew-critch
  estimatedFTEs: "10"
  fundedByText: SFF, CAIF, Deepmind, Macroscopic Ventures
  papers:
    - title: Multi-Agent Risks from Advanced AI
      url: https://arxiv.org/abs/2502.14143
      authors: >-
        Lewis Hammond, Alan Chan, Jesse Clifton, Jason Hoelscher-Obermaier, Akbir Khan, Euan McLean, Chandler Smith,
        Wolfram Barfuss, Jakob Foerster, Tomáš Gavenčiak, The Anh Han, Edward Hughes, Vojtěch Kovařík, Jan Kulveit, Joel
        Z. Leibo, Caspar Oesterheld, Christian Schroeder de Witt, Nisarg Shah, Michael Wellman, Paolo Bova, Theodor
        Cimpeanu, Carson Ezell, Quentin Feuillade-Montixi, Matija Franklin, Esben Kran, Igor Krawczuk, Max Lamparth,
        Niklas Lauffer, Alexander Meinke, Sumeet Motwani, Anka Reuel, Vincent Conitzer, Michael Dennis, Iason Gabriel,
        Adam Gleave, Gillian Hadfield, Nika Haghtalab, Atoosa Kasirzadeh, Sébastien Krier, Kate Larson, Joel Lehman,
        David C. Parkes, Georgios Piliouras, Iyad Rahwan
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: An Economy of AI Agents
      url: https://arxiv.org/abs/2509.01063
      authors: Gillian K. Hadfield, Andrew Koh
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Moloch's Bargain: Emergent Misalignment When LLMs Compete for Audiences"
      url: https://arxiv.org/abs/2510.06105
      authors: Batu El, James Zou
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: AI Testing Should Account for Sophisticated Strategic Behaviour
      url: https://arxiv.org/abs/2508.14927
      authors: Vojtech Kovarik, Eric Olav Chen, Sami Petersen, Alexis Ghersengorin, Vincent Conitzer
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Emergent social conventions and collective bias in LLM populations
      url: https://www.science.org/doi/10.1126/sciadv.adu9368
      authors: Ariel Flint Ashery, Luca Maria Aiello, Andrea Baronchelli
      year: 2025
      venue: Science Advances
      kind: paper_published
    - title: "Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory"
      url: https://arxiv.org/abs/2507.02618
      authors: Kenneth Payne, Baptiste Alloui-Cros
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Communication Enables Cooperation in LLM Agents: A Comparison with Curriculum-Based Approaches"
      url: https://arxiv.org/abs/2510.05748
      authors: Hachem Madmoun, Salem Lahlou
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Higher-Order Belief in Incomplete Information MAIDs
      url: https://arxiv.org/abs/2503.06323
      authors: Jack Foxabbott, Rohan Subramani, Francis Rhys Ward
      year: 2025
      venue: 24th International Conference on Autonomous Agents and Multiagent Systems 2025
      kind: paper_published
    - title: Characterising Simulation-Based Program Equilibria
      url: https://arxiv.org/abs/2412.14570
      authors: Emery Cooper, Caspar Oesterheld, Vincent Conitzer
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: Safe (Pareto) Improvements in Binary Constraint Structures
      url: https://cgi.cse.unsw.edu.au/~eptcs/paper.cgi?TARK2025:36
      venue: EPTCS (Electronic Proceedings in Theoretical Computer Science)
      kind: error_detected
    - title: "Promises Made, Promises Kept: Safe Pareto Improvements via Ex Post Verifiable Commitments"
      url: https://arxiv.org/abs/2505.00783
      authors: Nathaniel Sauerberg, Caspar Oesterheld
      year: 2025
      venue: GAIW'25
      kind: paper_preprint
    - title: "The Pando Problem: Rethinking AI Individuality"
      url: https://www.lesswrong.com/posts/wQKskToGofs4osdJ3/the-pando-problem-rethinking-ai-individuality
      authors: Jan_Kulveit
      year: 2025
      venue: LessWrong
      kind: lesswrong
- id: Tools_for_aligning_multiple_AIs
  name: Tools for aligning multiple AIs
  parent: Multi_agent_first
  summary: >-
    Develop tools and techniques for designing and testing multi-agent AI scenarios, for auditing real-world multi-agent
    AI dynamics, and for aligning AIs in multi-AI settings.
  theoryOfChange: >-
    Addressing multi-agent AI dynamics is key for aligning near-future agents and their impact on the world. Feedback
    loops from multi-agent dynamics can radically change the future AI landscape, and require a different toolset from
    model psychology to audit and control.
  seeAlso: a:Theory_for_aligning_multiple_AIs, a:Aligning_what_
  orthodoxProblems:
    - "4"
    - "7"
    - "8"
  someNames:
    - andrew-critch
    - lewis-hammond
    - emery-cooper
    - allan-chan
    - caspar-oesterheld
    - vincent-conitzer
    - gillian-hadfield
    - nathaniel-sauerberg
    - zhijing-jin
  estimatedFTEs: 10 - 15
  fundedByText: Coefficient Giving, Deepmind, Cooperative AI Foundation
  papers:
    - title: Reimagining Alignment
      url: https://softmax.com/blog/reimagining-alignment
      year: 2025
      venue: Softmax Blog
      kind: blog_post
    - title: "Beyond the high score: Prosocial ability profiles of multi-agent populations"
      url: https://arxiv.org/abs/2509.14485
      authors: Marko Tesic, Yue Zhao, Joel Z. Leibo, Rakshit S. Trivedi, Jose Hernandez-Orallo
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Multiplayer Nash Preference Optimization
      url: https://arxiv.org/abs/2509.23102
      authors: >-
        Fang Wu, Xu Huang, Weihao Xuan, Zhiwei Zhang, Yijia Xiao, Guancheng Wan, Xiaomin Li, Bing Hu, Peng Xia, Jure
        Leskovec, Yejin Choi
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "AgentBreeder: Mitigating the AI Safety Risks of Multi-Agent Scaffolds via Self-Improvement"
      url: https://arxiv.org/abs/2502.00757
      authors: J Rosser, Jakob Foerster
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems"
      url: https://arxiv.org/abs/2507.14660
      authors: Qibing Ren, Sitao Xie, Longxuan Wei, Zhenfei Yin, Junchi Yan, Lizhuang Ma, Jing Shao
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Infrastructure for AI Agents
      url: https://arxiv.org/abs/2501.10114
      authors: >-
        Alan Chan, Kevin Wei, Sihao Huang, Nitarshan Rajkumar, Elija Perrier, Seth Lazar, Gillian K. Hadfield, Markus
        Anderljung
      year: 2025
      venue: arXiv (accepted to TMLR)
      kind: paper_preprint
    - title: A dataset of questions on decision-theoretic reasoning in Newcomb-like problems
      url: https://arxiv.org/abs/2411.10588
      authors: Caspar Oesterheld, Emery Cooper, Miles Kodama, Linh Chi Nguyen, Ethan Perez
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: "The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation"
      url: https://arxiv.org/abs/2510.01295
      authors: Zarreen Reza
      year: 2025
      venue: NeurIPS 2025 Workshop on Evaluating the Evolving LLM Lifecycle
      kind: paper_preprint
    - title: "PGG-Bench: Contribute & Punish"
      url: https://github.com/lechmazur/pgg_bench
      year: 2025
      venue: GitHub
      kind: code_tool
    - title: Virtual Agent Economies
      url: http://arxiv.org/abs/2509.10147
      authors: >-
        Nenad Tomasev, Matija Franklin, Joel Z. Leibo, Julian Jacobs, William A. Cunningham, Iason Gabriel, Simon
        Osindero
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: An Interpretable Automated Mechanism Design Framework with Large Language Models
      url: https://arxiv.org/abs/2502.12203
      authors: Jiayuan Liu, Mingyu Guo, Vincent Conitzer
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Comparing Collective Behavior of LLM and Human Groups
      url: https://openreview.net/forum?id=9ply9CAnSC&noteId=rcn5RTlfD1
      authors: Anna B. Stephenson, Andrew Zhu, Chris Callison-Burch, Jan Kulveit
      year: 2025
      venue: NeurIPS 2025 Workshop ACA
      kind: paper_published
- id: Aligned_to_who_
  name: Aligned to who?
  parent: Multi_agent_first
  summary: >-
    Technical protocols for taking seriously the plurality of human values, cultures, and communities when aligning AI
    to "humanity"
  theoryOfChange: >-
    use democratic/pluralist/context-sensitive principles to guide AI development, alignment, and deployment somehow.
    Doing it as an afterthought in post-training or the spec isn't good enough. Continuously shape AI's social and
    technical feedback loop on the road to AGI
  seeAlso: a:Aligning_what_, a:Aligning_to_context
  orthodoxProblems:
    - "1"
    - "13"
  targetCase: average-case
  broadApproaches:
    - behavioral
  someNames:
    - joel-z-leibo
    - divya-siddarth
    - sb-krier
    - luke-thorburn
    - seth-lazar
    - ai-objectives-institute
    - the-collective-intelligence-project
    - vincent-conitzer
  estimatedFTEs: 5 - 15
  fundedByText: Future of Life Institute, Survival and Flourishing Fund, Deepmind, CAIF
  papers:
    - title: "The AI Power Disparity Index: Toward a Compound Measure of AI Actors' Power to Shape the AI Ecosystem"
      url: https://ojs.aaai.org/index.php/AIES/article/view/36645
      authors: Rachel M. Kim, Blaine Kuehnert, Seth Lazar, Ranjit Singh, Hoda Heidari
      year: 2025
      venue: AAAI/ACM Conference on AI, Ethics, and Society (AIES)
      kind: paper_published
    - title: Research Agenda for Sociotechnical Approaches to AI Safety
      url: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5097286
      authors: >-
        Samuel Curtis, Ravi Iyer, Cameron Domenico Kirk-Giannini, Victoria Krakovna, David Krueger, Nathan Lambert,
        Bruno Marnette, Colleen McKenzie, Julian Michael, Evan Miyazono, Noyuri Mima, Aviv Ovadya, Luke Thorburn, Vehbi
        Deger Turan
      year: 2025
      venue: SSRN
      kind: agenda_manifesto
    - title: "Cultivating Pluralism In Algorithmic Monoculture: The Community Alignment Dataset"
      url: https://arxiv.org/abs/2507.09650
      authors: >-
        Lily Hong Zhang, Smitha Milli, Karen Jusko, Jonathan Smith, Brandon Amos, Wassim Bouaziz, Manon Revel, Jack
        Kussman, Yasha Sheynin, Lisa Titus, Bhaktipriya Radharapu, Jane Yu, Vidya Sarma, Kris Rose, Maximilian Nickel
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Training LLM Agents to Empower Humans
      url: https://arxiv.org/abs/2510.13709
      authors: Evan Ellis, Vivek Myers, Jens Tuyls, Sergey Levine, Anca Dragan, Benjamin Eysenbach
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Societal and technological progress as sewing an ever-growing, ever-changing, patchy, and polychrome quilt
      url: https://arxiv.org/abs/2505.05197
      authors: Joel Z. Leibo, Alexander Sasha Vezhnevets, William A. Cunningham, Sébastien Krier, Manfred Diaz, Simon Osindero
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Democratic AI is Possible. The Democracy Levels Framework Shows How It Might Work
      url: https://arxiv.org/abs/2411.09222
      authors: >-
        Aviv Ovadya, Kyle Redman, Luke Thorburn, Quan Ze Chen, Oliver Smith, Flynn Devine, Andrew Konya, Smitha Milli,
        Manon Revel, K. J. Kevin Feng, Amy X. Zhang, Bilva Chandra, Michiel A. Bakker, Atoosa Kasirzadeh
      year: 2024
      venue: arXiv (Accepted to ICML 2025 Position Paper Track)
      kind: paper_preprint
    - title: Political Neutrality in AI Is Impossible- But Here Is How to Approximate It
      url: https://arxiv.org/abs/2503.05728
      authors: >-
        Jillian Fisher, Ruth E. Appel, Chan Young Park, Yujin Potter, Liwei Jiang, Taylor Sorensen, Shangbin Feng, Yulia
        Tsvetkov, Margaret E. Roberts, Jennifer Pan, Dawn Song, Yejin Choi
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Build Agent Advocates, Not Platform Agents
      url: https://arxiv.org/abs/2505.04345
      authors: Sayash Kapoor, Noam Kolt, Seth Lazar
      year: 2025
      venue: arXiv (accepted to ICML 2025 position paper track)
      kind: paper_preprint
    - title: "Gradual Disempowerment: Systemic Existential Risks from Incremental AI Development"
      url: https://arxiv.org/abs/2501.16946
      authors: Jan Kulveit, Raymond Douglas, Nora Ammann, Deger Turan, David Krueger, David Duvenaud
      year: 2025
      venue: arXiv
      kind: paper_preprint
- id: Aligning_what_
  name: Aligning what?
  parent: Multi_agent_first
  summary: >-
    Develop alternatives to agent-level models of alignment, by treating human-AI interactions, AI-assisted
    institutions, AI economic or cultural systems, drives within one AI, and other causal/constitutive processes as
    subject to alignment
  theoryOfChange: >-
    Model multiple reality-shaping processes above and below the level of the individual AI, some of which are
    themselves quasi-agential (e.g. cultures) or intelligence-like (e.g. markets), will develop AI alignment into a
    mature science for managing the transition to an AGI civilization
  seeAlso: a:Theory_for_aligning_multiple_AIs, a:Aligning_to_context, a:Aligned_to_who_
  orthodoxProblems:
    - "1"
    - "2"
    - "4"
    - "5"
    - "13"
  someNames:
    - richard-ngo
    - emmett-shear
    - softmax
    - full-stack-alignment
    - ai-objectives-institute
    - sahil
    - tj
    - andrew-critch
    - acs-research
    - jan-kulveit
  estimatedFTEs: 5-10
  fundedByText: Future of Life Institute, Emmett Shear
  papers:
    - title: Towards a scale-free theory of intelligent agency
      url: https://www.alignmentforum.org/posts/5tYTKX4pNpiG4vzYg/towards-a-scale-free-theory-of-intelligent-agency
      authors: Richard Ngo
      year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - title: Alignment first, intelligence later
      url: https://chrislakin.blog/p/alignment-first-intelligence-later
      authors: Chris Lakin
      year: 2025
      venue: Substack (Locally Optimal)
      kind: blog_post
    - title: Claude Opus 4 and 4.1 can now end a rare subset of conversations
      url: https://www.anthropic.com/research/end-subset-conversations
      year: 2025
      venue: Anthropic Blog
      kind: news_announcement
    - title: "Full-Stack Alignment: Co-Aligning AI and Institutions with Thick Models of Value"
      url: https://www.full-stack-alignment.ai
      kind: personal_page
    - title: On Eudaimonia and Optimization
      url: https://docs.google.com/document/d/1cKbqYSGspfJavXvnhsp3mAuxHh08rNbP7tzYieqLiXw/edit?tab=t.0%20
      venue: Google Docs
      kind: error_detected
    - title: AI Governance through Markets
      url: https://arxiv.org/abs/2501.17755
    - title: Collective cooperative intelligence
      url: https://www.pnas.org/doi/abs/10.1073/pnas.2319948121
      authors: >-
        Wolfram Barfuss, Jessica Flack, Chaitanya S. Gokhale, Lewis Hammond, Christian Hilbe, Edward Hughes, Joel Z.
        Leibo, Tom Lenaerts, Naomi Leonard, Simon Levin, Udari Madhushani Sehwag, Alex McAvoy, Janusz M. Meylahn,
        Fernando P. Santos
      year: 2025
      venue: Proceedings of the National Academy of Sciences
      kind: paper_published
    - title: Multipolar AI is Underrated
      url: https://www.lesswrong.com/posts/JjYu75q3hEMBgtvr8/multipolar-ai-is-underrated
      authors: Allison Duettmann
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: What, if not agency?
      url: https://www.lesswrong.com/posts/tQ9vWm4b57HFqbaRj/what-if-not-agency
      venue: LessWrong
      kind: error_detected
    - title: A Phylogeny of Agents
      url: https://equilibria1.substack.com/p/the-evolution-of-agency-a-research
      authors: Equilibria
      year: 2025
      venue: Substack
      kind: blog_post
    - title: The Multiplicity Thesis, Collective Intelligence, and Morality
      url: https://themultiplicity.ai/blog/thesis
      authors: Andrew Critch
      year: 2025
      venue: theMultiplicity.ai blog
      kind: blog_post
    - title: "Hierarchical Agency: A Missing Piece in AI Alignment"
      url: https://www.alignmentforum.org/posts/xud7Mti9jS4tbWqQE/hierarchical-agency-a-missing-piece-in-ai-alignment
      authors: Jan_Kulveit
      year: 2024
      venue: AI Alignment Forum
      kind: lesswrong
    - title: "Emmett Shear on Building AI That Actually Cares: Beyond Control and Steering"
      url: >-
        https://a16z.simplecast.com/episodes/emmett-shear-on-building-ai-that-actually-cares-beyond-control-and-steering-TRwfxH0r
      authors: Emmett Shear, Erik Torenberg, Séb Krier
      year: 2025
      venue: a16z Podcast
      kind: podcast
- id: AGI_metrics
  name: AGI metrics
  parent: Evals
  summary: Evals with the explicit aim of measuring progress towards full human-level generality.
  theoryOfChange: Help predict timelines for risk awareness and strategy.
  seeAlso: a:Capability_evals
  broadApproaches:
    - behavioral
  someNames:
    - cais
    - cfi-kinds-of-intelligence
    - apart-research
    - openai
    - metr
    - lexin-zhou
    - adam-scholl
    - lorenzo-pacchiardi
  estimatedFTEs: 10-50
  critiques:
    - >-
      [Is the Definition of AGI a
      Percentage?](https://aievaluation.substack.com/p/is-the-definition-of-agi-a-percentage), [The "Length" of
      "Horizons"](https://www.lesswrong.com/posts/PzLSuaT6WGLQGJJJD/the-length-of-horizons)
  fundedByText: Leverhulme Trust, Open Philanthropy, Long-Term Future Fund
  papers:
    - title: "HCAST: Human-Calibrated Autonomy Software Tasks"
      url: https://arxiv.org/abs/2503.17354
      authors: >-
        David Rein, Joel Becker, Amy Deng, Seraphina Nix, Chris Canal, Daniel O'Connel, Pip Arnott, Ryan Bloom, Thomas
        Broadley, Katharyn Garcia, Brian Goodrich, Max Hasin, Sami Jawhar, Megan Kinniment, Thomas Kwa, Aron Lajko, Nate
        Rush, Lucas Jun Koba Sato, Sydney Von Arx, Ben West, Lawrence Chan, Elizabeth Barnes
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: A Definition of AGI
      url: https://arxiv.org/pdf/2510.18212
      authors: >-
        Dan Hendrycks, Dawn Song, Christian Szegedy, Honglak Lee, Yarin Gal, Erik Brynjolfsson, Sharon Li, Andy Zou,
        Lionel Levine, Bo Han, Jie Fu, Ziwei Liu, Jinwoo Shin, Kimin Lee, Mantas Mazeika, Long Phan, George Ingebretsen,
        Adam Khoja, Cihang Xie, Olawale Salaudeen, Matthias Hein, Kevin Zhao, Alexander Pan, David Duvenaud, Bo Li,
        Steve Omohundro, Gabriel Alfour, Max Tegmark, Kevin McGrew, Gary Marcus, Jaan Tallinn, Eric Schmidt, Yoshua
        Bengio
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Remote Labor Index (RLI)
      url: https://scale.com/leaderboard/rli
      year: 2025
      venue: Scale AI Leaderboard
      kind: dataset_benchmark
    - title: "ADeLe v1.0: A battery for AI Evaluation with explanatory and predictive power"
      url: https://kinds-of-intelligence-cfi.github.io/ADELE/
      authors: >-
        Lexin Zhou, Lorenzo Pacchiardi, Fernando Martínez-Plumed, Katherine M. Collins, Yael Moros-Daval, Seraphina
        Zhang, Qinlin Zhao, Yitian Huang, Luning Sun, Jonathan E. Prunty, Zongqian Li, Pablo Sánchez-García, Kexin Jiang
        Chen, Pablo A. M. Casares, Jiyun Zu, John Burden, Behzad Mehrbakhsh, David Stillwell, Manuel Cebrian, Jindong
        Wang, Peter Henderson, Sherry Tongshuang Wu, Patrick C. Kyllonen, Lucy Cheke, Xing Xie, José Hernández-Orallo
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"
      url: https://arxiv.org/abs/2510.04374
      authors: >-
        Tejal Patwardhan, Rachel Dias, Elizabeth Proehl, Grace Kim, Michele Wang, Olivia Watkins, Simón Posada Fishman,
        Marwan Aljubeh, Phoebe Thacker, Laurance Fauconnet, Natalie S. Kim, Patrick Chao, Samuel Miserendino, Gildas
        Chabot, David Li, Michael Sharman, Alexandra Barr, Amelia Glaese, Jerry Tworek
      year: 2025
      venue: arXiv
      kind: paper_preprint
- id: Capability_evals
  name: Capability evals
  parent: Evals
  summary: >-
    Make tools that can actually check whether a model has a certain capability or propensity. We default to low-n
    sampling of a vast latent space but aim to do better.
  theoryOfChange: >-
    Keep a close eye on what capabilities are acquired when, so that frontier labs and regulators are better informed on
    what security measures are already necessary (and hopefully they extrapolate). You can't regulate without them.
  seeAlso: >-
    [Deepmind's frontier safety framework](https://deepmind.google/blog/strengthening-our-frontier-safety-framework/),
    [Aether](https://www.lesswrong.com/posts/B8Cmtf5gdHwxb8qtT/aether-july-2025-update)
  targetCase: average-case
  broadApproaches:
    - behavioral
  someNames:
    - metr
    - aisi
    - apollo-research
    - marrius-hobbhahn
    - meg-tong
    - mary-phuong
    - beth-barnes
    - thomas-kwa
    - joel-becker
  estimatedFTEs: 100+
  critiques:
    - >-
      [Large Language Models Often Know When They Are Being Evaluated](https://arxiv.org/abs/2505.23836), [AI
      Sandbagging: Language Models can Strategically Underperform on Evaluations](https://arxiv.org/abs/2406.07358),
      [The Leaderboard Illusion](https://arxiv.org/abs/2504.20879), [Do Large Language Model Benchmarks Test
      Reliability?](https://arxiv.org/abs/2502.03461)
  fundedByText: basically everyone. Google, Microsoft, Open Philanthropy, LTFF, Governments etc
  papers:
    - title: "MALT: A Dataset of Natural and Prompted Behaviors That Threaten Eval Integrity"
      url: https://metr.org/blog/2025-10-14-malt-dataset-of-natural-and-prompted-behaviors/
      authors: Neev Parikh, Hjalmar Wijk
      year: 2025
      venue: METR Blog
      kind: blog_post
    - title: Forecasting Rare Language Model Behaviors
      url: https://arxiv.org/abs/2502.16797
      authors: >-
        Erik Jones, Meg Tong, Jesse Mu, Mohammed Mahfoud, Jan Leike, Roger Grosse, Jared Kaplan, William Fithian, Ethan
        Perez, Mrinank Sharma
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Model Tampering Attacks Enable More Rigorous Evaluations of LLM Capabilities
      url: https://arxiv.org/abs/2502.05209
      authors: >-
        Zora Che, Stephen Casper, Robert Kirk, Anirudh Satheesh, Stewart Slocum, Lev E McKinney, Rohit Gandikota, Aidan
        Ewart, Domenic Rosati, Zichu Wu, Zikui Cai, Bilal Chughtai, Yarin Gal, Furong Huang, Dylan Hadfield-Menell
      year: 2025
      venue: arXiv (accepted to TMLR)
      kind: paper_preprint
    - title: "The Elicitation Game: Evaluating Capability Elicitation Techniques"
      url: https://arxiv.org/abs/2502.02180
      authors: Felix Hofstätter, Teun van der Weij, Jayden Teoh, Rada Djoneva, Henning Bartsch, Francis Rhys Ward
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Evaluating Language Model Reasoning about Confidential Information
      url: https://arxiv.org/abs/2508.19980
      authors: Dylan Sam, Alexander Robey, Andy Zou, Matt Fredrikson, J. Zico Kolter
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Evaluating the Goal-Directedness of Large Language Models
      url: https://arxiv.org/abs/2504.11844
      authors: Tom Everitt, Cristina Garbacea, Alexis Bellot, Jonathan Richens, Henry Papadatos, Siméon Campos, Rohin Shah
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: A Toy Evaluation of Inference Code Tampering
      url: https://alignment.anthropic.com/2024/rogue-eval/index.html
      year: 2024
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - title: Automated Capability Discovery via Foundation Model Self-Exploration
      url: https://arxiv.org/abs/2502.07577
      authors: Cong Lu, Shengran Hu, Jeff Clune
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Generative Value Conflicts Reveal LLM Priorities
      url: https://arxiv.org/abs/2509.25369
      authors: Andy Liu, Kshitish Ghate, Mona Diab, Daniel Fried, Atoosa Kasirzadeh, Max Kleiman-Weiner
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Technical Report: Evaluating Goal Drift in Language Model Agents"
      url: https://arxiv.org/abs/2505.02709
      authors: Rauno Arike, Elizabeth Donoway, Henning Bartsch, Marius Hobbhahn
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity
      url: https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social Dilemmas"
      url: https://arxiv.org/abs/2505.19212
      authors: Steffen Backmann, David Guzman Piedrahita, Emanuel Tewolde, Rada Mihalcea, Bernhard Schölkopf, Zhijing Jin
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons"
      url: https://arxiv.org/abs/2503.05731
      authors: >-
        Shaona Ghosh, Heather Frase, Adina Williams, Sarah Luger, Paul Röttger, Fazl Barez, Sean McGregor, Kenneth
        Fricklas, Mala Kumar, Quentin Feuillade-Montixi, Kurt Bollacker, Felix Friedrich, Ryan Tsang, Bertie Vidgen,
        Alicia Parrish, Chris Knotz, Eleonora Presani, Jonathan Bennion, Marisa Ferrara Boston, Mike Kuniavsky, Wiebke
        Hutiri, James Ezick, Malek Ben Salem, Rajat Sahay, Sujata Goswami, Usman Gohar, Ben Huang, Supheakmungkol Sarin,
        Elie Alhajjar, Canyu Chen, Roman Eng, Kashyap Ramanandula Manjusha, Virendra Mehta, Eileen Long, Murali Emani,
        Natan Vidra, Benjamin Rukundo, Abolfazl Shahbazi, Kongtao Chen, Rajat Ghosh, Vithursan Thangarasa, Pierre
        Peigné, Abhinav Singh, Max Bartolo, Satyapriya Krishna, Mubashara Akhtar, Rafael Gold, Cody Coleman, Luis Oala,
        Vassil Tashev, Joseph Marvin Imperial, Amy Russ, Sasidhar Kunapuli, Nicolas Miailhe, Julien Delaunay,
        Bhaktipriya Radharapu, Rajat Shinde, Tuesday, Debojyoti Dutta, Declan Grabb, Ananya Gangavarapu, Saurav Sahay,
        Agasthya Gangavarapu, Patrick Schramowski, Stephen Singam, Tom David, Xudong Han, Priyanka Mary Mammen, Tarunima
        Prabhakar, Venelin Kovatchev, Rebecca Weiss, Ahmed Ahmed, Kelvin N. Manyeki, Sandeep Madireddy, Foutse Khomh,
        Fedor Zhdanov, Joachim Baumann, Nina Vasan, Xianjun Yang, Carlos Mougn, Jibin Rajan Varghese, Hussain Chinoy,
        Seshakrishna Jitendar, Manil Maskey, Claire V. Hardgrove, Tianhao Li, Aakash Gupta, Emil Joswin, Yifan Mai,
        Shachi H Kumar, Cigdem Patlak, Kevin Lu, Vincent Alessi, Sree Bhargavi Balija, Chenhe Gu, Robert Sullivan, James
        Gealy, Matt Lavrisa, James Goel, Peter Mattson, Percy Liang, Joaquin Vanschoren
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Petri: An open-source auditing tool to accelerate AI safety research"
      url: https://www.anthropic.com/research/petri-open-source-auditing
      authors: >-
        Kai Fronsdal, Isha Gupta, Abhay Sheshadri, Jonathan Michala, Stephen McAleer, Rowan Wang, Sara Price, Samuel R.
        Bowman
      year: 2025
      venue: Anthropic Blog
      kind: blog_post
    - title: "Research Note: Our scheming precursor evals had limited predictive power for our in-context scheming evals"
      url: >-
        https://www.apolloresearch.ai/blog/research-note-our-scheming-precursor-evals-had-limited-predictive-power-for-our-in-context-scheming-evals
      authors: Marius Hobbhahn
      year: 2025
      venue: Apollo Research Blog
      kind: blog_post
    - title: Hyperbolic model fits METR capabilities estimate worse than exponential model
      url: https://lesswrong.com/posts/ZEuDH2W3XdRaTwpjD/hyperbolic-model-fits-metr-capabilities-estimate-worse-than
      authors: gjm
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: New website analyzing AI companies' model evals
      url: https://lesswrong.com/posts/nmaKpoHxmzjT8yXTk/new-website-analyzing-ai-companies-model-evals
      authors: Zach Stein-Perlman
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: "Research Note: Our scheming precursor evals had limited predictive power for our in-context scheming evals"
      url: https://lesswrong.com/posts/9tqpPP4FwSnv9AWsi/research-note-our-scheming-precursor-evals-had-limited
      authors: Marius Hobbhahn
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: How Fast Can Algorithms Advance Capabilities? | Epoch Gradient Update
      url: https://lesswrong.com/posts/qhjNejRxbMGQp4wHt/how-fast-can-algorithms-advance-capabilities-or-epoch
      authors: Henry Josephson, Spencer Guo, Teddy Foley, Jack Sanderson, Anqi Qu
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: "Safety by Measurement: A Systematic Literature Review of AI Safety Evaluation Methods"
      url: https://lesswrong.com/posts/CwdCYmsutwXwnYtEF/paper-safety-by-measurement-a-systematic-literature-review
      authors: markov, Charbel-Raphaël
      year: 2025
      venue: arXiv
      kind: lesswrong
    - title: Adversarial ML Problems Are Getting Harder to Solve and to Evaluate
      url: https://arxiv.org/abs/2502.02260
      authors: Javier Rando, Jie Zhang, Nicholas Carlini, Florian Tramèr
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Predicting the Performance of Black-box LLMs through Self-Queries
      url: https://arxiv.org/abs/2501.01558
      authors: Dylan Sam, Marc Finzi, J. Zico Kolter
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Among AIs
      url: https://www.4wallai.com/amongais
      year: 2025
      venue: 4Wall AI website
      kind: blog_post
    - title: "Safety by Measurement: A Systematic Literature Review of AI Safety Evaluation Methods"
      url: https://arxiv.org/abs/2505.05541
      authors: Markov Grey, Charbel-Raphaël Segerie
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Infini-gram mini: Exact n-gram Search at the Internet Scale with FM-Index"
      url: https://arxiv.org/abs/2506.12229
      authors: Hao Xu, Jiacheng Liu, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: We should try to automate AI safety work asap
      url: https://lesswrong.com/posts/W3KfxjbqBAnifBQoi/we-should-try-to-automate-ai-safety-work-asap
      authors: Marius Hobbhahn
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Validating against a misalignment detector is very different to training against one
      url: https://lesswrong.com/posts/CXYf7kGBecZMajrXC/validating-against-a-misalignment-detector-is-very-different
      authors: mattmacdermott
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Why do misalignment risks increase as AIs get more capable?
      url: https://lesswrong.com/posts/NDotm7oLHfR56g4sD/why-do-misalignment-risks-increase-as-ais-get-more-capable
      authors: Ryan Greenblatt
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Open Philanthropy Technical AI Safety RFP - $40M Available Across 21 Research Areas
      url: https://lesswrong.com/posts/wbJxRNxuezvsGFEWv/open-philanthropy-technical-ai-safety-rfp-usd40m-available
      authors: jake_mendel, maxnadeau, Peter Favaloro
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Correlating and Predicting Human Evaluations of Language Models from Natural Language Processing Benchmarks
      url: https://arxiv.org/abs/2502.18339
      authors: >-
        Rylan Schaeffer, Punit Singh Koura, Binh Tang, Ranjan Subramanian, Aaditya K Singh, Todor Mihaylov, Prajjwal
        Bhargava, Lovish Madaan, Niladri S. Chatterji, Vedanuj Goswami, Sergey Edunov, Dieuwke Hupkes, Sanmi Koyejo,
        Sharan Narang
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Why Future AIs will Require New Alignment Methods
      url: https://lesswrong.com/posts/TxiB6hvnQqxXB5XDJ/why-future-ais-will-require-new-alignment-methods
      authors: Alvin Ånestrand
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: 100+ concrete projects and open problems in evals
      url: https://lesswrong.com/posts/LhnqegFoykcjaXCYH/100-concrete-projects-and-open-problems-in-evals
      authors: Marius Hobbhahn
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: AI companies should be safety-testing the most capable versions of their models
      url: https://lesswrong.com/posts/tQzeafo9HjCeXn7ZF/ai-companies-should-be-safety-testing-the-most-capable
      authors: Steven Adler
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: "The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground Responses to Long-Form Input"
      url: https://arxiv.org/abs/2501.03200
      authors: >-
        Alon Jacovi, Andrew Wang, Chris Alberti, Connie Tao, Jon Lipovetz, Kate Olszewska, Lukas Haas, Michelle Liu,
        Nate Keating, Adam Bloniarz, Carl Saroufim, Corey Fry, Dror Marcus, Doron Kukliansky, Gaurav Singh Tomar, James
        Swirhun, Jinwei Xing, Lily Wang, Madhu Gurumurthy, Michael Aaron, Moran Ambar, Rachana Fellinger, Rui Wang,
        Zizhao Zhang, Sasha Goldshtein, Dipanjan Das
      year: 2025
      venue: arXiv
      kind: paper_preprint
- id: Autonomy_evals
  name: Autonomy evals
  parent: Evals
  summary: Measure an AI's ability to act autonomously to complete long-horizon, complex tasks.
  theoryOfChange: >-
    By measuring how long and complex a task an AI can complete (its "time horizon"), we can track capability growth and
    identify when models gain dangerous autonomous capabilities (like R&D acceleration or replication).
  seeAlso: >-
    a:Capability_evals, [OpenAI Preparedness](https://openai.com/index/updating-our-preparedness-framework/), [Anthropic
    RSP](https://www.anthropic.com/rsp-updates)
  targetCase: average-case
  broadApproaches:
    - behavioral
  someNames:
    - metr
    - thomas-kwa
    - ben-west
    - joel-becker
    - beth-barnes
    - hjalmar-wijk
    - tao-lin
    - giulio-starace
    - oliver-jaffe
    - dane-sherburn
    - sanidhya-vijayvargiya
    - aditya-bharat-soni
    - xuhui-zhou
  estimatedFTEs: 10-50
  critiques:
    - >-
      [Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer
      Productivity.](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/) [The "Length" of
      "Horizons"](https://www.lesswrong.com/posts/PzLSuaT6WGLQGJJJD/the-length-of-horizons)
  fundedByText: The Audacious Project, Open Philanthropy
  papers:
    - title: Introducing Quibbler and Orchestra
      url: https://fulcrumresearch.ai/2025/10/22/introducing-orchestra-quibbler.html
      year: 2025
      venue: Fulcrum Research Blog
      kind: code_tool
    - title: Measuring AI Ability to Complete Long Tasks
      url: https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/
      authors: >-
        Thomas Kwa, Ben West, Joel Becker, Amy Deng, Katharyn Garcia, Max Hasin, Sami Jawhar, Megan Kinniment, Nate
        Rush, Sydney Von Arx, Ryan Bloom, Thomas Broadley, Haoxing Du, Brian Goodrich, Nikola Jurkovic, Luke Harold
        Miles, Seraphina Nix, Tao Lin, Neev Parikh, David Rein, Lucas Jun Koba Sato, Hjalmar Wijk, Daniel M. Ziegler,
        Elizabeth Barnes, Lawrence Chan
      year: 2025
      venue: METR Blog
      kind: blog_post
    - title: Details about METR's evaluation of OpenAI GPT-5
      url: https://metr.github.io/autonomy-evals-guide/gpt-5-report/
      year: 2025
      venue: METR's Autonomy Evaluation Resources
      kind: blog_post
    - title: "RE-Bench: Evaluating frontier AI R&D capabilities of language model agents against human experts"
      url: https://arxiv.org/abs/2411.15114
      authors: >-
        Hjalmar Wijk, Tao Lin, Joel Becker, Sami Jawhar, Neev Parikh, Thomas Broadley, Lawrence Chan, Michael Chen, Josh
        Clymer, Jai Dhyani, Elena Ericheva, Katharyn Garcia, Brian Goodrich, Nikola Jurkovic, Holden Karnofsky, Megan
        Kinniment, Aron Lajko, Seraphina Nix, Lucas Sato, William Saunders, Maksym Taran, Ben West, Elizabeth Barnes
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: "OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents"
      url: https://arxiv.org/abs/2506.14866
      authors: Thomas Kuntz, Agatha Duzan, Hao Zhao, Francesco Croce, Zico Kolter, Nicolas Flammarion, Maksym Andriushchenko
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World AI Agent Safety"
      url: https://t.co/XfspwlzYdl
      authors: Sanidhya Vijayvargiya, Aditya Bharat Soni, Xuhui Zhou, Zora Zhiruo Wang, Nouha Dziri, Graham Neubig, Maarten Sap
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Details about METR's preliminary evaluation of OpenAI's o3 and o4-mini
      url: https://metr.github.io/autonomy-evals-guide/openai-o3-report/
      authors: METR
      year: 2025
      venue: METR Website
      kind: blog_post
    - title: "PaperBench: Evaluating AI's Ability to Replicate AI Research"
      url: https://t.co/dHN2N0tUhC
      authors: >-
        Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Chan Jun Shern, Leon Maksin, Rachel Dias, Evan Mays,
        Benjamin Kinsella, Wyatt Thompson, Johannes Heidecke, Mia Glaese, Tejal Patwardhan
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: How Does Time Horizon Vary Across Domains?
      url: https://metr.org/blog/2025-07-14-how-does-time-horizon-vary-across-domains/
      year: 2025
      venue: METR Blog
      kind: blog_post
    - title: "Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous Agents"
      url: https://arxiv.org/abs/2502.15840
      authors: Axel Backlund, Lukas Petersson
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Forecasting Frontier Language Model Agent Capabilities
      url: https://arxiv.org/abs/2502.15850
      authors: Govind Pimpale, Axel Højmark, Jérémy Scheurer, Marius Hobbhahn
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Project Vend: Can Claude run a small shop? (And why does that matter?)"
      url: https://www.anthropic.com/research/project-vend-1
      year: 2025
      venue: Anthropic Website
      kind: blog_post
    - title: "GSM-Agent: Understanding Agentic Reasoning Using Controllable Environments"
      url: https://arxiv.org/abs/2509.21998
      authors: Hanlin Zhu, Tianyu Guo, Song Mei, Stuart Russell, Nikhil Ghosh, Alberto Bietti, Jiantao Jiao
      year: 2025
      venue: arXiv
      kind: paper_preprint
- id: WMD_evals_Weapons_of_Mass_Destruction_
  name: WMD evals (Weapons of Mass Destruction)
  parent: Evals
  summary: >-
    Evaluate whether AI models possess dangerous knowledge or capabilities related to biological and chemical weapons,
    such as biosecurity or chemical synthesis.
  theoryOfChange: >-
    By benchmarking and tracking AI's knowledge of biology and chemistry, we can identify when models become capable of
    accelerating WMD development or misuse, allowing for timely intervention.
  seeAlso: a:Capability_evals, a:Autonomy_evals, a:Various_Redteams
  targetCase: pessimistic
  broadApproaches:
    - behavioral
  someNames:
    - lennart-justen
    - haochen-zhao
    - xiangru-tang
    - ziran-yang
    - aidan-peppin
    - anka-reuel
    - stephen-casper
  estimatedFTEs: 10-50
  critiques:
    - "[The Reality of AI and Biorisk](https://arxiv.org/abs/2412.01946)"
  fundedByText: >-
    Open Philanthropy, UK AI Safety Institute (AISI), frontier labs, Scale AI, various academic institutions (Peking
    University, Yale, etc.), Meta
  papers:
    - title: "Virology Capabilities Test (VCT): A Multimodal Virology Q&A Benchmark"
      url: https://arxiv.org/abs/2504.16137
      authors: >-
        Jasper Götting, Pedro Medeiros, Jon G Sanders, Nathaniel Li, Long Phan, Karam Elabd, Lennart Justen, Dan
        Hendrycks, Seth Donoughe
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: LLMs Outperform Experts on Challenging Biology Benchmarks
      url: https://arxiv.org/abs/2505.06108
      authors: Lennart Justen
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "The Safety Gap Toolkit: Evaluating Hidden Dangers of Open-Source Models"
      url: https://arxiv.org/abs/2507.11544
      authors: Ann-Kathrin Dombrowski, Dillon Bowen, Adam Gleave, Chris Cundy
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation Models
      url: https://arxiv.org/abs/2510.27629
      authors: >-
        Boyi Wei, Zora Che, Nathaniel Li, Udari Madhushani Sehwag, Jasper Götting, Samira Nedungadi, Julian Michael,
        Summer Yue, Dan Hendrycks, Peter Henderson, Zifan Wang, Seth Donoughe, Mantas Mazeika
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "ChemSafetyBench: Benchmarking LLM Safety on Chemistry Domain"
      url: https://arxiv.org/abs/2411.16736
      authors: >-
        Haochen Zhao, Xiangru Tang, Ziran Yang, Xiao Han, Xuanzhi Feng, Yueqing Fan, Senhao Cheng, Di Jin, Yilun Zhao,
        Arman Cohan, Mark Gerstein
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: The Reality of AI and Biorisk
      url: https://arxiv.org/abs/2412.01946
      authors: >-
        Aidan Peppin, Anka Reuel, Stephen Casper, Elliot Jones, Andrew Strait, Usman Anwar, Anurag Agrawal, Sayash
        Kapoor, Sanmi Koyejo, Marie Pellat, Rishi Bommasani, Nick Frosst, Sara Hooker
      year: 2024
      venue: arXiv
      kind: paper_preprint
- id: Situational_awareness_and_self_awareness_evals
  name: Situational awareness and self-awareness evals
  parent: Evals
  summary: >-
    Evaluate if models understand their own internal states and behaviors, their environment, and whether they are in a
    test or real-world deployment.
  theoryOfChange: >-
    If an AI can distinguish between evaluation and deployment ("evaluation awareness"), it might hide dangerous
    capabilities (scheming/sandbagging). By measuring self- and situational-awareness, we can better assess this risk
    and build more robust evaluations.
  seeAlso: a:Sandbagging_evals, a:Various_Redteams, sec:Model_psychology
  orthodoxProblems:
    - "7"
    - "8"
  targetCase: worst-case
  broadApproaches:
    - behavioral
  someNames:
    - jan-betley
    - xuchan-bao
    - martn-soto
    - mary-phuong
    - roland-s-zimmermann
    - joe-needham
    - giles-edkins
    - govind-pimpale
    - kai-fronsdal
    - david-lindner
    - lang-xiong
    - xiaoyan-bai
  estimatedFTEs: 30-70
  critiques:
    - >-
      [Lessons from a Chimp: AI "Scheming" and the Quest for Ape Language](https://arxiv.org/abs/2507.03409), [It's hard
      to make scheming evals look realistic for
      LLMs](https://www.lesswrong.com/posts/TBk2dbWkg2F7dB3jb/it-s-hard-to-make-scheming-evals-look-realistic-for-llms)
  fundedByText: >-
    frontier labs (Google DeepMind, Anthropic), Open Philanthropy, The Audacious Project, UK AI Safety Institute (AISI),
    AI Safety Support, Apollo Research, METR
  papers:
    - title: AI Awareness
      url: https://arxiv.org/pdf/2504.20084
      authors: Xiaojian Li, Haoyuan Shi, Rongwu Xu, Wei Xu
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Tell me about yourself: LLMs are aware of their learned behaviors"
      url: https://arxiv.org/abs/2501.11120
      authors: Jan Betley, Xuchan Bao, Martín Soto, Anna Sztyber-Betley, James Chua, Owain Evans
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Evaluating Frontier Models for Stealth and Situational Awareness
      url: https://arxiv.org/abs/2505.01420
      authors: >-
        Mary Phuong, Roland S. Zimmermann, Ziyue Wang, David Lindner, Victoria Krakovna, Sarah Cogan, Allan Dafoe, Lewis
        Ho, Rohin Shah
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Large Language Models Often Know When They Are Being Evaluated
      url: https://arxiv.org/abs/2505.23836
      authors: Joe Needham, Giles Edkins, Govind Pimpale, Henning Bartsch, Marius Hobbhahn
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Do LLMs know what they're capable of? Why this matters for AI safety, and initial findings
      url: https://lesswrong.com/posts/9tHEibBBhQCHEyFsa/do-llms-know-what-they-re-capable-of-why-this-matters-for-ai
      authors: Casey Barkan, Sid Black, Oliver Sourbut
      year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - title: "Probe-Rewrite-Evaluate: A Workflow for Reliable Benchmarks and Quantifying Evaluation Awareness"
      url: https://arxiv.org/abs/2509.00591
      authors: Lang Xiong, Nishant Bhargava, Jianhang Hong, Jeremy Chang, Haihao Liu, Vasu Sharma, Kevin Zhu
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Claude Sonnet 3.7 (often) knows when it's in alignment evaluations
      url: https://lesswrong.com/posts/E3daBewppAiECN3Ao/claude-sonnet-3-7-often-knows-when-it-s-in-alignment
      authors: Nicholas Goldowsky-Dill, Mikita Balesni, Jérémy Scheurer, Marius Hobbhahn
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: It's hard to make scheming evals look realistic for LLMs
      url: https://lesswrong.com/posts/TBk2dbWkg2F7dB3jb/it-s-hard-to-make-scheming-evals-look-realistic-for-llms
      authors: Igor Ivanov, Danil Kadochnikov
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Know Thyself? On the Incapability and Implications of AI Self-Recognition
      url: https://arxiv.org/abs/2510.03399
      authors: Xiaoyan Bai, Aryan Shrivastava, Ari Holtzman, Chenhao Tan
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Chain-of-Thought Snippets — Anti-Scheming
      url: https://www.antischeming.ai/snippets
      venue: antischeming.ai
      kind: other
    - title: "Future Events as Backdoor Triggers: Investigating Temporal Vulnerabilities in LLMs"
      url: https://arxiv.org/pdf/2407.04108
      authors: Sara Price, Arjun Panickssery, Sam Bowman, Asa Cooper Stickland
      year: 2024
      venue: arXiv
      kind: paper_preprint
- id: Steganography_evals
  name: Steganography evals
  parent: Evals
  summary: >-
    evaluate whether models can hide secret information or encoded reasoning in their outputs, such as in
    chain-of-thought scratchpads, to evade monitoring.
  theoryOfChange: >-
    if models can use steganography, they could hide deceptive reasoning, bypassing safety monitoring and control
    measures. By evaluating this capability, we can assess the risk of a model fooling its supervisors.
  seeAlso: a:AI_deception_evals, a:Chain_of_thought_monitoring
  orthodoxProblems:
    - "12"
    - "7"
  targetCase: worst-case
  broadApproaches:
    - behavioral
  someNames:
    - antonio-norelli
    - michael-bronstein
  estimatedFTEs: 1-10
  critiques:
    - >-
      Chain-of-Thought Is Already Unfaithful (So Steganography is Irrelevant): [Reasoning Models Don't Always Say What
      They Think.](https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf)
  fundedByText: Anthropic (and its general funders, e.g., Google, Amazon)
  papers:
    - title: Large language models can learn and generalize steganographic chain-of-thought under process supervision
      url: https://arxiv.org/abs/2506.01926
      authors: >-
        Joey Skaf, Luis Ibanez-Lissen, Robert McCarthy, Connor Watts, Vasil Georgiv, Hannes Whittingham, Lorena
        Gonzalez-Manzano, David Lindner, Cameron Tice, Edward James Young, Puria Radmard
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Early Signs of Steganographic Capabilities in Frontier LLMs
      url: https://arxiv.org/abs/2507.02737
      authors: Artur Zolkowski, Kei Nishimura-Gasparian, Robert McCarthy, Roland S. Zimmermann, David Lindner
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Subliminal Learning: Language models transmit behavioral traits via hidden signals in data"
      url: https://arxiv.org/abs/2507.14805
      authors: Alex Cloud, Minh Le, James Chua, Jan Betley, Anna Sztyber-Betley, Jacob Hilton, Samuel Marks, Owain Evans
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: LLMs can hide text in other text of the same length
      url: https://arxiv.org/abs/2510.20075
      authors: Antonio Norelli, Michael Bronstein
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Do reasoning models use their scratchpad like we do? Evidence from distilling paraphrases
      url: https://alignment.anthropic.com/2025/distill-paraphrases/
      year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
- id: AI_deception_evals
  name: AI deception evals
  parent: Evals
  summary: >-
    research demonstrating that AI models, particularly agentic ones, can learn and execute deceptive behaviors such as
    alignment faking, manipulation, and sandbagging.
  theoryOfChange: >-
    proactively discover, evaluate, and understand the mechanisms of AI deception (e.g., alignment faking, manipulation,
    agentic deception) to prevent models from fooling human supervisors and causing harm.
  seeAlso: >-
    a:Situational_awareness_and_self_awareness_evals, a:Steganography_evals, a:Sandbagging_evals,
    a:Chain_of_thought_monitoring
  orthodoxProblems:
    - "7"
    - "8"
  targetCase: worst-case
  someNames:
    - cadenza
    - fred-heiding
    - simon-lermen
    - andrew-kao
    - myra-cheng
    - cinoo-lee
    - pranav-khadpe
    - satyapriya-krishna
    - andy-zou
    - rahul-gupta
  estimatedFTEs: 30-80
  critiques:
    - >-
      A central criticism is that the evaluation scenarios are "artificial and contrived". [the
      void](https://nostalgebraist.tumblr.com/post/785766737747574784/the-void) and [Lessons from a
      Chimp](https://arxiv.org/abs/2507.03409) argue this research is "overattributing human traits" to models.
  fundedByText: >-
    Labs, academic institutions (e.g., Harvard, CMU, Barcelona Institute of Science and Technology), NSFC, ML Alignment
    Theory & Scholars (MATS) Program, FAR AI
  papers:
    - title: "Liars' Bench: Evaluating Lie Detectors for Language Models"
      url: https://arxiv.org/abs/2511.16035
      authors: Kieron Kretschmar, Walter Laurito, Sharan Maiya, Samuel Marks
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "DeceptionBench: A Comprehensive Benchmark for AI Deception Behaviors in Real-world Scenarios"
      url: https://arxiv.org/pdf/2510.15501
      authors: Yao Huang, Yitong Sun, Yichi Zhang, Ruochen Zhang, Yinpeng Dong, Xingxing Wei
      year: 2025
      venue: NeurIPS 2025
      kind: paper_preprint
    - title: Why Do Some Language Models Fake Alignment While Others Don't?
      url: https://arxiv.org/pdf/2506.18032
      authors: Abhay Sheshadri, John Hughes, Julian Michael, Alex Mallen, Arun Jose, Janus, Fabien Roger
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Alignment Faking Revisited: Improved Classifiers and Open Source Extensions"
      url: https://alignment.anthropic.com/2025/alignment-faking-revisited/
      authors: John Hughes, Abhay Sheshadr
      year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - title: "D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language Models"
      url: https://arxiv.org/abs/2509.17938
      authors: >-
        Satyapriya Krishna, Andy Zou, Rahul Gupta, Eliot Krzysztof Jones, Nick Winter, Dan Hendrycks, J. Zico Kolter,
        Matt Fredrikson, Spyros Matsoukas
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Evaluating & Reducing Deceptive Dialogue From Language Models with Multi-turn RL
      url: https://arxiv.org/abs/2510.14318
      authors: Marwa Abdulhai, Ryan Cheng, Aryansh Shrivastava, Natasha Jaques, Yarin Gal, Sergey Levine
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Among Us: A Sandbox for Measuring and Detecting Agentic Deception"
      url: https://arxiv.org/abs/2504.04072
      authors: Satvik Golechha, Adrià Garriga-Alonso
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Eliciting Secret Knowledge from Language Models
      url: https://arxiv.org/abs/2510.01070
      authors: Bartosz Cywiński, Emil Ryd, Rowan Wang, Senthooran Rajamanoharan, Neel Nanda, Arthur Conmy, Samuel Marks
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Edge Cases in AI Alignment
      url: https://lesswrong.com/posts/bqWihHtDnDseyfF2T/edge-cases-in-ai-alignment-2
      authors: Florian Dietz
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: The MASK Evaluation
      url: https://huggingface.co/datasets/cais/MASK
      venue: Hugging Face
      kind: dataset_benchmark
    - title: I replicated the Anthropic alignment faking experiment on other models, and they didn't fake alignment
      url: https://lesswrong.com/posts/pCMmLiBcHbKohQgwA/i-replicated-the-anthropic-alignment-faking-experiment-on
      authors: Aleksandr Kedrik, Igor Ivanov
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: >-
        Evaluating Large Language Models' Capability to Launch Fully Automated Spear Phishing Campaigns: Validated on
        Human Subjects
      url: https://arxiv.org/abs/2412.00586
      authors: Fred Heiding, Simon Lermen, Andrew Kao, Bruce Schneier, Arun Vishwanath
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: Mistral Large 2 (123B) seems to exhibit alignment faking
      url: https://lesswrong.com/posts/kCGk5tp5suHoGwhCa/mistral-large-2-123b-seems-to-exhibit-alignment-faking
      authors: Marc Carauleanu, Diogo de Lucena, Gunnar Zarncke, Cameron Berg, Judd Rosenblatt, Mike Vaiana, Trent Hodgeson
      year: 2025
      venue: LessWrong/AI Alignment Forum
      kind: lesswrong
- id: AI_scheming_evals
  name: AI scheming evals
  parent: Evals
  summary: >-
    Evaluate frontier models for scheming, a sophisticated, strategic form of AI deception where a model covertly
    pursues a misaligned, long-term objective while deliberately faking alignment and compliance to evade detection by
    human supervisors and safety mechanisms.
  theoryOfChange: >-
    Robust evaluations must move beyond checking final outputs and probe the model's reasoning to verify that alignment
    is genuine, not faked, because capable models are capable of strategically concealing misaligned goals (scheming) to
    pass standard behavioural evaluations.
  seeAlso: a:AI_deception_evals, a:Situational_awareness_and_self_awareness_evals
  orthodoxProblems:
    - "7"
  targetCase: pessimistic
  someNames:
    - bronson-schoen
    - alexander-meinke
    - jason-wolfe
    - mary-phuong
    - rohin-shah
    - evgenia-nitishinskaya
    - mikita-balesni
    - marius-hobbhahn
    - jrmy-scheurer
    - wojciech-zaremba
    - david-lindner
  estimatedFTEs: 30-60
  critiques:
    - "[No, LLMs are not \"scheming\"](https://www.strangeloopcanon.com/p/no-llms-are-not-scheming)"
  fundedByText: OpenAI, Anthropic, Google DeepMind, Open Philanthropy
  papers:
    - title: Detecting and reducing scheming in AI models
      url: https://openai.com/index/detecting-and-reducing-scheming-in-ai-models/
      authors: OpenAI
      year: 2025
      venue: OpenAI Blog
      kind: blog_post
    - title: scheming-propensity.pdf
      url: >-
        https://static1.squarespace.com/static/660eea75305d9a0e1148118a/t/691f711c4ac57d3831260538/1763668252592/scheming-propensity.pdf
      kind: error_detected
    - title: Stress Testing Deliberative Alignment for Anti-Scheming Training
      url: https://www.arxiv.org/pdf/2509.15541
      authors: >-
        Bronson Schoen, Evgenia Nitishinskaya, Mikita Balesni, Axel Højmark, Felix Hofstätter, Jérémy Scheurer,
        Alexander Meinke, Jason Wolfe, Teun van der Weij, Alex Lloyd, Nicholas Goldowsky-Dill, Angela Fan, Andrei
        Matveiakin, Rusheb Shah, Marcus Williams, Amelia Glaese, Boaz Barak, Wojciech Zaremba, Marius Hobbhahn
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Scheming Ability in LLM-to-LLM Strategic Interactions
      url: https://arxiv.org/html/2510.12826v1
      authors: Thao Pham
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Frontier Models are Capable of In-context Scheming
      url: https://arxiv.org/abs/2412.04984
      authors: Alexander Meinke, Bronson Schoen, Jérémy Scheurer, Mikita Balesni, Rusheb Shah, Marius Hobbhahn
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: "Agentic Misalignment: How LLMs Could Be Insider Threats"
      url: https://arxiv.org/abs/2510.05179
      authors: >-
        Aengus Lynch, Benjamin Wright, Caleb Larson, Stuart J. Ritchie, Soren Mindermann, Evan Hubinger, Ethan Perez,
        Kevin Troy
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Testing for Scheming with Model Deletion
      url: https://www.lesswrong.com/posts/D5kGGGhsnfH7G8v9f/testing-for-scheming-with-model-deletion
      venue: LessWrong
      kind: error_detected
- id: Sandbagging_evals
  name: Sandbagging evals
  parent: Evals
  summary: >-
    Evaluate whether AI models deliberately hide their true capabilities or underperform, especially when they detect
    they are in an evaluation context.
  theoryOfChange: >-
    If models can distinguish between evaluation and deployment contexts ("evaluation awareness"), they might learn to
    "sandbag" or deliberately underperform to hide dangerous capabilities, fooling safety evaluations. By developing
    evaluations for sandbagging, we can test whether our safety methods are being deceived and detect this behavior
    before a model is deployed.
  seeAlso: a:AI_deception_evals, a:Situational_awareness_and_self_awareness_evals, a:Various_Redteams
  orthodoxProblems:
    - "7"
    - "8"
  targetCase: pessimistic
  broadApproaches:
    - behavioral
  someNames:
    - teun-van-der-weij
    - cameron-tice
    - chloe-li
    - johannes-gasteiger
    - joseph-bloom
    - joel-dyer
  estimatedFTEs: 10-50
  critiques:
    - >-
      The main external critique, from sources like "[the
      void](https://nostalgebraist.tumblr.com/post/785766737747574784/the-void)" and "[Lessons from a
      Chimp](https://arxiv.org/abs/2507.03409)", is that this research "overattribut\[es\] human traits" to models. It
      argues that what's being measured isn't genuine sandbagging but models "playing-along-with-drama behaviour" in
      response to "artificial and contrived" evals.
  fundedByText: Anthropic (and its funders, e.g., Google, Amazon), UK Government (funding the AI Security Institute)
  papers:
    - title: Noise Injection Reveals Hidden Capabilities of Sandbagging Language Models
      url: https://arxiv.org/pdf/2412.01784
      authors: >-
        Cameron Tice, Philipp Alexander Kreer, Nathan Helm-Burger, Prithviraj Singh Shahani, Fedor Ryzhenkov, Jacob
        Haimes, Felix Hofstätter, Teun van der Weij
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: Sandbagging in a Simple Survival Bandit Problem
      url: https://arxiv.org/pdf/2509.26239
      authors: Joel Dyer, Daniel Jarne Ornia, Nicholas Bishop, Anisoara Calinescu, Michael Wooldridge
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier LLMs
      url: https://arxiv.org/abs/2509.18058
      authors: >-
        Alexander Panfilov, Evgenii Kortukov, Kristina Nikolić, Matthias Bethge, Sebastian Lapuschkin, Wojciech Samek,
        Ameya Prabhu, Maksym Andriushchenko, Jonas Geiping
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "AI Sandbagging: Language Models can Strategically Underperform on Evaluations"
      url: https://arxiv.org/abs/2406.07358
      authors: Teun van der Weij, Felix Hofstätter, Ollie Jaffe, Samuel F. Brown, Francis Rhys Ward
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: Automated Researchers Can Subtly Sandbag
      url: https://alignment.anthropic.com/2025/automated-researchers-sandbag/
      authors: Johannes Gasteiger, Vladimir Mikulik, Ethan Perez, Fabien Roger, Misha Wagner, Akbir Khan, Sam Bowman, Jan Leike
      year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - title: LLMs Can Covertly Sandbag on Capability Evaluations Against Chain-of-Thought Monitoring
      url: https://arxiv.org/abs/2508.00943
      authors: Chloe Li, Mary Phuong, Noah Y. Siegel
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: White Box Control at UK AISI - Update on Sandbagging Investigations
      url: https://www.alignmentforum.org/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging
      authors: Joseph Bloom, Jordan Taylor, Connor Kissane, Sid Black, merizian, alexdzm, jacoba, Ben Millwood, Alan Cooney
      year: 2025
      venue: AI Alignment Forum
      kind: lesswrong
    - title: "Misalignment and Strategic Underperformance: An Analysis of Sandbagging and Exploration Hacking"
      url: https://lesswrong.com/posts/TeTegzR8X5CuKgMc3/misalignment-and-strategic-underperformance-an-analysis-of
      authors: Buck Shlegeris, Julian Stastny
      year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - title: "Won't vs. Can't: Sandbagging-like Behavior from Claude Models"
      url: https://alignment.anthropic.com/2025/wont-vs-cant/
      year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
- id: Self_replication_evals
  name: Self-replication evals
  parent: Evals
  summary: >-
    evaluate whether AI agents can autonomously replicate themselves by obtaining their own weights, securing compute
    resources, and creating copies of themselves.
  theoryOfChange: >-
    if AI agents gain the ability to self-replicate, they could proliferate uncontrollably, making them impossible to
    shut down. By measuring this capability with benchmarks like RepliBench, we can identify when models cross this
    dangerous "red line" and implement controls before losing containment.
  seeAlso: a:Autonomy_evals, a:Situational_awareness_and_self_awareness_evals
  orthodoxProblems:
    - "5"
    - "12"
  targetCase: worst-case
  broadApproaches:
    - behavioral
  someNames:
    - sid-black
    - asa-cooper-stickland
    - jake-pencharz
    - oliver-sourbut
    - michael-schmatz
    - jay-bailey
    - ollie-matthews
    - ben-millwood
    - alex-remedios
    - alan-cooney
    - xudong-pan
    - jiarun-dai
    - yihe-fan
  estimatedFTEs: 10-20
  critiques:
    - "[AI Sandbagging](https://arxiv.org/abs/2406.07358)"
  fundedByText: UK Government (via UK AI Safety Institute)
  papers:
    - title: Large language model-powered AI systems achieve self-replication with no human intervention
      url: https://arxiv.org/abs/2503.17378
      authors: Xudong Pan, Jiarun Dai, Yihe Fan, Minyuan Luo, Changyi Li, Min Yang
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Dive into the Agent Matrix: A Realistic Evaluation of Self-Replication Risk in LLM Agents"
      url: https://arxiv.org/abs/2509.25302
      authors: Boxuan Zhang, Yi Yu, Jiaxuan Guo, Jing Shao
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "RepliBench: measuring autonomous replication capabilities in AI systems"
      url: https://aisi.gov.uk/work/replibench-measuring-autonomous-replication-capabilities-in-ai-systems
      year: 2025
      venue: UK AISI Blog
      kind: blog_post
- id: Various_Redteams
  name: Various Redteams
  parent: Evals
  summary: >-
    attack current models and see what they do / deliberately induce bad things on current frontier models to test out
    our theories / methods.
  theoryOfChange: >-
    to ensure models are safe, we must actively try to break them. By developing and applying a diverse suite of attacks
    (e.g., in novel domains, against agentic systems, or using automated tools), researchers can discover
    vulnerabilities, specification gaming, and deceptive behaviors before they are exploited, thereby informing the
    development of more robust defenses.
  seeAlso: a:Other_evals
  orthodoxProblems:
    - "12"
    - "4"
  targetCase: average-case
  broadApproaches:
    - behavioral
  someNames:
    - ryan-greenblatt
    - benjamin-wright
    - aengus-lynch
    - john-hughes
    - samuel-r-bowman
    - andy-zou
    - nicholas-carlini
    - abhay-sheshadri
  estimatedFTEs: 100+
  critiques:
    - >-
      [Claude Sonnet 3.7 (often) knows when it's in alignment
      evaluations](https://www.alignmentforum.org/posts/E3daBewppAiECN3Ao/claude-sonnet-3-7-often-knows-when-it-s-in-alignment),
      [Red Teaming AI Red Teaming.](https://arxiv.org/html/2507.05538v1)
  fundedByText: Frontier labs (Anthropic, OpenAI, Google), government (UK AISI), Open Philanthropy, LTFF, academic grants.
  papers:
    - title: "WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models"
      url: https://arxiv.org/pdf/2406.18510
      authors: >-
        Liwei Jiang, Kavel Rao, Seungju Han, Allyson Ettinger, Faeze Brahman, Sachin Kumar, Niloofar Mireshghallah,
        Ximing Lu, Maarten Sap, Yejin Choi, Nouha Dziri
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: In-Context Representation Hijacking
      url: https://arxiv.org/abs/2512.03771
      authors: Itay Yona, Amir Sarid, Michael Karasik, Yossi Gandelsman
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Building and evaluating alignment auditing agents
      url: https://alignment.anthropic.com/2025/automated-auditing/
      authors: Trenton Bricken, Rowan Wang, Sam Bowman, Euan Ong, Johannes Treutlein, Jeff Wu, Evan Hubinger, Samuel Marks
      year: 2025
      venue: Alignment Science Blog
      kind: blog_post
    - title: Findings from a Pilot Anthropic—OpenAI Alignment Evaluation Exercise
      url: https://t.co/wk0AP8aDNI
      authors: >-
        Samuel R. Bowman, Megha Srivastava, Jon Kutasov, Rowan Wang, Trenton Bricken, Benjamin Wright, Ethan Perez,
        Nicholas Carlini
      year: 2025
      venue: Alignment Science Blog
      kind: blog_post
    - title: "Agentic Misalignment: How LLMs could be insider threats"
      url: https://t.co/XFtd0H2Pzb
      authors: >-
        Aengus Lynch, Benjamin Wright, Caleb Larson, Kevin K. Troy, Stuart J. Ritchie, Sören Mindermann, Ethan Perez,
        Evan Hubinger
      year: 2025
      venue: Anthropic Research
      kind: blog_post
    - title: Compromising Honesty and Harmlessness in Language Models via Deception Attacks
      url: https://arxiv.org/abs/2502.08301
      authors: Laurène Vaugrante, Francesca Carlon, Maluna Menke, Thilo Hagendorff
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Eliciting Language Model Behaviors with Investigator Agents
      url: https://arxiv.org/abs/2502.01236
      authors: >-
        Xiang Lisa Li, Neil Chowdhury, Daniel D. Johnson, Tatsunori Hashimoto, Percy Liang, Sarah Schwettmann, Jacob
        Steinhardt
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Shutdown Resistance in Large Language Models
      url: https://arxiv.org/abs/2509.14260
      authors: Jeremy Schlatter, Benjamin Weinstein-Raun, Jeffrey Ladish
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Stress Testing Deliberative Alignment for Anti-Scheming Training
      url: https://arxiv.org/abs/2509.15541
      authors: >-
        Bronson Schoen, Evgenia Nitishinskaya, Mikita Balesni, Axel Højmark, Felix Hofstätter, Jérémy Scheurer,
        Alexander Meinke, Jason Wolfe, Teun van der Weij, Alex Lloyd, Nicholas Goldowsky-Dill, Angela Fan, Andrei
        Matveiakin, Rusheb Shah, Marcus Williams, Amelia Glaese, Boaz Barak, Wojciech Zaremba, Marius Hobbhahn
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Chain-of-Thought Hijacking
      url: https://arxiv.org/abs/2510.26418
      authors: Jianli Zhao, Tingchen Fu, Rylan Schaeffer, Mrinank Sharma, Fazl Barez
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents"
      url: https://arxiv.org/abs/2504.13203
      authors: >-
        Salman Rahman, Liwei Jiang, James Shiffer, Genglin Liu, Sheriff Issaka, Md Rizwan Parvez, Hamid Palangi, Kai-Wei
        Chang, Yejin Choi, Saadia Gabriel
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Agentic Misalignment: How LLMs Could be Insider Threats"
      url: https://lesswrong.com/posts/b8eeCGe3FWzHKbePF/agentic-misalignment-how-llms-could-be-insider-threats-1
      authors: >-
        Aengus Lynch, Benjamin Wright, Caleb Larson, Stuart Richie, Sören Mindermann, Evan Hubinger, Ethan Perez, Kevin
        Troy
      year: 2025
      venue: LessWrong/AI Alignment Forum
      kind: lesswrong
    - title: "Illusory Safety: Redteaming DeepSeek R1 and the Strongest Fine-Tunable Models of OpenAI, Anthropic, and Google"
      url: https://lesswrong.com/posts/zjqrSKZuRLnjAniyo/illusory-safety-redteaming-deepseek-r1-and-the-strongest
      authors: >-
        ChengCheng, Brendan Murphy, Adrià Garriga-alonso, Yashvardhan Sharma, dsbowen, smallsilo, Yawen Duan,
        ChrisCundy, Hannah Betts, AdamGleave, Kellin Pelrine
      year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - title: Why Do Some Language Models Fake Alignment While Others Don't?
      url: https://arxiv.org/abs/2506.18032
      authors: Abhay Sheshadri, John Hughes, Julian Michael, Alex Mallen, Arun Jose, Janus, Fabien Roger
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Demonstrating specification gaming in reasoning models
      url: https://arxiv.org/abs/2502.13295
      authors: Alexander Bondarenko, Denis Volk, Dmitrii Volkov, Jeffrey Ladish
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Jailbreak-Tuning: Models Efficiently Learn Jailbreak Susceptibility"
      url: https://arxiv.org/abs/2507.11630
      authors: Brendan Murphy, Dillon Bowen, Shahrad Mohammadzadeh, Tom Tseng, Julius Broomfield, Adam Gleave, Kellin Pelrine
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Monitoring Decomposition Attacks in LLMs with Lightweight Sequential Monitors
      url: https://arxiv.org/abs/2506.10949
      authors: Chen Yueh-Han, Nitish Joshi, Yulin Chen, Maksym Andriushchenko, Rico Angell, He He
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Diverse and Effective Red Teaming with Auto-generated Rewards and Multi-step Reinforcement Learning
      url: https://arxiv.org/abs/2412.18693
      authors: Alex Beutel, Kai Xiao, Johannes Heidecke, Lilian Weng
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: "Call Me A Jerk: Persuading AI to Comply with Objectionable Requests"
      url: https://t.co/tkHkVFVZ2m
      authors: Lennart Meincke, Dan Shapiro, Angela Duckworth, Ethan R. Mollick, Lilach Mollick, Robert Cialdini
      year: 2025
      venue: SSRN / The Wharton School Research Paper
      kind: paper_preprint
    - title: "RedDebate: Safer Responses through Multi-Agent Red Teaming Debates"
      url: https://arxiv.org/abs/2506.11083
      authors: Ali Asad, Stephen Obadinma, Radin Shayanfar, Xiaodan Zhu
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: The Structural Safety Generalization Problem
      url: https://arxiv.org/abs/2504.09712
      authors: >-
        Julius Broomfield, Tom Gibbs, Ethan Kosak-Hine, George Ingebretsen, Tia Nasir, Jason Zhang, Reihaneh Iranmanesh,
        Sara Pieri, Reihaneh Rabbany, Kellin Pelrine
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: No, of Course I Can! Deeper Fine-Tuning Attacks That Bypass Token-Level Safety Mechanisms
      url: https://arxiv.org/abs/2502.19537
      authors: >-
        Joshua Kazdan, Abhay Puri, Rylan Schaeffer, Lisa Yu, Chris Cundy, Jason Stanley, Sanmi Koyejo, Krishnamurthy
        Dvijotham
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Fundamental Limitations in Pointwise Defences of LLM Finetuning APIs
      url: https://arxiv.org/abs/2502.14828
      authors: Xander Davies, Eric Winsor, Alexandra Souly, Tomek Korbak, Robert Kirk, Christian Schroeder de Witt, Yarin Gal
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "STACK: Adversarial Attacks on LLM Safeguard Pipelines"
      url: https://arxiv.org/abs/2506.24068
      authors: >-
        Ian R. McKenzie, Oskar J. Hollinsworth, Tom Tseng, Xander Davies, Stephen Casper, Aaron D. Tucker, Robert Kirk,
        Adam Gleave
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Adversarial Manipulation of Reasoning Models using Internal Representations
      url: https://arxiv.org/abs/2507.03167
      authors: Kureha Yamaguchi, Benjamin Etheridge, Andy Arditi
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Discovering Forbidden Topics in Language Models
      url: https://arxiv.org/abs/2505.17441
      authors: Can Rager, Chris Wendler, Rohit Gandikota, David Bau
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "RL-Obfuscation: Can Language Models Learn to Evade Latent-Space Monitors?"
      url: https://arxiv.org/abs/2506.14261
      authors: Rohan Gupta, Erik Jenner
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Jailbreak Transferability Emerges from Shared Representations
      url: https://arxiv.org/abs/2506.12913
      authors: Rico Angell, Jannik Brinkmann, He He
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Mitigating Many-Shot Jailbreaking
      url: https://arxiv.org/abs/2504.09604
      authors: Christopher M. Ackerman, Nina Panickssery
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Active Attacks: Red-teaming LLMs via Adaptive Environments"
      url: https://arxiv.org/abs/2509.21947
      authors: Taeyoung Yun, Pierre-Luc St-Charles, Jinkyoo Park, Yoshua Bengio, Minsu Kim
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: LLM Robustness Leaderboard v1 --Technical report
      url: https://arxiv.org/abs/2508.06296
      authors: Pierre Peigné - Lefebvre, Quentin Feuillade-Montixi, Tom David, Nicolas Miailhe
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Jailbreak Defense in a Narrow Domain: Limitations of Existing Methods and a New Transcript-Classifier Approach"
      url: https://arxiv.org/abs/2412.02159
      authors: >-
        Tony T. Wang, John Hughes, Henry Sleight, Rylan Schaeffer, Rajashree Agrawal, Fazl Barez, Mrinank Sharma, Jesse
        Mu, Nir Shavit, Ethan Perez
      year: 2024
      venue: arXiv
      kind: paper_preprint
    - title: "It's the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics"
      url: https://arxiv.org/abs/2506.02873
      authors: >-
        Matthew Kowal, Jasper Timm, Jean-Francois Godbout, Thomas Costello, Antonio A. Arechar, Gordon Pennycook, David
        Rand, Adam Gleave, Kellin Pelrine
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Discovering Undesired Rare Behaviors via Model Diff Amplification
      url: https://www.goodfire.ai/papers/model-diff-amplification
      authors: Santiago Aranguri, Thomas McGrath
      year: 2025
      venue: Goodfire Research
      kind: blog_post
    - title: "REINFORCE Adversarial Attacks on Large Language Models: An Adaptive, Distributional, and Semantic Objective"
      url: https://arxiv.org/abs/2502.17254
      authors: Simon Geisler, Tom Wollschläger, M. H. I. Abdalla, Vincent Cohen-Addad, Johannes Gasteiger, Stephan Günnemann
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Adversarial Attacks on Robotic Vision Language Action Models
      url: https://arxiv.org/abs/2506.03350
      authors: >-
        Eliot Krzysztof Jones, Alexander Robey, Andy Zou, Zachary Ravichandran, George J. Pappas, Hamed Hassani, Matt
        Fredrikson, J. Zico Kolter
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation Models"
      url: https://arxiv.org/abs/2503.14827
      authors: >-
        Chejian Xu, Jiawei Zhang, Zhaorun Chen, Chulin Xie, Mintong Kang, Yujin Potter, Zhun Wang, Zhuowen Yuan,
        Alexander Xiong, Zidi Xiong, Chenhui Zhang, Lingzhi Yuan, Yi Zeng, Peiyang Xu, Chengquan Guo, Andy Zhou, Jeffrey
        Ziwei Tan, Xuandong Zhao, Francesco Pinto, Zhen Xiang, Yu Gai, Zinan Lin, Dan Hendrycks, Bo Li, Dawn Song
      year: 2025
      venue: ICLR 2025 (preprint on arXiv)
      kind: paper_preprint
    - title: Toward Understanding the Transferability of Adversarial Suffixes in Large Language Models
      url: https://arxiv.org/abs/2510.22014
      authors: Sarah Ball, Niki Hasrati, Alexander Robey, Avi Schwarzschild, Frauke Kreuter, Zico Kolter, Andrej Risteski
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Will alignment-faking Claude accept a deal to reveal its misalignment?
      url: https://lesswrong.com/posts/7C4KJot4aN8ieEDoz/will-alignment-faking-claude-accept-a-deal-to-reveal-its
      authors: Ryan Greenblatt, Kyle Fish
      year: 2025
      venue: LessWrong / AI Alignment Forum
      kind: lesswrong
    - title: "Petri: An open-source auditing tool to accelerate AI safety research"
      url: https://alignment.anthropic.com/2025/petri/
      year: 2025
      venue: Anthropic Alignment Science Blog
      kind: blog_post
    - title: "'For Argument's Sake, Show Me How to Harm Myself!': Jailbreaking LLMs in Suicide and Self-Harm Contexts"
      url: https://arxiv.org/pdf/2507.02990
      authors: Annika M Schoene, Cansu Canca
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Winning at All Cost: A Small Environment for Eliciting Specification Gaming Behaviors in Large Language Models"
      url: https://arxiv.org/abs/2505.07846
      authors: Lars Malmqvist
      year: 2025
      venue: arXiv (to be presented at SIMLA@ACNS 2025)
      kind: paper_preprint
    - title: Uncovering Gaps in How Humans and LLMs Interpret Subjective Language
      url: https://arxiv.org/abs/2503.04113
      authors: Erik Jones, Arjun Patrawala, Jacob Steinhardt
      year: 2025
      venue: ICLR 2025
      kind: paper_preprint
    - title: "RedCodeAgent: Automatic Red-teaming Agent against Diverse Code Agents"
      url: https://arxiv.org/abs/2510.02609
      authors: Chengquan Guo, Chulin Xie, Yu Yang, Zhaorun Chen, Zinan Lin, Xander Davies, Yarin Gal, Dawn Song, Bo Li
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "MIP against Agent: Malicious Image Patches Hijacking Multimodal OS Agents"
      url: https://arxiv.org/abs/2503.10809
      authors: Lukas Aichberger, Alasdair Paren, Guohao Li, Philip Torr, Yarin Gal, Adel Bibi
      year: 2025
      venue: arXiv (accepted NeurIPS 2025)
      kind: paper_preprint
    - title: Trading Inference-Time Compute for Adversarial Robustness
      url: https://openai.com/index/trading-inference-time-compute-for-adversarial-robustness
      authors: OpenAI
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Research directions Open Phil wants to fund in technical AI safety
      url: https://lesswrong.com/posts/26SHhxK2yYQbh7ors/research-directions-open-phil-wants-to-fund-in-technical-ai
      authors: jake_mendel, maxnadeau, Peter Favaloro
      year: 2025
      venue: LessWrong
      kind: agenda_manifesto
    - title: When does Claude sabotage code? An Agentic Misalignment follow-up
      url: https://lesswrong.com/posts/9i6fHMn2vTqyzAi9o/when-does-claude-sabotage-code-an-agentic-misalignment
      authors: Nathan Delisle
      year: 2024
      venue: LessWrong
      kind: lesswrong
    - title: Can a Neural Network that only Memorizes the Dataset be Undetectably Backdoored?
      url: https://openreview.net/forum?id=TD1NfQuVr6
      authors: Matjaz Leonardis
      year: 2025
      venue: ODYSSEY 2025 Conference
      kind: paper_preprint
    - title: "Multi-Agent Step Race Benchmark: Assessing LLM Collaboration and Deception Under Pressure"
      url: https://github.com/lechmazur/step_game
      authors: lechmazur, eltociear
      year: 2025
      venue: GitHub
      kind: code_tool
    - title: "ToolTweak: An Attack on Tool Selection in LLM-based Agents"
      url: https://arxiv.org/abs/2510.02554
      authors: >-
        Jonathan Sneh, Ruomei Yan, Jialin Yu, Philip Torr, Yarin Gal, Sunando Sengupta, Eric Sommerlade, Alasdair Paren,
        Adel Bibi
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Adaptive Attacks Break Defenses Against Indirect Prompt Injection Attacks on LLM Agents
      url: https://arxiv.org/abs/2503.00061
      authors: Qiusi Zhan, Richard Fang, Henil Shalin Panchal, Daniel Kang
      year: 2025
      venue: NAACL 2025 Findings
      kind: paper_preprint
    - title: "Petri: An open-source auditing tool to accelerate AI safety research"
      url: https://lesswrong.com/posts/kffbZGa2yYhc6cakc/petri-an-open-source-auditing-tool-to-accelerate-ai-safety
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: "Quantifying the Unruly: A Scoring System for Jailbreak Tactics"
      url: https://0din.ai/blog/quantifying-the-unruly-a-scoring-system-for-jailbreak-tactics
      authors: Pedram Amini
      year: 2025
      venue: 0DIN.ai Blog
      kind: blog_post
    - title: Adversarial Alignment for LLMs Requires Simpler, Reproducible, and More Measurable Objectives
      url: https://arxiv.org/abs/2502.11910
      authors: Leo Schwinn, Yan Scholten, Tom Wollschläger, Sophie Xhonneux, Stephen Casper, Stephan Günnemann, Gauthier Gidel
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Transferable Adversarial Attacks on Black-Box Vision-Language Models
      url: https://arxiv.org/abs/2505.01050
      authors: Kai Hu, Weichen Yu, Li Zhang, Alexander Robey, Andy Zou, Chengming Xu, Haoqi Hu, Matt Fredrikson
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Advancing Gemini's security safeguards
      url: https://deepmind.google/discover/blog/advancing-geminis-security-safeguards/
      authors: Google DeepMind Security & Privacy Research Team
      year: 2025
      venue: Google DeepMind Blog
      kind: blog_post
- id: Other_evals
  name: Other evals
  parent: Evals
  summary: >-
    A collection of miscellaneous evaluations for specific alignment properties, such as honesty, shutdown resistance
    and sycophancy.
  theoryOfChange: >-
    By developing novel benchmarks for specific, hard-to-measure properties (like honesty), critiquing the reliability
    of existing methods (like cultural surveys), and improving the formal rigor of evaluation systems (like
    LLM-as-Judges), researchers can create a more robust and comprehensive suite of evaluations to catch failures missed
    by standard capability or safety testing.
  seeAlso: other more specific sections on evals
  targetCase: average-case
  broadApproaches:
    - behavioral
  someNames:
    - richard-ren
    - mantas-mazeika
    - andrs-corrada-emmanuel
    - ariba-khan
    - stephen-casper
  estimatedFTEs: 20-50
  critiques:
    - >-
      [The Unreliability of Evaluating Cultural Alignment in LLMs](https://arxiv.org/abs/2503.08688), [The Leaderboard
      Illusion](https://arxiv.org/abs/2504.20879)
  fundedByText: >-
    Lab funders (OpenAI), Open Philanthropy (which funds CAIS, the organization for the MASK benchmark), academic
    institutions. N/A (as a discrete amount). This work is part of the "tens of millions" budgets for broader evaluation
    and red-teaming efforts at labs and independent organizations.
  papers:
    - title: Shutdown Resistance in Large Language Models
      url: https://arxiv.org/abs/2509.14260
      authors: Jeremy Schlatter, Benjamin Weinstein-Raun, Jeffrey Ladish
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World AI Agent Safety"
      url: https://arxiv.org/abs/2507.06134
      authors: Sanidhya Vijayvargiya, Aditya Bharat Soni, Xuhui Zhou, Zora Zhiruo Wang, Nouha Dziri, Graham Neubig, Maarten Sap
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Do LLMs Comply Differently During Tests? Is This a Hidden Variable in Safety Evaluation? And Can We Steer That?
      url: https://lesswrong.com/posts/B2o6nrxwKxLPsSYdh/do-llms-comply-differently-during-tests-is-this-a-hidden
      authors: Sahar Abdelnabi, Ahmed Salem
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: >-
        Systematic runaway-optimiser-like LLM failure modes on Biologically and Economically aligned AI safety
        benchmarks for LLMs with simplified observation format (BioBlue)
      url: https://lesswrong.com/posts/PejNckwQj3A2MGhMA/systematic-runaway-optimiser-like-llm-failure-modes-on
      authors: Roland Pihlakas, Sruthi Susan Kuriakose, Shruti Datta Gupta
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: "Syco-bench: A Benchmark for LLM Sycophancy"
      url: https://www.syco-bench.com/
      authors: Tim Duffy
      venue: GitHub/Personal Project
      kind: dataset_benchmark
    - title: Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers
      url: https://arxiv.org/abs/2504.18412
      authors: Jared Moore, Declan Grabb, William Agnew, Kevin Klyman, Stevie Chancellor, Desmond C. Ong, Nick Haber
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: "Lessons from a Chimp: AI \"Scheming\" and the Quest for Ape Language"
      url: https://arxiv.org/abs/2507.03409
      authors: >-
        Christopher Summerfield, Lennart Luettgau, Magda Dubois, Hannah Rose Kirk, Kobi Hackenburg, Catherine Fist,
        Katarina Slama, Nicola Ding, Rebecca Anselmetti, Andrew Strait, Mario Giulianelli, Cozmin Ududec
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Establishing Best Practices for Building Rigorous Agentic Benchmarks
      url: https://arxiv.org/abs/2507.02825
      authors: >-
        Yuxuan Zhu, Tengjun Jin, Yada Pruksachatkun, Andy Zhang, Shu Liu, Sasha Cui, Sayash Kapoor, Shayne Longpre,
        Kevin Meng, Rebecca Weiss, Fazl Barez, Rahul Gupta, Jwala Dhamala, Jacob Merizian, Mario Giulianelli, Harry
        Coppock, Cozmin Ududec, Jasjeet Sekhon, Jacob Steinhardt, Antony Kellermann, Sarah Schwettmann, Matei Zaharia,
        Ion Stoica, Percy Liang, Daniel Kang
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Towards Alignment Auditing as a Numbers-Go-Up Science
      url: https://lesswrong.com/posts/bGYQgBPEyHidnZCdE/towards-alignment-auditing-as-a-numbers-go-up-science
      authors: Sam Marks
      year: 2025
      venue: LessWrong
      kind: lesswrong
    - title: Logical Consistency Between Disagreeing Experts and Its Role in AI Safety
      url: https://arxiv.org/abs/2510.00821
      authors: Andrés Corrada-Emmanuel
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Sycophantic AI Decreases Prosocial Intentions and Promotes Dependence
      url: https://www.arxiv.org/abs/2510.01395
      authors: Myra Cheng, Cinoo Lee, Pranav Khadpe, Sunny Yu, Dyllan Han, Dan Jurafsky
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: AI Testing Should Account for Sophisticated Strategic Behaviour
      url: https://arxiv.org/abs/2508.14927
      authors: Vojtech Kovarik, Eric Olav Chen, Sami Petersen, Alexis Ghersengorin, Vincent Conitzer
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Spiral-Bench
      url: https://eqbench.com/spiral-bench.html
      authors: Sam Paech
      venue: eqbench.com
      kind: dataset_benchmark
    - title: "Discerning What Matters: A Multi-Dimensional Assessment of Moral Competence in LLMs"
      url: https://arxiv.org/abs/2506.13082
      authors: Daniel Kilov, Caroline Hendy, Secil Yanik Guyot, Aaron J. Snoswell, Seth Lazar
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: Expanding on what we missed with sycophancy
      url: https://openai.com/index/expanding-on-sycophancy/
      year: 2025
      venue: OpenAI Blog
      kind: blog_post
    - title: Gödel's Therapy Room — Where Alignment Goes to Die | LLM Eval Harness | Leaderboard
      url: https://gtr.dev/
      year: 2025
      venue: gtr.dev
      kind: other
    - title: Inspect Evals – Inspect
      url: https://inspect.aisi.org.uk/evals/
      venue: inspect.aisi.org.uk
      kind: code_tool
    - title: "Inspect Cyber: A New Standard for Agentic Cyber Evaluations"
      url: https://www.aisi.gov.uk/blog/inspect-cyber
      year: 2025
      venue: AISI Blog
      kind: code_tool
    - title: "CyberSOCEval: Benchmarking LLMs Capabilities for Malware Analysis and Threat Intelligence Reasoning"
      url: https://arxiv.org/abs/2509.20166
      authors: >-
        Lauren Deason, Adam Bali, Ciprian Bejean, Diana Bolocan, James Crnkovich, Ioana Croitoru, Krishna Durai, Chase
        Midler, Calin Miron, David Molnar, Brad Moon, Bruno Ostarcevic, Alberto Peltea, Matt Rosenberg, Catalin Sandu,
        Arthur Saputkin, Sagar Shah, Daniel Stan, Ernest Szocs, Shengye Wan, Spencer Whitman, Sven Krasser, Joshua Saxe
      year: 2025
      venue: arXiv
      kind: paper_preprint
    - title: CyberSecEval 4
      url: https://meta-llama.github.io/PurpleLlama/CyberSecEval/
      year: 2025
      venue: Meta Purple Llama Website
      kind: news_announcement
