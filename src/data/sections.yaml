# GENERATED FILE - Do not edit manually
# Source: src/data/pipelineData.yaml
# Regenerate with: npm run convert-pipeline
#
# This file is regenerated from the shallow-review pipeline output.
# Manual edits will be overwritten.

- id: Labs_giant_companies_
  name: Labs (giant companies)
- id: Black_box_safety_understand_and_control_current_model_behaviour_
  name: Black-box safety (understand and control current model behaviour)
- id: Iterative_alignment
  name: Iterative alignment
  description: >-
    Nudging base models by optimising their output. Worked on by the post-training teams at most labs, estimating the
    FTEs at \>500 in some sense. Funded by most of the industry.


    * *General theory of change:* "LLMs don't seem very dangerous and might scale to AGI, things are generally smooth,
    relevant capabilities are harder than alignment, assume no mesaoptimisers, assume that zero-shot deception is hard,
    assume a fundamentally humanish ontology is learned, assume no simulated agents, assume that noise in the data means
    that human preferences are not ruled out, assume that alignment is a superficial feature, assume that tuning for
    what we want will also get us to avoid what we don't want. Maybe assume that thoughts are translucent."


    * *General
    [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):*
    engineering  ·  *Target
    [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):*
    average


    * *General critiques:* [Bellot](https://arxiv.org/abs/2506.02923),
    [Alfour](https://cognition.cafe/p/ai-alignment-based-on-intentions), [STACK](https://arxiv.org/abs/2506.24068)*,*
    [AI Alignment Strategies from a Risk Perspective](https://arxiv.org/abs/2510.11235), [AI Alignment based on
    Intentions does not work](https://t.co/OTnrYRVsPS)*,* [Distortion of AI Alignment: Does Preference Optimization
    Optimize for Preferences?](https://arxiv.org/abs/2505.23749)*,* [Murphy’s Laws of AI Alignment: Why the Gap Always
    Wins](https://arxiv.org/abs/2509.05381), [Alignment remains a hard, unsolved
    problem](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)
  parent: Black_box_safety_understand_and_control_current_model_behaviour_
- id: Model_psychology
  name: Model psychology
  description: This section consists of a bottom-up set of things people happen to be doing, rather than a normative taxonomy.
  parent: Black_box_safety_understand_and_control_current_model_behaviour_
- id: Better_data
  name: Better data
  parent: Black_box_safety_understand_and_control_current_model_behaviour_
- id: Goal_robustness
  name: Goal robustness
  parent: Black_box_safety_understand_and_control_current_model_behaviour_
- id: White_box_safety_i_e_Interpretability_
  name: White-box safety (i.e. Interpretability)
  description: >-
    This section isn't very conceptually clean. See the [Open Problems](https://arxiv.org/abs/2501.16496) paper or
    [Deepmind](https://arxiv.org/pdf/2504.01849#page=92.33) for strong frames which are not useful for descriptive
    purposes.
- id: Concept_based_interpretability
  name: Concept-based interpretability
  parent: White_box_safety_i_e_Interpretability_
- id: Safety_by_construction
  name: Safety by construction
  description: Approaches which try to get assurances about system outputs while still being scalable.
- id: Make_AI_solve_it
  name: Make AI solve it
- id: Theory
  name: Theory
  description: >-
    *Develop a principled scientific understanding that will help us reliably understand and control current and future
    AI systems.*
- id: Corrigibility
  name: Corrigibility
  parent: Theory
- id: Ontology_Identification
  name: Ontology Identification
  parent: Theory
- id: Multi_agent_first
  name: Multi-agent first
- id: Evals
  name: Evals
