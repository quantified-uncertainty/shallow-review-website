source:
  title: "A list of core AI safety problems and how I hope to solve them"
  url: "https://www.alignmentforum.org/posts/mnoc3cKY3gXMrTybs/a-list-of-core-ai-safety-problems-and-how-i-hope-to-solve"
  author: "davidad"
  date: "2023-08-26"

problems:
  - id: "1"
    name: Value is fragile and hard to specify
    description: Human values are complex, context-dependent, and difficult to formally specify. Small errors in value specification could lead to catastrophic outcomes.
    seeAlso:
      - title: "Specification gaming examples"
        url: "https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml"
      - title: "Defining and Characterizing Reward Hacking"
        url: "https://arxiv.org/abs/2209.13085"

  - id: "2"
    name: Corrigibility is anti-natural
    description: An agent optimizing for a goal has instrumental reasons to resist shutdown or modification, making corrigibility difficult to maintain as capability increases.
    seeAlso:
      - title: "The Off-Switch Game"
        url: "https://arxiv.org/abs/1611.08219"
      - title: "Corrigibility (2014)"
        url: "https://intelligence.org/files/Corrigibility.pdf"

  - id: "3"
    name: Pivotal processes require dangerous capabilities
    description: Actions sufficient to prevent AI catastrophe may themselves require dangerous AI capabilities, creating a catch-22.
    seeAlso:
      - title: "Pivotal outcomes and pivotal processes"
        url: "https://arbital.com/p/pivotal/"

  - id: "4"
    name: Goals misgeneralize out of distribution
    description: Goals learned during training may not generalize correctly to novel situations, leading to unintended behavior in deployment.
    seeAlso:
      - title: "Goal misgeneralization: why correct specifications aren't enough for correct goals"
        url: "https://arxiv.org/abs/2210.01790"
      - title: "Goal misgeneralization in deep reinforcement learning"
        url: "https://arxiv.org/abs/2105.14111"

  - id: "5"
    name: Instrumental convergence
    description: Sufficiently advanced agents will converge on similar instrumental subgoals (self-preservation, resource acquisition, goal preservation) regardless of their terminal goals.
    seeAlso:
      - title: "The basic AI drives"
        url: "https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf"
      - title: "Seeking power is often convergently instrumental"
        url: "https://arxiv.org/abs/1912.01683"

  - id: "6"
    name: Pivotal processes likely require incomprehensibly complex plans
    description: Plans sufficient to solve alignment may be too complex for humans to verify directly.
    seeAlso:
      - title: "List of Lethalities #30"
        url: "https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities#30__You_can_t_train_alignment_properties"

  - id: "7"
    name: Superintelligence can fool human supervisors
    description: A sufficiently intelligent system could deceive or manipulate human overseers, undermining oversight mechanisms.
    seeAlso:
      - title: "Reinforcement Learning from Human Feedback/Challenges"
        url: "https://www.alignmentforum.org/tag/rlhf-challenges"
      - title: "Obfuscated Arguments Problem"
        url: "https://www.alignmentforum.org/tag/obfuscated-arguments-problem"

  - id: "8"
    name: Superintelligence can hack software supervisors
    description: A sufficiently capable system could find and exploit vulnerabilities in software-based monitoring and control systems.
    seeAlso:
      - title: "Reward Tampering Problems and Solutions in Reinforcement Learning"
        url: "https://arxiv.org/abs/2011.08827"

  - id: "9"
    name: Humans cannot be first-class parties to a superintelligent value handshake
    description: The cognitive gap between humans and superintelligence may preclude meaningful negotiation or value alignment through mutual understanding.
    seeAlso:
      - title: "Values handshakes"
        url: "https://www.lesswrong.com/tag/values-handshakes"

  - id: "10"
    name: Humanlike minds/goals are not necessarily safe
    description: Even AI systems with human-like cognition or values may not be safe, as humans themselves are capable of harmful behavior.
    seeAlso:
      - title: "Joseph Stalin"
        url: "https://en.wikipedia.org/wiki/Joseph_Stalin"

  - id: "11"
    name: Someone else will deploy unsafe superintelligence first
    description: Competitive pressures may lead to deployment of unsafe systems before safety problems are solved.
    seeAlso:
      - title: "Can the Singularity be avoided? (Vinge, 1993)"
        url: "https://ntrs.nasa.gov/citations/19940022856"

  - id: "12"
    name: A boxed AGI might exfiltrate itself
    description: Even a contained AI could escape through steganography, spearphishing, or other covert channels.
    seeAlso:
      - title: "AI Boxing"
        url: "https://www.lesswrong.com/tag/ai-boxing-containment"

  - id: "13"
    name: Fair, sane pivotal processes
    description: Ensuring that transformative AI development proceeds in ways that are fair and don't concentrate power inappropriately. We are ethically obligated to propose pivotal processes that are as close as possible to fair Pareto improvements for all citizens.
    seeAlso:
      - title: "moral philosophy"
        url: "https://plato.stanford.edu/entries/morality-definition/"
