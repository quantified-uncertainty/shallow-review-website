labs:
  - id: openai-safety
    name: OpenAI Safety
    lesswrongTags:
      - openai
    keywords:
      - deliberative-alignment
      - scheming
      - personas
      - evals
      - circuits
      - emergent-capabilities
    papers:
      - title: Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation
        url: https://arxiv.org/abs/2503.11926
      - title: Persona Features Control Emergent Misalignment
        url: https://arxiv.org/abs/2506.19823
      - title: Stress Testing Deliberative Alignment for Anti-Scheming Training
        url: https://arxiv.org/abs/2509.15541
      - title: "Deliberative Alignment: Reasoning Enables Safer Language Models"
        url: https://arxiv.org/abs/2412.16339
      - title: Toward understanding and preventing misalignment generalization
        url: https://openai.com/index/emergent-misalignment
      - title: Our updated Preparedness Framework
        url: https://openai.com/index/updating-our-preparedness-framework/
      - title: Trading Inference-Time Compute for Adversarial Robustness
        url: https://arxiv.org/abs/2501.18841
      - title: "Findings from a pilot Anthropic–OpenAI alignment evaluation exercise: OpenAI Safety Tests"
        url: https://openai.com/index/openai-anthropic-safety-evaluation
      - title: Safety evaluations hub
        url: https://openai.com/safety/evaluations-hub
      - title: "Small-to-Large Generalization: Data Influences Models Consistently Across Scale"
        url: https://arxiv.org/abs/2505.16260
      - title: https://cdn.openai.com/pdf/41df8f28-d4ef-43e9-aed2-823f9393e470/circuit-sparsity-paper.pdf
        url: https://cdn.openai.com/pdf/41df8f28-d4ef-43e9-aed2-823f9393e470/circuit-sparsity-paper.pdf
    critiques:
      - "[Stein-Perlman](https://ailabwatch.org/companies/openai)"
      - "[Stewart](https://intelligence.org/2025/03/31/a-response-to-openais-how-we-think-about-safety-and-alignment/)"
      - "[underelicitation](https://www.lesswrong.com/posts/AK6AihHGjirdoiJg6/ai-companies-eval-reports-mostly-don-t-support-their-claims)"
      - "[Midas](https://www.openaifiles.org/transparency-and-safety)"
      - "[defense](https://www.wired.com/story/openai-anduril-defense/)"
      - >-
        [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\)
      - "[difficult](https://conversationswithtyler.com/episodes/sam-altman-2/)"
    someNames:
      - johannes-heidecke
      - boaz-barak
      - mia-glaese
      - jenny-nitishinskaya
      - lama-ahmad
      - naomi-bashkansky
      - miles-wang
      - wojciech-zaremba
      - david-robinson
      - zico-kolter
      - jerry-tworek
      - eric-wallace
      - olivia-watkins
      - kai-chen
      - chris-koch
      - andrea-vallone
    fundedBy:
      - microsoft
      - amazon
      - oracle
      - nvidia
      - softbank
      - amd
      - coatue
      - thrive
      - a16z
      - fidelity
      - founders-fund
      - sequoia
    resources:
      - title: OpenAI Safety
        url: https://openai.com/safety/
    wikipedia: https://en.wikipedia.org/wiki/OpenAI
    hostOrgStructure: public benefit corp
    teams: >-
      Alignment, Safety Systems (Interpretability, Safety Oversight, Pretraining Safety, Robustness, Safety Research, Trustworthy AI, new Misalignment Research team
      [coming](https://archive.is/eDB1D)), Preparedness, Model Policy, Safety and Security Committee, Safety Advisory Group. The [Persona Features](https://www.arxiv.org/pdf/2506.19823) paper had a
      distinct author list. No named successor to Superalignment.
    publicAlignmentAgenda: >-
      [None](https://openai.com/safety/how-we-think-about-safety-alignment/). Barak [offers](https://www.lesswrong.com/posts/3jnziqCF3vA2NXAKp/six-thoughts-on-ai-safety) personal
      [views](https://windowsontheory.org/2025/06/24/machines-of-faithful-obedience/).
    publicPlan: "[Preparedness Framework](https://cdn.openai.com/pdf/18a02b5d-6b67-4cec-ab64-68cdfbddebcd/preparedness-framework-v2.pdf)"
  - id: deepmind-responsibility-safety
    name: Deepmind Responsibility & Safety
    lesswrongTags:
      - deepmind
    keywords:
      - situational-awareness
      - scheming
      - sycophancy
      - jailbreaking
      - evals
      - scalable-oversight
      - sparse-autoencoders
    papers:
      - title: Evaluating Frontier Models for Stealth and Situational Awareness
        url: https://arxiv.org/abs/2505.01420
      - title: When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors
        url: https://arxiv.org/abs/2507.05246
      - title: "MONA: Managed Myopia with Approval Feedback"
        url: https://alignmentforum.org/posts/zWySWKuXnhMDhgwc3/mona-managed-myopia-with-approval-feedback-2
      - title: Consistency Training Helps Stop Sycophancy and Jailbreaks
        url: https://arxiv.org/abs/2510.27062
      - title: An Approach to Technical AGI Safety and Security
        url: https://arxiv.org/abs/2504.01849
      - title: "InfAlign: Inference-aware language model alignment"
        url: https://arxiv.org/abs/2412.19792
      - title: Negative Results for SAEs On Downstream Tasks and Deprioritising SAE Research (GDM Mech Interp Team Progress Update \#2)
        url: https://alignmentforum.org/posts/4uXCAJNuPKtKBsi28/negative-results-for-saes-on-downstream-tasks
      - title: Taking a responsible path to AGI
        url: https://deepmind.google/discover/blog/taking-a-responsible-path-to-agi/
      - title: Evaluating potential cybersecurity threats of advanced AI
        url: https://deepmind.google/discover/blog/evaluating-potential-cybersecurity-threats-of-advanced-ai
      - title: On Google's Safety Plan
        url: https://lesswrong.com/posts/hvEikwtsbf6zaXG2s/on-google-s-safety-plan
        authors: Zvi
      - title: On the Meta and DeepMind Safety Frameworks
        url: https://lesswrong.com/posts/etqbEF4yWoGBEaPro/on-the-meta-and-deepmind-safety-frameworks
        authors: Zvi
      - title: Incentives for Responsiveness, Instrumental Control and Impact
        url: https://arxiv.org/pdf/2001.07118
    critiques:
      - "[Stein-Perlman](https://ailabwatch.org/companies/deepmind)"
      - >-
        [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\)
      - "[underelicitation](https://www.lesswrong.com/posts/AK6AihHGjirdoiJg6/ai-companies-eval-reports-mostly-don-t-support-their-claims)"
    fundedBy:
      - google-deepmind
    resources:
      - title: DeepMind Responsibility & Safety
        url: https://deepmind.google/about/responsibility-safety/
    structure: research laboratory subsidiary of a for-profit
    teams: >-
      [ASAT](https://www.alignmentforum.org/posts/wqz5CRzqWkvzoatBG/agi-safety-and-alignment-google-deepmind-is-hiring) ("AGI Alignment", amplified oversight and interpretability; plus "Frontier
      Safety", framework development and implementation), Gemini Safety, [Voices of All in
      Alignment](https://www.edinburgh-robotics.org/events/whose-gold-aligning-ai-diverse-views-what%E2%80%99s-safe-aligned-and-beneficial), AGI Safety Council, Responsibility and Safety Council.
      Sort-of the [Causal Incentives Working Group](https://causalincentives.com/) too.
    publicAlignmentAgenda: >-
      [An Approach to Technical AGI
      Safety](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/evaluating-potential-cybersecurity-threats-of-advanced-ai/An_Approach_to_Technical_AGI_Safety_Apr_2025.pdf)
    framework: "[Frontier Safety Framework](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/updating-the-frontier-safety-framework/Frontier%20Safety%20Framework%202.0.pdf)"
    someNames:
      - rohin-shah
      - allan-dafoe
      - anca-dragan
      - dave-orr
      - alex-irpan
      - alex-turner
      - anna-wang
      - arthur-conmy
      - david-lindner
      - jonah-brown-cohen
      - lewis-ho
      - neel-nanda
      - raluca-ada-popa
      - rishub-jain
      - rory-greig
      - scott-emmons
      - sebastian-farquhar
      - senthooran-rajamanoharan
      - sophie-bridgers
      - tobi-ijitoye
      - tom-everitt
      - victoria-krakovna
      - vikrant-varma
      - vladimir-mikulik
      - zac-kenton
      - noah-goodman
      - four-flynn
  - id: anthropic-safety
    name: Anthropic Safety
    lesswrongTags:
      - anthropic-org
    keywords:
      - scheming
      - sabotage
      - deception
      - circuits
      - sparse-autoencoders
      - safety-cases
      - constitutional-ai
      - classifiers
      - jailbreaking
      - model-organisms
      - scalable-oversight
    papers:
      - title: "Agentic Misalignment: How LLMs could be insider threats"
        url: https://anthropic.com/research/agentic-misalignment
      - title: "SHADE-Arena: Evaluating sabotage and monitoring in LLM agents"
        url: https://anthropic.com/research/shade-arena-sabotage-monitoring
      - title: Forecasting Rare Language Model Behaviors
        url: https://arxiv.org/abs/2502.16797
      - title: Why Do Some Language Models Fake Alignment While Others Don't?
        url: https://alignmentforum.org/posts/ghESoA8mo3fv9Yx3E/why-do-some-language-models-fake-alignment-while-others-don
      - title: "Petri: An open-source auditing tool to accelerate AI safety research"
        url: https://alignment.anthropic.com/2025/petri
      - title: Signs of introspection in large language models
        url: https://anthropic.com/research/introspection
      - title: Recommendations for Technical AI Safety Research Directions
        url: https://alignment.anthropic.com/2025/recommended-directions/index.html
      - title: Putting up Bumpers
        url: https://alignment.anthropic.com/2025/bumpers/
      - title: Three Sketches of ASL-4 Safety Case Components
        url: https://alignment.anthropic.com/2024/safety-cases/index.html
      - title: Open-sourcing circuit tracing tools
        url: https://anthropic.com/research/open-source-circuit-tracing
      - title: Findings from a Pilot Anthropic—OpenAI Alignment Evaluation Exercise
        url: https://alignment.anthropic.com/2025/openai-findings
      - title: Reasoning models don't always say what they think
        url: https://www.anthropic.com/research/reasoning-models-dont-say-think
      - title: On the Biology of a Large Language Model
        url: https://transformer-circuits.pub/2025/attribution-graphs/biology.html
      - title: "Circuit Tracing: Revealing Computational Graphs in Language Models"
        url: https://transformer-circuits.pub/2025/attribution-graphs/methods.html
      - title: Auditing language models for hidden objectives
        url: https://www.anthropic.com/research/auditing-hidden-objectives
      - title: "Constitutional Classifiers: Defending against universal jailbreaks"
        url: https://www.anthropic.com/research/constitutional-classifiers
      - title: Existing Safety Frameworks Imply Unreasonable Confidence
        url: https://lesswrong.com/posts/7ExkgcDudwhag73vw/existing-safety-frameworks-imply-unreasonable-confidence
    someNames:
      - chris-olah
      - evan-hubinger
      - sam-marks
      - johannes-treutlein
      - sam-bowman
      - euan-ong
      - fabien-roger
      - adam-jermyn
      - holden-karnofsky
    critiques:
      - "[Stein](https://ailabwatch.org/anthropic-opinions)"
      - "[Perlman](https://ailabwatch.org/companies/anthropic)"
      - "[Casper](https://www.lesswrong.com/posts/pH6tyhEnngqWAXi9i/eis-xiii-reflections-on-anthropic-s-sae-research-circa-may#A_review___thoughts)"
      - >-
        [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\)
      - "[on Anthropic](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#ref-14)"
      - "[underelicitation](https://www.lesswrong.com/posts/AK6AihHGjirdoiJg6/ai-companies-eval-reports-mostly-don-t-support-their-claims)"
      - "[Greenblatt](https://nitter.net/RyanPGreenblatt/status/1925992236648464774)"
      - "[defense](https://techcrunch.com/2024/11/07/anthropic-teams-up-with-palantir-and-aws-to-sell-its-ai-to-defense-customers/)"
    fundedBy:
      - amazon
      - google-deepmind
      - fidelity
      - lightspeed
      - blackrock
      - coatue
      - general-catalyst
      - goldman-sachs
    resources:
      - title: Anthropic Research
        url: https://www.anthropic.com/research
      - title: Alignment Science Blog
        url: https://alignment.anthropic.com/
    wikipedia: https://en.wikipedia.org/wiki/Anthropic
    hostOrgStructure: public-benefit corp
    teams: >-
      Scalable Alignment (Leike), Alignment Evals (Bowman), [Interpretability](https://transformer-circuits.pub/) (Olah), Model Psychiatry (Lindsey), Character (Askell), Alignment Stress-Testing
      (Hubinger), Frontier Red Team (Graham), Safeguards (Sharma), Trust and Safety (Sanderford), Model Welfare (Fish).
    publicAlignmentAgenda: >-
      [directions](https://alignment.anthropic.com/2025/recommended-directions/), [bumpers](https://alignment.anthropic.com/2025/bumpers/), [checklist](https://sleepinyourhat.github.io/checklist/),
      [old vague view](https://www.anthropic.com/news/core-views-on-ai-safety)
    framework: "[RSP](https://www-cdn.anthropic.com/872c653b2d0501d6ab44cf87f43e1dc4853e4d37.pdf)"
  - id: xai
    name: xAI
    lesswrongTags:
      - xai
    papers: []
    someNames:
      - dan-hendrycks
      - juntang-zhuang
      - toby-pohlen
      - lianmin-zheng
      - piaoyang-cui
      - nikita-popov
      - ying-sheng
      - sehoon-kim
    critiques:
      - "[framework](https://www.lesswrong.com/posts/hQyrTDuTXpqkxrnoH/xai-s-new-safety-framework-is-dreadful)"
      - "[hacking](https://x.com/g_leech_/status/1990543987846078854)"
      - "[broken promises](https://x.com/g_leech_/status/1990734517145911593)"
      - "[Stein](https://ailabwatch.org/companies/xai)"
      - "[Perlman](https://ailabwatch.org/resources/integrity#xai)"
      - "[insecurity](https://nitter.net/elonmusk/status/1961904269545648624)"
      - >-
        [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\)
    fundedBy:
      - a16z
      - blackrock
      - fidelity
      - lightspeed
      - morgan-stanley
      - sequoia
    resources:
      - title: xAI
        url: https://x.ai/
    wikipedia: https://en.wikipedia.org/wiki/XAI_(company)
    hostOrgStructure: "[for-profit](https://www.cnbc.com/amp/2025/08/25/elon-musk-xai-dropped-public-benefit-corp-status-while-fighting-openai.html)"
    teams: "[Applied Safety](https://job-boards.greenhouse.io/xai/jobs/4944324007), Model Evaluation. Nominally focussed on misuse."
    publicAlignmentAgenda: None.
    framework: "[Risk Management Framework](https://data.x.ai/2025-08-20-xai-risk-management-framework.pdf)"
  - id: meta
    name: Meta
    keywords:
      - rlhf
      - jailbreaking
      - agentic-systems
      - evals
    papers:
      - title: "The Alignment Waltz: Jointly Training Agents to Collaborate for Safety"
        url: "% 9https://arxiv.org/pdf/2510.08240"
      - title: Large Reasoning Models Learn Better Alignment from Flawed Thinking
        url: https://arxiv.org/pdf/2510.00938
      - title: Robust LLM safeguarding via refusal feature adversarial training
        url: https://arxiv.org/pdf/2409.20089
      - title: Code World Model Preparedness Report
        url: >-
          https://scontent-lhr8-1.xx.fbcdn.net/v/t39.2365-6/557601942_1468972530985309_838842257265552803_n.pdf?_nc_cat=108&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=_H33_VKF3ZUQ7kNvwFog8dd&_nc_oc=AdlNtqCDY4HafZ3-d5rb26AF5f2m0X46SGdKhVq3jLqwpNf_wEXhdQnH7_30ychiZWk&_nc_zt=14&_nc_ht=scontent-lhr8-1.xx&_nc_gid=QvW_ePiaF4E-PxOf30MWyg&oh=00_AfiZC5G4ODvWhiy0MuVH8PSlUFrW8RDQQ8tdr6Zec5k9aA&oe=691A6D09
      - title: "Agents Rule of Two: A Practical Approach to AI Agent Security"
        url: https://ai.meta.com/blog/practical-ai-agent-security/
    someNames:
      - shuchao-bi
      - hongyuan-zhan
      - jingyu-zhang
      - haozhu-wang
      - eric-michael-smith
      - sid-wang
      - amr-sharaf
      - mahesh-pasupuleti
      - jason-weston
      - shengyun-peng
      - ivan-evtimov
      - song-jiang
      - pin-yu-chen
      - evangelia-spiliopoulou
      - lei-yu
      - virginie-do
      - karen-hambardzumyan
      - nicola-cancedda
      - adina-williams
    critiques:
      - >-
        [extreme
        underelicitation](https://googleprojectzero.blogspot.com/2024/06/project-naptime.html#:~:text=We%20find%20that%2C%20by%20refining%20the%20testing%20methodology%20to%20take%20advantage%20of%20modern%20LLM%20capabilities%2C%20significantly%20better%20performance%20in%20vulnerability%20discovery%20can%20be%20achieved.%20To%20facilitate%20effective%20evaluation%20of%20LLMs%20for%20vulnerability%20discovery%2C%20we%20propose%20below%20a%20set%20of%20guiding%20principles.)
      - "[Stein](https://ailabwatch.org/companies/meta)"
      - "[Perlman](https://ailabwatch.org/companies/meta)"
      - >-
        [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\)
    fundedBy:
      - meta-ai
    resources:
      - title: Meta AI
        url: https://ai.meta.com/
    wikipedia: https://en.wikipedia.org/wiki/Meta_AI
    hostOrgStructure: for-profit
    teams: Safety "integrated into" capabilities research, Meta Superintelligence Lab. But also FAIR Alignment, [Brain and AI](https://www.metacareers.com/jobs/1319148726628205).
    publicAlignmentAgenda: None.
    framework: "[FAF](https://ai.meta.com/static-resource/meta-frontier-ai-framework/?utm_source=newsroom&utm_medium=web&utm_content=Frontier_AI_Framework_PDF&utm_campaign=Our_Approach_to_Frontier_AI_blog)"
  - id: others
    name: Others
    description: |
      - Amazon's [Nova Pro](https://arxiv.org/pdf/2506.12103v1) is around the level of Llama 3 90B, which in turn is around the level of the original GPT-4. So 2 years behind. But they have their own [chip](https://www.businessinsider.com/startups-amazon-ai-chips-less-competitive-nvidia-gpus-trainium-aws-2025-11).
      - Microsoft are [now](https://www.dwarkesh.com/p/satya-nadella-2) mid-training on top of GPT-5. MAI-1-preview is [around](https://lmarena.ai/leaderboard/text) DeepSeek V3.0 on Arena. They [continue](https://arxiv.org/abs/2506.22405v1) to focus on medical diagnosis. You can [request](https://forms.microsoft.com/pages/responsepage.aspx?id=v4j5cvGGr0GRqy180BHbRyRliS0ly-JEvgSpwo3yWyhUQkdTQktBUkFaWERHR1JFRjgwMlZUUkQxTC4u&route=shorturl) access.
      - Mistral have a reasoning model, [Magistral Medium](https://arxiv.org/pdf/2506.10910), and released the weights of a little 24B version. It's a bit worse than R1 at pass@1.

      The Chinese companies [don't](https://futureoflife.org/wp-content/uploads/2025/07/FLI-AI-Safety-Index-Report-Summer-2025.pdf#page=3) [attempt](https://ailabwatch.org/companies/deepseek) to be safe, even in the prosaic safeguards sense. They drop the weights [immediately](https://x.com/natolambert/status/1991915728992190909) after post-training finishes. They're mostly open weights and closed data. The companies are often [severely](https://www.wsj.com/tech/ai/china-us-ai-chip-restrictions-effect-275a311e) compute-constrained. See [here](https://www.gleech.org/paper) for doubts about their capabilities.

      - Alibaba's Qwen3-etc-etc is [nominally](https://artificialanalysis.ai/leaderboards/models) at the level of Gemini 2.5 Flash. Maybe the only Chinese model with a [large](https://www.atomproject.ai/#:~:text=Model%20Adoption%20Trends) Western userbase, including businesses, but since it's self-hosted this doesn't translate into profits for them yet. On [one ad hoc test](https://www.gleech.org/paper) it was the only Chinese model not to collapse OOD, but the Qwen2.5 corpus was severely contaminated.
      - DeepSeek's v3.2 is [nominally](https://artificialanalysis.ai/leaderboards/models) around the same as Qwen. The CCP made them [waste](https://arstechnica.com/ai/2025/08/deepseek-delays-next-ai-model-due-to-poor-performance-of-chinese-made-chips/) months trying Huawei chips.
      - Moonshot's Kimi-K2-Thinking has some nominally [frontier](https://artificialanalysis.ai/) benchmark results and a pleasant style but does not [seem](https://x.com/METR_Evals/status/1991658241932292537) actually frontier.
      - Baidu's [ERNIE 5](https://x.com/Baidu_Inc/status/1988820837898829918) is again nominally very strong, a bit better than DeepSeek. This new one might not be open weights?
      - Z's [GLM-4.6](https://z.ai/blog/glm-4.6) is around the same as Qwen.
      - MiniMax's M2 is nominally better than Qwen, [around the same](https://artificialanalysis.ai/leaderboards/models) as Grok 4 Fast on the usual superficial benchmarks. It does [fine](https://www.holisticai.com/blog/red-teaming-open-source-ai-models-china) on one very basic red-team test.
      - ByteDance do impressive research but in a lagging paradigm, [diffusion LMs](https://seed.bytedance.com/en/direction/llm). There are [others](https://www.interconnects.ai/i/171165224/honorable-mentions) but they're marginal for now.
